{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Apache Spark 3.0.1 \u00b6 Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"Welcome"},{"location":"#the-internals-of-apache-spark-301","text":"Welcome to The Internals of Apache Spark online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Apache Spark as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Apache Spark .","title":"The Internals of Apache Spark 3.0.1"},{"location":"AccumulatorContext/","text":"== [[AccumulatorContext]] AccumulatorContext AccumulatorContext is a private[spark] internal object used to track accumulators by Spark itself using an internal originals lookup table. Spark uses the AccumulatorContext object to register and unregister accumulators. The originals lookup table maps accumulator identifier to the accumulator itself. Every accumulator has its own unique accumulator id that is assigned using the internal nextId counter. === [[register]] register Method CAUTION: FIXME === [[newId]] newId Method CAUTION: FIXME === [[AccumulatorContext-SQL_ACCUM_IDENTIFIER]] AccumulatorContext.SQL_ACCUM_IDENTIFIER AccumulatorContext.SQL_ACCUM_IDENTIFIER is an internal identifier for Spark SQL's internal accumulators. The value is sql and Spark uses it to distinguish spark-sql-SparkPlan.md#SQLMetric[Spark SQL metrics] from others.","title":"AccumulatorContext"},{"location":"AppStatusPlugin/","text":"== [[AppStatusPlugin]] AppStatusPlugin -- Contract for AppStatusPlugin is the < > for...FIXME [source, scala] \u00b6 package org.apache.spark.status trait AppStatusPlugin { def setupListeners( conf: SparkConf, store: KVStore, addListenerFn: SparkListener => Unit, live: Boolean): Unit def setupUI(ui: SparkUI): Unit } NOTE: AppStatusPlugin is a private[spark] Scala trait. .AppStatusPlugin Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description [[setupListeners]] setupListeners [[setupUI]] setupUI === loadPlugins \u00b6 loadPlugins () : Iterable [ AppStatusPlugin ] loadPlugins ...FIXME loadPlugins is used when: SparkContext is created FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[create a web UI] AppStatusStore creates an in-memory store for a live Spark application","title":"AppStatusPlugin"},{"location":"AppStatusPlugin/#source-scala","text":"package org.apache.spark.status trait AppStatusPlugin { def setupListeners( conf: SparkConf, store: KVStore, addListenerFn: SparkListener => Unit, live: Boolean): Unit def setupUI(ui: SparkUI): Unit } NOTE: AppStatusPlugin is a private[spark] Scala trait. .AppStatusPlugin Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description [[setupListeners]] setupListeners [[setupUI]] setupUI ===","title":"[source, scala]"},{"location":"AppStatusPlugin/#loadplugins","text":"loadPlugins () : Iterable [ AppStatusPlugin ] loadPlugins ...FIXME loadPlugins is used when: SparkContext is created FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[create a web UI] AppStatusStore creates an in-memory store for a live Spark application","title":" loadPlugins"},{"location":"AsyncEventQueue/","text":"== [[AsyncEventQueue]] AsyncEventQueue AsyncEventQueue is...FIXME","title":"AsyncEventQueue"},{"location":"Broadcast/","text":"= Broadcast Variable From http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables[the official documentation about Broadcast Variables]: Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. And later in the document: Explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] To use a broadcast value in a Spark transformation you have to create it first using ROOT:SparkContext.md#broadcast[SparkContext.broadcast] and then use value method to access the shared value. Learn it in < > section. The Broadcast feature in Spark uses SparkContext to create broadcast values and core:BroadcastManager.md[] and core:ContextCleaner.md[ContextCleaner] to manage their lifecycle. .SparkContext to broadcast using BroadcastManager and ContextCleaner image::sparkcontext-broadcastmanager-contextcleaner.png[align=\"center\"] TIP: Not only can Spark developers use broadcast variables for efficient data distribution, but Spark itself uses them quite often. A very notable use case is when scheduler:DAGScheduler.md#submitMissingTasks[Spark distributes tasks to executors for their execution]. That does change my perspective on the role of broadcast variables in Spark. The idea is to transfer values used in transformations from a driver to executors in a most effective way so they are copied once and used many times by tasks (rather than being copied every time a task is launched). == [[developer-contract]] Broadcast Spark Developer-Facing Contract The developer-facing Broadcast contract allows Spark developers to use it in their applications. .Broadcast API [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Method Name | Description | id | The unique identifier | < > | The value | < > | Asynchronously deletes cached copies of this broadcast on the executors. | < > | Destroys all data and metadata related to this broadcast variable. | toString | The string representation |=== == [[lifecycle]] Lifecycle of Broadcast Variable You can create a broadcast variable of type T using ROOT:SparkContext.md#broadcast[SparkContext.broadcast] method. scala> val b = sc.broadcast(1) b: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0) [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.storage.BlockManager logger to debug broadcast method. Read storage:BlockManager.md[BlockManager] to find out how to enable the logging level. \u00b6 With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Put block broadcast_0 locally took 430 ms DEBUG BlockManager: Putting block broadcast_0 without replication took 431 ms DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 4 ms DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 4 ms After creating an instance of a broadcast variable, you can then reference the value using < > method. [source, scala] \u00b6 scala> b.value res0: Int = 1 NOTE: value method is the only way to access the value of a broadcast variable. With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Getting local block broadcast_0 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas) When you are done with a broadcast variable, you should < > it to release memory. [source, scala] \u00b6 scala> b.destroy \u00b6 With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Removing broadcast 0 DEBUG BlockManager: Removing block broadcast_0_piece0 DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Removing block broadcast_0 Before < > a broadcast variable, you may want to < > it. [source, scala] \u00b6 scala> b.unpersist \u00b6 == [[value]] Getting the Value of Broadcast Variable -- value Method [source, scala] \u00b6 value: T \u00b6 value returns the value of a broadcast variable. You can only access the value until it is < > after which you will see the following SparkException exception in the logs: org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:69) ... 48 elided Internally, value makes sure that the broadcast variable is valid , i.e. < > was not called, and, if so, calls the abstract getValue method. [NOTE] \u00b6 getValue is abstracted and broadcast variable implementations are supposed to provide a concrete behaviour. Refer to core:TorrentBroadcast.md#getValue[TorrentBroadcast]. \u00b6 == [[unpersist]] Unpersisting Broadcast Variable -- unpersist Methods [source, scala] \u00b6 unpersist(): Unit unpersist(blocking: Boolean): Unit == [[destroy]] Destroying Broadcast Variable -- destroy Method [source, scala] \u00b6 destroy(): Unit \u00b6 destroy removes a broadcast variable. NOTE: Once a broadcast variable has been destroyed, it cannot be used again. If you try to destroy a broadcast variable more than once, you will see the following SparkException exception in the logs: scala> b.destroy org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:107) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98) ... 48 elided Internally, destroy executes the internal < > (with blocking enabled). == [[destroy-internal]] Removing Persisted Data of Broadcast Variable -- destroy Internal Method [source, scala] \u00b6 destroy(blocking: Boolean): Unit \u00b6 destroy destroys all data and metadata of a broadcast variable. NOTE: destroy is a private[spark] method. Internally, destroy marks a broadcast variable destroyed, i.e. the internal _isValid flag is disabled. You should see the following INFO message in the logs: INFO TorrentBroadcast: Destroying Broadcast([id]) (from [destroySite]) In the end, doDestroy method is executed (that broadcast implementations are supposed to provide). NOTE: doDestroy is part of the < > for broadcast implementations so they can provide their own custom behaviour. == [[introductory-example]] Introductory Example Let's start with an introductory example to check out how to use broadcast variables and build your initial understanding. You're going to use a static mapping of interesting projects with their websites, i.e. Map[String, String] that the tasks, i.e. closures (anonymous functions) in transformations, use. scala> val pws = Map(\"Apache Spark\" -> \"http://spark.apache.org/\", \"Scala\" -> \"http://www.scala-lang.org/\") pws: scala.collection.immutable.Map[String,String] = Map(Apache Spark -> http://spark.apache.org/, Scala -> http://www.scala-lang.org/) scala> val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pws).collect ... websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) It works, but is very ineffective as the pws map is sent over the wire to executors while it could have been there already. If there were more tasks that need the pws map, you could improve their performance by minimizing the number of bytes that are going to be sent over the network for task execution. Enter broadcast variables. val pwsB = sc.broadcast(pws) val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pwsB.value).collect // websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) Semantically, the two computations - with and without the broadcast value - are exactly the same, but the broadcast-based one wins performance-wise when there are more executors spawned to execute many tasks that use pws map. == [[introduction]] Introduction Broadcast is part of Spark that is responsible for broadcasting information across nodes in a cluster. You use broadcast variable to implement map-side join , i.e. a join using a map . For this, lookup tables are distributed across nodes in a cluster using broadcast and then looked up inside map (to do the join implicitly). When you broadcast a value, it is copied to executors only once (while it is copied multiple times for tasks otherwise). It means that broadcast can help to get your Spark application faster if you have a large value to use in tasks or there are more tasks than executors. It appears that a Spark idiom emerges that uses broadcast with collectAsMap to create a Map for broadcast. When an RDD is map over to a smaller dataset (column-wise not record-wise), collectAsMap , and broadcast , using the very big RDD to map its elements to the broadcast RDDs is computationally faster. [source, scala] \u00b6 val acMap = sc.broadcast(myRDD.map { case (a,b,c,b) => (a, c) }.collectAsMap) val otherMap = sc.broadcast(myOtherRDD.collectAsMap) myBigRDD.map { case (a, b, c, d) => (acMap.value.get(a).get, otherMap.value.get\u00a9.get) }.collect Use large broadcasted HashMaps over RDDs whenever possible and leave RDDs with a key to lookup necessary data as demonstrated above. Spark comes with a BitTorrent implementation. It is not enabled by default. == [[contract]] Broadcast Contract The Broadcast contract is made up of the following methods that custom Broadcast implementations are supposed to provide: getValue doUnpersist doDestroy NOTE: core:TorrentBroadcast.md[TorrentBroadcast] is the only implementation of the Broadcast contract. NOTE: < Broadcast Spark Developer-Facing Contract>> is the developer-facing Broadcast contract that allows Spark developers to use it in their applications. == [[i-want-more]] Further Reading or Watching http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/[Map-Side Join in Spark]","title":"Broadcast"},{"location":"Broadcast/#tip","text":"Enable DEBUG logging level for org.apache.spark.storage.BlockManager logger to debug broadcast method.","title":"[TIP]"},{"location":"Broadcast/#read-storageblockmanagermdblockmanager-to-find-out-how-to-enable-the-logging-level","text":"With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Put block broadcast_0 locally took 430 ms DEBUG BlockManager: Putting block broadcast_0 without replication took 431 ms DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 4 ms DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 4 ms After creating an instance of a broadcast variable, you can then reference the value using < > method.","title":"Read storage:BlockManager.md[BlockManager] to find out how to enable the logging level."},{"location":"Broadcast/#source-scala","text":"scala> b.value res0: Int = 1 NOTE: value method is the only way to access the value of a broadcast variable. With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Getting local block broadcast_0 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas) When you are done with a broadcast variable, you should < > it to release memory.","title":"[source, scala]"},{"location":"Broadcast/#source-scala_1","text":"","title":"[source, scala]"},{"location":"Broadcast/#scala-bdestroy","text":"With DEBUG logging level enabled, you should see the following messages in the logs: DEBUG BlockManager: Removing broadcast 0 DEBUG BlockManager: Removing block broadcast_0_piece0 DEBUG BlockManager: Told master about block broadcast_0_piece0 DEBUG BlockManager: Removing block broadcast_0 Before < > a broadcast variable, you may want to < > it.","title":"scala&gt; b.destroy"},{"location":"Broadcast/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Broadcast/#scala-bunpersist","text":"== [[value]] Getting the Value of Broadcast Variable -- value Method","title":"scala&gt; b.unpersist"},{"location":"Broadcast/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Broadcast/#value-t","text":"value returns the value of a broadcast variable. You can only access the value until it is < > after which you will see the following SparkException exception in the logs: org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:69) ... 48 elided Internally, value makes sure that the broadcast variable is valid , i.e. < > was not called, and, if so, calls the abstract getValue method.","title":"value: T"},{"location":"Broadcast/#note","text":"getValue is abstracted and broadcast variable implementations are supposed to provide a concrete behaviour.","title":"[NOTE]"},{"location":"Broadcast/#refer-to-coretorrentbroadcastmdgetvaluetorrentbroadcast","text":"== [[unpersist]] Unpersisting Broadcast Variable -- unpersist Methods","title":"Refer to core:TorrentBroadcast.md#getValue[TorrentBroadcast]."},{"location":"Broadcast/#source-scala_4","text":"unpersist(): Unit unpersist(blocking: Boolean): Unit == [[destroy]] Destroying Broadcast Variable -- destroy Method","title":"[source, scala]"},{"location":"Broadcast/#source-scala_5","text":"","title":"[source, scala]"},{"location":"Broadcast/#destroy-unit","text":"destroy removes a broadcast variable. NOTE: Once a broadcast variable has been destroyed, it cannot be used again. If you try to destroy a broadcast variable more than once, you will see the following SparkException exception in the logs: scala> b.destroy org.apache.spark.SparkException: Attempted to use Broadcast(0) after it was destroyed (destroy at <console>:27) at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:107) at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98) ... 48 elided Internally, destroy executes the internal < > (with blocking enabled). == [[destroy-internal]] Removing Persisted Data of Broadcast Variable -- destroy Internal Method","title":"destroy(): Unit"},{"location":"Broadcast/#source-scala_6","text":"","title":"[source, scala]"},{"location":"Broadcast/#destroyblocking-boolean-unit","text":"destroy destroys all data and metadata of a broadcast variable. NOTE: destroy is a private[spark] method. Internally, destroy marks a broadcast variable destroyed, i.e. the internal _isValid flag is disabled. You should see the following INFO message in the logs: INFO TorrentBroadcast: Destroying Broadcast([id]) (from [destroySite]) In the end, doDestroy method is executed (that broadcast implementations are supposed to provide). NOTE: doDestroy is part of the < > for broadcast implementations so they can provide their own custom behaviour. == [[introductory-example]] Introductory Example Let's start with an introductory example to check out how to use broadcast variables and build your initial understanding. You're going to use a static mapping of interesting projects with their websites, i.e. Map[String, String] that the tasks, i.e. closures (anonymous functions) in transformations, use. scala> val pws = Map(\"Apache Spark\" -> \"http://spark.apache.org/\", \"Scala\" -> \"http://www.scala-lang.org/\") pws: scala.collection.immutable.Map[String,String] = Map(Apache Spark -> http://spark.apache.org/, Scala -> http://www.scala-lang.org/) scala> val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pws).collect ... websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) It works, but is very ineffective as the pws map is sent over the wire to executors while it could have been there already. If there were more tasks that need the pws map, you could improve their performance by minimizing the number of bytes that are going to be sent over the network for task execution. Enter broadcast variables. val pwsB = sc.broadcast(pws) val websites = sc.parallelize(Seq(\"Apache Spark\", \"Scala\")).map(pwsB.value).collect // websites: Array[String] = Array(http://spark.apache.org/, http://www.scala-lang.org/) Semantically, the two computations - with and without the broadcast value - are exactly the same, but the broadcast-based one wins performance-wise when there are more executors spawned to execute many tasks that use pws map. == [[introduction]] Introduction Broadcast is part of Spark that is responsible for broadcasting information across nodes in a cluster. You use broadcast variable to implement map-side join , i.e. a join using a map . For this, lookup tables are distributed across nodes in a cluster using broadcast and then looked up inside map (to do the join implicitly). When you broadcast a value, it is copied to executors only once (while it is copied multiple times for tasks otherwise). It means that broadcast can help to get your Spark application faster if you have a large value to use in tasks or there are more tasks than executors. It appears that a Spark idiom emerges that uses broadcast with collectAsMap to create a Map for broadcast. When an RDD is map over to a smaller dataset (column-wise not record-wise), collectAsMap , and broadcast , using the very big RDD to map its elements to the broadcast RDDs is computationally faster.","title":"destroy(blocking: Boolean): Unit"},{"location":"Broadcast/#source-scala_7","text":"val acMap = sc.broadcast(myRDD.map { case (a,b,c,b) => (a, c) }.collectAsMap) val otherMap = sc.broadcast(myOtherRDD.collectAsMap) myBigRDD.map { case (a, b, c, d) => (acMap.value.get(a).get, otherMap.value.get\u00a9.get) }.collect Use large broadcasted HashMaps over RDDs whenever possible and leave RDDs with a key to lookup necessary data as demonstrated above. Spark comes with a BitTorrent implementation. It is not enabled by default. == [[contract]] Broadcast Contract The Broadcast contract is made up of the following methods that custom Broadcast implementations are supposed to provide: getValue doUnpersist doDestroy NOTE: core:TorrentBroadcast.md[TorrentBroadcast] is the only implementation of the Broadcast contract. NOTE: < Broadcast Spark Developer-Facing Contract>> is the developer-facing Broadcast contract that allows Spark developers to use it in their applications. == [[i-want-more]] Further Reading or Watching http://dmtolpeko.com/2015/02/20/map-side-join-in-spark/[Map-Side Join in Spark]","title":"[source, scala]"},{"location":"CompressionCodec/","text":"= CompressionCodec CompressionCodec is an abstraction of < >. A concrete CompressionCodec is supposed to < >. The default compression codec is configured using ROOT:configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property. == [[implementations]][[shortCompressionCodecNames]] Available CompressionCodecs [cols=\"30,10m,60\",options=\"header\",width=\"100%\"] |=== | CompressionCodec | Alias | Description | LZ4CompressionCodec | lz4 a| [[LZ4CompressionCodec]] https://github.com/lz4/lz4-java[LZ4 compression] The default compression codec based on ROOT:configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property Uses ROOT:configuration-properties.md#spark.io.compression.lz4.blockSize[spark.io.compression.lz4.blockSize] configuration property for the block size | LZFCompressionCodec | lzf | [[LZFCompressionCodec]] https://github.com/ning/compress[LZF compression] | SnappyCompressionCodec | snappy a| [[SnappyCompressionCodec]] https://google.github.io/snappy/[Snappy compression] Uses ROOT:configuration-properties.md#spark.io.compression.snappy.blockSize[spark.io.compression.snappy.blockSize] configuration property for the block size | ZStdCompressionCodec | zstd a| [[ZStdCompressionCodec]] https://facebook.github.io/zstd/[ZStandard compression] ROOT:configuration-properties.md#spark.io.compression.zstd.bufferSize[spark.io.compression.zstd.bufferSize] for the buffer size ROOT:configuration-properties.md#spark.io.compression.zstd.level[spark.io.compression.zstd.level] for the compression level |=== == [[compressedOutputStream]] Compressing Output Stream [source,scala] \u00b6 compressedOutputStream( s: OutputStream): OutputStream compressedOutputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#blockifyObject[blockifyObject] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#start[start] GenericAvroSerializer is requested to compress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an output stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[compressedInputStream]] Compressing Input Stream [source,scala] \u00b6 compressedInputStream( s: InputStream): InputStream compressedInputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#unBlockifyObject[unBlockifyObject] ReliableCheckpointRDD is requested to readCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is requested to decompress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an input stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[createCodec]] Creating CompressionCodec [source, scala] \u00b6 createCodec( conf: SparkConf): CompressionCodec createCodec( conf: SparkConf, codecName: String): CompressionCodec createCodec creates an instance of the compression codec by the given name (using a constructor that accepts a ROOT:SparkConf.md[SparkConf]). createCodec uses < > utility to find the codec name unless specified explicitly. createCodec finds the class name in the < > internal lookup table or assumes that the codec name is already a fully-qualified class name. createCodec throws an IllegalArgumentException exception if a compression codec could not be found: [source,plaintext] \u00b6 Codec [codecName] is not available. Consider setting spark.io.compression.codec=snappy \u00b6 createCodec is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#setConf[setConf] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile and readCheckpointFile spark-history-server:EventLoggingListener.md[EventLoggingListener] is created and requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is created SerializerManager is created UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills] Finding Compression Codec Name \u00b6 getCodecName ( conf : SparkConf ) : String getCodecName takes the name of a compression codec based on ROOT:configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property (using the ROOT:SparkConf.md[SparkConf]) if available or defaults to lz4 . getCodecName is used when: SparkContext is created CompressionCodec utility is used to creating a CompressionCodec == [[supportsConcatenationOfSerializedStreams]] supportsConcatenationOfSerializedStreams Method [source, scala] \u00b6 supportsConcatenationOfSerializedStreams( codec: CompressionCodec): Boolean supportsConcatenationOfSerializedStreams returns true when the given CompressionCodec is one of the < >. supportsConcatenationOfSerializedStreams is used when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills].","title":"CompressionCodec"},{"location":"CompressionCodec/#sourcescala","text":"compressedOutputStream( s: OutputStream): OutputStream compressedOutputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#blockifyObject[blockifyObject] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#start[start] GenericAvroSerializer is requested to compress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an output stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[compressedInputStream]] Compressing Input Stream","title":"[source,scala]"},{"location":"CompressionCodec/#sourcescala_1","text":"compressedInputStream( s: InputStream): InputStream compressedInputStream is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#unBlockifyObject[unBlockifyObject] ReliableCheckpointRDD is requested to readCheckpointFile EventLoggingListener is requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is requested to decompress a schema SerializerManager is requested to serializer:SerializerManager.md#wrapForCompression[wrap an input stream of a block for compression] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpillsWithFileStream[mergeSpillsWithFileStream] == [[createCodec]] Creating CompressionCodec","title":"[source,scala]"},{"location":"CompressionCodec/#source-scala","text":"createCodec( conf: SparkConf): CompressionCodec createCodec( conf: SparkConf, codecName: String): CompressionCodec createCodec creates an instance of the compression codec by the given name (using a constructor that accepts a ROOT:SparkConf.md[SparkConf]). createCodec uses < > utility to find the codec name unless specified explicitly. createCodec finds the class name in the < > internal lookup table or assumes that the codec name is already a fully-qualified class name. createCodec throws an IllegalArgumentException exception if a compression codec could not be found:","title":"[source, scala]"},{"location":"CompressionCodec/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"CompressionCodec/#codec-codecname-is-not-available-consider-setting-sparkiocompressioncodecsnappy","text":"createCodec is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#setConf[setConf] ReliableCheckpointRDD is requested to writePartitionToCheckpointFile and readCheckpointFile spark-history-server:EventLoggingListener.md[EventLoggingListener] is created and requested to spark-history-server:EventLoggingListener.md#openEventLog[openEventLog] GenericAvroSerializer is created SerializerManager is created UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills]","title":"Codec [codecName] is not available. Consider setting spark.io.compression.codec=snappy"},{"location":"CompressionCodec/#finding-compression-codec-name","text":"getCodecName ( conf : SparkConf ) : String getCodecName takes the name of a compression codec based on ROOT:configuration-properties.md#spark.io.compression.codec[spark.io.compression.codec] configuration property (using the ROOT:SparkConf.md[SparkConf]) if available or defaults to lz4 . getCodecName is used when: SparkContext is created CompressionCodec utility is used to creating a CompressionCodec == [[supportsConcatenationOfSerializedStreams]] supportsConcatenationOfSerializedStreams Method","title":" Finding Compression Codec Name"},{"location":"CompressionCodec/#source-scala_1","text":"supportsConcatenationOfSerializedStreams( codec: CompressionCodec): Boolean supportsConcatenationOfSerializedStreams returns true when the given CompressionCodec is one of the < >. supportsConcatenationOfSerializedStreams is used when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spills].","title":"[source, scala]"},{"location":"ConsoleProgressBar/","text":"ConsoleProgressBar \u00b6 ConsoleProgressBar shows the progress of active stages to standard error, i.e. stderr . It uses spark-sparkcontext-SparkStatusTracker.md[SparkStatusTracker] to poll the status of stages periodically and print out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time. [Stage 0:====> (316 + 4) / 1000][Stage 1:> (0 + 0) / 1000][Stage 2:> (0 + 0) / 1000]]] The progress includes the stage id, the number of completed, active, and total tasks. TIP: ConsoleProgressBar may be useful when you ssh to workers and want to see the progress of active stages. < ConsoleProgressBar is created>> when SparkContext is created with spark-webui-properties.md#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] enabled and the logging level of ROOT:SparkContext.md[org.apache.spark.SparkContext] logger as WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ). [source, scala] \u00b6 import org.apache.log4j._ Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) To print the progress nicely ConsoleProgressBar uses COLUMNS environment variable to know the width of the terminal. It assumes 80 columns. The progress bar prints out the status after a stage has ran at least 500 milliseconds every spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] milliseconds. NOTE: The initial delay of 500 milliseconds before ConsoleProgressBar show the progress is not configurable. See the progress bar in Spark shell with the following: [source] \u00b6 $ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true # <1> scala> sc.setLogLevel(\"OFF\") // <2> import org.apache.log4j._ scala> Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) // <3> scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count // <4> [Stage 2:> (0 + 4) / 4] [Stage 2:==============> (1 + 3) / 4] [Stage 2:=============================> (2 + 2) / 4] [Stage 2:============================================> (3 + 1) / 4] <1> Make sure spark.ui.showConsoleProgress is true . It is by default. <2> Disable ( OFF ) the root logger (that includes Spark's logger) <3> Make sure org.apache.spark.SparkContext logger is at least WARN . <4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar. TIP: https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action. You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017] show progress bar in console #3029]): > ./bin/spark-shell scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> a.union(b).count() === [[creating-instance]] Creating ConsoleProgressBar Instance ConsoleProgressBar requires a ROOT:SparkContext.md[SparkContext]. When being created, ConsoleProgressBar reads spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] configuration property to set up the update interval and COLUMNS environment variable for the terminal width (or assumes 80 columns). ConsoleProgressBar starts the internal timer refresh progress that does < > and shows progress. NOTE: ConsoleProgressBar is created when SparkContext is created, spark-webui-properties.md#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] configuration property is enabled, and the logging level of ROOT:SparkContext.md[org.apache.spark.SparkContext] logger is WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ). NOTE: Once created, ConsoleProgressBar is available internally as _progressBar . === [[finishAll]] finishAll Method CAUTION: FIXME === [[stop]] stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop cancels (stops) the internal timer. NOTE: stop is executed when ROOT:SparkContext.md#stop[ SparkContext stops]. === [[refresh]] refresh Internal Method [source, scala] \u00b6 refresh(): Unit \u00b6 refresh ...FIXME NOTE: refresh is used when...FIXME","title":"ConsoleProgressBar"},{"location":"ConsoleProgressBar/#consoleprogressbar","text":"ConsoleProgressBar shows the progress of active stages to standard error, i.e. stderr . It uses spark-sparkcontext-SparkStatusTracker.md[SparkStatusTracker] to poll the status of stages periodically and print out active stages with more than one task. It keeps overwriting itself to hold in one line for at most 3 first concurrent stages at a time. [Stage 0:====> (316 + 4) / 1000][Stage 1:> (0 + 0) / 1000][Stage 2:> (0 + 0) / 1000]]] The progress includes the stage id, the number of completed, active, and total tasks. TIP: ConsoleProgressBar may be useful when you ssh to workers and want to see the progress of active stages. < ConsoleProgressBar is created>> when SparkContext is created with spark-webui-properties.md#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] enabled and the logging level of ROOT:SparkContext.md[org.apache.spark.SparkContext] logger as WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ).","title":"ConsoleProgressBar"},{"location":"ConsoleProgressBar/#source-scala","text":"import org.apache.log4j._ Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) To print the progress nicely ConsoleProgressBar uses COLUMNS environment variable to know the width of the terminal. It assumes 80 columns. The progress bar prints out the status after a stage has ran at least 500 milliseconds every spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] milliseconds. NOTE: The initial delay of 500 milliseconds before ConsoleProgressBar show the progress is not configurable. See the progress bar in Spark shell with the following:","title":"[source, scala]"},{"location":"ConsoleProgressBar/#source","text":"$ ./bin/spark-shell --conf spark.ui.showConsoleProgress=true # <1> scala> sc.setLogLevel(\"OFF\") // <2> import org.apache.log4j._ scala> Logger.getLogger(\"org.apache.spark.SparkContext\").setLevel(Level.WARN) // <3> scala> sc.parallelize(1 to 4, 4).map { n => Thread.sleep(500 + 200 * n); n }.count // <4> [Stage 2:> (0 + 4) / 4] [Stage 2:==============> (1 + 3) / 4] [Stage 2:=============================> (2 + 2) / 4] [Stage 2:============================================> (3 + 1) / 4] <1> Make sure spark.ui.showConsoleProgress is true . It is by default. <2> Disable ( OFF ) the root logger (that includes Spark's logger) <3> Make sure org.apache.spark.SparkContext logger is at least WARN . <4> Run a job with 4 tasks with 500ms initial sleep and 200ms sleep chunks to see the progress bar. TIP: https://youtu.be/uEmcGo8rwek[Watch the short video] that show ConsoleProgressBar in action. You may want to use the following example to see the progress bar in full glory - all 3 concurrent stages in console (borrowed from https://github.com/apache/spark/pull/3029#issuecomment-63244719[a comment to [SPARK-4017] show progress bar in console #3029]): > ./bin/spark-shell scala> val a = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> val b = sc.makeRDD(1 to 1000, 10000).map(x => (x, x)).reduceByKey(_ + _) scala> a.union(b).count() === [[creating-instance]] Creating ConsoleProgressBar Instance ConsoleProgressBar requires a ROOT:SparkContext.md[SparkContext]. When being created, ConsoleProgressBar reads spark-webui-properties.md#spark.ui.consoleProgress.update.interval[spark.ui.consoleProgress.update.interval] configuration property to set up the update interval and COLUMNS environment variable for the terminal width (or assumes 80 columns). ConsoleProgressBar starts the internal timer refresh progress that does < > and shows progress. NOTE: ConsoleProgressBar is created when SparkContext is created, spark-webui-properties.md#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] configuration property is enabled, and the logging level of ROOT:SparkContext.md[org.apache.spark.SparkContext] logger is WARN or higher (i.e. less messages are printed out and so there is a \"space\" for ConsoleProgressBar ). NOTE: Once created, ConsoleProgressBar is available internally as _progressBar . === [[finishAll]] finishAll Method CAUTION: FIXME === [[stop]] stop Method","title":"[source]"},{"location":"ConsoleProgressBar/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ConsoleProgressBar/#stop-unit","text":"stop cancels (stops) the internal timer. NOTE: stop is executed when ROOT:SparkContext.md#stop[ SparkContext stops]. === [[refresh]] refresh Internal Method","title":"stop(): Unit"},{"location":"ConsoleProgressBar/#source-scala_2","text":"","title":"[source, scala]"},{"location":"ConsoleProgressBar/#refresh-unit","text":"refresh ...FIXME NOTE: refresh is used when...FIXME","title":"refresh(): Unit"},{"location":"ExecutorAllocationClient/","text":"ExecutorAllocationClient \u00b6 ExecutorAllocationClient is a < > for clients to communicate with a cluster manager to request or kill executors. === [[contract]] ExecutorAllocationClient Contract [source, scala] \u00b6 trait ExecutorAllocationClient { def getExecutorIds(): Seq[String] def requestTotalExecutors(numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean def requestExecutors(numAdditionalExecutors: Int): Boolean def killExecutor(executorId: String): Boolean def killExecutors(executorIds: Seq[String]): Seq[String] def killExecutorsOnHost(host: String): Boolean } NOTE: ExecutorAllocationClient is a private[spark] contract. .ExecutorAllocationClient Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[getExecutorIds]] getExecutorIds | Finds identifiers of the executors in use. Used when ROOT:SparkContext.md#getExecutorIds[ SparkContext calculates the executors in use] and also when spark-streaming/spark-streaming-dynamic-allocation.md[Spark Streaming manages executors]. | [[requestTotalExecutors]] requestTotalExecutors a| Updates the cluster manager with the exact number of executors desired. It returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: ROOT:SparkContext.md#requestTotalExecutors[ SparkContext requests executors] (for coarse-grained scheduler backends only). ExecutorAllocationManager spark-ExecutorAllocationManager.md#start[starts], does spark-ExecutorAllocationManager.md#updateAndSyncNumExecutorsTarget[updateAndSyncNumExecutorsTarget], and spark-ExecutorAllocationManager.md#addExecutors[addExecutors]. Streaming ExecutorAllocationManager spark-streaming/spark-streaming-ExecutorAllocationManager.md#requestExecutors[requests executors]. yarn/spark-yarn-yarnschedulerbackend.md#stop[ YarnSchedulerBackend stops]. | [[requestExecutors]] requestExecutors | Requests additional executors from a cluster manager and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when ROOT:SparkContext.md#requestExecutors[ SparkContext requests additional executors] (for coarse-grained scheduler backends only). | [[killExecutor]] killExecutor a| Requests a cluster manager to kill a single executor that is no longer in use and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). The default implementation simply calls < > (with a single-element collection of executors to kill). Used when: ExecutorAllocationManager spark-ExecutorAllocationManager.md#removeExecutor[removes an executor]. SparkContext ROOT:SparkContext.md#killExecutors[is requested to kill executors]. Streaming ExecutorAllocationManager spark-streaming/spark-streaming-ExecutorAllocationManager.md#killExecutor[is requested to kill executors]. | [[killExecutors]] killExecutors | Requests that a cluster manager to kill one or many executors that are no longer in use and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Interestingly , it is only used for < >. | [[killExecutorsOnHost]] killExecutorsOnHost | Used exclusively when BlacklistTracker kills blacklisted executors. |===","title":"ExecutorAllocationClient"},{"location":"ExecutorAllocationClient/#executorallocationclient","text":"ExecutorAllocationClient is a < > for clients to communicate with a cluster manager to request or kill executors. === [[contract]] ExecutorAllocationClient Contract","title":"ExecutorAllocationClient"},{"location":"ExecutorAllocationClient/#source-scala","text":"trait ExecutorAllocationClient { def getExecutorIds(): Seq[String] def requestTotalExecutors(numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean def requestExecutors(numAdditionalExecutors: Int): Boolean def killExecutor(executorId: String): Boolean def killExecutors(executorIds: Seq[String]): Seq[String] def killExecutorsOnHost(host: String): Boolean } NOTE: ExecutorAllocationClient is a private[spark] contract. .ExecutorAllocationClient Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[getExecutorIds]] getExecutorIds | Finds identifiers of the executors in use. Used when ROOT:SparkContext.md#getExecutorIds[ SparkContext calculates the executors in use] and also when spark-streaming/spark-streaming-dynamic-allocation.md[Spark Streaming manages executors]. | [[requestTotalExecutors]] requestTotalExecutors a| Updates the cluster manager with the exact number of executors desired. It returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when: ROOT:SparkContext.md#requestTotalExecutors[ SparkContext requests executors] (for coarse-grained scheduler backends only). ExecutorAllocationManager spark-ExecutorAllocationManager.md#start[starts], does spark-ExecutorAllocationManager.md#updateAndSyncNumExecutorsTarget[updateAndSyncNumExecutorsTarget], and spark-ExecutorAllocationManager.md#addExecutors[addExecutors]. Streaming ExecutorAllocationManager spark-streaming/spark-streaming-ExecutorAllocationManager.md#requestExecutors[requests executors]. yarn/spark-yarn-yarnschedulerbackend.md#stop[ YarnSchedulerBackend stops]. | [[requestExecutors]] requestExecutors | Requests additional executors from a cluster manager and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Used when ROOT:SparkContext.md#requestExecutors[ SparkContext requests additional executors] (for coarse-grained scheduler backends only). | [[killExecutor]] killExecutor a| Requests a cluster manager to kill a single executor that is no longer in use and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). The default implementation simply calls < > (with a single-element collection of executors to kill). Used when: ExecutorAllocationManager spark-ExecutorAllocationManager.md#removeExecutor[removes an executor]. SparkContext ROOT:SparkContext.md#killExecutors[is requested to kill executors]. Streaming ExecutorAllocationManager spark-streaming/spark-streaming-ExecutorAllocationManager.md#killExecutor[is requested to kill executors]. | [[killExecutors]] killExecutors | Requests that a cluster manager to kill one or many executors that are no longer in use and returns whether the request has been acknowledged by the cluster manager ( true ) or not ( false ). Interestingly , it is only used for < >. | [[killExecutorsOnHost]] killExecutorsOnHost | Used exclusively when BlacklistTracker kills blacklisted executors. |===","title":"[source, scala]"},{"location":"ExecutorAllocationListener/","text":"== ExecutorAllocationListener CAUTION: FIXME ExecutorAllocationListener is a ROOT:SparkListener.md[] that intercepts events about stages, tasks, and executors, i.e. onStageSubmitted, onStageCompleted, onTaskStart, onTaskEnd, onExecutorAdded, and onExecutorRemoved. Using the events spark-ExecutorAllocationManager.md[ExecutorAllocationManager] can manage the pool of dynamically managed executors. NOTE: ExecutorAllocationListener is an internal class of spark-ExecutorAllocationManager.md[ExecutorAllocationManager] with full access to spark-ExecutorAllocationManager.md#internal-registries[its internal registries].","title":"ExecutorAllocationListener"},{"location":"ExecutorAllocationManager/","text":"ExecutorAllocationManager \u2014 Allocation Manager for Spark Core \u00b6 ExecutorAllocationManager is responsible for dynamically allocating and removing executor:Executor.md[executors] based on the workload. It intercepts Spark events using the internal spark-SparkListener-ExecutorAllocationListener.md[ExecutorAllocationListener] that keeps track of the workload (changing the < > that the allocation manager uses for executors management). It uses spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient], scheduler:LiveListenerBus.md[], and ROOT:SparkConf.md[SparkConf] (that are all passed in when ExecutorAllocationManager is created). ExecutorAllocationManager is created when SparkContext is created. NOTE: SparkContext expects that SchedulerBackend follows the spark-service-ExecutorAllocationClient.md#contract[ExecutorAllocationClient contract] when dynamic allocation of executors is enabled. [[internal-properties]] .ExecutorAllocationManager's Internal Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description | [[executorAllocationManagerSource]] executorAllocationManagerSource | spark-service-ExecutorAllocationManagerSource.md[ExecutorAllocationManagerSource] | FIXME | tasksPerExecutorForFullParallelism | a| [[tasksPerExecutorForFullParallelism]] |=== [[internal-registries]] .ExecutorAllocationManager's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[executorsPendingToRemove]] executorsPendingToRemove | Internal cache with...FIXME Used when...FIXME | [[removeTimes]] removeTimes | Internal cache with...FIXME Used when...FIXME | [[executorIds]] executorIds | Internal cache with...FIXME Used when...FIXME | [[initialNumExecutors]] initialNumExecutors | FIXME | [[numExecutorsTarget]] numExecutorsTarget | FIXME | [[numExecutorsToAdd]] numExecutorsToAdd | FIXME | [[initializing]] initializing | Flag whether...FIXME Starts enabled (i.e. true ). |=== [TIP] \u00b6 Enable INFO logging level for org.apache.spark.ExecutorAllocationManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ExecutorAllocationManager=INFO Refer to spark-logging.md[Logging]. \u00b6 === [[addExecutors]] addExecutors Method CAUTION: FIXME === [[removeExecutor]] removeExecutor Method CAUTION: FIXME === [[maxNumExecutorsNeeded]] maxNumExecutorsNeeded Method CAUTION: FIXME === [[start]] Starting ExecutorAllocationManager -- start Method [source, scala] \u00b6 start(): Unit \u00b6 start registers spark-SparkListener-ExecutorAllocationListener.md[ExecutorAllocationListener] (with scheduler:LiveListenerBus.md[]) to monitor scheduler events and make decisions when to add and remove executors. It then immediately starts < > that is responsible for the < > every 100 milliseconds. NOTE: 100 milliseconds for the period between successive < > is fixed, i.e. not configurable. It spark-service-ExecutorAllocationClient.md#requestTotalExecutors[requests executors] using the input spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient]. It requests ROOT:spark-dynamic-allocation.md#spark.dynamicAllocation.initialExecutors[spark.dynamicAllocation.initialExecutors]. start is used when SparkContext is created. === [[schedule]] Scheduling Executors -- schedule Method [source, scala] \u00b6 schedule(): Unit \u00b6 schedule calls < > to...FIXME It then go over < > to remove expired executors, i.e. executors for which expiration time has elapsed. === [[updateAndSyncNumExecutorsTarget]] updateAndSyncNumExecutorsTarget Method [source, scala] \u00b6 updateAndSyncNumExecutorsTarget(now: Long): Int \u00b6 updateAndSyncNumExecutorsTarget ...FIXME If ExecutorAllocationManager is < > it returns 0 . === [[reset]] Resetting ExecutorAllocationManager -- reset Method [source, scala] \u00b6 reset(): Unit \u00b6 reset resets ExecutorAllocationManager to its initial state, i.e. < > is enabled (i.e. true ). The < > is set to < >. The < > is set to 1 . All < > are cleared. All < > are cleared. === [[stop]] Stopping ExecutorAllocationManager -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop shuts down < >. NOTE: stop waits 10 seconds for the termination to be complete. === [[creating-instance]] Creating ExecutorAllocationManager Instance ExecutorAllocationManager takes the following when created: [[client]] spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient] [[listenerBus]] scheduler:LiveListenerBus.md[] [[conf]] ROOT:SparkConf.md[SparkConf] ExecutorAllocationManager initializes the < >. === [[validateSettings]] Validating Configuration of Dynamic Allocation -- validateSettings Internal Method [source, scala] \u00b6 validateSettings(): Unit \u00b6 validateSettings makes sure that the ROOT:spark-dynamic-allocation.md#settings[settings for dynamic allocation] are correct. validateSettings validates the following and throws a SparkException if not set correctly. . < > must be positive . < > must be 0 or greater . < > must be less than or equal to < > . < > must be greater than 0 . ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] must be enabled. . The number of tasks per core, i.e. executor:Executor.md#spark.executor.cores[spark.executor.cores] divided by ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus], is not zero. NOTE: validateSettings is used when < ExecutorAllocationManager is created>>. === [[spark-dynamic-executor-allocation]] spark-dynamic-executor-allocation Allocation Executor spark-dynamic-executor-allocation allocation executor is a...FIXME It is started... It is stopped...","title":"ExecutorAllocationManager"},{"location":"ExecutorAllocationManager/#executorallocationmanager-allocation-manager-for-spark-core","text":"ExecutorAllocationManager is responsible for dynamically allocating and removing executor:Executor.md[executors] based on the workload. It intercepts Spark events using the internal spark-SparkListener-ExecutorAllocationListener.md[ExecutorAllocationListener] that keeps track of the workload (changing the < > that the allocation manager uses for executors management). It uses spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient], scheduler:LiveListenerBus.md[], and ROOT:SparkConf.md[SparkConf] (that are all passed in when ExecutorAllocationManager is created). ExecutorAllocationManager is created when SparkContext is created. NOTE: SparkContext expects that SchedulerBackend follows the spark-service-ExecutorAllocationClient.md#contract[ExecutorAllocationClient contract] when dynamic allocation of executors is enabled. [[internal-properties]] .ExecutorAllocationManager's Internal Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description | [[executorAllocationManagerSource]] executorAllocationManagerSource | spark-service-ExecutorAllocationManagerSource.md[ExecutorAllocationManagerSource] | FIXME | tasksPerExecutorForFullParallelism | a| [[tasksPerExecutorForFullParallelism]] |=== [[internal-registries]] .ExecutorAllocationManager's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[executorsPendingToRemove]] executorsPendingToRemove | Internal cache with...FIXME Used when...FIXME | [[removeTimes]] removeTimes | Internal cache with...FIXME Used when...FIXME | [[executorIds]] executorIds | Internal cache with...FIXME Used when...FIXME | [[initialNumExecutors]] initialNumExecutors | FIXME | [[numExecutorsTarget]] numExecutorsTarget | FIXME | [[numExecutorsToAdd]] numExecutorsToAdd | FIXME | [[initializing]] initializing | Flag whether...FIXME Starts enabled (i.e. true ). |===","title":"ExecutorAllocationManager &mdash; Allocation Manager for Spark Core"},{"location":"ExecutorAllocationManager/#tip","text":"Enable INFO logging level for org.apache.spark.ExecutorAllocationManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ExecutorAllocationManager=INFO","title":"[TIP]"},{"location":"ExecutorAllocationManager/#refer-to-spark-loggingmdlogging","text":"=== [[addExecutors]] addExecutors Method CAUTION: FIXME === [[removeExecutor]] removeExecutor Method CAUTION: FIXME === [[maxNumExecutorsNeeded]] maxNumExecutorsNeeded Method CAUTION: FIXME === [[start]] Starting ExecutorAllocationManager -- start Method","title":"Refer to spark-logging.md[Logging]."},{"location":"ExecutorAllocationManager/#source-scala","text":"","title":"[source, scala]"},{"location":"ExecutorAllocationManager/#start-unit","text":"start registers spark-SparkListener-ExecutorAllocationListener.md[ExecutorAllocationListener] (with scheduler:LiveListenerBus.md[]) to monitor scheduler events and make decisions when to add and remove executors. It then immediately starts < > that is responsible for the < > every 100 milliseconds. NOTE: 100 milliseconds for the period between successive < > is fixed, i.e. not configurable. It spark-service-ExecutorAllocationClient.md#requestTotalExecutors[requests executors] using the input spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient]. It requests ROOT:spark-dynamic-allocation.md#spark.dynamicAllocation.initialExecutors[spark.dynamicAllocation.initialExecutors]. start is used when SparkContext is created. === [[schedule]] Scheduling Executors -- schedule Method","title":"start(): Unit"},{"location":"ExecutorAllocationManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ExecutorAllocationManager/#schedule-unit","text":"schedule calls < > to...FIXME It then go over < > to remove expired executors, i.e. executors for which expiration time has elapsed. === [[updateAndSyncNumExecutorsTarget]] updateAndSyncNumExecutorsTarget Method","title":"schedule(): Unit"},{"location":"ExecutorAllocationManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"ExecutorAllocationManager/#updateandsyncnumexecutorstargetnow-long-int","text":"updateAndSyncNumExecutorsTarget ...FIXME If ExecutorAllocationManager is < > it returns 0 . === [[reset]] Resetting ExecutorAllocationManager -- reset Method","title":"updateAndSyncNumExecutorsTarget(now: Long): Int"},{"location":"ExecutorAllocationManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"ExecutorAllocationManager/#reset-unit","text":"reset resets ExecutorAllocationManager to its initial state, i.e. < > is enabled (i.e. true ). The < > is set to < >. The < > is set to 1 . All < > are cleared. All < > are cleared. === [[stop]] Stopping ExecutorAllocationManager -- stop Method","title":"reset(): Unit"},{"location":"ExecutorAllocationManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"ExecutorAllocationManager/#stop-unit","text":"stop shuts down < >. NOTE: stop waits 10 seconds for the termination to be complete. === [[creating-instance]] Creating ExecutorAllocationManager Instance ExecutorAllocationManager takes the following when created: [[client]] spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient] [[listenerBus]] scheduler:LiveListenerBus.md[] [[conf]] ROOT:SparkConf.md[SparkConf] ExecutorAllocationManager initializes the < >. === [[validateSettings]] Validating Configuration of Dynamic Allocation -- validateSettings Internal Method","title":"stop(): Unit"},{"location":"ExecutorAllocationManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"ExecutorAllocationManager/#validatesettings-unit","text":"validateSettings makes sure that the ROOT:spark-dynamic-allocation.md#settings[settings for dynamic allocation] are correct. validateSettings validates the following and throws a SparkException if not set correctly. . < > must be positive . < > must be 0 or greater . < > must be less than or equal to < > . < > must be greater than 0 . ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] must be enabled. . The number of tasks per core, i.e. executor:Executor.md#spark.executor.cores[spark.executor.cores] divided by ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus], is not zero. NOTE: validateSettings is used when < ExecutorAllocationManager is created>>. === [[spark-dynamic-executor-allocation]] spark-dynamic-executor-allocation Allocation Executor spark-dynamic-executor-allocation allocation executor is a...FIXME It is started... It is stopped...","title":"validateSettings(): Unit"},{"location":"ExecutorAllocationManagerSource/","text":"ExecutorAllocationManagerSource -- Metric Source for Dynamic Allocation \u00b6 ExecutorAllocationManagerSource is a spark-metrics-MetricsSystem.md[metric source] for ROOT:spark-dynamic-allocation.md[] with name ExecutorAllocationManager and the following gauges: executors/numberExecutorsToAdd which exposes spark-ExecutorAllocationManager.md#numExecutorsToAdd[numExecutorsToAdd]. executors/numberExecutorsPendingToRemove which corresponds to the number of elements in spark-ExecutorAllocationManager.md#executorsPendingToRemove[executorsPendingToRemove]. executors/numberAllExecutors which corresponds to the number of elements in spark-ExecutorAllocationManager.md#executorIds[executorIds]. executors/numberTargetExecutors which is spark-ExecutorAllocationManager.md#numExecutorsTarget[numExecutorsTarget]. executors/numberMaxNeededExecutors which simply calls spark-ExecutorAllocationManager.md#maxNumExecutorsNeeded[maxNumExecutorsNeeded]. NOTE: Spark uses http://metrics.dropwizard.io/[Metrics ] Java library to expose internal state of its services to measure.","title":"ExecutorAllocationManagerSource"},{"location":"ExecutorAllocationManagerSource/#executorallocationmanagersource-metric-source-for-dynamic-allocation","text":"ExecutorAllocationManagerSource is a spark-metrics-MetricsSystem.md[metric source] for ROOT:spark-dynamic-allocation.md[] with name ExecutorAllocationManager and the following gauges: executors/numberExecutorsToAdd which exposes spark-ExecutorAllocationManager.md#numExecutorsToAdd[numExecutorsToAdd]. executors/numberExecutorsPendingToRemove which corresponds to the number of elements in spark-ExecutorAllocationManager.md#executorsPendingToRemove[executorsPendingToRemove]. executors/numberAllExecutors which corresponds to the number of elements in spark-ExecutorAllocationManager.md#executorIds[executorIds]. executors/numberTargetExecutors which is spark-ExecutorAllocationManager.md#numExecutorsTarget[numExecutorsTarget]. executors/numberMaxNeededExecutors which simply calls spark-ExecutorAllocationManager.md#maxNumExecutorsNeeded[maxNumExecutorsNeeded]. NOTE: Spark uses http://metrics.dropwizard.io/[Metrics ] Java library to expose internal state of its services to measure.","title":"ExecutorAllocationManagerSource -- Metric Source for Dynamic Allocation"},{"location":"FileCommitProtocol/","text":"= FileCommitProtocol FileCommitProtocol is an < > of < > that can setup, commit or abort a Spark job or task (while writing out a key-value RDD and the partitions). FileCommitProtocol is used for rdd:PairRDDFunctions.md#saveAsNewAPIHadoopDataset[saveAsNewAPIHadoopDataset] and rdd:PairRDDFunctions.md#saveAsHadoopDataset[saveAsHadoopDataset] transformations (that use SparkHadoopWriter utility to < >). A concrete < > is created using < > utility. [[contract]] .FileCommitProtocol Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | abortJob a| [[abortJob]] [source, scala] \u00b6 abortJob( jobContext: JobContext): Unit Aborts a job Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatWriter utility is used to write a result of a structured query | abortTask a| [[abortTask]] [source, scala] \u00b6 abortTask( taskContext: TaskAttemptContext): Unit Intercepts that a Spark task is (about to be) aborted Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatDataWriter is requested to abort (when FileFormatWriter utility is used to write a result of a structured query) | commitJob a| [[commitJob]] [source, scala] \u00b6 commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatWriter utility is used to write a result of a structured query | commitTask a| [[commitTask]] [source, scala] \u00b6 commitTask( taskContext: TaskAttemptContext): TaskCommitMessage Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatDataWriter is requested to commit (when FileFormatWriter utility is used to write a result of a structured query) | newTaskTempFile a| [[newTaskTempFile]] [source, scala] \u00b6 newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String Used when: (Spark SQL) SingleDirectoryDataWriter and DynamicPartitionDataWriter are requested to write (and in turn newOutputWriter ) | newTaskTempFileAbsPath a| [[newTaskTempFileAbsPath]] [source, scala] \u00b6 newTaskTempFileAbsPath( taskContext: TaskAttemptContext, absoluteDir: String, ext: String): String Used when: (Spark SQL) DynamicPartitionDataWriter is requested to write | onTaskCommit a| [[onTaskCommit]] [source, scala] \u00b6 onTaskCommit( taskCommit: TaskCommitMessage): Unit = {} Used when: (Spark SQL) FileFormatWriter is requested to write | setupJob a| [[setupJob]] [source, scala] \u00b6 setupJob( jobContext: JobContext): Unit Used when: SparkHadoopWriter utility is used to < > (while < >) (Spark SQL) FileFormatWriter utility is used to write a result of a structured query | setupTask a| [[setupTask]] [source, scala] \u00b6 setupTask( taskContext: TaskAttemptContext): Unit Sets up the task with the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/TaskAttemptContext.html[TaskAttemptContext ] Used when: SparkHadoopWriter is requested to < > (while < >) (Spark SQL) FileFormatWriter utility is used to write out a RDD partition (while writing out a result of a structured query) |=== [[implementations]] .FileCommitProtocols [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | FileCommitProtocol | Description | < > | [[HadoopMapReduceCommitProtocol]] | < > | [[HadoopMapRedCommitProtocol]] | SQLHadoopMapReduceCommitProtocol | [[SQLHadoopMapReduceCommitProtocol]] Used for batch queries (Spark SQL) | ManifestFileCommitProtocol | [[ManifestFileCommitProtocol]] Used for streaming queries (Spark Structured Streaming) |=== == [[instantiate]] Instantiating FileCommitProtocol Committer -- instantiate Utility [source, scala] \u00b6 instantiate( className: String, jobId: String, outputPath: String, dynamicPartitionOverwrite: Boolean = false): FileCommitProtocol instantiate prints out the following DEBUG message to the logs: Creating committer [className]; job [jobId]; output=[outputPath]; dynamic=[dynamicPartitionOverwrite] instantiate tries to find a constructor method that takes three arguments (two of type String and one Boolean ) for the given jobId , outputPath and dynamicPartitionOverwrite flag. If found, instantiate prints out the following DEBUG message to the logs: Using (String, String, Boolean) constructor In case of NoSuchMethodException , instantiate prints out the following DEBUG message to the logs: Falling back to (String, String) constructor instantiate tries to find a constructor method that takes two arguments (two of type String ) for the given jobId and outputPath . With two String arguments, instantiate requires that the given dynamicPartitionOverwrite flag is disabled ( false ) or throws an IllegalArgumentException : [options=\"wrap\"] \u00b6 requirement failed: Dynamic Partition Overwrite is enabled but the committer [className] does not have the appropriate constructor \u00b6 [NOTE] \u00b6 instantiate is used when: < > and < > are requested to create a < > committer (Spark SQL) InsertIntoHadoopFsRelationCommand , InsertIntoHiveDirCommand , and InsertIntoHiveTable logical commands are executed * (Spark Structured Streaming) FileStreamSink is requested to addBatch \u00b6 == [[deleteWithJob]] deleteWithJob Method [source, scala] \u00b6 deleteWithJob( fs: FileSystem, path: Path, recursive: Boolean): Boolean deleteWithJob simply requests the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] to delete a directory. NOTE: deleteWithJob is used when InsertIntoHadoopFsRelationCommand logical command (Spark SQL) is executed.","title":"FileCommitProtocol"},{"location":"FileCommitProtocol/#source-scala","text":"abortJob( jobContext: JobContext): Unit Aborts a job Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatWriter utility is used to write a result of a structured query | abortTask a| [[abortTask]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_1","text":"abortTask( taskContext: TaskAttemptContext): Unit Intercepts that a Spark task is (about to be) aborted Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatDataWriter is requested to abort (when FileFormatWriter utility is used to write a result of a structured query) | commitJob a| [[commitJob]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_2","text":"commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatWriter utility is used to write a result of a structured query | commitTask a| [[commitTask]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_3","text":"commitTask( taskContext: TaskAttemptContext): TaskCommitMessage Used when: SparkHadoopWriter utility is used to < > (Spark SQL) FileFormatDataWriter is requested to commit (when FileFormatWriter utility is used to write a result of a structured query) | newTaskTempFile a| [[newTaskTempFile]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_4","text":"newTaskTempFile( taskContext: TaskAttemptContext, dir: Option[String], ext: String): String Used when: (Spark SQL) SingleDirectoryDataWriter and DynamicPartitionDataWriter are requested to write (and in turn newOutputWriter ) | newTaskTempFileAbsPath a| [[newTaskTempFileAbsPath]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_5","text":"newTaskTempFileAbsPath( taskContext: TaskAttemptContext, absoluteDir: String, ext: String): String Used when: (Spark SQL) DynamicPartitionDataWriter is requested to write | onTaskCommit a| [[onTaskCommit]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_6","text":"onTaskCommit( taskCommit: TaskCommitMessage): Unit = {} Used when: (Spark SQL) FileFormatWriter is requested to write | setupJob a| [[setupJob]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_7","text":"setupJob( jobContext: JobContext): Unit Used when: SparkHadoopWriter utility is used to < > (while < >) (Spark SQL) FileFormatWriter utility is used to write a result of a structured query | setupTask a| [[setupTask]]","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_8","text":"setupTask( taskContext: TaskAttemptContext): Unit Sets up the task with the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/TaskAttemptContext.html[TaskAttemptContext ] Used when: SparkHadoopWriter is requested to < > (while < >) (Spark SQL) FileFormatWriter utility is used to write out a RDD partition (while writing out a result of a structured query) |=== [[implementations]] .FileCommitProtocols [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | FileCommitProtocol | Description | < > | [[HadoopMapReduceCommitProtocol]] | < > | [[HadoopMapRedCommitProtocol]] | SQLHadoopMapReduceCommitProtocol | [[SQLHadoopMapReduceCommitProtocol]] Used for batch queries (Spark SQL) | ManifestFileCommitProtocol | [[ManifestFileCommitProtocol]] Used for streaming queries (Spark Structured Streaming) |=== == [[instantiate]] Instantiating FileCommitProtocol Committer -- instantiate Utility","title":"[source, scala]"},{"location":"FileCommitProtocol/#source-scala_9","text":"instantiate( className: String, jobId: String, outputPath: String, dynamicPartitionOverwrite: Boolean = false): FileCommitProtocol instantiate prints out the following DEBUG message to the logs: Creating committer [className]; job [jobId]; output=[outputPath]; dynamic=[dynamicPartitionOverwrite] instantiate tries to find a constructor method that takes three arguments (two of type String and one Boolean ) for the given jobId , outputPath and dynamicPartitionOverwrite flag. If found, instantiate prints out the following DEBUG message to the logs: Using (String, String, Boolean) constructor In case of NoSuchMethodException , instantiate prints out the following DEBUG message to the logs: Falling back to (String, String) constructor instantiate tries to find a constructor method that takes two arguments (two of type String ) for the given jobId and outputPath . With two String arguments, instantiate requires that the given dynamicPartitionOverwrite flag is disabled ( false ) or throws an IllegalArgumentException :","title":"[source, scala]"},{"location":"FileCommitProtocol/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"FileCommitProtocol/#requirement-failed-dynamic-partition-overwrite-is-enabled-but-the-committer-classname-does-not-have-the-appropriate-constructor","text":"","title":"requirement failed: Dynamic Partition Overwrite is enabled but the committer [className] does not have the appropriate constructor"},{"location":"FileCommitProtocol/#note","text":"instantiate is used when: < > and < > are requested to create a < > committer (Spark SQL) InsertIntoHadoopFsRelationCommand , InsertIntoHiveDirCommand , and InsertIntoHiveTable logical commands are executed","title":"[NOTE]"},{"location":"FileCommitProtocol/#spark-structured-streaming-filestreamsink-is-requested-to-addbatch","text":"== [[deleteWithJob]] deleteWithJob Method","title":"* (Spark Structured Streaming) FileStreamSink is requested to addBatch"},{"location":"FileCommitProtocol/#source-scala_10","text":"deleteWithJob( fs: FileSystem, path: Path, recursive: Boolean): Boolean deleteWithJob simply requests the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] to delete a directory. NOTE: deleteWithJob is used when InsertIntoHadoopFsRelationCommand logical command (Spark SQL) is executed.","title":"[source, scala]"},{"location":"HadoopMapRedCommitProtocol/","text":"= HadoopMapRedCommitProtocol HadoopMapRedCommitProtocol is...FIXME","title":"HadoopMapRedCommitProtocol"},{"location":"HadoopMapRedWriteConfigUtil/","text":"= HadoopMapRedWriteConfigUtil HadoopMapRedWriteConfigUtil is...FIXME == [[createCommitter]] createCommitter Method [source, scala] \u00b6 createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"HadoopMapRedWriteConfigUtil"},{"location":"HadoopMapRedWriteConfigUtil/#source-scala","text":"createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"[source, scala]"},{"location":"HadoopMapReduceCommitProtocol/","text":"= HadoopMapReduceCommitProtocol HadoopMapReduceCommitProtocol is...FIXME","title":"HadoopMapReduceCommitProtocol"},{"location":"HadoopMapReduceWriteConfigUtil/","text":"= HadoopMapReduceWriteConfigUtil HadoopMapReduceWriteConfigUtil is...FIXME == [[createCommitter]] createCommitter Method [source, scala] \u00b6 createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"HadoopMapReduceWriteConfigUtil"},{"location":"HadoopMapReduceWriteConfigUtil/#source-scala","text":"createCommitter( jobId: Int): HadoopMapReduceCommitProtocol NOTE: createCommitter is part of the < > contract to...FIXME. createCommitter ...FIXME","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/","text":"= HadoopWriteConfigUtil HadoopWriteConfigUtil[K, V] is an < > of < >. HadoopWriteConfigUtil is used for < > utility when requested to < > (for rdd:PairRDDFunctions.md#saveAsNewAPIHadoopDataset[saveAsNewAPIHadoopDataset] and rdd:PairRDDFunctions.md#saveAsHadoopDataset[saveAsHadoopDataset] transformations). [[contract]] .HadoopWriteConfigUtil Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | assertConf a| [[assertConf]] [source, scala] \u00b6 assertConf( jobContext: JobContext, conf: SparkConf): Unit | closeWriter a| [[closeWriter]] [source, scala] \u00b6 closeWriter( taskContext: TaskAttemptContext): Unit | createCommitter a| [[createCommitter]] [source, scala] \u00b6 createCommitter( jobId: Int): HadoopMapReduceCommitProtocol | createJobContext a| [[createJobContext]] [source, scala] \u00b6 createJobContext( jobTrackerId: String, jobId: Int): JobContext | createTaskAttemptContext a| [[createTaskAttemptContext]] [source, scala] \u00b6 createTaskAttemptContext( jobTrackerId: String, jobId: Int, splitId: Int, taskAttemptId: Int): TaskAttemptContext Creates a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/TaskAttemptContext.html[TaskAttemptContext ] | initOutputFormat a| [[initOutputFormat]] [source, scala] \u00b6 initOutputFormat( jobContext: JobContext): Unit | initWriter a| [[initWriter]] [source, scala] \u00b6 initWriter( taskContext: TaskAttemptContext, splitId: Int): Unit | write a| [[write]] [source, scala] \u00b6 write( pair: (K, V)): Unit Writes out the key-value pair Used when SparkHadoopWriter is requested to < > (while < >) |=== [[implementations]] .HadoopWriteConfigUtils [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | HadoopWriteConfigUtil | Description | < > | [[HadoopMapReduceWriteConfigUtil]] | < > | [[HadoopMapRedWriteConfigUtil]] |===","title":"HadoopWriteConfigUtil"},{"location":"HadoopWriteConfigUtil/#source-scala","text":"assertConf( jobContext: JobContext, conf: SparkConf): Unit | closeWriter a| [[closeWriter]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_1","text":"closeWriter( taskContext: TaskAttemptContext): Unit | createCommitter a| [[createCommitter]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_2","text":"createCommitter( jobId: Int): HadoopMapReduceCommitProtocol | createJobContext a| [[createJobContext]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_3","text":"createJobContext( jobTrackerId: String, jobId: Int): JobContext | createTaskAttemptContext a| [[createTaskAttemptContext]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_4","text":"createTaskAttemptContext( jobTrackerId: String, jobId: Int, splitId: Int, taskAttemptId: Int): TaskAttemptContext Creates a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/TaskAttemptContext.html[TaskAttemptContext ] | initOutputFormat a| [[initOutputFormat]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_5","text":"initOutputFormat( jobContext: JobContext): Unit | initWriter a| [[initWriter]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_6","text":"initWriter( taskContext: TaskAttemptContext, splitId: Int): Unit | write a| [[write]]","title":"[source, scala]"},{"location":"HadoopWriteConfigUtil/#source-scala_7","text":"write( pair: (K, V)): Unit Writes out the key-value pair Used when SparkHadoopWriter is requested to < > (while < >) |=== [[implementations]] .HadoopWriteConfigUtils [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | HadoopWriteConfigUtil | Description | < > | [[HadoopMapReduceWriteConfigUtil]] | < > | [[HadoopMapRedWriteConfigUtil]] |===","title":"[source, scala]"},{"location":"InterruptibleIterator/","text":"== [[InterruptibleIterator]] InterruptibleIterator -- Iterator With Support For Task Cancellation InterruptibleIterator is a custom Scala https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator[Iterator ] that supports task cancellation, i.e. < >. Quoting the official Scala https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator[Iterator ] documentation: Iterators are data structures that allow to iterate over a sequence of elements. They have a hasNext method for checking if there is a next element available, and a next method which returns the next element and discards it from the iterator. InterruptibleIterator is < > when: RDD is requested to rdd:RDD.md#getOrCompute[get or compute a RDD partition] CoGroupedRDD , rdd:spark-rdd-HadoopRDD.md#compute[HadoopRDD], rdd:spark-rdd-NewHadoopRDD.md#compute[NewHadoopRDD], rdd:spark-rdd-ParallelCollectionRDD.md#compute[ParallelCollectionRDD] are requested to compute a partition BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined key-value records for a reduce task] PairRDDFunctions is requested to rdd:PairRDDFunctions.md#combineByKeyWithClassTag[combineByKeyWithClassTag] Spark SQL's DataSourceRDD and JDBCRDD are requested to compute a partition Spark SQL's RangeExec physical operator is requested to doExecute PySpark's BasePythonRunner is requested to compute [[creating-instance]] InterruptibleIterator takes the following when created: [[context]] spark-TaskContext.md[TaskContext] [[delegate]] Scala Iterator[T] NOTE: InterruptibleIterator is a Developer API which is a lower-level, unstable API intended for Spark developers that may change or be removed in minor versions of Apache Spark. === [[hasNext]] hasNext Method [source, scala] \u00b6 hasNext: Boolean \u00b6 NOTE: hasNext is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext requests the < > to spark-TaskContext.md#killTaskIfInterrupted[kill the task if interrupted] (that simply throws a TaskKilledException that in turn breaks the task execution). In the end, hasNext requests the < > to hasNext . === [[next]] next Method [source, scala] \u00b6 next(): T \u00b6 NOTE: next is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@next():A++[Iterator Contract] to produce the next element of this iterator. next simply requests the < > to next .","title":"InterruptibleIterator"},{"location":"InterruptibleIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"InterruptibleIterator/#hasnext-boolean","text":"NOTE: hasNext is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext requests the < > to spark-TaskContext.md#killTaskIfInterrupted[kill the task if interrupted] (that simply throws a TaskKilledException that in turn breaks the task execution). In the end, hasNext requests the < > to hasNext . === [[next]] next Method","title":"hasNext: Boolean"},{"location":"InterruptibleIterator/#source-scala_1","text":"","title":"[source, scala]"},{"location":"InterruptibleIterator/#next-t","text":"NOTE: next is part of ++ https://www.scala-lang.org/api/2.11.x/index.html#scala.collection.Iterator@next():A++[Iterator Contract] to produce the next element of this iterator. next simply requests the < > to next .","title":"next(): T"},{"location":"KVStoreView/","text":"== [[KVStoreView]] KVStoreView KVStoreView ...FIXME === [[index]] index Method [source, java] \u00b6 KVStoreView index(String name) \u00b6 index ...FIXME NOTE: index is used when...FIXME","title":"KVStoreView"},{"location":"KVStoreView/#source-java","text":"","title":"[source, java]"},{"location":"KVStoreView/#kvstoreview-indexstring-name","text":"index ...FIXME NOTE: index is used when...FIXME","title":"KVStoreView index(String name)"},{"location":"RDDBarrier/","text":"== [[RDDBarrier]] RDDBarrier RDDBarrier is used to mark the current stage as a < > in < >. RDDBarrier is < > exclusively as the result of < > transformation (which is new in Spark 2.4.0 ). [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 [[creating-instance]] [[rdd]] RDDBarrier takes a single rdd:RDD.md[RDD] to be created and gives the single < > transformation (on the RDD ) that simply changes the regular rdd:spark-rdd-transformations.md#mapPartitions[RDD.mapPartitions] transformation to create a rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:spark-rdd-MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] [[example]] val rdd = sc.parallelize(0 to 3, 1) scala> :type rdd.barrier org.apache.spark.rdd.RDDBarrier[Int] val barrierRdd = rdd .barrier .mapPartitions(identity) scala> :type barrierRdd org.apache.spark.rdd.RDD[Int] scala> println(barrierRdd.toDebugString) (1) MapPartitionsRDD[5] at mapPartitions at <console>:26 [] | ParallelCollectionRDD[3] at parallelize at <console>:25 [] // MapPartitionsRDD is private[spark] // so is RDD.isBarrier // Use org.apache.spark package then // :paste -raw the following code in spark-shell / Scala REPL // BEGIN package org.apache.spark object IsBarrier { import org.apache.spark.rdd.RDD implicit class BypassPrivateSpark[T](rdd: RDD[T]) { def myIsBarrier = rdd.isBarrier } } // END import org.apache.spark.IsBarrier._ assert(barrierRdd.myIsBarrier)","title":"RDDBarrier"},{"location":"RDDBarrier/#source-scala","text":"","title":"[source, scala]"},{"location":"RDDBarrier/#barrier-rddbarriert","text":"[[creating-instance]] [[rdd]] RDDBarrier takes a single rdd:RDD.md[RDD] to be created and gives the single < > transformation (on the RDD ) that simply changes the regular rdd:spark-rdd-transformations.md#mapPartitions[RDD.mapPartitions] transformation to create a rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:spark-rdd-MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] [[example]] val rdd = sc.parallelize(0 to 3, 1) scala> :type rdd.barrier org.apache.spark.rdd.RDDBarrier[Int] val barrierRdd = rdd .barrier .mapPartitions(identity) scala> :type barrierRdd org.apache.spark.rdd.RDD[Int] scala> println(barrierRdd.toDebugString) (1) MapPartitionsRDD[5] at mapPartitions at <console>:26 [] | ParallelCollectionRDD[3] at parallelize at <console>:25 [] // MapPartitionsRDD is private[spark] // so is RDD.isBarrier // Use org.apache.spark package then // :paste -raw the following code in spark-shell / Scala REPL // BEGIN package org.apache.spark object IsBarrier { import org.apache.spark.rdd.RDD implicit class BypassPrivateSpark[T](rdd: RDD[T]) { def myIsBarrier = rdd.isBarrier } } // END import org.apache.spark.IsBarrier._ assert(barrierRdd.myIsBarrier)","title":"barrier(): RDDBarrier[T]"},{"location":"SparkConf/","text":"SparkConf \u00b6 Every user program starts with creating an instance of SparkConf that holds the ROOT:spark-deployment-environments.md#master-urls[master URL] to connect to ( spark.master ), the name for your Spark application (that is later displayed in webui:index.md[web UI] and becomes spark.app.name ) and other Spark properties required for proper runs. The instance of SparkConf can be used to create ROOT:SparkContext.md[SparkContext]. [TIP] \u00b6 Start tools:spark-shell.md[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ... Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing. \u00b6 You can query for the values of Spark properties in tools:spark-shell.md[Spark shell] as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read ROOT:spark-properties.md#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by tools:spark-submit.md[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code: [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out. [source, scala] \u00b6 scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method [source, scala] \u00b6 getAppId: String \u00b6 getAppId returns the value of ROOT:configuration-properties.md#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#init[init] (and creates a storage:NettyBlockRpcServer.md#creating-instance[NettyBlockRpcServer] as well as storage:NettyBlockTransferService.md#appId[saves the identifier for later use]). Executor executor:Executor.md#creating-instance[is created] (in non-local mode and storage:BlockManager.md#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method [source, scala] \u00b6 getAvroSchema: Map[Long, String] \u00b6 getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"SparkConf"},{"location":"SparkConf/#sparkconf","text":"Every user program starts with creating an instance of SparkConf that holds the ROOT:spark-deployment-environments.md#master-urls[master URL] to connect to ( spark.master ), the name for your Spark application (that is later displayed in webui:index.md[web UI] and becomes spark.app.name ) and other Spark properties required for proper runs. The instance of SparkConf can be used to create ROOT:SparkContext.md[SparkContext].","title":"SparkConf"},{"location":"SparkConf/#tip","text":"Start tools:spark-shell.md[Spark shell] with --conf spark.logConf=true to log the effective Spark configuration as INFO when SparkContext is started. $ ./bin/spark-shell --conf spark.logConf=true ... 15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT 15/10/19 17:13:49 INFO SparkContext: Spark configuration: spark.app.name=Spark shell spark.home=/Users/jacek/dev/oss/spark spark.jars= spark.logConf=true spark.master=local[*] spark.repl.class.uri=http://10.5.10.20:64055 spark.submit.deployMode=client ...","title":"[TIP]"},{"location":"SparkConf/#use-scgetconftodebugstring-to-have-a-richer-output-once-sparkcontext-has-finished-initializing","text":"You can query for the values of Spark properties in tools:spark-shell.md[Spark shell] as follows: scala> sc.getConf.getOption(\"spark.local.dir\") res0: Option[String] = None scala> sc.getConf.getOption(\"spark.app.name\") res1: Option[String] = Some(Spark shell) scala> sc.getConf.get(\"spark.master\") res2: String = local[*] == [[setIfMissing]] setIfMissing Method CAUTION: FIXME == [[isExecutorStartupConf]] isExecutorStartupConf Method CAUTION: FIXME == [[set]] set Method CAUTION: FIXME == Setting up Spark Properties There are the following places where a Spark application looks for Spark properties (in the order of importance from the least important to the most important): conf/spark-defaults.conf - the configuration file with the default Spark properties. Read ROOT:spark-properties.md#spark-defaults-conf[spark-defaults.conf]. --conf or -c - the command-line option used by tools:spark-submit.md[spark-submit] (and other shell scripts that use spark-submit or spark-class under the covers, e.g. spark-shell ) SparkConf == [[default-configuration]] Default Configuration The default Spark configuration is created when you execute the following code:","title":"Use sc.getConf.toDebugString to have a richer output once SparkContext has finished initializing."},{"location":"SparkConf/#source-scala","text":"import org.apache.spark.SparkConf val conf = new SparkConf It simply loads spark.* system properties. You can use conf.toDebugString or conf.getAll to have the spark.* system properties loaded printed out.","title":"[source, scala]"},{"location":"SparkConf/#source-scala_1","text":"scala> conf.getAll res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,\"\"), (spark.master,local[*]), (spark.submit.deployMode,client)) scala> conf.toDebugString res1: String = spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client scala> println(conf.toDebugString) spark.app.name=Spark shell spark.jars= spark.master=local[*] spark.submit.deployMode=client == [[getAppId]] Unique Identifier of Spark Application -- getAppId Method","title":"[source, scala]"},{"location":"SparkConf/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkConf/#getappid-string","text":"getAppId returns the value of ROOT:configuration-properties.md#spark.app.id[spark.app.id] configuration property or throws a NoSuchElementException if not set. getAppId is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#init[init] (and creates a storage:NettyBlockRpcServer.md#creating-instance[NettyBlockRpcServer] as well as storage:NettyBlockTransferService.md#appId[saves the identifier for later use]). Executor executor:Executor.md#creating-instance[is created] (in non-local mode and storage:BlockManager.md#initialize[requests BlockManager to initialize]). == [[getAvroSchema]] getAvroSchema Method","title":"getAppId: String"},{"location":"SparkConf/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkConf/#getavroschema-maplong-string","text":"getAvroSchema takes all avro.schema -prefixed configuration properties from < > and...FIXME getAvroSchema is used when KryoSerializer is created (and initializes avroSchemas).","title":"getAvroSchema: Map[Long, String]"},{"location":"SparkContext-creating-instance-internals/","text":"== Inside Creating SparkContext This document describes what happens when you ROOT:SparkContext.md#creating-instance[create a new SparkContext]. [source, scala] \u00b6 import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // local mode // 2. Create Spark context val sc = new SparkContext(conf) NOTE: The example uses Spark in local/spark-local.md[local mode], but the initialization with spark-cluster.md[the other cluster modes] would follow similar steps. Creating SparkContext instance starts by setting the internal allowMultipleContexts field with the value of ROOT:SparkContext.md#spark.driver.allowMultipleContexts[spark.driver.allowMultipleContexts] and marking this SparkContext instance as partially constructed. It makes sure that no other thread is creating a SparkContext instance in this JVM. It does so by synchronizing on SPARK_CONTEXT_CONSTRUCTOR_LOCK and using the internal atomic reference activeContext (that eventually has a fully-created SparkContext instance). [NOTE] \u00b6 The entire code of SparkContext that creates a fully-working SparkContext instance is between two statements: [source, scala] \u00b6 SparkContext.markPartiallyConstructed(this, allowMultipleContexts) // the SparkContext code goes here SparkContext.setActiveContext(this, allowMultipleContexts) \u00b6 ==== ROOT:SparkContext.md#startTime[startTime] is set to the current time in milliseconds. < > internal flag is set to false . The very first information printed out is the version of Spark as an INFO message: INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT TIP: You can use ROOT:SparkContext.md#version[version] method to learn about the current Spark version or org.apache.spark.SPARK_VERSION value. A scheduler:LiveListenerBus.md#creating-instance[LiveListenerBus instance is created] (as listenerBus ). [[sparkUser]] The ROOT:SparkContext.md#sparkUser[current user name] is computed. CAUTION: FIXME Where is sparkUser used? It saves the input SparkConf (as _conf ). CAUTION: FIXME Review _conf.validateSettings() It ensures that the first mandatory setting - spark.master is defined. SparkException is thrown if not. A master URL must be set in your configuration It ensures that the other mandatory setting - spark.app.name is defined. SparkException is thrown if not. An application name must be set in your configuration For yarn/spark-yarn-cluster-yarnclusterschedulerbackend.md[Spark on YARN in cluster deploy mode], it checks existence of spark.yarn.app.id . SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. CAUTION: FIXME How to \"trigger\" the exception? What are the steps? When spark.logConf is enabled ROOT:SparkConf.md[SparkConf.toDebugString] is called. NOTE: SparkConf.toDebugString is called very early in the initialization process and other settings configured afterwards are not included. Use sc.getConf.toDebugString once SparkContext is initialized. The driver's host and port are set if missing. spark-driver.md#spark_driver_host[spark.driver.host] becomes the value of < > (or an exception is thrown) while spark-driver.md#spark_driver_port[spark.driver.port] is set to 0 . NOTE: spark-driver.md#spark_driver_host[spark.driver.host] and spark-driver.md#spark_driver_port[spark.driver.port] are expected to be set on the driver. It is later asserted by core:SparkEnv.md#createDriverEnv[SparkEnv]. executor:Executor.md#spark.executor.id[spark.executor.id] setting is set to driver . TIP: Use sc.getConf.get(\"spark.executor.id\") to know where the code is executed -- core:SparkEnv.md[driver or executors]. It sets the jars and files based on spark.jars and spark.files , respectively. These are files that are required for proper task execution on executors. If spark-history-server:EventLoggingListener.md[event logging] is enabled, i.e. EventLoggingListener.md#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of EventLoggingListener.md#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . [[_eventLogCodec]] Also, if spark-history-server:EventLoggingListener.md#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the io:CompressionCodec.md[CompressionCodec] is assigned to _eventLogCodec . The config key is core:BroadcastManager.md#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). TIP: Read about compression codecs in core:BroadcastManager.md#compression[Compression]. === [[_listenerBus]] Creating LiveListenerBus SparkContext creates a scheduler:LiveListenerBus.md#creating-instance[LiveListenerBus]. === [[_statusStore]] Creating Live AppStatusStore SparkContext requests AppStatusStore to create a core:AppStatusStore.md#createLiveStore[live store] (i.e. the AppStatusStore for a live Spark application) and requests < > to add the core:AppStatusStore.md#listener[AppStatusListener] to the scheduler:LiveListenerBus.md#addToStatusQueue[status queue]. NOTE: The current AppStatusStore is available as ROOT:SparkContext.md#statusStore[statusStore] property of the SparkContext . === [[_env]] Creating SparkEnv SparkContext creates a < > and requests SparkEnv to core:SparkEnv.md#set[use the instance as the default SparkEnv]. CAUTION: FIXME Describe the following steps. MetadataCleaner is created. CAUTION: FIXME What's MetadataCleaner? === [[_statusTracker]] Creating SparkStatusTracker SparkContext creates a spark-sparkcontext-SparkStatusTracker.md#creating-instance[SparkStatusTracker] (with itself and the <<_statusStore, AppStatusStore>>). === [[_progressBar]] Creating ConsoleProgressBar SparkContext creates the optional ConsoleProgressBar when spark-webui-properties.md#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] property is enabled and the INFO logging level for SparkContext is disabled. === [[_ui]][[ui]] Creating SparkUI SparkContext creates a spark-webui-SparkUI.md#create[SparkUI] when spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] configuration property is enabled (i.e. true ) with the following: <<_statusStore, AppStatusStore>> Name of the Spark application that is exactly the value of ROOT:SparkConf.md#spark.app.name[spark.app.name] configuration property Empty base path NOTE: spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] Spark property is assumed enabled when undefined. CAUTION: FIXME Where's _ui used? A Hadoop configuration is created. See ROOT:SparkContext.md#hadoopConfiguration[Hadoop Configuration]. [[jars]] If there are jars given through the SparkContext constructor, they are added using addJar . [[files]] If there were files specified, they are added using ROOT:SparkContext.md#addFile[addFile]. At this point in time, the amount of memory to allocate to each executor (as _executorMemory ) is calculated. It is the value of executor:Executor.md#spark.executor.memory[spark.executor.memory] setting, or ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, spark-standalone.md#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs . [CAUTION] \u00b6 FIXME What's _executorMemory ? What's the unit of the value of _executorMemory exactly? What are \"SPARK_TESTING\", \"spark.testing\"? How do they contribute to executorEnvs ? What's executorEnvs ? \u00b6 The Mesos scheduler backend's configuration is included in executorEnvs , i.e. ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . [[_heartbeatReceiver]] SparkContext registers spark-HeartbeatReceiver.md[HeartbeatReceiver RPC endpoint]. SparkContext object is requested to ROOT:SparkContext.md#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . NOTE: The internal _schedulerBackend and _taskScheduler are used by schedulerBackend and taskScheduler methods, respectively. scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created] (as _dagScheduler ). [[TaskSchedulerIsSet]] SparkContext sends a blocking spark-HeartbeatReceiver.md#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint] (to inform that the TaskScheduler is now available). === [[taskScheduler-start]] Starting TaskScheduler SparkContext scheduler:TaskScheduler.md#start[starts TaskScheduler ]. === [[_applicationId]][[_applicationAttemptId]] Setting Spark Application's and Execution Attempt's IDs -- _applicationId and _applicationAttemptId SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the scheduler:TaskScheduler.md#applicationId[unique identifier of a Spark application] (that is currently only implemented by scheduler:TaskSchedulerImpl.md#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to scheduler:SchedulerBackend.md#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize spark-webui-SparkUI.md#setAppId[SparkUI] and storage:BlockManager.md#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the ROOT:SparkContext.md#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener spark-history-server:EventLoggingListener.md#creating-instance[is created]. === [[spark.app.id]] Setting spark.app.id Spark Property in SparkConf SparkContext sets ROOT:SparkConf.md#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, spark-webui-SparkUI.md#setAppId[passes it on to SparkUI ]. === [[BlockManager-initialization]] Initializing BlockManager The storage:BlockManager.md#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). === [[MetricsSystem-start]] Starting MetricsSystem SparkContext requests the MetricsSystem to spark-metrics-MetricsSystem.md#start[start]. NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to spark-metrics-MetricsSystem.md#buildRegistryName[build unique identifiers fo metrics sources]. === [[MetricsSystem-getServletHandlers]] Requesting JSON Servlet Handler SparkContext requests the MetricsSystem for a spark-metrics-MetricsSystem.md#getServletHandlers[JSON servlet handler] and requests the <<_ui, SparkUI>> to spark-webui-WebUI.md#attachHandler[attach it]. [[_eventLogger]] _eventLogger is created and started if isEventLogEnabled . It uses spark-history-server:EventLoggingListener.md[EventLoggingListener] that gets registered to scheduler:LiveListenerBus.md[]. CAUTION: FIXME Why is _eventLogger required to be the internal field of SparkContext? Where is this used? [[ExecutorAllocationManager]] For ROOT:spark-dynamic-allocation.md[], spark-ExecutorAllocationManager.md#creating-instance[ ExecutorAllocationManager is created] (as _executorAllocationManager ) and immediately spark-ExecutorAllocationManager.md#start[started]. NOTE: _executorAllocationManager is exposed (as a method) to yarn/spark-yarn-yarnschedulerbackend.md#reset[YARN scheduler backends to reset their state to the initial state]. [[_cleaner]][[ContextCleaner]] With ROOT:configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled, SparkContext core:ContextCleaner.md#creating-instance[creates ContextCleaner ] (as _cleaner ) and core:ContextCleaner.md#start[started] immediately. Otherwise, _cleaner is empty. CAUTION: FIXME It'd be quite useful to have all the properties with their default values in sc.getConf.toDebugString , so when a configuration is not included but does change Spark runtime configuration, it should be added to _conf . [[registering_SparkListeners]] It < SparkListenerEvent event delivery to the listeners>>. [[postEnvironmentUpdate]] postEnvironmentUpdate is called that posts ROOT:SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on scheduler:LiveListenerBus.md[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in web UI's spark-webui-environment.md[Environment tab]. [[postApplicationStart]] ROOT:SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to scheduler:LiveListenerBus.md[] (using the internal postApplicationStart method). [[postStartHook]] TaskScheduler scheduler:TaskScheduler.md#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: scheduler:TaskScheduler.md#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl scheduler:TaskSchedulerImpl.md#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. === [[registerSource]] Registering Metrics Sources SparkContext requests MetricsSystem to spark-metrics-MetricsSystem.md#registerSource[register metrics sources] for the following services: . scheduler:DAGScheduler.md#metricsSource[DAGScheduler] . spark-BlockManager-BlockManagerSource.md[BlockManager] . spark-ExecutorAllocationManager.md#executorAllocationManagerSource[ExecutorAllocationManager] (for ROOT:spark-dynamic-allocation.md[]) === [[addShutdownHook]] Adding Shutdown Hook SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). You should see the following DEBUG message in the logs: DEBUG Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me === [[nextShuffleId]][[nextRddId]] Initializing nextShuffleId and nextRddId Internal Counters nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. === [[getClusterManager]] Loading External Cluster Manager for URL (getClusterManager method) [source, scala] \u00b6 getClusterManager(url: String): Option[ExternalClusterManager] \u00b6 getClusterManager loads scheduler:ExternalClusterManager.md[] that scheduler:ExternalClusterManager.md#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when ROOT:SparkContext.md#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. === [[setupAndStartListenerBus]] setupAndStartListenerBus [source, scala] \u00b6 setupAndStartListenerBus(): Unit \u00b6 setupAndStartListenerBus is an internal method that reads ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property from the current ROOT:SparkConf.md[SparkConf] to create and register ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts ROOT:SparkConf.md[SparkConf] a zero-argument constructor setupAndStartListenerBus scheduler:LiveListenerBus.md#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It scheduler:LiveListenerBus.md#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] listener ROOT:SparkContext.md#stop[stops the SparkContext] and a SparkException is thrown and the source exception's message. Exception when registering SparkListener [TIP] \u00b6 Set INFO on org.apache.spark.SparkContext logger to see the extra listeners being registered. INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener \u00b6 === [[createSparkEnv]] Creating SparkEnv for Driver -- createSparkEnv Method [source, scala] \u00b6 createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv createSparkEnv simply delegates the call to core:SparkEnv.md#createDriverEnv[SparkEnv to create a SparkEnv for the driver]. It calculates the number of cores to 1 for local master URL, the number of processors available for JVM for * or the exact number in the master URL, or 0 for the cluster master URLs. === [[getCurrentUserName]] Utils.getCurrentUserName Method [source, scala] \u00b6 getCurrentUserName(): String \u00b6 getCurrentUserName computes the user name who has started the ROOT:SparkContext.md[SparkContext] instance. NOTE: It is later available as ROOT:SparkContext.md#sparkUser[SparkContext.sparkUser]. Internally, it reads ROOT:SparkContext.md#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. === [[localHostName]] Utils.localHostName Method localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while ROOT:SparkContext.md#creating-instance[ SparkContext is created] and also to compute the default value of spark-driver.md#spark_driver_host[spark.driver.host Spark property]. CAUTION: FIXME Review the rest. === [[stopped]] stopped Flag CAUTION: FIXME Where is this used?","title":"Inside Creating SparkContext"},{"location":"SparkContext-creating-instance-internals/#source-scala","text":"import org.apache.spark.{SparkConf, SparkContext} // 1. Create Spark configuration val conf = new SparkConf() .setAppName(\"SparkMe Application\") .setMaster(\"local[*]\") // local mode // 2. Create Spark context val sc = new SparkContext(conf) NOTE: The example uses Spark in local/spark-local.md[local mode], but the initialization with spark-cluster.md[the other cluster modes] would follow similar steps. Creating SparkContext instance starts by setting the internal allowMultipleContexts field with the value of ROOT:SparkContext.md#spark.driver.allowMultipleContexts[spark.driver.allowMultipleContexts] and marking this SparkContext instance as partially constructed. It makes sure that no other thread is creating a SparkContext instance in this JVM. It does so by synchronizing on SPARK_CONTEXT_CONSTRUCTOR_LOCK and using the internal atomic reference activeContext (that eventually has a fully-created SparkContext instance).","title":"[source, scala]"},{"location":"SparkContext-creating-instance-internals/#note","text":"The entire code of SparkContext that creates a fully-working SparkContext instance is between two statements:","title":"[NOTE]"},{"location":"SparkContext-creating-instance-internals/#source-scala_1","text":"SparkContext.markPartiallyConstructed(this, allowMultipleContexts) // the SparkContext code goes here","title":"[source, scala]"},{"location":"SparkContext-creating-instance-internals/#sparkcontextsetactivecontextthis-allowmultiplecontexts","text":"==== ROOT:SparkContext.md#startTime[startTime] is set to the current time in milliseconds. < > internal flag is set to false . The very first information printed out is the version of Spark as an INFO message: INFO SparkContext: Running Spark version 2.0.0-SNAPSHOT TIP: You can use ROOT:SparkContext.md#version[version] method to learn about the current Spark version or org.apache.spark.SPARK_VERSION value. A scheduler:LiveListenerBus.md#creating-instance[LiveListenerBus instance is created] (as listenerBus ). [[sparkUser]] The ROOT:SparkContext.md#sparkUser[current user name] is computed. CAUTION: FIXME Where is sparkUser used? It saves the input SparkConf (as _conf ). CAUTION: FIXME Review _conf.validateSettings() It ensures that the first mandatory setting - spark.master is defined. SparkException is thrown if not. A master URL must be set in your configuration It ensures that the other mandatory setting - spark.app.name is defined. SparkException is thrown if not. An application name must be set in your configuration For yarn/spark-yarn-cluster-yarnclusterschedulerbackend.md[Spark on YARN in cluster deploy mode], it checks existence of spark.yarn.app.id . SparkException is thrown if it does not exist. Detected yarn cluster mode, but isn't running on a cluster. Deployment to YARN is not supported directly by SparkContext. Please use spark-submit. CAUTION: FIXME How to \"trigger\" the exception? What are the steps? When spark.logConf is enabled ROOT:SparkConf.md[SparkConf.toDebugString] is called. NOTE: SparkConf.toDebugString is called very early in the initialization process and other settings configured afterwards are not included. Use sc.getConf.toDebugString once SparkContext is initialized. The driver's host and port are set if missing. spark-driver.md#spark_driver_host[spark.driver.host] becomes the value of < > (or an exception is thrown) while spark-driver.md#spark_driver_port[spark.driver.port] is set to 0 . NOTE: spark-driver.md#spark_driver_host[spark.driver.host] and spark-driver.md#spark_driver_port[spark.driver.port] are expected to be set on the driver. It is later asserted by core:SparkEnv.md#createDriverEnv[SparkEnv]. executor:Executor.md#spark.executor.id[spark.executor.id] setting is set to driver . TIP: Use sc.getConf.get(\"spark.executor.id\") to know where the code is executed -- core:SparkEnv.md[driver or executors]. It sets the jars and files based on spark.jars and spark.files , respectively. These are files that are required for proper task execution on executors. If spark-history-server:EventLoggingListener.md[event logging] is enabled, i.e. EventLoggingListener.md#spark_eventLog_enabled[spark.eventLog.enabled] flag is true , the internal field _eventLogDir is set to the value of EventLoggingListener.md#spark_eventLog_dir[spark.eventLog.dir] setting or the default value /tmp/spark-events . [[_eventLogCodec]] Also, if spark-history-server:EventLoggingListener.md#spark_eventLog_compress[spark.eventLog.compress] is enabled (it is not by default), the short name of the io:CompressionCodec.md[CompressionCodec] is assigned to _eventLogCodec . The config key is core:BroadcastManager.md#spark_io_compression_codec[spark.io.compression.codec] (default: lz4 ). TIP: Read about compression codecs in core:BroadcastManager.md#compression[Compression]. === [[_listenerBus]] Creating LiveListenerBus SparkContext creates a scheduler:LiveListenerBus.md#creating-instance[LiveListenerBus]. === [[_statusStore]] Creating Live AppStatusStore SparkContext requests AppStatusStore to create a core:AppStatusStore.md#createLiveStore[live store] (i.e. the AppStatusStore for a live Spark application) and requests < > to add the core:AppStatusStore.md#listener[AppStatusListener] to the scheduler:LiveListenerBus.md#addToStatusQueue[status queue]. NOTE: The current AppStatusStore is available as ROOT:SparkContext.md#statusStore[statusStore] property of the SparkContext . === [[_env]] Creating SparkEnv SparkContext creates a < > and requests SparkEnv to core:SparkEnv.md#set[use the instance as the default SparkEnv]. CAUTION: FIXME Describe the following steps. MetadataCleaner is created. CAUTION: FIXME What's MetadataCleaner? === [[_statusTracker]] Creating SparkStatusTracker SparkContext creates a spark-sparkcontext-SparkStatusTracker.md#creating-instance[SparkStatusTracker] (with itself and the <<_statusStore, AppStatusStore>>). === [[_progressBar]] Creating ConsoleProgressBar SparkContext creates the optional ConsoleProgressBar when spark-webui-properties.md#spark.ui.showConsoleProgress[spark.ui.showConsoleProgress] property is enabled and the INFO logging level for SparkContext is disabled. === [[_ui]][[ui]] Creating SparkUI SparkContext creates a spark-webui-SparkUI.md#create[SparkUI] when spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] configuration property is enabled (i.e. true ) with the following: <<_statusStore, AppStatusStore>> Name of the Spark application that is exactly the value of ROOT:SparkConf.md#spark.app.name[spark.app.name] configuration property Empty base path NOTE: spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] Spark property is assumed enabled when undefined. CAUTION: FIXME Where's _ui used? A Hadoop configuration is created. See ROOT:SparkContext.md#hadoopConfiguration[Hadoop Configuration]. [[jars]] If there are jars given through the SparkContext constructor, they are added using addJar . [[files]] If there were files specified, they are added using ROOT:SparkContext.md#addFile[addFile]. At this point in time, the amount of memory to allocate to each executor (as _executorMemory ) is calculated. It is the value of executor:Executor.md#spark.executor.memory[spark.executor.memory] setting, or ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable (or currently-deprecated SPARK_MEM ), or defaults to 1024 . _executorMemory is later available as sc.executorMemory and used for LOCAL_CLUSTER_REGEX, spark-standalone.md#SparkDeploySchedulerBackend[Spark Standalone's SparkDeploySchedulerBackend], to set executorEnvs(\"SPARK_EXECUTOR_MEMORY\") , MesosSchedulerBackend, CoarseMesosSchedulerBackend. The value of SPARK_PREPEND_CLASSES environment variable is included in executorEnvs .","title":"SparkContext.setActiveContext(this, allowMultipleContexts)"},{"location":"SparkContext-creating-instance-internals/#caution","text":"FIXME What's _executorMemory ? What's the unit of the value of _executorMemory exactly? What are \"SPARK_TESTING\", \"spark.testing\"? How do they contribute to executorEnvs ?","title":"[CAUTION]"},{"location":"SparkContext-creating-instance-internals/#whats-executorenvs","text":"The Mesos scheduler backend's configuration is included in executorEnvs , i.e. ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY], _conf.getExecutorEnv , and SPARK_USER . [[_heartbeatReceiver]] SparkContext registers spark-HeartbeatReceiver.md[HeartbeatReceiver RPC endpoint]. SparkContext object is requested to ROOT:SparkContext.md#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] (for the given master URL) and the result becomes the internal _schedulerBackend and _taskScheduler . NOTE: The internal _schedulerBackend and _taskScheduler are used by schedulerBackend and taskScheduler methods, respectively. scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created] (as _dagScheduler ). [[TaskSchedulerIsSet]] SparkContext sends a blocking spark-HeartbeatReceiver.md#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint] (to inform that the TaskScheduler is now available). === [[taskScheduler-start]] Starting TaskScheduler SparkContext scheduler:TaskScheduler.md#start[starts TaskScheduler ]. === [[_applicationId]][[_applicationAttemptId]] Setting Spark Application's and Execution Attempt's IDs -- _applicationId and _applicationAttemptId SparkContext sets the internal fields -- _applicationId and _applicationAttemptId -- (using applicationId and applicationAttemptId methods from the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]). NOTE: SparkContext requests TaskScheduler for the scheduler:TaskScheduler.md#applicationId[unique identifier of a Spark application] (that is currently only implemented by scheduler:TaskSchedulerImpl.md#applicationId[TaskSchedulerImpl] that uses SchedulerBackend to scheduler:SchedulerBackend.md#applicationId[request the identifier]). NOTE: The unique identifier of a Spark application is used to initialize spark-webui-SparkUI.md#setAppId[SparkUI] and storage:BlockManager.md#initialize[BlockManager]. NOTE: _applicationAttemptId is used when SparkContext is requested for the ROOT:SparkContext.md#applicationAttemptId[unique identifier of execution attempt of a Spark application] and when EventLoggingListener spark-history-server:EventLoggingListener.md#creating-instance[is created]. === [[spark.app.id]] Setting spark.app.id Spark Property in SparkConf SparkContext sets ROOT:SparkConf.md#spark.app.id[spark.app.id] property to be the <<_applicationId, unique identifier of a Spark application>> and, if enabled, spark-webui-SparkUI.md#setAppId[passes it on to SparkUI ]. === [[BlockManager-initialization]] Initializing BlockManager The storage:BlockManager.md#initialize[BlockManager (for the driver) is initialized] (with _applicationId ). === [[MetricsSystem-start]] Starting MetricsSystem SparkContext requests the MetricsSystem to spark-metrics-MetricsSystem.md#start[start]. NOTE: SparkContext starts MetricsSystem after < > as MetricsSystem uses it to spark-metrics-MetricsSystem.md#buildRegistryName[build unique identifiers fo metrics sources]. === [[MetricsSystem-getServletHandlers]] Requesting JSON Servlet Handler SparkContext requests the MetricsSystem for a spark-metrics-MetricsSystem.md#getServletHandlers[JSON servlet handler] and requests the <<_ui, SparkUI>> to spark-webui-WebUI.md#attachHandler[attach it]. [[_eventLogger]] _eventLogger is created and started if isEventLogEnabled . It uses spark-history-server:EventLoggingListener.md[EventLoggingListener] that gets registered to scheduler:LiveListenerBus.md[]. CAUTION: FIXME Why is _eventLogger required to be the internal field of SparkContext? Where is this used? [[ExecutorAllocationManager]] For ROOT:spark-dynamic-allocation.md[], spark-ExecutorAllocationManager.md#creating-instance[ ExecutorAllocationManager is created] (as _executorAllocationManager ) and immediately spark-ExecutorAllocationManager.md#start[started]. NOTE: _executorAllocationManager is exposed (as a method) to yarn/spark-yarn-yarnschedulerbackend.md#reset[YARN scheduler backends to reset their state to the initial state]. [[_cleaner]][[ContextCleaner]] With ROOT:configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled, SparkContext core:ContextCleaner.md#creating-instance[creates ContextCleaner ] (as _cleaner ) and core:ContextCleaner.md#start[started] immediately. Otherwise, _cleaner is empty. CAUTION: FIXME It'd be quite useful to have all the properties with their default values in sc.getConf.toDebugString , so when a configuration is not included but does change Spark runtime configuration, it should be added to _conf . [[registering_SparkListeners]] It < SparkListenerEvent event delivery to the listeners>>. [[postEnvironmentUpdate]] postEnvironmentUpdate is called that posts ROOT:SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] message on scheduler:LiveListenerBus.md[] with information about Task Scheduler's scheduling mode, added jar and file paths, and other environmental details. They are displayed in web UI's spark-webui-environment.md[Environment tab]. [[postApplicationStart]] ROOT:SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] message is posted to scheduler:LiveListenerBus.md[] (using the internal postApplicationStart method). [[postStartHook]] TaskScheduler scheduler:TaskScheduler.md#postStartHook[is notified that SparkContext is almost fully initialized]. NOTE: scheduler:TaskScheduler.md#postStartHook[TaskScheduler.postStartHook] does nothing by default, but custom implementations offer more advanced features, i.e. TaskSchedulerImpl scheduler:TaskSchedulerImpl.md#postStartHook[blocks the current thread until SchedulerBackend is ready]. There is also YarnClusterScheduler for Spark on YARN in cluster deploy mode. === [[registerSource]] Registering Metrics Sources SparkContext requests MetricsSystem to spark-metrics-MetricsSystem.md#registerSource[register metrics sources] for the following services: . scheduler:DAGScheduler.md#metricsSource[DAGScheduler] . spark-BlockManager-BlockManagerSource.md[BlockManager] . spark-ExecutorAllocationManager.md#executorAllocationManagerSource[ExecutorAllocationManager] (for ROOT:spark-dynamic-allocation.md[]) === [[addShutdownHook]] Adding Shutdown Hook SparkContext adds a shutdown hook (using ShutdownHookManager.addShutdownHook() ). You should see the following DEBUG message in the logs: DEBUG Adding shutdown hook CAUTION: FIXME ShutdownHookManager.addShutdownHook() Any non-fatal Exception leads to termination of the Spark context instance. CAUTION: FIXME What does NonFatal represent in Scala? CAUTION: FIXME Finish me === [[nextShuffleId]][[nextRddId]] Initializing nextShuffleId and nextRddId Internal Counters nextShuffleId and nextRddId start with 0 . CAUTION: FIXME Where are nextShuffleId and nextRddId used? A new instance of Spark context is created and ready for operation. === [[getClusterManager]] Loading External Cluster Manager for URL (getClusterManager method)","title":"What's executorEnvs?"},{"location":"SparkContext-creating-instance-internals/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkContext-creating-instance-internals/#getclustermanagerurl-string-optionexternalclustermanager","text":"getClusterManager loads scheduler:ExternalClusterManager.md[] that scheduler:ExternalClusterManager.md#canCreate[can handle the input url ]. If there are two or more external cluster managers that could handle url , a SparkException is thrown: Multiple Cluster Managers ([serviceLoaders]) registered for the url [url]. NOTE: getClusterManager uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader.load ] method. NOTE: getClusterManager is used to find a cluster manager for a master URL when ROOT:SparkContext.md#createTaskScheduler[creating a SchedulerBackend and a TaskScheduler for the driver]. === [[setupAndStartListenerBus]] setupAndStartListenerBus","title":"getClusterManager(url: String): Option[ExternalClusterManager]"},{"location":"SparkContext-creating-instance-internals/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkContext-creating-instance-internals/#setupandstartlistenerbus-unit","text":"setupAndStartListenerBus is an internal method that reads ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property from the current ROOT:SparkConf.md[SparkConf] to create and register ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] listeners. It expects that the class name represents a SparkListenerInterface listener with one of the following constructors (in this order): a single-argument constructor that accepts ROOT:SparkConf.md[SparkConf] a zero-argument constructor setupAndStartListenerBus scheduler:LiveListenerBus.md#ListenerBus-addListener[registers every listener class]. You should see the following INFO message in the logs: INFO Registered listener [className] It scheduler:LiveListenerBus.md#start[starts LiveListenerBus] and records it in the internal _listenerBusStarted . When no single- SparkConf or zero-argument constructor could be found for a class name in ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property, a SparkException is thrown with the message: [className] did not have a zero-argument constructor or a single-argument constructor that accepts SparkConf. Note: if the class is defined inside of another Scala class, then its constructors may accept an implicit parameter that references the enclosing class; in this case, you must define the listener as a top-level class in order to prevent this extra parameter from breaking Spark's ability to find a valid constructor. Any exception while registering a ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] listener ROOT:SparkContext.md#stop[stops the SparkContext] and a SparkException is thrown and the source exception's message. Exception when registering SparkListener","title":"setupAndStartListenerBus(): Unit"},{"location":"SparkContext-creating-instance-internals/#tip","text":"Set INFO on org.apache.spark.SparkContext logger to see the extra listeners being registered.","title":"[TIP]"},{"location":"SparkContext-creating-instance-internals/#info-sparkcontext-registered-listener-pljapilasparkcustomsparklistener","text":"=== [[createSparkEnv]] Creating SparkEnv for Driver -- createSparkEnv Method","title":"INFO SparkContext: Registered listener pl.japila.spark.CustomSparkListener\n"},{"location":"SparkContext-creating-instance-internals/#source-scala_4","text":"createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv createSparkEnv simply delegates the call to core:SparkEnv.md#createDriverEnv[SparkEnv to create a SparkEnv for the driver]. It calculates the number of cores to 1 for local master URL, the number of processors available for JVM for * or the exact number in the master URL, or 0 for the cluster master URLs. === [[getCurrentUserName]] Utils.getCurrentUserName Method","title":"[source, scala]"},{"location":"SparkContext-creating-instance-internals/#source-scala_5","text":"","title":"[source, scala]"},{"location":"SparkContext-creating-instance-internals/#getcurrentusername-string","text":"getCurrentUserName computes the user name who has started the ROOT:SparkContext.md[SparkContext] instance. NOTE: It is later available as ROOT:SparkContext.md#sparkUser[SparkContext.sparkUser]. Internally, it reads ROOT:SparkContext.md#SPARK_USER[SPARK_USER] environment variable and, if not set, reverts to Hadoop Security API's UserGroupInformation.getCurrentUser().getShortUserName() . NOTE: It is another place where Spark relies on Hadoop API for its operation. === [[localHostName]] Utils.localHostName Method localHostName computes the local host name. It starts by checking SPARK_LOCAL_HOSTNAME environment variable for the value. If it is not defined, it uses SPARK_LOCAL_IP to find the name (using InetAddress.getByName ). If it is not defined either, it calls InetAddress.getLocalHost for the name. NOTE: Utils.localHostName is executed while ROOT:SparkContext.md#creating-instance[ SparkContext is created] and also to compute the default value of spark-driver.md#spark_driver_host[spark.driver.host Spark property]. CAUTION: FIXME Review the rest. === [[stopped]] stopped Flag CAUTION: FIXME Where is this used?","title":"getCurrentUserName(): String"},{"location":"SparkContext/","text":"SparkContext \u00b6 SparkContext is the entry point to all of the components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly). Important There should be one active SparkContext per JVM and Spark developers should use SparkContext.getOrCreate utility for sharing it (e.g. across threads). Creating Instance \u00b6 SparkContext takes the following to be created: SparkConf SparkContext is created (directly or indirectly using getOrCreate utility). While being created, SparkContext sets up core services and establishes a connection to a Spark execution environment . getOrCreate Utility \u00b6 getOrCreate () : SparkContext getOrCreate ( config : SparkConf ) : SparkContext getOrCreate ...FIXME PluginContainer \u00b6 SparkContext creates a PluginContainer when created . PluginContainer is created (for the driver where SparkContext lives) using PluginContainer.apply utility. PluginContainer is then requested to registerMetrics with the applicationId . PluginContainer is requested to shutdown when SparkContext is requested to stop . Creating SchedulerBackend and TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , master : String , deployMode : String ) : ( SchedulerBackend , TaskScheduler ) createTaskScheduler creates a SchedulerBackend and a TaskScheduler for the given master URL and deployment mode. Internally, createTaskScheduler branches off per the given master URL ( master URL ) to select the requested implementations. createTaskScheduler accepts the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker Other URLs are simply handed over to getClusterManager to load an external cluster manager if available createTaskScheduler is used when SparkContext is created . Finding ExternalClusterManager for Master URL \u00b6 getClusterManager ( url : String ) : Option [ ExternalClusterManager ] getClusterManager ...FIXME getClusterManager is used when SparkContext is requested for a SchedulerBackend and TaskScheduler . Running Job Synchronously \u00b6 runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U ) : Array [ U ] runJob [ T , U: ClassTag ]( rdd : RDD [ T ], processPartition : ( TaskContext , Iterator [ T ]) => U , resultHandler : ( Int , U ) => Unit ) : Unit runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ]) : Array [ U ] runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], resultHandler : ( Int , U ) => Unit ) : Unit runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U ) : Array [ U ] runJob [ T , U: ClassTag ]( rdd : RDD [ T ], processPartition : Iterator [ T ] => U , resultHandler : ( Int , U ) => Unit ) : Unit runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U , partitions : Seq [ Int ]) : Array [ U ] runJob finds the call site and cleans up the given func function. runJob prints out the following INFO message to the logs: Starting job: [callSite] With spark.logLineage enabled, runJob requests the given RDD for the recursive dependencies and prints out the following INFO message to the logs: RDD's recursive dependencies: [toDebugString] runJob requests the DAGScheduler to run a job . runJob requests the ConsoleProgressBar to finishAll if defined. In the end, runJob requests the given RDD to doCheckpoint . runJob throws an IllegalStateException when SparkContext is stopped : SparkContext has been shutdown runJob Demo \u00b6 runJob is essentially executing a func function on all or a subset of partitions of an RDD and returning the result as an array (with elements being the results per partition). sc . setLocalProperty ( \"callSite.short\" , \"runJob Demo\" ) val partitionsNumber = 4 val rdd = sc . parallelize ( Seq ( \"hello world\" , \"nice to see you\" ), numSlices = partitionsNumber ) import org.apache.spark.TaskContext val func = ( t : TaskContext , ss : Iterator [ String ]) => 1 val result = sc . runJob ( rdd , func ) assert ( result . length == partitionsNumber ) sc . clearCallSite () Call Site \u00b6 getCallSite () : CallSite getCallSite ...FIXME getCallSite is used when: SparkContext is requested to broadcast , runJob , runApproximateJob , submitJob and submitMapStage AsyncRDDActions is requested to takeAsync RDD is created Closure Cleaning \u00b6 clean ( f : F , checkSerializable : Boolean = true ) : F clean cleans up the given f closure (using ClosureCleaner.clean utility). Tip Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG Refer to Logging . With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++ Old Information \u00b6 SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of spark-rdd-partitions.md[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** spark-sparkcontext-local-properties.md[Local Properties -- Creating Logical Job Groups] ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, scheduler:LiveListenerBus.md[], storage:BlockManager.md[BlockManager], scheduler:SchedulerBackend.md[SchedulerBackends], shuffle:ShuffleManager.md[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. == [[addFile]] addFile Method [source, scala] \u00b6 addFile( path: String): Unit // <1> addFile( path: String, recursive: Boolean): Unit <1> recursive flag is off addFile adds the path file to be downloaded...FIXME == [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- unpersistRDD Internal Method [source, scala] \u00b6 unpersistRDD(rddId: Int, blocking: Boolean = true): Unit \u00b6 unpersistRDD requests BlockManagerMaster to storage:BlockManagerMaster.md#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv core:SparkEnv.md#blockManager[to access the current BlockManager ] that is in turn used to storage:BlockManager.md#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a ROOT:SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >. [NOTE] \u00b6 unpersistRDD is used when: ContextCleaner does core:ContextCleaner.md#doCleanupRDD[doCleanupRDD] SparkContext < > (i.e. marks an RDD as non-persistent) \u00b6 == [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method [source, scala] \u00b6 postApplicationStart(): Unit \u00b6 postApplicationStart ...FIXME postApplicationStart is used exclusively when SparkContext is created. == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method [source, scala] \u00b6 getPersistentRDDs: Map[Int, RDD[_]] \u00b6 getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via spark-rdd-caching.md#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method [source, scala] \u00b6 cancelJob(jobId: Int) \u00b6 cancelJob requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods [source, scala] \u00b6 cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab spark-webui-StagesTab.md#handleKillRequest[handles a kill request] (from a user in web UI). == [[dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers the following methods as the developer API for ROOT:spark-dynamic-allocation.md[]: < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method [source, scala] \u00b6 requestExecutors(numAdditionalExecutors: Int): Boolean \u00b6 requestExecutors requests numAdditionalExecutors executors from scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method [source, scala] \u00b6 killExecutors(executorIds: Seq[String]): Boolean \u00b6 CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method [source, scala] \u00b6 requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that scheduler:CoarseGrainedSchedulerBackend.md#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode === [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method getExecutorIds is a private[spark] method that is part of spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient contract]. It simply scheduler:CoarseGrainedSchedulerBackend.md#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds ]. NOTE: It works for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods [source, scala] \u00b6 getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one. [source, scala] \u00b6 import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using spark-submit.md[spark-submit]. === [[constructors]] Constructors [source, scala] \u00b6 SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors. [source, scala] \u00b6 import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method [source, scala] \u00b6 getConf: SparkConf \u00b6 getConf returns the current ROOT:SparkConf.md[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method [source, scala] \u00b6 master: String \u00b6 master method returns the current value of ROOT:configuration-properties.md#spark.master[spark.master] which is the spark-deployment-environments.md[deployment environment] in use. == [[appName]] Application Name -- appName Method [source, scala] \u00b6 appName: String \u00b6 appName gives the value of the mandatory ROOT:SparkConf.md#spark.app.name[spark.app.name] setting. NOTE: appName is used when spark-standalone.md#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], spark-webui-SparkUI.md#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method [source, scala] \u00b6 applicationAttemptId: Option[String] \u00b6 applicationAttemptId gives the unique identifier of the execution attempt of a Spark application. [NOTE] \u00b6 applicationAttemptId is used when: scheduler:ShuffleMapTask.md#creating-instance[ShuffleMapTask] and scheduler:ResultTask.md#creating-instance[ResultTask] are created * SparkContext < > \u00b6 == [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method [source, scala] \u00b6 getExecutorStorageStatus: Array[StorageStatus] \u00b6 getExecutorStorageStatus storage:BlockManagerMaster.md#getStorageStatus[requests BlockManagerMaster for storage status] (of all storage:BlockManager.md[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API. [NOTE] \u00b6 getExecutorStorageStatus is used when: SparkContext < > * SparkStatusTracker spark-sparkcontext-SparkStatusTracker.md#getExecutorInfos[is requested for information about all known executors] \u00b6 == [[deployMode]] Deploy Mode -- deployMode Method [source,scala] \u00b6 deployMode: String \u00b6 deployMode returns the current value of spark-deploy-mode.md[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method [source, scala] \u00b6 getSchedulingMode: SchedulingMode.SchedulingMode \u00b6 getSchedulingMode returns the current spark-scheduler-SchedulingMode.md[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method [source, scala] \u00b6 getPoolForName(pool: String): Option[Schedulable] \u00b6 getPoolForName returns a spark-scheduler-Schedulable.md[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the scheduler:TaskScheduler.md#rootPool[TaskScheduler for the root pool] and spark-scheduler-Pool.md#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to spark-webui-PoolPage.md[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method [source, scala] \u00b6 getAllPools: Seq[Schedulable] \u00b6 getAllPools collects the spark-scheduler-Pool.md[Pools] in scheduler:TaskScheduler.md#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for spark-webui-AllStagesPage.md#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism [source, scala] \u00b6 defaultParallelism: Int \u00b6 defaultParallelism requests < > for the scheduler:TaskScheduler.md#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of spark-rdd-partitions.md[partitions] in RDDs when created without specifying them explicitly by a user. [NOTE] \u00b6 defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.). defaultParallelism is also used to instantiate rdd:HashPartitioner.md[HashPartitioner] and for the minimum number of partitions in rdd:spark-rdd-HadoopRDD.md[HadoopRDDs]. \u00b6 == [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property [source, scala] \u00b6 taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property [source, scala] \u00b6 version: String \u00b6 version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method [source, scala] \u00b6 submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to scheduler:DAGScheduler.md#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of spark-rdd-actions.md#FutureAction[SimpleFutureAction] that holds the JobWaiter instance. CAUTION: FIXME What are resultFunc ? It is used in: spark-rdd-actions.md#AsyncRDDActions[AsyncRDDActions] methods spark-streaming/spark-streaming.md[Spark Streaming] for spark-streaming/spark-streaming-receivertracker.md#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align=\"center\"] == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read rdd:index.md#creating-rdds[Creating RDDs] in rdd:index.md[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's storage:BlockManager.md[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts ROOT:SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method [source, scala] \u00b6 setCheckpointDir(directory: String) \u00b6 setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods [source, scala] \u00b6 register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc spark-accumulators.md[accumulator]. You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register spark-accumulators.md#register[registers acc accumulator] (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators [source, scala] \u00b6 longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register spark-accumulators.md[accumulators] for simple and collection values. longAccumulator returns spark-accumulators.md#LongAccumulator[LongAccumulator] with the zero value 0 . doubleAccumulator returns spark-accumulators.md#DoubleAccumulator[DoubleAccumulator] with the zero value 0.0 . collectionAccumulator returns spark-accumulators.md#CollectionAccumulator[CollectionAccumulator] with the zero value java.util.List[T] . [source, scala] \u00b6 scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in spark-webui-StagePage.md#accumulators[Spark UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] TIP: You can register custom accumulators using < > methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method [source, scala] \u00b6 broadcast T : Broadcast[T] broadcast method creates a ROOT:Broadcast.md[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors. [source,plaintext] \u00b6 val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] Internally, broadcast requests BroadcastManager for a core:BroadcastManager.md#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using core:SparkEnv.md#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always core:BroadcastManager.md[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the core:ContextCleaner.md#[new broadcast variable is registered for cleanup]. [NOTE] \u00b6 Spark does not support broadcasting RDDs. scala> sc.broadcast(sc.range(0, 10)) java.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result. at scala.Predef$.require(Predef.scala:224) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392) ... 48 elided \u00b6 Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under spark-webui-executors.md[Executors tab]). .Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align=\"center\"] == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for scheduler:ShuffleMapStage.md[registering shuffle dependencies] to shuffle:ShuffleManager.md[Shuffle Service]. == [[stop]][[stopping]] Stopping SparkContext -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) spark-webui-SparkUI.md#stop[Stops web UI] spark-metrics-MetricsSystem.md#report[Requests MetricSystem to report metrics] (from all registered sinks) core:ContextCleaner.md#stop[Stops ContextCleaner ] spark-ExecutorAllocationManager.md#stop[Requests ExecutorAllocationManager to stop] If LiveListenerBus was started, scheduler:LiveListenerBus.md#stop[requests LiveListenerBus to stop] Requests spark-history-server:EventLoggingListener.md#stop[ EventLoggingListener to stop] Requests scheduler:DAGScheduler.md#stop[ DAGScheduler to stop] Requests rpc:index.md#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests ConsoleProgressBar to stop Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests core:SparkEnv.md#stop[ SparkEnv to stop] and clears SparkEnv Clears yarn/spark-yarn-client.md#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: Successfully stopped SparkContext == [[addSparkListener]] Registering SparkListener -- addSparkListener Method [source, scala] \u00b6 addSparkListener(listener: SparkListenerInterface): Unit \u00b6 You can register a custom ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] using addSparkListener method NOTE: You can also register custom listeners using ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers ROOT:SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and ROOT:SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method [source, scala] \u00b6 setLogLevel(logLevel: String) \u00b6 setLogLevel allows you to set the root logging level in a Spark application, e.g. spark-shell.md[Spark shell]. Internally, setLogLevel calls ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )]. [TIP] \u00b6 You can directly set the logging level using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()]. [source, scala] \u00b6 LogManager.getLogger(\"org\").setLevel(Level.OFF) \u00b6 ==== == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: spark-SparkHadoopUtil.md#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a scheduler:LiveListenerBus.md[] object that acts as a mechanism to announce events to other services on the spark-driver.md[driver]. LiveListenerBus is created and started when SparkContext is created and, since it is a single-JVM event bus, is exclusively used on the driver. == [[startTime]] Time when SparkContext was Created -- startTime Property [source, scala] \u00b6 startTime: Long \u00b6 startTime is the time in milliseconds when < >. [source, scala] \u00b6 scala> sc.startTime res0: Long = 1464425605653 == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method [source, scala] \u00b6 submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage scheduler:DAGScheduler.md#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's spark-sql-SparkPlan-ShuffleExchange.md[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method [source, scala] \u00b6 cancelJobGroup(groupId: String) \u00b6 cancelJobGroup requests DAGScheduler scheduler:DAGScheduler.md#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when spark-shell.md[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- setJobGroup Method [source, scala] \u00b6 setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit setJobGroup spark-sparkcontext-local-properties.md#setLocalProperty[sets local properties]: spark.jobGroup.id as groupId spark.job.description as description spark.job.interruptOnCancel as interruptOnCancel [NOTE] \u00b6 setJobGroup is used when: Spark Thrift Server's SparkExecuteStatementOperation runs a query Structured Streaming's StreamExecution runs batches \u00b6 == [[cleaner]] ContextCleaner [source, scala] \u00b6 cleaner: Option[ContextCleaner] \u00b6 SparkContext may have a core:ContextCleaner.md[ContextCleaner] defined. ContextCleaner is created when SparkContext is created with ROOT:configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition [source, scala] \u00b6 getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply scheduler:DAGScheduler.md#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method [source, scala] \u00b6 persistRDD(rdd: RDD[_]): Unit \u00b6 persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is rdd:index.md#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods [source, scala] \u00b6 getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of storage:RDDInfo.md[RDDInfo] instances. getRDDStorageInfo then spark-webui-StorageListener.md#StorageUtils.updateRddInfo[updates the RDDInfos] with the < > (in a Spark application). In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD spark-rdd-lineage.md#toDebugString[is requested for RDD lineage graph]. == [[settings]] Settings === [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts Quoting the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]: Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. You can however control the behaviour using spark.driver.allowMultipleContexts flag. It is disabled, i.e. false , by default. If enabled (i.e. true ), Spark prints the following WARN message to the logs: WARN Multiple running SparkContexts detected in the same JVM! If disabled (default), it will throw an SparkException exception: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: [ctx.creationSite.longForm] When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process). CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress. == [[statusStore]] Accessing AppStatusStore [source, scala] \u00b6 statusStore: AppStatusStore \u00b6 statusStore gives the current core:AppStatusStore.md[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to refresh SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method [source, scala] \u00b6 uiWebUrl: Option[String] \u00b6 uiWebUrl requests the SparkUI for webUrl . == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method [source, scala] \u00b6 maxNumConcurrentTasks(): Int \u00b6 maxNumConcurrentTasks simply requests the < > for the scheduler:SchedulerBackend.md#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See executor:Executor.md#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event [source, scala] \u00b6 postEnvironmentUpdate(): Unit \u00b6 postEnvironmentUpdate ...FIXME postEnvironmentUpdate is used when SparkContext is created, and requested to < > and < >. == [[addJar-internals]] addJar Method [source, scala] \u00b6 addJar(path: String): Unit \u00b6 addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job [source, scala] \u00b6 runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to scheduler:DAGScheduler.md#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method [source, scala] \u00b6 checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.SparkContext=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory [source,scala] \u00b6 checkpointDir: Option[String] = None \u00b6 checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > === [[stopped]] stopped Flag Flag that says whether...FIXME ( true ) or not ( false ) === [[_taskScheduler]] TaskScheduler scheduler:TaskScheduler.md[TaskScheduler]","title":"SparkContext"},{"location":"SparkContext/#sparkcontext","text":"SparkContext is the entry point to all of the components of Apache Spark (execution engine) and so the heart of a Spark application. In fact, you can consider an application a Spark application only when it uses a SparkContext (directly or indirectly). Important There should be one active SparkContext per JVM and Spark developers should use SparkContext.getOrCreate utility for sharing it (e.g. across threads).","title":"SparkContext"},{"location":"SparkContext/#creating-instance","text":"SparkContext takes the following to be created: SparkConf SparkContext is created (directly or indirectly using getOrCreate utility). While being created, SparkContext sets up core services and establishes a connection to a Spark execution environment .","title":"Creating Instance"},{"location":"SparkContext/#getorcreate-utility","text":"getOrCreate () : SparkContext getOrCreate ( config : SparkConf ) : SparkContext getOrCreate ...FIXME","title":" getOrCreate Utility"},{"location":"SparkContext/#plugincontainer","text":"SparkContext creates a PluginContainer when created . PluginContainer is created (for the driver where SparkContext lives) using PluginContainer.apply utility. PluginContainer is then requested to registerMetrics with the applicationId . PluginContainer is requested to shutdown when SparkContext is requested to stop .","title":" PluginContainer"},{"location":"SparkContext/#creating-schedulerbackend-and-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , master : String , deployMode : String ) : ( SchedulerBackend , TaskScheduler ) createTaskScheduler creates a SchedulerBackend and a TaskScheduler for the given master URL and deployment mode. Internally, createTaskScheduler branches off per the given master URL ( master URL ) to select the requested implementations. createTaskScheduler accepts the following master URLs: local - local mode with 1 thread only local[n] or local[*] - local mode with n threads local[n, m] or local[*, m] -- local mode with n threads and m number of failures spark://hostname:port for Spark Standalone local-cluster[n, m, z] -- local cluster with n workers, m cores per worker, and z memory per worker Other URLs are simply handed over to getClusterManager to load an external cluster manager if available createTaskScheduler is used when SparkContext is created .","title":" Creating SchedulerBackend and TaskScheduler"},{"location":"SparkContext/#finding-externalclustermanager-for-master-url","text":"getClusterManager ( url : String ) : Option [ ExternalClusterManager ] getClusterManager ...FIXME getClusterManager is used when SparkContext is requested for a SchedulerBackend and TaskScheduler .","title":" Finding ExternalClusterManager for Master URL"},{"location":"SparkContext/#running-job-synchronously","text":"runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U ) : Array [ U ] runJob [ T , U: ClassTag ]( rdd : RDD [ T ], processPartition : ( TaskContext , Iterator [ T ]) => U , resultHandler : ( Int , U ) => Unit ) : Unit runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ]) : Array [ U ] runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], resultHandler : ( Int , U ) => Unit ) : Unit runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U ) : Array [ U ] runJob [ T , U: ClassTag ]( rdd : RDD [ T ], processPartition : Iterator [ T ] => U , resultHandler : ( Int , U ) => Unit ) : Unit runJob [ T , U: ClassTag ]( rdd : RDD [ T ], func : Iterator [ T ] => U , partitions : Seq [ Int ]) : Array [ U ] runJob finds the call site and cleans up the given func function. runJob prints out the following INFO message to the logs: Starting job: [callSite] With spark.logLineage enabled, runJob requests the given RDD for the recursive dependencies and prints out the following INFO message to the logs: RDD's recursive dependencies: [toDebugString] runJob requests the DAGScheduler to run a job . runJob requests the ConsoleProgressBar to finishAll if defined. In the end, runJob requests the given RDD to doCheckpoint . runJob throws an IllegalStateException when SparkContext is stopped : SparkContext has been shutdown","title":" Running Job Synchronously"},{"location":"SparkContext/#runjob-demo","text":"runJob is essentially executing a func function on all or a subset of partitions of an RDD and returning the result as an array (with elements being the results per partition). sc . setLocalProperty ( \"callSite.short\" , \"runJob Demo\" ) val partitionsNumber = 4 val rdd = sc . parallelize ( Seq ( \"hello world\" , \"nice to see you\" ), numSlices = partitionsNumber ) import org.apache.spark.TaskContext val func = ( t : TaskContext , ss : Iterator [ String ]) => 1 val result = sc . runJob ( rdd , func ) assert ( result . length == partitionsNumber ) sc . clearCallSite ()","title":" runJob Demo"},{"location":"SparkContext/#call-site","text":"getCallSite () : CallSite getCallSite ...FIXME getCallSite is used when: SparkContext is requested to broadcast , runJob , runApproximateJob , submitJob and submitMapStage AsyncRDDActions is requested to takeAsync RDD is created","title":" Call Site"},{"location":"SparkContext/#closure-cleaning","text":"clean ( f : F , checkSerializable : Boolean = true ) : F clean cleans up the given f closure (using ClosureCleaner.clean utility). Tip Enable DEBUG logging level for org.apache.spark.util.ClosureCleaner logger to see what happens inside the class. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ClosureCleaner=DEBUG Refer to Logging . With DEBUG logging level you should see the following messages in the logs: +++ Cleaning closure [func] ([func.getClass.getName]) +++ + declared fields: [declaredFields.size] [field] ... +++ closure [func] ([func.getClass.getName]) is now cleaned +++","title":" Closure Cleaning"},{"location":"SparkContext/#old-information","text":"SparkContext offers the following functions: Getting current status of a Spark application ** < > ** < > ** < > ** < > ** < > ** < > ** < > that specifies the number of spark-rdd-partitions.md[partitions] in RDDs when they are created without specifying the number explicitly by a user. ** < > ** < > ** < > ** < > ** < > Setting Configuration ** < > ** spark-sparkcontext-local-properties.md[Local Properties -- Creating Logical Job Groups] ** < > ** < > Creating Distributed Entities ** < > ** < > ** < > Accessing services, e.g. < >, < >, scheduler:LiveListenerBus.md[], storage:BlockManager.md[BlockManager], scheduler:SchedulerBackend.md[SchedulerBackends], shuffle:ShuffleManager.md[ShuffleManager] and the < >. < > < > < > < > < > < > < > < > < > < > TIP: Read the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]. == [[addFile]] addFile Method","title":"Old Information"},{"location":"SparkContext/#source-scala","text":"addFile( path: String): Unit // <1> addFile( path: String, recursive: Boolean): Unit <1> recursive flag is off addFile adds the path file to be downloaded...FIXME == [[unpersistRDD]] Removing RDD Blocks from BlockManagerMaster -- unpersistRDD Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_1","text":"","title":"[source, scala]"},{"location":"SparkContext/#unpersistrddrddid-int-blocking-boolean-true-unit","text":"unpersistRDD requests BlockManagerMaster to storage:BlockManagerMaster.md#removeRdd[remove the blocks for the RDD] (given rddId ). NOTE: unpersistRDD uses SparkEnv core:SparkEnv.md#blockManager[to access the current BlockManager ] that is in turn used to storage:BlockManager.md#master[access the current BlockManagerMaster ]. unpersistRDD removes rddId from < > registry. In the end, unpersistRDD posts a ROOT:SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] (with rddId ) to < >.","title":"unpersistRDD(rddId: Int, blocking: Boolean = true): Unit"},{"location":"SparkContext/#note","text":"unpersistRDD is used when: ContextCleaner does core:ContextCleaner.md#doCleanupRDD[doCleanupRDD]","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext-ie-marks-an-rdd-as-non-persistent","text":"== [[applicationId]] Unique Identifier of Spark Application -- applicationId Method CAUTION: FIXME == [[postApplicationStart]] postApplicationStart Internal Method","title":"SparkContext &lt;&gt; (i.e. marks an RDD as non-persistent)"},{"location":"SparkContext/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkContext/#postapplicationstart-unit","text":"postApplicationStart ...FIXME postApplicationStart is used exclusively when SparkContext is created. == [[postApplicationEnd]] postApplicationEnd Method CAUTION: FIXME == [[clearActiveContext]] clearActiveContext Method CAUTION: FIXME == [[getPersistentRDDs]] Accessing persistent RDDs -- getPersistentRDDs Method","title":"postApplicationStart(): Unit"},{"location":"SparkContext/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpersistentrdds-mapint-rdd_","text":"getPersistentRDDs returns the collection of RDDs that have marked themselves as persistent via spark-rdd-caching.md#cache[cache]. Internally, getPersistentRDDs returns < > internal registry. == [[cancelJob]] Cancelling Job -- cancelJob Method","title":"getPersistentRDDs: Map[Int, RDD[_]]"},{"location":"SparkContext/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobjobid-int","text":"cancelJob requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark job]. == [[cancelStage]] Cancelling Stage -- cancelStage Methods","title":"cancelJob(jobId: Int)"},{"location":"SparkContext/#source-scala_5","text":"cancelStage(stageId: Int): Unit cancelStage(stageId: Int, reason: String): Unit cancelStage simply requests DAGScheduler scheduler:DAGScheduler.md#cancelJob[to cancel a Spark stage] (with an optional reason ). NOTE: cancelStage is used when StagesTab spark-webui-StagesTab.md#handleKillRequest[handles a kill request] (from a user in web UI). == [[dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers the following methods as the developer API for ROOT:spark-dynamic-allocation.md[]: < > < > < > (private!) < > === [[requestExecutors]] Requesting New Executors -- requestExecutors Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_6","text":"","title":"[source, scala]"},{"location":"SparkContext/#requestexecutorsnumadditionalexecutors-int-boolean","text":"requestExecutors requests numAdditionalExecutors executors from scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend]. === [[killExecutors]] Requesting to Kill Executors -- killExecutors Method","title":"requestExecutors(numAdditionalExecutors: Int): Boolean"},{"location":"SparkContext/#source-scala_7","text":"","title":"[source, scala]"},{"location":"SparkContext/#killexecutorsexecutorids-seqstring-boolean","text":"CAUTION: FIXME === [[requestTotalExecutors]] Requesting Total Executors -- requestTotalExecutors Method","title":"killExecutors(executorIds: Seq[String]): Boolean"},{"location":"SparkContext/#source-scala_8","text":"requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a private[spark] method that scheduler:CoarseGrainedSchedulerBackend.md#requestTotalExecutors[requests the exact number of executors from a coarse-grained scheduler backend]. NOTE: It works for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode === [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method getExecutorIds is a private[spark] method that is part of spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient contract]. It simply scheduler:CoarseGrainedSchedulerBackend.md#getExecutorIds[passes the call on to the current coarse-grained scheduler backend, i.e. calls getExecutorIds ]. NOTE: It works for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] only. When called for other scheduler backends you should see the following WARN message in the logs: WARN Requesting executors is only supported in coarse-grained mode CAUTION: FIXME Why does SparkContext implement the method for coarse-grained scheduler backends? Why doesn't SparkContext throw an exception when the method is called? Nobody seems to be using it (!) === [[getOrCreate]] Getting Existing or Creating New SparkContext -- getOrCreate Methods","title":"[source, scala]"},{"location":"SparkContext/#source-scala_9","text":"getOrCreate(): SparkContext getOrCreate(conf: SparkConf): SparkContext getOrCreate methods allow you to get the existing SparkContext or create a new one.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_10","text":"import org.apache.spark.SparkContext val sc = SparkContext.getOrCreate() // Using an explicit SparkConf object import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") val sc = SparkContext.getOrCreate(conf) The no-param getOrCreate method requires that the two mandatory Spark settings - < > and < > - are specified using spark-submit.md[spark-submit]. === [[constructors]] Constructors","title":"[source, scala]"},{"location":"SparkContext/#source-scala_11","text":"SparkContext() SparkContext(conf: SparkConf) SparkContext(master: String, appName: String, conf: SparkConf) SparkContext( master: String, appName: String, sparkHome: String = null, jars: Seq[String] = Nil, environment: Map[String, String] = Map()) You can create a SparkContext instance using the four constructors.","title":"[source, scala]"},{"location":"SparkContext/#source-scala_12","text":"import org.apache.spark.SparkConf val conf = new SparkConf() .setMaster(\"local[*]\") .setAppName(\"SparkMe App\") import org.apache.spark.SparkContext val sc = new SparkContext(conf) When a Spark context starts up you should see the following INFO in the logs (amongst the other messages that come from the Spark services): Running Spark version 2.0.0-SNAPSHOT NOTE: Only one SparkContext may be running in a single JVM (check out https://issues.apache.org/jira/browse/SPARK-2243[SPARK-2243 Support multiple SparkContexts in the same JVM]). Sharing access to a SparkContext in the JVM is the solution to share data within Spark (without relying on other means of data sharing using external data stores). == [[env]] Accessing Current SparkEnv -- env Method CAUTION: FIXME == [[getConf]] Getting Current SparkConf -- getConf Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_13","text":"","title":"[source, scala]"},{"location":"SparkContext/#getconf-sparkconf","text":"getConf returns the current ROOT:SparkConf.md[SparkConf]. NOTE: Changing the SparkConf object does not change the current configuration (as the method returns a copy). == [[master]][[master-url]] Deployment Environment -- master Method","title":"getConf: SparkConf"},{"location":"SparkContext/#source-scala_14","text":"","title":"[source, scala]"},{"location":"SparkContext/#master-string","text":"master method returns the current value of ROOT:configuration-properties.md#spark.master[spark.master] which is the spark-deployment-environments.md[deployment environment] in use. == [[appName]] Application Name -- appName Method","title":"master: String"},{"location":"SparkContext/#source-scala_15","text":"","title":"[source, scala]"},{"location":"SparkContext/#appname-string","text":"appName gives the value of the mandatory ROOT:SparkConf.md#spark.app.name[spark.app.name] setting. NOTE: appName is used when spark-standalone.md#SparkDeploySchedulerBackend[ SparkDeploySchedulerBackend starts], spark-webui-SparkUI.md#createLiveUI[ SparkUI creates a web UI], when postApplicationStart is executed, and for Mesos and checkpointing in Spark Streaming. == [[applicationAttemptId]] Unique Identifier of Execution Attempt -- applicationAttemptId Method","title":"appName: String"},{"location":"SparkContext/#source-scala_16","text":"","title":"[source, scala]"},{"location":"SparkContext/#applicationattemptid-optionstring","text":"applicationAttemptId gives the unique identifier of the execution attempt of a Spark application.","title":"applicationAttemptId: Option[String]"},{"location":"SparkContext/#note_1","text":"applicationAttemptId is used when: scheduler:ShuffleMapTask.md#creating-instance[ShuffleMapTask] and scheduler:ResultTask.md#creating-instance[ResultTask] are created","title":"[NOTE]"},{"location":"SparkContext/#sparkcontext_1","text":"== [[getExecutorStorageStatus]] Storage Status (of All BlockManagers) -- getExecutorStorageStatus Method","title":"* SparkContext &lt;&gt;"},{"location":"SparkContext/#source-scala_17","text":"","title":"[source, scala]"},{"location":"SparkContext/#getexecutorstoragestatus-arraystoragestatus","text":"getExecutorStorageStatus storage:BlockManagerMaster.md#getStorageStatus[requests BlockManagerMaster for storage status] (of all storage:BlockManager.md[BlockManagers]). NOTE: getExecutorStorageStatus is a developer API.","title":"getExecutorStorageStatus: Array[StorageStatus]"},{"location":"SparkContext/#note_2","text":"getExecutorStorageStatus is used when: SparkContext < >","title":"[NOTE]"},{"location":"SparkContext/#sparkstatustracker-spark-sparkcontext-sparkstatustrackermdgetexecutorinfosis-requested-for-information-about-all-known-executors","text":"== [[deployMode]] Deploy Mode -- deployMode Method","title":"* SparkStatusTracker spark-sparkcontext-SparkStatusTracker.md#getExecutorInfos[is requested for information about all known executors]"},{"location":"SparkContext/#sourcescala","text":"","title":"[source,scala]"},{"location":"SparkContext/#deploymode-string","text":"deployMode returns the current value of spark-deploy-mode.md[spark.submit.deployMode] setting or client if not set. == [[getSchedulingMode]] Scheduling Mode -- getSchedulingMode Method","title":"deployMode: String"},{"location":"SparkContext/#source-scala_18","text":"","title":"[source, scala]"},{"location":"SparkContext/#getschedulingmode-schedulingmodeschedulingmode","text":"getSchedulingMode returns the current spark-scheduler-SchedulingMode.md[Scheduling Mode]. == [[getPoolForName]] Schedulable (Pool) by Name -- getPoolForName Method","title":"getSchedulingMode: SchedulingMode.SchedulingMode"},{"location":"SparkContext/#source-scala_19","text":"","title":"[source, scala]"},{"location":"SparkContext/#getpoolfornamepool-string-optionschedulable","text":"getPoolForName returns a spark-scheduler-Schedulable.md[Schedulable] by the pool name, if one exists. NOTE: getPoolForName is part of the Developer's API and may change in the future. Internally, it requests the scheduler:TaskScheduler.md#rootPool[TaskScheduler for the root pool] and spark-scheduler-Pool.md#schedulableNameToSchedulable[looks up the Schedulable by the pool name]. It is exclusively used to spark-webui-PoolPage.md[show pool details in web UI (for a stage)]. == [[getAllPools]] All Schedulable Pools -- getAllPools Method","title":"getPoolForName(pool: String): Option[Schedulable]"},{"location":"SparkContext/#source-scala_20","text":"","title":"[source, scala]"},{"location":"SparkContext/#getallpools-seqschedulable","text":"getAllPools collects the spark-scheduler-Pool.md[Pools] in scheduler:TaskScheduler.md#contract[TaskScheduler.rootPool]. NOTE: TaskScheduler.rootPool is part of the scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. NOTE: getAllPools is part of the Developer's API. CAUTION: FIXME Where is the method used? NOTE: getAllPools is used to calculate pool names for spark-webui-AllStagesPage.md#pool-names[Stages tab in web UI] with FAIR scheduling mode used. == [[defaultParallelism]] Default Level of Parallelism","title":"getAllPools: Seq[Schedulable]"},{"location":"SparkContext/#source-scala_21","text":"","title":"[source, scala]"},{"location":"SparkContext/#defaultparallelism-int","text":"defaultParallelism requests < > for the scheduler:TaskScheduler.md#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism specifies the number of spark-rdd-partitions.md[partitions] in RDDs when created without specifying them explicitly by a user.","title":"defaultParallelism: Int"},{"location":"SparkContext/#note_3","text":"defaultParallelism is used in < >, SparkContext.range and < > (as well as Spark Streaming's DStream.countByValue and DStream.countByValueAndWindow et al.).","title":"[NOTE]"},{"location":"SparkContext/#defaultparallelism-is-also-used-to-instantiate-rddhashpartitionermdhashpartitioner-and-for-the-minimum-number-of-partitions-in-rddspark-rdd-hadooprddmdhadooprdds","text":"== [[taskScheduler]] Current Spark Scheduler (aka TaskScheduler) -- taskScheduler Property","title":"defaultParallelism is also used to instantiate rdd:HashPartitioner.md[HashPartitioner] and for the minimum number of partitions in rdd:spark-rdd-HadoopRDD.md[HadoopRDDs]."},{"location":"SparkContext/#source-scala_22","text":"taskScheduler: TaskScheduler taskScheduler_=(ts: TaskScheduler): Unit taskScheduler manages (i.e. reads or writes) <<_taskScheduler, _taskScheduler>> internal property. == [[version]] Getting Spark Version -- version Property","title":"[source, scala]"},{"location":"SparkContext/#source-scala_23","text":"","title":"[source, scala]"},{"location":"SparkContext/#version-string","text":"version returns the Spark version this SparkContext uses. == [[makeRDD]] makeRDD Method CAUTION: FIXME == [[submitJob]] Submitting Jobs Asynchronously -- submitJob Method","title":"version: String"},{"location":"SparkContext/#source-scala_24","text":"submitJob T, U, R : SimpleFutureAction[R] submitJob submits a job in an asynchronous, non-blocking way to scheduler:DAGScheduler.md#submitJob[DAGScheduler]. It cleans the processPartition input function argument and returns an instance of spark-rdd-actions.md#FutureAction[SimpleFutureAction] that holds the JobWaiter instance. CAUTION: FIXME What are resultFunc ? It is used in: spark-rdd-actions.md#AsyncRDDActions[AsyncRDDActions] methods spark-streaming/spark-streaming.md[Spark Streaming] for spark-streaming/spark-streaming-receivertracker.md#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] == [[spark-configuration]] Spark Configuration CAUTION: FIXME == [[sparkcontext-and-rdd]] SparkContext and RDDs You use a Spark context to create RDDs (see < >). When an RDD is created, it belongs to and is completely owned by the Spark context it originated from. RDDs can't by design be shared between SparkContexts. .A Spark context creates a living space for RDDs. image::diagrams/sparkcontext-rdds.png[align=\"center\"] == [[creating-rdds]][[parallelize]] Creating RDD -- parallelize Method SparkContext allows you to create many different RDDs from input sources like: Scala's collections, i.e. sc.parallelize(0 to 100) local or remote filesystems, i.e. sc.textFile(\"README.md\") Any Hadoop InputSource using sc.newAPIHadoopFile Read rdd:index.md#creating-rdds[Creating RDDs] in rdd:index.md[RDD - Resilient Distributed Dataset]. == [[unpersist]] Unpersisting RDD (Marking RDD as Non-Persistent) -- unpersist Method CAUTION: FIXME unpersist removes an RDD from the master's storage:BlockManager.md[Block Manager] (calls removeRdd(rddId: Int, blocking: Boolean) ) and the internal < > mapping. It finally posts ROOT:SparkListener.md#SparkListenerUnpersistRDD[SparkListenerUnpersistRDD] message to listenerBus . == [[setCheckpointDir]] Setting Checkpoint Directory -- setCheckpointDir Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_25","text":"","title":"[source, scala]"},{"location":"SparkContext/#setcheckpointdirdirectory-string","text":"setCheckpointDir method is used to set up the checkpoint directory...FIXME CAUTION: FIXME == [[register]] Registering Accumulator -- register Methods","title":"setCheckpointDir(directory: String)"},{"location":"SparkContext/#source-scala_26","text":"register(acc: AccumulatorV2[ , _]): Unit register(acc: AccumulatorV2[ , _], name: String): Unit register registers the acc spark-accumulators.md[accumulator]. You can optionally give an accumulator a name . TIP: You can create built-in accumulators for longs, doubles, and collection types using < >. Internally, register spark-accumulators.md#register[registers acc accumulator] (with the current SparkContext). == [[creating-accumulators]][[longAccumulator]][[doubleAccumulator]][[collectionAccumulator]] Creating Built-In Accumulators","title":"[source, scala]"},{"location":"SparkContext/#source-scala_27","text":"longAccumulator: LongAccumulator longAccumulator(name: String): LongAccumulator doubleAccumulator: DoubleAccumulator doubleAccumulator(name: String): DoubleAccumulator collectionAccumulator[T]: CollectionAccumulator[T] collectionAccumulator T : CollectionAccumulator[T] You can use longAccumulator , doubleAccumulator or collectionAccumulator to create and register spark-accumulators.md[accumulators] for simple and collection values. longAccumulator returns spark-accumulators.md#LongAccumulator[LongAccumulator] with the zero value 0 . doubleAccumulator returns spark-accumulators.md#DoubleAccumulator[DoubleAccumulator] with the zero value 0.0 . collectionAccumulator returns spark-accumulators.md#CollectionAccumulator[CollectionAccumulator] with the zero value java.util.List[T] .","title":"[source, scala]"},{"location":"SparkContext/#source-scala_28","text":"scala> val acc = sc.longAccumulator acc: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: None, value: 0) scala> val counter = sc.longAccumulator(\"counter\") counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 1, name: Some(counter), value: 0) scala> counter.value res0: Long = 0 scala> sc.parallelize(0 to 9).foreach(n => counter.add(n)) scala> counter.value res3: Long = 45 The name input parameter allows you to give a name to an accumulator and have it displayed in spark-webui-StagePage.md#accumulators[Spark UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] TIP: You can register custom accumulators using < > methods. == [[broadcast]] Creating Broadcast Variable -- broadcast Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_29","text":"broadcast T : Broadcast[T] broadcast method creates a ROOT:Broadcast.md[]. It is a shared memory with value (as broadcast blocks) on the driver and later on all Spark executors.","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext","text":"val sc: SparkContext = ??? scala> val hello = sc.broadcast(\"hello\") hello: org.apache.spark.broadcast.Broadcast[String] = Broadcast(0) Spark transfers the value to Spark executors once , and tasks can share it without incurring repetitive network transmissions when the broadcast variable is used multiple times. .Broadcasting a value to executors image::sparkcontext-broadcast-executors.png[align=\"center\"] Internally, broadcast requests BroadcastManager for a core:BroadcastManager.md#newBroadcast[new broadcast variable]. NOTE: The current BroadcastManager is available using core:SparkEnv.md#broadcastManager[ SparkEnv.broadcastManager ] attribute and is always core:BroadcastManager.md[BroadcastManager] (with few internal configuration changes to reflect where it runs, i.e. inside the driver or executors). You should see the following INFO message in the logs: Created broadcast [id] from [callSite] If ContextCleaner is defined, the core:ContextCleaner.md#[new broadcast variable is registered for cleanup].","title":"[source,plaintext]"},{"location":"SparkContext/#note_4","text":"Spark does not support broadcasting RDDs.","title":"[NOTE]"},{"location":"SparkContext/#scala-scbroadcastscrange0-10-javalangillegalargumentexception-requirement-failed-can-not-directly-broadcast-rdds-instead-call-collect-and-broadcast-the-result-at-scalapredefrequirepredefscala224-at-orgapachesparksparkcontextbroadcastsparkcontextscala1392-48-elided","text":"Once created, the broadcast variable (and other blocks) are displayed per executor and the driver in web UI (under spark-webui-executors.md[Executors tab]). .Broadcast Variables In web UI's Executors Tab image::spark-broadcast-webui-executors-rdd-blocks.png[align=\"center\"] == [[jars]] Distribute JARs to workers The jar you specify with SparkContext.addJar will be copied to all the worker nodes. The configuration setting spark.jars is a comma-separated list of jar paths to be included in all tasks executed from this SparkContext. A path can either be a local file, a file in HDFS (or other Hadoop-supported filesystems), an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node. scala> sc.addJar(\"build.sbt\") 15/11/11 21:54:54 Added JAR build.sbt at http://192.168.1.4:49427/jars/build.sbt with timestamp 1447275294457 CAUTION: FIXME Why is HttpFileServer used for addJar? === SparkContext as Application-Wide Counter SparkContext keeps track of: [[nextShuffleId]] * shuffle ids using nextShuffleId internal counter for scheduler:ShuffleMapStage.md[registering shuffle dependencies] to shuffle:ShuffleManager.md[Shuffle Service]. == [[stop]][[stopping]] Stopping SparkContext -- stop Method","title":"scala&gt; sc.broadcast(sc.range(0, 10))\njava.lang.IllegalArgumentException: requirement failed: Can not directly broadcast RDDs; instead, call collect() and broadcast the result.\n  at scala.Predef$.require(Predef.scala:224)\n  at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1392)\n  ... 48 elided\n"},{"location":"SparkContext/#source-scala_30","text":"","title":"[source, scala]"},{"location":"SparkContext/#stop-unit","text":"stop stops the SparkContext. Internally, stop enables stopped internal flag. If already stopped, you should see the following INFO message in the logs: SparkContext already stopped. stop then does the following: Removes _shutdownHookRef from ShutdownHookManager < SparkListenerApplicationEnd >> (to < >) spark-webui-SparkUI.md#stop[Stops web UI] spark-metrics-MetricsSystem.md#report[Requests MetricSystem to report metrics] (from all registered sinks) core:ContextCleaner.md#stop[Stops ContextCleaner ] spark-ExecutorAllocationManager.md#stop[Requests ExecutorAllocationManager to stop] If LiveListenerBus was started, scheduler:LiveListenerBus.md#stop[requests LiveListenerBus to stop] Requests spark-history-server:EventLoggingListener.md#stop[ EventLoggingListener to stop] Requests scheduler:DAGScheduler.md#stop[ DAGScheduler to stop] Requests rpc:index.md#stop[RpcEnv to stop HeartbeatReceiver endpoint] Requests ConsoleProgressBar to stop Clears the reference to TaskScheduler , i.e. _taskScheduler is null Requests core:SparkEnv.md#stop[ SparkEnv to stop] and clears SparkEnv Clears yarn/spark-yarn-client.md#SPARK_YARN_MODE[ SPARK_YARN_MODE flag] < > Ultimately, you should see the following INFO message in the logs: Successfully stopped SparkContext == [[addSparkListener]] Registering SparkListener -- addSparkListener Method","title":"stop(): Unit"},{"location":"SparkContext/#source-scala_31","text":"","title":"[source, scala]"},{"location":"SparkContext/#addsparklistenerlistener-sparklistenerinterface-unit","text":"You can register a custom ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] using addSparkListener method NOTE: You can also register custom listeners using ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property. == [[custom-schedulers]] Custom SchedulerBackend, TaskScheduler and DAGScheduler By default, SparkContext uses ( private[spark] class) org.apache.spark.scheduler.DAGScheduler , but you can develop your own custom DAGScheduler implementation, and use ( private[spark] ) SparkContext.dagScheduler_=(ds: DAGScheduler) method to assign yours. It is also applicable to SchedulerBackend and TaskScheduler using schedulerBackend_=(sb: SchedulerBackend) and taskScheduler_=(ts: TaskScheduler) methods, respectively. CAUTION: FIXME Make it an advanced exercise. == [[events]] Events When a Spark context starts, it triggers ROOT:SparkListener.md#SparkListenerEnvironmentUpdate[SparkListenerEnvironmentUpdate] and ROOT:SparkListener.md#SparkListenerApplicationStart[SparkListenerApplicationStart] messages. Refer to the section < >. == [[setLogLevel]][[setting-default-log-level]] Setting Default Logging Level -- setLogLevel Method","title":"addSparkListener(listener: SparkListenerInterface): Unit"},{"location":"SparkContext/#source-scala_32","text":"","title":"[source, scala]"},{"location":"SparkContext/#setloglevelloglevel-string","text":"setLogLevel allows you to set the root logging level in a Spark application, e.g. spark-shell.md[Spark shell]. Internally, setLogLevel calls ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html#toLevel(java.lang.String)++[org.apache.log4j.Level.toLevel(logLevel )] that it then uses to set using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getRootLogger()++[org.apache.log4j.LogManager.getRootLogger().setLevel(level )].","title":"setLogLevel(logLevel: String)"},{"location":"SparkContext/#tip","text":"You can directly set the logging level using ++ http://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/LogManager.html#getLogger()++[org.apache.log4j.LogManager.getLogger ()].","title":"[TIP]"},{"location":"SparkContext/#source-scala_33","text":"","title":"[source, scala]"},{"location":"SparkContext/#logmanagergetloggerorgsetlevelleveloff","text":"==== == [[hadoopConfiguration]] Hadoop Configuration While a < >, so is a Hadoop configuration (as an instance of https://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html[org.apache.hadoop.conf.Configuration ] that is available as _hadoopConfiguration ). NOTE: spark-SparkHadoopUtil.md#newConfiguration[SparkHadoopUtil.get.newConfiguration] is used. If a SparkConf is provided it is used to build the configuration as described. Otherwise, the default Configuration object is returned. If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are both available, the following settings are set for the Hadoop configuration: fs.s3.awsAccessKeyId , fs.s3n.awsAccessKeyId , fs.s3a.access.key are set to the value of AWS_ACCESS_KEY_ID fs.s3.awsSecretAccessKey , fs.s3n.awsSecretAccessKey , and fs.s3a.secret.key are set to the value of AWS_SECRET_ACCESS_KEY Every spark.hadoop. setting becomes a setting of the configuration with the prefix spark.hadoop. removed for the key. The value of spark.buffer.size (default: 65536 ) is used as the value of io.file.buffer.size . == [[listenerBus]] listenerBus -- LiveListenerBus Event Bus listenerBus is a scheduler:LiveListenerBus.md[] object that acts as a mechanism to announce events to other services on the spark-driver.md[driver]. LiveListenerBus is created and started when SparkContext is created and, since it is a single-JVM event bus, is exclusively used on the driver. == [[startTime]] Time when SparkContext was Created -- startTime Property","title":"LogManager.getLogger(\"org\").setLevel(Level.OFF)"},{"location":"SparkContext/#source-scala_34","text":"","title":"[source, scala]"},{"location":"SparkContext/#starttime-long","text":"startTime is the time in milliseconds when < >.","title":"startTime: Long"},{"location":"SparkContext/#source-scala_35","text":"scala> sc.startTime res0: Long = 1464425605653 == [[submitMapStage]] Submitting ShuffleDependency for Execution -- submitMapStage Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_36","text":"submitMapStage K, V, C : SimpleFutureAction[MapOutputStatistics] submitMapStage scheduler:DAGScheduler.md#submitMapStage[submits the input ShuffleDependency to DAGScheduler for execution] and returns a SimpleFutureAction . Internally, submitMapStage < > first and submits it with localProperties . NOTE: Interestingly, submitMapStage is used exclusively when Spark SQL's spark-sql-SparkPlan-ShuffleExchange.md[ShuffleExchange] physical operator is executed. NOTE: submitMapStage seems related to scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. == [[cancelJobGroup]] Cancelling Job Group -- cancelJobGroup Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_37","text":"","title":"[source, scala]"},{"location":"SparkContext/#canceljobgroupgroupid-string","text":"cancelJobGroup requests DAGScheduler scheduler:DAGScheduler.md#cancelJobGroup[to cancel a group of active Spark jobs]. NOTE: cancelJobGroup is used exclusively when SparkExecuteStatementOperation does cancel . == [[cancelAllJobs]] Cancelling All Running and Scheduled Jobs -- cancelAllJobs Method CAUTION: FIXME NOTE: cancelAllJobs is used when spark-shell.md[spark-shell] is terminated (e.g. using Ctrl+C, so it can in turn terminate all active Spark jobs) or SparkSQLCLIDriver is terminated. == [[setJobGroup]] Setting Local Properties to Group Spark Jobs -- setJobGroup Method","title":"cancelJobGroup(groupId: String)"},{"location":"SparkContext/#source-scala_38","text":"setJobGroup( groupId: String, description: String, interruptOnCancel: Boolean = false): Unit setJobGroup spark-sparkcontext-local-properties.md#setLocalProperty[sets local properties]: spark.jobGroup.id as groupId spark.job.description as description spark.job.interruptOnCancel as interruptOnCancel","title":"[source, scala]"},{"location":"SparkContext/#note_5","text":"setJobGroup is used when: Spark Thrift Server's SparkExecuteStatementOperation runs a query","title":"[NOTE]"},{"location":"SparkContext/#structured-streamings-streamexecution-runs-batches","text":"== [[cleaner]] ContextCleaner","title":"Structured Streaming's StreamExecution runs batches"},{"location":"SparkContext/#source-scala_39","text":"","title":"[source, scala]"},{"location":"SparkContext/#cleaner-optioncontextcleaner","text":"SparkContext may have a core:ContextCleaner.md[ContextCleaner] defined. ContextCleaner is created when SparkContext is created with ROOT:configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[getPreferredLocs]] Finding Preferred Locations (Placement Preferences) for RDD Partition","title":"cleaner: Option[ContextCleaner]"},{"location":"SparkContext/#source-scala_40","text":"getPreferredLocs( rdd: RDD[_], partition: Int): Seq[TaskLocation] getPreferredLocs simply scheduler:DAGScheduler.md#getPreferredLocs[requests DAGScheduler for the preferred locations for partition ]. NOTE: Preferred locations of a partition of a RDD are also called placement preferences or locality preferences . getPreferredLocs is used in CoalescedRDDPartition, DefaultPartitionCoalescer and PartitionerAwareUnionRDD. == [[persistRDD]] Registering RDD in persistentRdds Internal Registry -- persistRDD Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_41","text":"","title":"[source, scala]"},{"location":"SparkContext/#persistrddrdd-rdd_-unit","text":"persistRDD registers rdd in < > internal registry. NOTE: persistRDD is used exclusively when RDD is rdd:index.md#persist-internal[persisted or locally checkpointed]. == [[getRDDStorageInfo]] Getting Storage Status of Cached RDDs (as RDDInfos) -- getRDDStorageInfo Methods","title":"persistRDD(rdd: RDD[_]): Unit"},{"location":"SparkContext/#source-scala_42","text":"getRDDStorageInfo: Array[RDDInfo] // <1> getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] // <2> <1> Part of Spark's Developer API that uses <2> filtering no RDDs getRDDStorageInfo takes all the RDDs (from < > registry) that match filter and creates a collection of storage:RDDInfo.md[RDDInfo] instances. getRDDStorageInfo then spark-webui-StorageListener.md#StorageUtils.updateRddInfo[updates the RDDInfos] with the < > (in a Spark application). In the end, getRDDStorageInfo gives only the RDD that are cached (i.e. the sum of memory and disk sizes as well as the number of partitions cached are greater than 0 ). NOTE: getRDDStorageInfo is used when RDD spark-rdd-lineage.md#toDebugString[is requested for RDD lineage graph]. == [[settings]] Settings === [[spark.driver.allowMultipleContexts]] spark.driver.allowMultipleContexts Quoting the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext[org.apache.spark.SparkContext ]: Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one. You can however control the behaviour using spark.driver.allowMultipleContexts flag. It is disabled, i.e. false , by default. If enabled (i.e. true ), Spark prints the following WARN message to the logs: WARN Multiple running SparkContexts detected in the same JVM! If disabled (default), it will throw an SparkException exception: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at: [ctx.creationSite.longForm] When creating an instance of SparkContext, Spark marks the current thread as having it being created (very early in the instantiation process). CAUTION: It's not guaranteed that Spark will work properly with two or more SparkContexts. Consider the feature a work in progress. == [[statusStore]] Accessing AppStatusStore","title":"[source, scala]"},{"location":"SparkContext/#source-scala_43","text":"","title":"[source, scala]"},{"location":"SparkContext/#statusstore-appstatusstore","text":"statusStore gives the current core:AppStatusStore.md[]. statusStore is used when: SparkContext is requested to < > ConsoleProgressBar is requested to refresh SharedState (Spark SQL) is requested for a SQLAppStatusStore == [[uiWebUrl]] Requesting URL of web UI -- uiWebUrl Method","title":"statusStore: AppStatusStore"},{"location":"SparkContext/#source-scala_44","text":"","title":"[source, scala]"},{"location":"SparkContext/#uiweburl-optionstring","text":"uiWebUrl requests the SparkUI for webUrl . == [[maxNumConcurrentTasks]] maxNumConcurrentTasks Method","title":"uiWebUrl: Option[String]"},{"location":"SparkContext/#source-scala_45","text":"","title":"[source, scala]"},{"location":"SparkContext/#maxnumconcurrenttasks-int","text":"maxNumConcurrentTasks simply requests the < > for the scheduler:SchedulerBackend.md#maxNumConcurrentTasks[maximum number of tasks that can be launched concurrently]. NOTE: maxNumConcurrentTasks is used exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#checkBarrierStageWithNumSlots[checkBarrierStageWithNumSlots]. == [[environment-variables]] Environment Variables .Environment Variables [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | [[SPARK_EXECUTOR_MEMORY]] SPARK_EXECUTOR_MEMORY | 1024 | Amount of memory to allocate for a Spark executor in MB. See executor:Executor.md#memory[Executor Memory]. [[SPARK_USER]] SPARK_USER The user who is running SparkContext. Available later as < >. === == [[postEnvironmentUpdate]] Posting SparkListenerEnvironmentUpdate Event","title":"maxNumConcurrentTasks(): Int"},{"location":"SparkContext/#source-scala_46","text":"","title":"[source, scala]"},{"location":"SparkContext/#postenvironmentupdate-unit","text":"postEnvironmentUpdate ...FIXME postEnvironmentUpdate is used when SparkContext is created, and requested to < > and < >. == [[addJar-internals]] addJar Method","title":"postEnvironmentUpdate(): Unit"},{"location":"SparkContext/#source-scala_47","text":"","title":"[source, scala]"},{"location":"SparkContext/#addjarpath-string-unit","text":"addJar ...FIXME NOTE: addJar is used when...FIXME == [[runApproximateJob]] Running Approximate Job","title":"addJar(path: String): Unit"},{"location":"SparkContext/#source-scala_48","text":"runApproximateJob T, U, R : PartialResult[R] runApproximateJob...FIXME runApproximateJob is used when: DoubleRDDFunctions is requested to meanApprox and sumApprox RDD is requested to countApprox and countByValueApprox == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"SparkContext/#source-scala_49","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean = true, reason: String = \"killed via SparkContext.killTaskAttempt\"): Boolean killTaskAttempt requests the < > to scheduler:DAGScheduler.md#killTaskAttempt[kill a task]. == [[checkpointFile]] checkpointFile Internal Method","title":"[source, scala]"},{"location":"SparkContext/#source-scala_50","text":"checkpointFile T: ClassTag : RDD[T] checkpointFile...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkContext logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkContext/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"SparkContext/#log4jloggerorgapachesparksparkcontextall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[checkpointDir]] Checkpoint Directory","title":"log4j.logger.org.apache.spark.SparkContext=ALL"},{"location":"SparkContext/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"SparkContext/#checkpointdir-optionstring-none","text":"checkpointDir is...FIXME === [[persistentRdds]] persistentRdds Lookup Table Lookup table of persistent/cached RDDs per their ids. Used when SparkContext is requested to: < > < > < > < > === [[stopped]] stopped Flag Flag that says whether...FIXME ( true ) or not ( false ) === [[_taskScheduler]] TaskScheduler scheduler:TaskScheduler.md[TaskScheduler]","title":"checkpointDir: Option[String] = None"},{"location":"SparkEnv/","text":"SparkEnv \u00b6 SparkEnv is a handle to Spark Execution Environment with the core services of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). There are two separate SparkEnv s of the driver and executors . Core Services \u00b6 Property Service blockManager BlockManager broadcastManager BroadcastManager closureSerializer Serializer conf SparkConf mapOutputTracker MapOutputTracker memoryManager MemoryManager metricsSystem MetricsSystem outputCommitCoordinator OutputCommitCoordinator rpcEnv RpcEnv securityManager SecurityManager serializer Serializer serializerManager SerializerManager shuffleManager ShuffleManager Creating Instance \u00b6 SparkEnv takes the following to be created: Executor ID RpcEnv Serializer Serializer SerializerManager MapOutputTracker ShuffleManager BroadcastManager BlockManager SecurityManager MetricsSystem MemoryManager OutputCommitCoordinator SparkConf SparkEnv is created using create utility. Creating SparkEnv for Driver \u00b6 createDriverEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus , numCores : Int , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. createDriverEnv accepts an instance of ROOT:SparkConf.md[SparkConf], spark-deployment-environments.md[whether it runs in local mode or not], scheduler:LiveListenerBus.md[], the number of cores to use for execution in local mode or 0 otherwise, and a scheduler:OutputCommitCoordinator.md[OutputCommitCoordinator] (default: none). createDriverEnv ensures that spark-driver.md#spark_driver_host[spark.driver.host] and spark-driver.md#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). createDriverEnv is used when SparkContext is created . Creating SparkEnv for Executor \u00b6 createExecutorEnv ( conf : SparkConf , executorId : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv ( conf : SparkConf , executorId : String , bindAddress : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. createExecutorEnv is used when CoarseGrainedExecutorBackend utility is requested to run . Creating \"Base\" SparkEnv (for Driver and Executors) \u00b6 create ( conf : SparkConf , executorId : String , bindAddress : String , advertiseAddress : String , port : Option [ Int ], isLocal : Boolean , numUsableCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], listenerBus : LiveListenerBus = null , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv create is an utility to create the \"base\" SparkEnv (that is \"enhanced\" for the driver and executors later on). .create's Input Arguments and Their Usage [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Input Argument | Usage | bindAddress | Used to create rpc:index.md[RpcEnv] and storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | advertiseAddress | Used to create rpc:index.md[RpcEnv] and storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | numUsableCores | Used to create memory:MemoryManager.md[MemoryManager], storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] and storage:BlockManager.md#creating-instance[BlockManager]. |=== [[create-Serializer]] create creates a Serializer (based on < > setting). You should see the following DEBUG message in the logs: Using serializer: [serializer] [[create-closure-Serializer]] create creates a closure Serializer (based on < >). [[ShuffleManager]][[create-ShuffleManager]] create creates a shuffle:ShuffleManager.md[ShuffleManager] given the value of ROOT:configuration-properties.md#spark.shuffle.manager[spark.shuffle.manager] configuration property. [[MemoryManager]][[create-MemoryManager]] create creates a memory:MemoryManager.md[MemoryManager] based on ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] setting (with memory:UnifiedMemoryManager.md[UnifiedMemoryManager] being the default and numCores the input numUsableCores ). [[NettyBlockTransferService]][[create-NettyBlockTransferService]] create creates a storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] with the following ports: spark-driver.md#spark_driver_blockManager_port[spark.driver.blockManager.port] for the driver (default: 0 ) storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] for an executor (default: 0 ) NOTE: create uses the NettyBlockTransferService to < >. CAUTION: FIXME A picture with SparkEnv, NettyBlockTransferService and the ports \"armed\". [[BlockManagerMaster]][[create-BlockManagerMaster]] create creates a storage:BlockManagerMaster.md#creating-instance[BlockManagerMaster] object with the BlockManagerMaster RPC endpoint reference (by < > and storage:BlockManagerMasterEndpoint.md[]), the input ROOT:SparkConf.md[SparkConf], and the input isDriver flag. .Creating BlockManager for the Driver image::sparkenv-driver-blockmanager.png[align=\"center\"] NOTE: create registers the BlockManagerMaster RPC endpoint for the driver and looks it up for executors. .Creating BlockManager for Executor image::sparkenv-executor-blockmanager.png[align=\"center\"] [[BlockManager]][[create-BlockManager]] create creates a storage:BlockManager.md#creating-instance[BlockManager] (using the above < >, < > and other services). create creates a core:BroadcastManager.md[]. [[MapOutputTracker]][[create-MapOutputTracker]] create creates a scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] or scheduler:MapOutputTrackerWorker.md[MapOutputTrackerWorker] for the driver and executors, respectively. NOTE: The choice of the real implementation of scheduler:MapOutputTracker.md[MapOutputTracker] is based on whether the input executorId is driver or not. [[MapOutputTrackerMasterEndpoint]][[create-MapOutputTrackerMasterEndpoint]] create < RpcEndpoint >> as MapOutputTracker . It registers scheduler:MapOutputTrackerMasterEndpoint.md[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the scheduler:MapOutputTracker.md#trackerEndpoint[MapOutputTracker RPC endpoint]. CAUTION: FIXME [[create-CacheManager]] It creates a CacheManager. [[create-MetricsSystem]] It creates a MetricsSystem for a driver and a worker separately. It initializes userFiles temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor. [[create-OutputCommitCoordinator]] An OutputCommitCoordinator is created. Usage \u00b6 create is used when SparkEnv utility is used to create a SparkEnv for the driver and executors . == [[get]] Accessing SparkEnv [source, scala] \u00b6 get: SparkEnv \u00b6 get returns the SparkEnv on the driver and executors. [source, scala] \u00b6 import org.apache.spark.SparkEnv assert(SparkEnv.get.isInstanceOf[SparkEnv]) == [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name [source, scala] \u00b6 registerOrLookupEndpoint( name: String, endpointCreator: => RpcEndpoint) registerOrLookupEndpoint registers or looks up a RPC endpoint by name . If called from the driver, you should see the following INFO message in the logs: Registering [name] And the RPC endpoint is registered in the RPC environment. Otherwise, it obtains a RPC endpoint reference by name . == [[stop]] Stopping SparkEnv [source, scala] \u00b6 stop(): Unit \u00b6 stop checks < > internal flag and does nothing when enabled already. Otherwise, stop turns isStopped flag on, stops all pythonWorkers and requests the following services to stop: scheduler:MapOutputTracker.md#stop[MapOutputTracker] shuffle:ShuffleManager.md#stop[ShuffleManager] core:BroadcastManager.md#stop[BroadcastManager] storage:BlockManager.md#stop[BlockManager] storage:BlockManagerMaster.md#stop[BlockManagerMaster] spark-metrics-MetricsSystem.md#stop[MetricsSystem] scheduler:OutputCommitCoordinator.md#stop[OutputCommitCoordinator] stop rpc:index.md#shutdown[requests RpcEnv to shut down] and rpc:index.md#awaitTermination[waits till it terminates]. Only on the driver, stop deletes the < >. You can see the following WARN message in the logs if the deletion fails. Exception while deleting Spark temp dir: [path] NOTE: stop is used when ROOT:SparkContext.md#stop[ SparkContext stops] (on the driver) and executor:Executor.md#stop[ Executor stops]. == [[set]] set Method [source, scala] \u00b6 set(e: SparkEnv): Unit \u00b6 set saves the input SparkEnv to < > internal registry (as the default SparkEnv). NOTE: set is used when...FIXME == [[environmentDetails]] environmentDetails Utility [source, scala] \u00b6 environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] environmentDetails...FIXME environmentDetails is used when SparkContext is requested to ROOT:SparkContext.md#postEnvironmentUpdate[post a SparkListenerEnvironmentUpdate event]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.SparkEnv=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isStopped | [[isStopped]] Used to mark SparkEnv stopped Default: false | driverTmpDir | [[driverTmpDir]] |===","title":"SparkEnv"},{"location":"SparkEnv/#sparkenv","text":"SparkEnv is a handle to Spark Execution Environment with the core services of Apache Spark (that interact with each other to establish a distributed computing platform for a Spark application). There are two separate SparkEnv s of the driver and executors .","title":"SparkEnv"},{"location":"SparkEnv/#core-services","text":"Property Service blockManager BlockManager broadcastManager BroadcastManager closureSerializer Serializer conf SparkConf mapOutputTracker MapOutputTracker memoryManager MemoryManager metricsSystem MetricsSystem outputCommitCoordinator OutputCommitCoordinator rpcEnv RpcEnv securityManager SecurityManager serializer Serializer serializerManager SerializerManager shuffleManager ShuffleManager","title":" Core Services"},{"location":"SparkEnv/#creating-instance","text":"SparkEnv takes the following to be created: Executor ID RpcEnv Serializer Serializer SerializerManager MapOutputTracker ShuffleManager BroadcastManager BlockManager SecurityManager MetricsSystem MemoryManager OutputCommitCoordinator SparkConf SparkEnv is created using create utility.","title":"Creating Instance"},{"location":"SparkEnv/#creating-sparkenv-for-driver","text":"createDriverEnv ( conf : SparkConf , isLocal : Boolean , listenerBus : LiveListenerBus , numCores : Int , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv createDriverEnv creates a SparkEnv execution environment for the driver. createDriverEnv accepts an instance of ROOT:SparkConf.md[SparkConf], spark-deployment-environments.md[whether it runs in local mode or not], scheduler:LiveListenerBus.md[], the number of cores to use for execution in local mode or 0 otherwise, and a scheduler:OutputCommitCoordinator.md[OutputCommitCoordinator] (default: none). createDriverEnv ensures that spark-driver.md#spark_driver_host[spark.driver.host] and spark-driver.md#spark_driver_port[spark.driver.port] settings are defined. It then passes the call straight on to the < > (with driver executor id, isDriver enabled, and the input parameters). createDriverEnv is used when SparkContext is created .","title":" Creating SparkEnv for Driver"},{"location":"SparkEnv/#creating-sparkenv-for-executor","text":"createExecutorEnv ( conf : SparkConf , executorId : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv ( conf : SparkConf , executorId : String , bindAddress : String , hostname : String , numCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], isLocal : Boolean ) : SparkEnv createExecutorEnv creates an executor's (execution) environment that is the Spark execution environment for an executor. createExecutorEnv simply < > (passing in all the input parameters) and < >. NOTE: The number of cores numCores is configured using --cores command-line option of CoarseGrainedExecutorBackend and is specific to a cluster manager. createExecutorEnv is used when CoarseGrainedExecutorBackend utility is requested to run .","title":" Creating SparkEnv for Executor"},{"location":"SparkEnv/#creating-base-sparkenv-for-driver-and-executors","text":"create ( conf : SparkConf , executorId : String , bindAddress : String , advertiseAddress : String , port : Option [ Int ], isLocal : Boolean , numUsableCores : Int , ioEncryptionKey : Option [ Array [ Byte ]], listenerBus : LiveListenerBus = null , mockOutputCommitCoordinator : Option [ OutputCommitCoordinator ] = None ) : SparkEnv create is an utility to create the \"base\" SparkEnv (that is \"enhanced\" for the driver and executors later on). .create's Input Arguments and Their Usage [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Input Argument | Usage | bindAddress | Used to create rpc:index.md[RpcEnv] and storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | advertiseAddress | Used to create rpc:index.md[RpcEnv] and storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService]. | numUsableCores | Used to create memory:MemoryManager.md[MemoryManager], storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] and storage:BlockManager.md#creating-instance[BlockManager]. |=== [[create-Serializer]] create creates a Serializer (based on < > setting). You should see the following DEBUG message in the logs: Using serializer: [serializer] [[create-closure-Serializer]] create creates a closure Serializer (based on < >). [[ShuffleManager]][[create-ShuffleManager]] create creates a shuffle:ShuffleManager.md[ShuffleManager] given the value of ROOT:configuration-properties.md#spark.shuffle.manager[spark.shuffle.manager] configuration property. [[MemoryManager]][[create-MemoryManager]] create creates a memory:MemoryManager.md[MemoryManager] based on ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] setting (with memory:UnifiedMemoryManager.md[UnifiedMemoryManager] being the default and numCores the input numUsableCores ). [[NettyBlockTransferService]][[create-NettyBlockTransferService]] create creates a storage:NettyBlockTransferService.md#creating-instance[NettyBlockTransferService] with the following ports: spark-driver.md#spark_driver_blockManager_port[spark.driver.blockManager.port] for the driver (default: 0 ) storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] for an executor (default: 0 ) NOTE: create uses the NettyBlockTransferService to < >. CAUTION: FIXME A picture with SparkEnv, NettyBlockTransferService and the ports \"armed\". [[BlockManagerMaster]][[create-BlockManagerMaster]] create creates a storage:BlockManagerMaster.md#creating-instance[BlockManagerMaster] object with the BlockManagerMaster RPC endpoint reference (by < > and storage:BlockManagerMasterEndpoint.md[]), the input ROOT:SparkConf.md[SparkConf], and the input isDriver flag. .Creating BlockManager for the Driver image::sparkenv-driver-blockmanager.png[align=\"center\"] NOTE: create registers the BlockManagerMaster RPC endpoint for the driver and looks it up for executors. .Creating BlockManager for Executor image::sparkenv-executor-blockmanager.png[align=\"center\"] [[BlockManager]][[create-BlockManager]] create creates a storage:BlockManager.md#creating-instance[BlockManager] (using the above < >, < > and other services). create creates a core:BroadcastManager.md[]. [[MapOutputTracker]][[create-MapOutputTracker]] create creates a scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] or scheduler:MapOutputTrackerWorker.md[MapOutputTrackerWorker] for the driver and executors, respectively. NOTE: The choice of the real implementation of scheduler:MapOutputTracker.md[MapOutputTracker] is based on whether the input executorId is driver or not. [[MapOutputTrackerMasterEndpoint]][[create-MapOutputTrackerMasterEndpoint]] create < RpcEndpoint >> as MapOutputTracker . It registers scheduler:MapOutputTrackerMasterEndpoint.md[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the scheduler:MapOutputTracker.md#trackerEndpoint[MapOutputTracker RPC endpoint]. CAUTION: FIXME [[create-CacheManager]] It creates a CacheManager. [[create-MetricsSystem]] It creates a MetricsSystem for a driver and a worker separately. It initializes userFiles temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor. [[create-OutputCommitCoordinator]] An OutputCommitCoordinator is created.","title":" Creating \"Base\" SparkEnv (for Driver and Executors)"},{"location":"SparkEnv/#usage","text":"create is used when SparkEnv utility is used to create a SparkEnv for the driver and executors . == [[get]] Accessing SparkEnv","title":"Usage"},{"location":"SparkEnv/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkEnv/#get-sparkenv","text":"get returns the SparkEnv on the driver and executors.","title":"get: SparkEnv"},{"location":"SparkEnv/#source-scala_1","text":"import org.apache.spark.SparkEnv assert(SparkEnv.get.isInstanceOf[SparkEnv]) == [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_2","text":"registerOrLookupEndpoint( name: String, endpointCreator: => RpcEndpoint) registerOrLookupEndpoint registers or looks up a RPC endpoint by name . If called from the driver, you should see the following INFO message in the logs: Registering [name] And the RPC endpoint is registered in the RPC environment. Otherwise, it obtains a RPC endpoint reference by name . == [[stop]] Stopping SparkEnv","title":"[source, scala]"},{"location":"SparkEnv/#source-scala_3","text":"","title":"[source, scala]"},{"location":"SparkEnv/#stop-unit","text":"stop checks < > internal flag and does nothing when enabled already. Otherwise, stop turns isStopped flag on, stops all pythonWorkers and requests the following services to stop: scheduler:MapOutputTracker.md#stop[MapOutputTracker] shuffle:ShuffleManager.md#stop[ShuffleManager] core:BroadcastManager.md#stop[BroadcastManager] storage:BlockManager.md#stop[BlockManager] storage:BlockManagerMaster.md#stop[BlockManagerMaster] spark-metrics-MetricsSystem.md#stop[MetricsSystem] scheduler:OutputCommitCoordinator.md#stop[OutputCommitCoordinator] stop rpc:index.md#shutdown[requests RpcEnv to shut down] and rpc:index.md#awaitTermination[waits till it terminates]. Only on the driver, stop deletes the < >. You can see the following WARN message in the logs if the deletion fails. Exception while deleting Spark temp dir: [path] NOTE: stop is used when ROOT:SparkContext.md#stop[ SparkContext stops] (on the driver) and executor:Executor.md#stop[ Executor stops]. == [[set]] set Method","title":"stop(): Unit"},{"location":"SparkEnv/#source-scala_4","text":"","title":"[source, scala]"},{"location":"SparkEnv/#sete-sparkenv-unit","text":"set saves the input SparkEnv to < > internal registry (as the default SparkEnv). NOTE: set is used when...FIXME == [[environmentDetails]] environmentDetails Utility","title":"set(e: SparkEnv): Unit"},{"location":"SparkEnv/#source-scala_5","text":"environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] environmentDetails...FIXME environmentDetails is used when SparkContext is requested to ROOT:SparkContext.md#postEnvironmentUpdate[post a SparkListenerEnvironmentUpdate event]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.SparkEnv logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"SparkEnv/#source","text":"","title":"[source]"},{"location":"SparkEnv/#log4jloggerorgapachesparksparkenvall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isStopped | [[isStopped]] Used to mark SparkEnv stopped Default: false | driverTmpDir | [[driverTmpDir]] |===","title":"log4j.logger.org.apache.spark.SparkEnv=ALL"},{"location":"SparkHadoopWriter/","text":"= SparkHadoopWriter SparkHadoopWriter utility is used to < >. SparkHadoopWriter utility is used by rdd:PairRDDFunctions.md#saveAsNewAPIHadoopDataset[saveAsNewAPIHadoopDataset] and rdd:PairRDDFunctions.md#saveAsHadoopDataset[saveAsHadoopDataset] transformations. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.internal.io.SparkHadoopWriter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.internal.io.SparkHadoopWriter=ALL Refer to < >. \u00b6 == [[write]] Writing Key-Value RDD Out (As Hadoop OutputFormat) -- write Utility [source, scala] \u00b6 write K, V: ClassTag : Unit [[write-commitJobId]] write uses the id of the given RDD as the commitJobId . [[write-jobTrackerId]] write creates a jobTrackerId with the current date. [[write-jobContext]] write requests the given HadoopWriteConfigUtil to < > (for the < > and < >). write requests the given HadoopWriteConfigUtil to < > with the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/JobContext.html[JobContext ]. write requests the given HadoopWriteConfigUtil to < >. write requests the given HadoopWriteConfigUtil to < > for the < >. write requests the HadoopMapReduceCommitProtocol to < > (with the < >). [[write-runJob]][[write-executeTask]] write uses the SparkContext (of the given RDD) to ROOT:SparkContext.md#runJob[run a Spark job asynchronously] for the given RDD with the < > partition function. [[write-commitJob]] In the end, write requests the < > to < > and prints out the following INFO message to the logs: Job [getJobID] committed. NOTE: write is used when PairRDDFunctions is requested to rdd:PairRDDFunctions.md#saveAsNewAPIHadoopDataset[saveAsNewAPIHadoopDataset] and rdd:PairRDDFunctions.md#saveAsHadoopDataset[saveAsHadoopDataset]. === [[write-Throwable]] write Utility And Throwables In case of any Throwable , write prints out the following ERROR message to the logs: Aborting job [getJobID]. [[write-abortJob]] write requests the < > to < > and throws a SparkException : Job aborted. == [[executeTask]] Writing RDD Partition -- executeTask Internal Utility [source, scala] \u00b6 executeTask K, V: ClassTag : TaskCommitMessage executeTask requests the given HadoopWriteConfigUtil to < >. executeTask requests the given FileCommitProtocol to < > with the TaskAttemptContext . executeTask requests the given HadoopWriteConfigUtil to < > (with the TaskAttemptContext and the given sparkPartitionId ). executeTask < >. executeTask writes all rows of the RDD partition (from the given Iterator[(K, V)] ). executeTask requests the given HadoopWriteConfigUtil to < >. In the end, executeTask requests the given HadoopWriteConfigUtil to < > and the given FileCommitProtocol to < >. executeTask updates metrics about writing data to external systems ( bytesWritten and recordsWritten ) every few records and at the end. In case of any errors, executeTask requests the given HadoopWriteConfigUtil to < > and the given FileCommitProtocol to < >. In the end, executeTask prints out the following ERROR message to the logs: Task [taskAttemptID] aborted. NOTE: executeTask is used when SparkHadoopWriter utility is used to < >. == [[initHadoopOutputMetrics]] initHadoopOutputMetrics Utility [source, scala] \u00b6 initHadoopOutputMetrics( context: TaskContext): (OutputMetrics, () => Long) initHadoopOutputMetrics ...FIXME NOTE: initHadoopOutputMetrics is used when SparkHadoopWriter utility is used to < >.","title":"SparkHadoopWriter"},{"location":"SparkHadoopWriter/#refer-to","text":"== [[write]] Writing Key-Value RDD Out (As Hadoop OutputFormat) -- write Utility","title":"Refer to &lt;&gt;."},{"location":"SparkHadoopWriter/#source-scala","text":"write K, V: ClassTag : Unit [[write-commitJobId]] write uses the id of the given RDD as the commitJobId . [[write-jobTrackerId]] write creates a jobTrackerId with the current date. [[write-jobContext]] write requests the given HadoopWriteConfigUtil to < > (for the < > and < >). write requests the given HadoopWriteConfigUtil to < > with the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/JobContext.html[JobContext ]. write requests the given HadoopWriteConfigUtil to < >. write requests the given HadoopWriteConfigUtil to < > for the < >. write requests the HadoopMapReduceCommitProtocol to < > (with the < >). [[write-runJob]][[write-executeTask]] write uses the SparkContext (of the given RDD) to ROOT:SparkContext.md#runJob[run a Spark job asynchronously] for the given RDD with the < > partition function. [[write-commitJob]] In the end, write requests the < > to < > and prints out the following INFO message to the logs: Job [getJobID] committed. NOTE: write is used when PairRDDFunctions is requested to rdd:PairRDDFunctions.md#saveAsNewAPIHadoopDataset[saveAsNewAPIHadoopDataset] and rdd:PairRDDFunctions.md#saveAsHadoopDataset[saveAsHadoopDataset]. === [[write-Throwable]] write Utility And Throwables In case of any Throwable , write prints out the following ERROR message to the logs: Aborting job [getJobID]. [[write-abortJob]] write requests the < > to < > and throws a SparkException : Job aborted. == [[executeTask]] Writing RDD Partition -- executeTask Internal Utility","title":"[source, scala]"},{"location":"SparkHadoopWriter/#source-scala_1","text":"executeTask K, V: ClassTag : TaskCommitMessage executeTask requests the given HadoopWriteConfigUtil to < >. executeTask requests the given FileCommitProtocol to < > with the TaskAttemptContext . executeTask requests the given HadoopWriteConfigUtil to < > (with the TaskAttemptContext and the given sparkPartitionId ). executeTask < >. executeTask writes all rows of the RDD partition (from the given Iterator[(K, V)] ). executeTask requests the given HadoopWriteConfigUtil to < >. In the end, executeTask requests the given HadoopWriteConfigUtil to < > and the given FileCommitProtocol to < >. executeTask updates metrics about writing data to external systems ( bytesWritten and recordsWritten ) every few records and at the end. In case of any errors, executeTask requests the given HadoopWriteConfigUtil to < > and the given FileCommitProtocol to < >. In the end, executeTask prints out the following ERROR message to the logs: Task [taskAttemptID] aborted. NOTE: executeTask is used when SparkHadoopWriter utility is used to < >. == [[initHadoopOutputMetrics]] initHadoopOutputMetrics Utility","title":"[source, scala]"},{"location":"SparkHadoopWriter/#source-scala_2","text":"initHadoopOutputMetrics( context: TaskContext): (OutputMetrics, () => Long) initHadoopOutputMetrics ...FIXME NOTE: initHadoopOutputMetrics is used when SparkHadoopWriter utility is used to < >.","title":"[source, scala]"},{"location":"SparkListener/","text":"= SparkListener SparkListener is a mechanism in Spark to intercept events from the Spark scheduler that are emitted over the course of execution of a Spark application. SparkListener extends < > with all the callback methods being no-op/do-nothing. Spark < SparkListeners internally>> to manage communication between internal components in the distributed environment for a Spark application, e.g. spark-webui.md[web UI], spark-history-server:EventLoggingListener.md[event persistence] (for History Server), spark-ExecutorAllocationManager.md[dynamic allocation of executors], spark-HeartbeatReceiver.md[keeping track of executors (using HeartbeatReceiver )] and others. You can develop your own custom SparkListener and register it using ROOT:SparkContext.md#addSparkListener[SparkContext.addSparkListener] method or ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property. With SparkListener you can focus on Spark events of your liking and process a subset of all scheduling events. [TIP] \u00b6 Enable INFO logging level for org.apache.spark.SparkContext logger to see when custom Spark listeners are registered. INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener See ROOT:SparkContext.md[]. \u00b6 == [[SparkListenerInterface]] SparkListenerInterface -- Internal Contract for Spark Listeners SparkListenerInterface is an private[spark] contract for Spark listeners to intercept events from the Spark scheduler. NOTE: < > and < > Spark listeners are direct implementations of SparkListenerInterface contract to help developing more sophisticated Spark listeners. .SparkListenerInterface Methods [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Method | Event | Reason | onApplicationEnd | [[SparkListenerApplicationEnd]] SparkListenerApplicationEnd | SparkContext does postApplicationEnd | [[onApplicationStart]] onApplicationStart | [[SparkListenerApplicationStart]] SparkListenerApplicationStart | SparkContext does postApplicationStart | [[onBlockManagerAdded]] onBlockManagerAdded | [[SparkListenerBlockManagerAdded]] SparkListenerBlockManagerAdded | BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#register[has registered a BlockManager ]. | [[onBlockManagerRemoved]] onBlockManagerRemoved | [[SparkListenerBlockManagerRemoved]] SparkListenerBlockManagerRemoved | BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#removeBlockManager[has removed a BlockManager ] (which is when...FIXME) | [[onBlockUpdated]] onBlockUpdated | [[SparkListenerBlockUpdated]] SparkListenerBlockUpdated | BlockManagerMasterEndpoint receives a storage:BlockManagerMasterEndpoint.md#UpdateBlockInfo[UpdateBlockInfo] event (which is when BlockManager storage:BlockManager.md#tryToReportBlockStatus[reports a block status update to driver]). | onEnvironmentUpdate | [[SparkListenerEnvironmentUpdate]] SparkListenerEnvironmentUpdate | SparkContext does postEnvironmentUpdate . | onExecutorMetricsUpdate | [[SparkListenerExecutorMetricsUpdate]] SparkListenerExecutorMetricsUpdate | | onExecutorAdded | [[SparkListenerExecutorAdded]] SparkListenerExecutorAdded | [[onExecutorAdded]] DriverEndpoint RPC endpoint (of CoarseGrainedSchedulerBackend ) scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RegisterExecutor[receives RegisterExecutor message], MesosFineGrainedSchedulerBackend does resourceOffers , and LocalSchedulerBackendEndpoint starts. | [[onExecutorBlacklisted]] onExecutorBlacklisted | [[SparkListenerExecutorBlacklisted]] SparkListenerExecutorBlacklisted | FIXME | [[onExecutorRemoved]] onExecutorRemoved | [[SparkListenerExecutorRemoved]] SparkListenerExecutorRemoved | DriverEndpoint RPC endpoint (of CoarseGrainedSchedulerBackend ) does scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#removeExecutor[removeExecutor] and MesosFineGrainedSchedulerBackend does removeExecutor . | [[onExecutorUnblacklisted]] onExecutorUnblacklisted | [[SparkListenerExecutorUnblacklisted]] SparkListenerExecutorUnblacklisted | FIXME | onJobEnd | [[SparkListenerJobEnd]] SparkListenerJobEnd | DAGScheduler does cleanUpAfterSchedulerStop , handleTaskCompletion , failJobAndIndependentStages , and markMapStageJobAsFinished. | [[onJobStart]] onJobStart | [[SparkListenerJobStart]] SparkListenerJobStart | DAGScheduler handles scheduler:DAGSchedulerEventProcessLoop.md#handleJobSubmitted[JobSubmitted] and scheduler:DAGSchedulerEventProcessLoop.md#handleMapStageSubmitted[MapStageSubmitted] messages | [[onNodeBlacklisted]] onNodeBlacklisted | [[SparkListenerNodeBlacklisted]] SparkListenerNodeBlacklisted | FIXME | [[onNodeUnblacklisted]] onNodeUnblacklisted | [[SparkListenerNodeUnblacklisted]] SparkListenerNodeUnblacklisted | FIXME | [[onStageCompleted]] onStageCompleted | [[SparkListenerStageCompleted]] SparkListenerStageCompleted | DAGScheduler scheduler:DAGScheduler.md#markStageAsFinished[marks a stage as finished]. | [[onStageSubmitted]] onStageSubmitted | [[SparkListenerStageSubmitted]] SparkListenerStageSubmitted | DAGScheduler scheduler:DAGScheduler.md#submitMissingTasks[submits the missing tasks of a stage (in a Spark job)]. | [[onTaskEnd]] onTaskEnd | [[SparkListenerTaskEnd]] SparkListenerTaskEnd | DAGScheduler scheduler:DAGScheduler.md#handleTaskCompletion[handles a task completion] | onTaskGettingResult | [[SparkListenerTaskGettingResult]] SparkListenerTaskGettingResult | DAGScheduler scheduler:DAGSchedulerEventProcessLoop.md#handleGetTaskResult[handles GettingResultEvent event] | [[onTaskStart]] onTaskStart | [[SparkListenerTaskStart]] SparkListenerTaskStart | DAGScheduler is informed that a scheduler:DAGSchedulerEventProcessLoop.md#handleBeginEvent[task is about to start]. | [[onUnpersistRDD]] onUnpersistRDD | [[SparkListenerUnpersistRDD]] SparkListenerUnpersistRDD | SparkContext ROOT:SparkContext.md#unpersistRDD[unpersists an RDD], i.e. removes RDD blocks from BlockManagerMaster (that can be triggered ROOT:SparkContext.md#unpersist[explicitly] or core:ContextCleaner.md#doCleanupRDD[implicitly]). | [[onOtherEvent]] onOtherEvent | [[SparkListenerEvent]] SparkListenerEvent | Catch-all callback that is often used in Spark SQL to handle custom events. |=== == [[builtin-implementations]] Built-In Spark Listeners .Built-In Spark Listeners [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Spark Listener | Description | spark-history-server:EventLoggingListener.md[EventLoggingListener] | Logs JSON-encoded events to a file that can later be read by spark-history-server:index.md[History Server] | spark-SparkListener-StatsReportListener.md[StatsReportListener] | | [[SparkFirehoseListener]] SparkFirehoseListener | Allows users to receive all < > events by overriding the single onEvent method only. | spark-SparkListener-ExecutorAllocationListener.md[ExecutorAllocationListener] | | spark-HeartbeatReceiver.md[HeartbeatReceiver] | | spark-streaming/spark-streaming-streaminglisteners.md#StreamingJobProgressListener[StreamingJobProgressListener] | | spark-webui-executors-ExecutorsListener.md[ExecutorsListener] | Prepares information for spark-webui-executors.md[Executors tab] in spark-webui.md[web UI] | spark-webui-StorageStatusListener.md[StorageStatusListener], spark-webui-RDDOperationGraphListener.md[RDDOperationGraphListener], spark-webui-EnvironmentListener.md[EnvironmentListener], spark-webui-BlockStatusListener.md[BlockStatusListener] and spark-webui-StorageListener.md[StorageListener] | For spark-webui.md[web UI] | SpillListener | | ApplicationEventListener | | spark-sql-streaming-StreamingQueryListenerBus.md[StreamingQueryListenerBus] | | spark-sql-SQLListener.md[SQLListener] / spark-history-server:SQLHistoryListener.md[SQLHistoryListener] | Support for spark-history-server:index.md[History Server] | spark-streaming/spark-streaming-jobscheduler.md#StreamingListenerBus[StreamingListenerBus] | | spark-webui-JobProgressListener.md[JobProgressListener] | |===","title":"SparkListener"},{"location":"SparkListener/#tip","text":"Enable INFO logging level for org.apache.spark.SparkContext logger to see when custom Spark listeners are registered. INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener","title":"[TIP]"},{"location":"SparkListener/#see-rootsparkcontextmd","text":"== [[SparkListenerInterface]] SparkListenerInterface -- Internal Contract for Spark Listeners SparkListenerInterface is an private[spark] contract for Spark listeners to intercept events from the Spark scheduler. NOTE: < > and < > Spark listeners are direct implementations of SparkListenerInterface contract to help developing more sophisticated Spark listeners. .SparkListenerInterface Methods [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Method | Event | Reason | onApplicationEnd | [[SparkListenerApplicationEnd]] SparkListenerApplicationEnd | SparkContext does postApplicationEnd | [[onApplicationStart]] onApplicationStart | [[SparkListenerApplicationStart]] SparkListenerApplicationStart | SparkContext does postApplicationStart | [[onBlockManagerAdded]] onBlockManagerAdded | [[SparkListenerBlockManagerAdded]] SparkListenerBlockManagerAdded | BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#register[has registered a BlockManager ]. | [[onBlockManagerRemoved]] onBlockManagerRemoved | [[SparkListenerBlockManagerRemoved]] SparkListenerBlockManagerRemoved | BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#removeBlockManager[has removed a BlockManager ] (which is when...FIXME) | [[onBlockUpdated]] onBlockUpdated | [[SparkListenerBlockUpdated]] SparkListenerBlockUpdated | BlockManagerMasterEndpoint receives a storage:BlockManagerMasterEndpoint.md#UpdateBlockInfo[UpdateBlockInfo] event (which is when BlockManager storage:BlockManager.md#tryToReportBlockStatus[reports a block status update to driver]). | onEnvironmentUpdate | [[SparkListenerEnvironmentUpdate]] SparkListenerEnvironmentUpdate | SparkContext does postEnvironmentUpdate . | onExecutorMetricsUpdate | [[SparkListenerExecutorMetricsUpdate]] SparkListenerExecutorMetricsUpdate | | onExecutorAdded | [[SparkListenerExecutorAdded]] SparkListenerExecutorAdded | [[onExecutorAdded]] DriverEndpoint RPC endpoint (of CoarseGrainedSchedulerBackend ) scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RegisterExecutor[receives RegisterExecutor message], MesosFineGrainedSchedulerBackend does resourceOffers , and LocalSchedulerBackendEndpoint starts. | [[onExecutorBlacklisted]] onExecutorBlacklisted | [[SparkListenerExecutorBlacklisted]] SparkListenerExecutorBlacklisted | FIXME | [[onExecutorRemoved]] onExecutorRemoved | [[SparkListenerExecutorRemoved]] SparkListenerExecutorRemoved | DriverEndpoint RPC endpoint (of CoarseGrainedSchedulerBackend ) does scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#removeExecutor[removeExecutor] and MesosFineGrainedSchedulerBackend does removeExecutor . | [[onExecutorUnblacklisted]] onExecutorUnblacklisted | [[SparkListenerExecutorUnblacklisted]] SparkListenerExecutorUnblacklisted | FIXME | onJobEnd | [[SparkListenerJobEnd]] SparkListenerJobEnd | DAGScheduler does cleanUpAfterSchedulerStop , handleTaskCompletion , failJobAndIndependentStages , and markMapStageJobAsFinished. | [[onJobStart]] onJobStart | [[SparkListenerJobStart]] SparkListenerJobStart | DAGScheduler handles scheduler:DAGSchedulerEventProcessLoop.md#handleJobSubmitted[JobSubmitted] and scheduler:DAGSchedulerEventProcessLoop.md#handleMapStageSubmitted[MapStageSubmitted] messages | [[onNodeBlacklisted]] onNodeBlacklisted | [[SparkListenerNodeBlacklisted]] SparkListenerNodeBlacklisted | FIXME | [[onNodeUnblacklisted]] onNodeUnblacklisted | [[SparkListenerNodeUnblacklisted]] SparkListenerNodeUnblacklisted | FIXME | [[onStageCompleted]] onStageCompleted | [[SparkListenerStageCompleted]] SparkListenerStageCompleted | DAGScheduler scheduler:DAGScheduler.md#markStageAsFinished[marks a stage as finished]. | [[onStageSubmitted]] onStageSubmitted | [[SparkListenerStageSubmitted]] SparkListenerStageSubmitted | DAGScheduler scheduler:DAGScheduler.md#submitMissingTasks[submits the missing tasks of a stage (in a Spark job)]. | [[onTaskEnd]] onTaskEnd | [[SparkListenerTaskEnd]] SparkListenerTaskEnd | DAGScheduler scheduler:DAGScheduler.md#handleTaskCompletion[handles a task completion] | onTaskGettingResult | [[SparkListenerTaskGettingResult]] SparkListenerTaskGettingResult | DAGScheduler scheduler:DAGSchedulerEventProcessLoop.md#handleGetTaskResult[handles GettingResultEvent event] | [[onTaskStart]] onTaskStart | [[SparkListenerTaskStart]] SparkListenerTaskStart | DAGScheduler is informed that a scheduler:DAGSchedulerEventProcessLoop.md#handleBeginEvent[task is about to start]. | [[onUnpersistRDD]] onUnpersistRDD | [[SparkListenerUnpersistRDD]] SparkListenerUnpersistRDD | SparkContext ROOT:SparkContext.md#unpersistRDD[unpersists an RDD], i.e. removes RDD blocks from BlockManagerMaster (that can be triggered ROOT:SparkContext.md#unpersist[explicitly] or core:ContextCleaner.md#doCleanupRDD[implicitly]). | [[onOtherEvent]] onOtherEvent | [[SparkListenerEvent]] SparkListenerEvent | Catch-all callback that is often used in Spark SQL to handle custom events. |=== == [[builtin-implementations]] Built-In Spark Listeners .Built-In Spark Listeners [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Spark Listener | Description | spark-history-server:EventLoggingListener.md[EventLoggingListener] | Logs JSON-encoded events to a file that can later be read by spark-history-server:index.md[History Server] | spark-SparkListener-StatsReportListener.md[StatsReportListener] | | [[SparkFirehoseListener]] SparkFirehoseListener | Allows users to receive all < > events by overriding the single onEvent method only. | spark-SparkListener-ExecutorAllocationListener.md[ExecutorAllocationListener] | | spark-HeartbeatReceiver.md[HeartbeatReceiver] | | spark-streaming/spark-streaming-streaminglisteners.md#StreamingJobProgressListener[StreamingJobProgressListener] | | spark-webui-executors-ExecutorsListener.md[ExecutorsListener] | Prepares information for spark-webui-executors.md[Executors tab] in spark-webui.md[web UI] | spark-webui-StorageStatusListener.md[StorageStatusListener], spark-webui-RDDOperationGraphListener.md[RDDOperationGraphListener], spark-webui-EnvironmentListener.md[EnvironmentListener], spark-webui-BlockStatusListener.md[BlockStatusListener] and spark-webui-StorageListener.md[StorageListener] | For spark-webui.md[web UI] | SpillListener | | ApplicationEventListener | | spark-sql-streaming-StreamingQueryListenerBus.md[StreamingQueryListenerBus] | | spark-sql-SQLListener.md[SQLListener] / spark-history-server:SQLHistoryListener.md[SQLHistoryListener] | Support for spark-history-server:index.md[History Server] | spark-streaming/spark-streaming-jobscheduler.md#StreamingListenerBus[StreamingListenerBus] | | spark-webui-JobProgressListener.md[JobProgressListener] | |===","title":"See ROOT:SparkContext.md[]."},{"location":"SparkListenerBus/","text":"SparkListenerBus \u00b6 SparkListenerBus is a private[spark] < > for ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] listeners that process ROOT:SparkListener.md#SparkListenerEvent[SparkListenerEvent] events. SparkListenerBus comes with a custom doPostEvent method that simply relays SparkListenerEvent events to appropriate SparkListenerInterface methods. NOTE: There are two custom SparkListenerBus listeners: scheduler:LiveListenerBus.md[] and spark-history-server:ReplayListenerBus.md[]. .SparkListenerEvent to SparkListenerInterface's Method \"mapping\" [width=\"100%\",options=\"header\"] |=== |SparkListenerEvent |SparkListenerInterface's Method | SparkListenerStageSubmitted | onStageSubmitted | SparkListenerStageCompleted | onStageCompleted | SparkListenerJobStart | onJobStart | SparkListenerJobEnd | onJobEnd | SparkListenerJobEnd | onJobEnd | SparkListenerTaskStart | onTaskStart | SparkListenerTaskGettingResult | onTaskGettingResult | ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] | onTaskEnd | SparkListenerEnvironmentUpdate | onEnvironmentUpdate | SparkListenerBlockManagerAdded | onBlockManagerAdded | SparkListenerBlockManagerRemoved | onBlockManagerRemoved | SparkListenerUnpersistRDD | onUnpersistRDD | SparkListenerApplicationStart | onApplicationStart | SparkListenerApplicationEnd | onApplicationEnd | SparkListenerExecutorMetricsUpdate | onExecutorMetricsUpdate | SparkListenerExecutorAdded | onExecutorAdded | SparkListenerExecutorRemoved | onExecutorRemoved | SparkListenerBlockUpdated | onBlockUpdated | SparkListenerLogStart | event ignored | other event types | onOtherEvent |=== == [[ListenerBus]][[ListenerBus-addListener]][[ListenerBus-doPostEvent]] ListenerBus Event Bus Contract [source, scala] \u00b6 ListenerBus[L <: AnyRef, E] \u00b6 ListenerBus is an event bus that post events (of type E ) to all registered listeners (of type L ). It manages listeners of type L , i.e. it can add to and remove listeners from an internal listeners collection. [source, scala] \u00b6 addListener(listener: L): Unit removeListener(listener: L): Unit It can post events of type E to all registered listeners (using postToAll method). It simply iterates over the internal listeners collection and executes the abstract doPostEvent method. [source, scala] \u00b6 doPostEvent(listener: L, event: E): Unit \u00b6 NOTE: doPostEvent is provided by more specialized ListenerBus event buses. In case of exception while posting an event to a listener you should see the following ERROR message in the logs and the exception. ERROR Listener [listener] threw an exception NOTE: There are three custom ListenerBus listeners: < >, spark-sql-streaming-StreamingQueryListenerBus.md[StreamingQueryListenerBus], and spark-streaming/spark-streaming-jobscheduler.md#StreamingListenerBus[StreamingListenerBus]. [TIP] \u00b6 Enable ERROR logging level for org.apache.spark.util.ListenerBus logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ListenerBus=ERROR Refer to spark-logging.md[Logging]. \u00b6","title":"SparkListenerBus"},{"location":"SparkListenerBus/#sparklistenerbus","text":"SparkListenerBus is a private[spark] < > for ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface] listeners that process ROOT:SparkListener.md#SparkListenerEvent[SparkListenerEvent] events. SparkListenerBus comes with a custom doPostEvent method that simply relays SparkListenerEvent events to appropriate SparkListenerInterface methods. NOTE: There are two custom SparkListenerBus listeners: scheduler:LiveListenerBus.md[] and spark-history-server:ReplayListenerBus.md[]. .SparkListenerEvent to SparkListenerInterface's Method \"mapping\" [width=\"100%\",options=\"header\"] |=== |SparkListenerEvent |SparkListenerInterface's Method | SparkListenerStageSubmitted | onStageSubmitted | SparkListenerStageCompleted | onStageCompleted | SparkListenerJobStart | onJobStart | SparkListenerJobEnd | onJobEnd | SparkListenerJobEnd | onJobEnd | SparkListenerTaskStart | onTaskStart | SparkListenerTaskGettingResult | onTaskGettingResult | ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] | onTaskEnd | SparkListenerEnvironmentUpdate | onEnvironmentUpdate | SparkListenerBlockManagerAdded | onBlockManagerAdded | SparkListenerBlockManagerRemoved | onBlockManagerRemoved | SparkListenerUnpersistRDD | onUnpersistRDD | SparkListenerApplicationStart | onApplicationStart | SparkListenerApplicationEnd | onApplicationEnd | SparkListenerExecutorMetricsUpdate | onExecutorMetricsUpdate | SparkListenerExecutorAdded | onExecutorAdded | SparkListenerExecutorRemoved | onExecutorRemoved | SparkListenerBlockUpdated | onBlockUpdated | SparkListenerLogStart | event ignored | other event types | onOtherEvent |=== == [[ListenerBus]][[ListenerBus-addListener]][[ListenerBus-doPostEvent]] ListenerBus Event Bus Contract","title":"SparkListenerBus"},{"location":"SparkListenerBus/#source-scala","text":"","title":"[source, scala]"},{"location":"SparkListenerBus/#listenerbusl-anyref-e","text":"ListenerBus is an event bus that post events (of type E ) to all registered listeners (of type L ). It manages listeners of type L , i.e. it can add to and remove listeners from an internal listeners collection.","title":"ListenerBus[L &lt;: AnyRef, E]"},{"location":"SparkListenerBus/#source-scala_1","text":"addListener(listener: L): Unit removeListener(listener: L): Unit It can post events of type E to all registered listeners (using postToAll method). It simply iterates over the internal listeners collection and executes the abstract doPostEvent method.","title":"[source, scala]"},{"location":"SparkListenerBus/#source-scala_2","text":"","title":"[source, scala]"},{"location":"SparkListenerBus/#doposteventlistener-l-event-e-unit","text":"NOTE: doPostEvent is provided by more specialized ListenerBus event buses. In case of exception while posting an event to a listener you should see the following ERROR message in the logs and the exception. ERROR Listener [listener] threw an exception NOTE: There are three custom ListenerBus listeners: < >, spark-sql-streaming-StreamingQueryListenerBus.md[StreamingQueryListenerBus], and spark-streaming/spark-streaming-jobscheduler.md#StreamingListenerBus[StreamingListenerBus].","title":"doPostEvent(listener: L, event: E): Unit"},{"location":"SparkListenerBus/#tip","text":"Enable ERROR logging level for org.apache.spark.util.ListenerBus logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.ListenerBus=ERROR","title":"[TIP]"},{"location":"SparkListenerBus/#refer-to-spark-loggingmdlogging","text":"","title":"Refer to spark-logging.md[Logging]."},{"location":"SparkListenerEvent/","text":"SparkListenerEvent \u00b6 SparkListenerEvent is...FIXME","title":"SparkListenerEvent"},{"location":"SparkListenerEvent/#sparklistenerevent","text":"SparkListenerEvent is...FIXME","title":"SparkListenerEvent"},{"location":"SpillListener/","text":"SpillListener \u00b6 SpillListener is a SparkListener that intercepts ( listens to ) the following events for detecting spills in jobs: onTaskEnd onStageCompleted SpillListener is used for testing only. Creating Instance \u00b6 SpillListener takes no input arguments to be created. SpillListener is created when TestUtils is requested to assertSpilled and assertNotSpilled . onTaskEnd Callback \u00b6 onTaskEnd ( taskEnd : SparkListenerTaskEnd ) : Unit onTaskEnd ...FIXME onTaskEnd is part of the SparkListener abstraction. onStageCompleted Callback \u00b6 onStageCompleted ( stageComplete : SparkListenerStageCompleted ) : Unit onStageCompleted ...FIXME onStageCompleted is part of the SparkListener abstraction.","title":"SpillListener"},{"location":"SpillListener/#spilllistener","text":"SpillListener is a SparkListener that intercepts ( listens to ) the following events for detecting spills in jobs: onTaskEnd onStageCompleted SpillListener is used for testing only.","title":"SpillListener"},{"location":"SpillListener/#creating-instance","text":"SpillListener takes no input arguments to be created. SpillListener is created when TestUtils is requested to assertSpilled and assertNotSpilled .","title":"Creating Instance"},{"location":"SpillListener/#ontaskend-callback","text":"onTaskEnd ( taskEnd : SparkListenerTaskEnd ) : Unit onTaskEnd ...FIXME onTaskEnd is part of the SparkListener abstraction.","title":" onTaskEnd Callback"},{"location":"SpillListener/#onstagecompleted-callback","text":"onStageCompleted ( stageComplete : SparkListenerStageCompleted ) : Unit onStageCompleted ...FIXME onStageCompleted is part of the SparkListener abstraction.","title":" onStageCompleted Callback"},{"location":"StatsReportListener/","text":"== [[StatsReportListener]] StatsReportListener -- Logging Summary Statistics org.apache.spark.scheduler.StatsReportListener (see https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.StatsReportListener[the listener's scaladoc]) is a ROOT:SparkListener.md[] that logs summary statistics when each stage completes. StatsReportListener listens to ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] and ROOT:SparkListener.md#SparkListenerStageCompleted[SparkListenerStageCompleted] events and prints them out at INFO logging level. [TIP] \u00b6 Enable INFO logging level for org.apache.spark.scheduler.StatsReportListener logger to see Spark events. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.StatsReportListener=INFO Refer to spark-logging.md[Logging]. \u00b6 === [[onStageCompleted]] Intercepting Stage Completed Events -- onStageCompleted Callback CAUTION: FIXME === [[example]] Example $ ./bin/spark-shell -c spark.extraListeners=org.apache.spark.scheduler.StatsReportListener ... INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener ... scala> spark.read.text(\"README.md\").count ... INFO StatsReportListener: Finished stage: Stage(0, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 212 msec INFO StatsReportListener: task runtime:(count: 1, mean: 198.000000, stdev: 0.000000, max: 198.000000, min: 198.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 59.000000, stdev: 0.000000, max: 59.000000, min: 59.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1885.000000, stdev: 0.000000, max: 1885.000000, min: 1885.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 73.737374, stdev: 0.000000, max: 73.737374, min: 73.737374) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 26.262626, stdev: 0.000000, max: 26.262626, min: 26.262626) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % INFO StatsReportListener: Finished stage: Stage(1, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 34 msec INFO StatsReportListener: task runtime:(count: 1, mean: 33.000000, stdev: 0.000000, max: 33.000000, min: 33.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1960.000000, stdev: 0.000000, max: 1960.000000, min: 1960.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 75.757576, stdev: 0.000000, max: 75.757576, min: 75.757576) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 24.242424, stdev: 0.000000, max: 24.242424, min: 24.242424) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % res0: Long = 99","title":"StatsReportListener"},{"location":"StatsReportListener/#tip","text":"Enable INFO logging level for org.apache.spark.scheduler.StatsReportListener logger to see Spark events. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.StatsReportListener=INFO","title":"[TIP]"},{"location":"StatsReportListener/#refer-to-spark-loggingmdlogging","text":"=== [[onStageCompleted]] Intercepting Stage Completed Events -- onStageCompleted Callback CAUTION: FIXME === [[example]] Example $ ./bin/spark-shell -c spark.extraListeners=org.apache.spark.scheduler.StatsReportListener ... INFO SparkContext: Registered listener org.apache.spark.scheduler.StatsReportListener ... scala> spark.read.text(\"README.md\").count ... INFO StatsReportListener: Finished stage: Stage(0, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 212 msec INFO StatsReportListener: task runtime:(count: 1, mean: 198.000000, stdev: 0.000000, max: 198.000000, min: 198.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms 198.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 59.000000, stdev: 0.000000, max: 59.000000, min: 59.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B 59.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1885.000000, stdev: 0.000000, max: 1885.000000, min: 1885.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B 1885.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 73.737374, stdev: 0.000000, max: 73.737374, min: 73.737374) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % 74 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 26.262626, stdev: 0.000000, max: 26.262626, min: 26.262626) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % 26 % INFO StatsReportListener: Finished stage: Stage(1, 0); Name: 'count at <console>:24'; Status: succeeded; numTasks: 1; Took: 34 msec INFO StatsReportListener: task runtime:(count: 1, mean: 33.000000, stdev: 0.000000, max: 33.000000, min: 33.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms 33.0 ms INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms 0.0 ms INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B 0.0 B INFO StatsReportListener: task result size:(count: 1, mean: 1960.000000, stdev: 0.000000, max: 1960.000000, min: 1960.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B 1960.0 B INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 75.757576, stdev: 0.000000, max: 75.757576, min: 75.757576) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % 76 % INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % 0 % INFO StatsReportListener: other time pct: (count: 1, mean: 24.242424, stdev: 0.000000, max: 24.242424, min: 24.242424) INFO StatsReportListener: 0% 5% 10% 25% 50% 75% 90% 95% 100% INFO StatsReportListener: 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % 24 % res0: Long = 99","title":"Refer to spark-logging.md[Logging]."},{"location":"Utils/","text":"== [[Utils]] Utils Helper Object Utils is a Scala object that...FIXME === [[getLocalDir]] getLocalDir Method [source, scala] \u00b6 getLocalDir(conf: SparkConf): String \u00b6 getLocalDir ...FIXME [NOTE] \u00b6 getLocalDir is used when: Utils is requested to < > SparkEnv is core:SparkEnv.md#create[created] (on the driver) spark-shell.md[spark-shell] is launched Spark on YARN's Client is requested to spark-yarn-client.md#prepareLocalResources[prepareLocalResources] and spark-yarn-client.md#createConfArchive[create ++ spark_conf .zip++ archive with configuration files and Spark configuration] PySpark's PythonBroadcast is requested to readObject * PySpark's EvalPythonExec is requested to doExecute \u00b6 === [[fetchFile]] fetchFile Method [source, scala] \u00b6 fetchFile( url: String, targetDir: File, conf: SparkConf, securityMgr: SecurityManager, hadoopConf: Configuration, timestamp: Long, useCache: Boolean): File fetchFile ...FIXME [NOTE] \u00b6 fetchFile is used when: SparkContext is requested to ROOT:SparkContext.md#addFile[addFile] Executor is requested to executor:Executor.md#updateDependencies[updateDependencies] * Spark Standalone's DriverRunner is requested to downloadUserJar \u00b6 === [[getOrCreateLocalRootDirsImpl]] getOrCreateLocalRootDirsImpl Internal Method [source, scala] \u00b6 getOrCreateLocalRootDirsImpl(conf: SparkConf): Array[String] \u00b6 getOrCreateLocalRootDirsImpl ...FIXME NOTE: getOrCreateLocalRootDirsImpl is used exclusively when Utils is requested to < > === [[getOrCreateLocalRootDirs]] getOrCreateLocalRootDirs Internal Method [source, scala] \u00b6 getOrCreateLocalRootDirs(conf: SparkConf): Array[String] \u00b6 getOrCreateLocalRootDirs ...FIXME [NOTE] \u00b6 getOrCreateLocalRootDirs is used when: Utils is requested to < > * Worker is requested to spark-standalone-worker.md#receive[handle a LaunchExecutor message] \u00b6","title":"Utils"},{"location":"Utils/#source-scala","text":"","title":"[source, scala]"},{"location":"Utils/#getlocaldirconf-sparkconf-string","text":"getLocalDir ...FIXME","title":"getLocalDir(conf: SparkConf): String"},{"location":"Utils/#note","text":"getLocalDir is used when: Utils is requested to < > SparkEnv is core:SparkEnv.md#create[created] (on the driver) spark-shell.md[spark-shell] is launched Spark on YARN's Client is requested to spark-yarn-client.md#prepareLocalResources[prepareLocalResources] and spark-yarn-client.md#createConfArchive[create ++ spark_conf .zip++ archive with configuration files and Spark configuration] PySpark's PythonBroadcast is requested to readObject","title":"[NOTE]"},{"location":"Utils/#pysparks-evalpythonexec-is-requested-to-doexecute","text":"=== [[fetchFile]] fetchFile Method","title":"* PySpark's  EvalPythonExec is requested to doExecute"},{"location":"Utils/#source-scala_1","text":"fetchFile( url: String, targetDir: File, conf: SparkConf, securityMgr: SecurityManager, hadoopConf: Configuration, timestamp: Long, useCache: Boolean): File fetchFile ...FIXME","title":"[source, scala]"},{"location":"Utils/#note_1","text":"fetchFile is used when: SparkContext is requested to ROOT:SparkContext.md#addFile[addFile] Executor is requested to executor:Executor.md#updateDependencies[updateDependencies]","title":"[NOTE]"},{"location":"Utils/#spark-standalones-driverrunner-is-requested-to-downloaduserjar","text":"=== [[getOrCreateLocalRootDirsImpl]] getOrCreateLocalRootDirsImpl Internal Method","title":"* Spark Standalone's DriverRunner is requested to downloadUserJar"},{"location":"Utils/#source-scala_2","text":"","title":"[source, scala]"},{"location":"Utils/#getorcreatelocalrootdirsimplconf-sparkconf-arraystring","text":"getOrCreateLocalRootDirsImpl ...FIXME NOTE: getOrCreateLocalRootDirsImpl is used exclusively when Utils is requested to < > === [[getOrCreateLocalRootDirs]] getOrCreateLocalRootDirs Internal Method","title":"getOrCreateLocalRootDirsImpl(conf: SparkConf): Array[String]"},{"location":"Utils/#source-scala_3","text":"","title":"[source, scala]"},{"location":"Utils/#getorcreatelocalrootdirsconf-sparkconf-arraystring","text":"getOrCreateLocalRootDirs ...FIXME","title":"getOrCreateLocalRootDirs(conf: SparkConf): Array[String]"},{"location":"Utils/#note_2","text":"getOrCreateLocalRootDirs is used when: Utils is requested to < >","title":"[NOTE]"},{"location":"Utils/#worker-is-requested-to-spark-standalone-workermdreceivehandle-a-launchexecutor-message","text":"","title":"* Worker is requested to spark-standalone-worker.md#receive[handle a LaunchExecutor message]"},{"location":"accumulators/","text":"== [[AccumulatorV2]] Accumulators Accumulators are variables that are \"added\" to through an associative and commutative \"add\" operation. They act as a container for accumulating partial values across multiple tasks (running on executors). They are designed to be used safely and efficiently in parallel and distributed Spark computations and are meant for distributed counters and sums (e.g. executor:TaskMetrics.md[task metrics]). You can create built-in accumulators for ROOT:SparkContext.md#creating-accumulators[longs, doubles, or collections] or register custom accumulators using the ROOT:SparkContext.md#register[SparkContext.register] methods. You can create accumulators with or without a name, but only < > are displayed in spark-webui-StagePage.md#accumulators[web UI] (under Stages tab for a given stage). .Accumulators in the Spark UI image::spark-webui-accumulators.png[align=\"center\"] Accumulator are write-only variables for executors. They can be added to by executors and read by the driver only. executor1: accumulator.add(incByExecutor1) executor2: accumulator.add(incByExecutor2) driver: println(accumulator.value) Accumulators are not thread-safe. They do not really have to since the scheduler:DAGScheduler.md#updateAccumulators[DAGScheduler.updateAccumulators] method that the driver uses to update the values of accumulators after a task completes (successfully or with a failure) is only executed on a scheduler:DAGScheduler.md#eventProcessLoop[single thread that runs scheduling loop]. Beside that, they are write-only data structures for workers that have their own local accumulator reference whereas accessing the value of an accumulator is only allowed by the driver. Accumulators are serializable so they can safely be referenced in the code executed in executors and then safely send over the wire for execution. [source, scala] \u00b6 val counter = sc.longAccumulator(\"counter\") sc.parallelize(1 to 9).foreach(x => counter.add(x)) Internally, ROOT:SparkContext.md#longAccumulator[longAccumulator], ROOT:SparkContext.md#doubleAccumulator[doubleAccumulator], and ROOT:SparkContext.md#collectionAccumulator[collectionAccumulator] methods create the built-in typed accumulators and call ROOT:SparkContext.md#register[SparkContext.register]. TIP: Read the official documentation about http://spark.apache.org/docs/latest/programming-guide.html#accumulators[Accumulators ]. [[internal-registries]] .AccumulatorV2's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[metadata]] metadata | < > Used when...FIXME | [[atDriverSide]] atDriverSide | Flag whether...FIXME Used when...FIXME |=== === [[merge]] merge Method CAUTION: FIXME === [[AccumulatorV2]] AccumulatorV2 [source, scala] \u00b6 abstract class AccumulatorV2[IN, OUT] \u00b6 AccumulatorV2 parameterized class represents an accumulator that accumulates IN values to produce OUT result. ==== [[register]] Registering Accumulator -- register Method [source, scala] \u00b6 register( sc: SparkContext, name: Option[String] = None, countFailedValues: Boolean = false): Unit register creates a < > metadata object for the accumulator (with a spark-AccumulatorContext.md#newId[new unique identifier]) that is then used to spark-AccumulatorContext.md#register[register the accumulator with]. In the end, register core:ContextCleaner.md#registerAccumulatorForCleanup[registers the accumulator for cleanup] (only when ROOT:SparkContext.md#cleaner[ ContextCleaner is defined in the SparkContext ]). register reports a IllegalStateException if < > is already defined (which means that register was called already). Cannot register an Accumulator twice. NOTE: register is a private[spark] method. [NOTE] \u00b6 register is used when: SparkContext ROOT:SparkContext.md#register[registers accumulators] TaskMetrics executor:TaskMetrics.md#register[registers the internal accumulators] spark-sql-SQLMetric.md[SQLMetrics] creates metrics. \u00b6 === [[AccumulatorMetadata]] AccumulatorMetadata AccumulatorMetadata is a container object with the metadata of an accumulator: [[id]] Accumulator ID [[name]] (optional) name [[countFailedValues]] Flag whether to include the latest value of an accumulator on failure NOTE: countFailedValues is used exclusively when scheduler:Task.md#collectAccumulatorUpdates[ Task collects the latest values of accumulators] (irrespective of task status -- a success or a failure). === [[named]] Named Accumulators An accumulator can have an optional name that you can specify when ROOT:SparkContext.md#creating-accumulators[creating an accumulator]. [source, scala] \u00b6 val counter = sc.longAccumulator(\"counter\") \u00b6 === [[AccumulableInfo]] AccumulableInfo AccumulableInfo contains information about a task's local updates to an < >. id of the accumulator optional name of the accumulator optional partial update to the accumulator from a task value whether or not it is internal whether or not to countFailedValues to the final value of the accumulator for failed tasks optional metadata AccumulableInfo is used to transfer accumulator updates from executors to the driver every executor heartbeat or when a task finishes. Create an [[AccumulableInfo]] representation of this [[Accumulable]] with the provided values. === When are Accumulators Updated? === [[examples]] Examples ==== [[example-distributed-counter]] Example: Distributed Counter Imagine you are requested to write a distributed counter. What do you think about the following solutions? What are the pros and cons of using it? [source, scala] \u00b6 val ints = sc.parallelize(0 to 9, 3) var counter = 0 ints.foreach { n => println(s\"int: $n\") counter = counter + 1 } println(s\"The number of elements is $counter\") How would you go about doing the calculation using accumulators? ==== [[example1]] Example: Using Accumulators in Transformations and Guarantee Exactly-Once Update CAUTION: FIXME Code with failing transformations (tasks) that update accumulator ( Map ) with TaskContext info. ==== [[example2]] Example: Custom Accumulator CAUTION: FIXME Improve the earlier example ==== [[example3]] Example: Distributed Stopwatch NOTE: This is almost a raw copy of org.apache.spark.ml.util.DistributedStopwatch. [source, scala] \u00b6 class DistributedStopwatch(sc: SparkContext, val name: String) { val elapsedTime: Accumulator[Long] = sc.accumulator(0L, s\"DistributedStopwatch($name)\") override def elapsed(): Long = elapsedTime.value override protected def add(duration: Long): Unit = { elapsedTime += duration } } === [[i-want-more]] Further reading or watching http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf[Performance and Scalability of Broadcast in Spark]","title":"accumulators"},{"location":"accumulators/#source-scala","text":"val counter = sc.longAccumulator(\"counter\") sc.parallelize(1 to 9).foreach(x => counter.add(x)) Internally, ROOT:SparkContext.md#longAccumulator[longAccumulator], ROOT:SparkContext.md#doubleAccumulator[doubleAccumulator], and ROOT:SparkContext.md#collectionAccumulator[collectionAccumulator] methods create the built-in typed accumulators and call ROOT:SparkContext.md#register[SparkContext.register]. TIP: Read the official documentation about http://spark.apache.org/docs/latest/programming-guide.html#accumulators[Accumulators ]. [[internal-registries]] .AccumulatorV2's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[metadata]] metadata | < > Used when...FIXME | [[atDriverSide]] atDriverSide | Flag whether...FIXME Used when...FIXME |=== === [[merge]] merge Method CAUTION: FIXME === [[AccumulatorV2]] AccumulatorV2","title":"[source, scala]"},{"location":"accumulators/#source-scala_1","text":"","title":"[source, scala]"},{"location":"accumulators/#abstract-class-accumulatorv2in-out","text":"AccumulatorV2 parameterized class represents an accumulator that accumulates IN values to produce OUT result. ==== [[register]] Registering Accumulator -- register Method","title":"abstract class AccumulatorV2[IN, OUT]"},{"location":"accumulators/#source-scala_2","text":"register( sc: SparkContext, name: Option[String] = None, countFailedValues: Boolean = false): Unit register creates a < > metadata object for the accumulator (with a spark-AccumulatorContext.md#newId[new unique identifier]) that is then used to spark-AccumulatorContext.md#register[register the accumulator with]. In the end, register core:ContextCleaner.md#registerAccumulatorForCleanup[registers the accumulator for cleanup] (only when ROOT:SparkContext.md#cleaner[ ContextCleaner is defined in the SparkContext ]). register reports a IllegalStateException if < > is already defined (which means that register was called already). Cannot register an Accumulator twice. NOTE: register is a private[spark] method.","title":"[source, scala]"},{"location":"accumulators/#note","text":"register is used when: SparkContext ROOT:SparkContext.md#register[registers accumulators] TaskMetrics executor:TaskMetrics.md#register[registers the internal accumulators]","title":"[NOTE]"},{"location":"accumulators/#spark-sql-sqlmetricmdsqlmetrics-creates-metrics","text":"=== [[AccumulatorMetadata]] AccumulatorMetadata AccumulatorMetadata is a container object with the metadata of an accumulator: [[id]] Accumulator ID [[name]] (optional) name [[countFailedValues]] Flag whether to include the latest value of an accumulator on failure NOTE: countFailedValues is used exclusively when scheduler:Task.md#collectAccumulatorUpdates[ Task collects the latest values of accumulators] (irrespective of task status -- a success or a failure). === [[named]] Named Accumulators An accumulator can have an optional name that you can specify when ROOT:SparkContext.md#creating-accumulators[creating an accumulator].","title":"spark-sql-SQLMetric.md[SQLMetrics] creates metrics."},{"location":"accumulators/#source-scala_3","text":"","title":"[source, scala]"},{"location":"accumulators/#val-counter-sclongaccumulatorcounter","text":"=== [[AccumulableInfo]] AccumulableInfo AccumulableInfo contains information about a task's local updates to an < >. id of the accumulator optional name of the accumulator optional partial update to the accumulator from a task value whether or not it is internal whether or not to countFailedValues to the final value of the accumulator for failed tasks optional metadata AccumulableInfo is used to transfer accumulator updates from executors to the driver every executor heartbeat or when a task finishes. Create an [[AccumulableInfo]] representation of this [[Accumulable]] with the provided values. === When are Accumulators Updated? === [[examples]] Examples ==== [[example-distributed-counter]] Example: Distributed Counter Imagine you are requested to write a distributed counter. What do you think about the following solutions? What are the pros and cons of using it?","title":"val counter = sc.longAccumulator(\"counter\")"},{"location":"accumulators/#source-scala_4","text":"val ints = sc.parallelize(0 to 9, 3) var counter = 0 ints.foreach { n => println(s\"int: $n\") counter = counter + 1 } println(s\"The number of elements is $counter\") How would you go about doing the calculation using accumulators? ==== [[example1]] Example: Using Accumulators in Transformations and Guarantee Exactly-Once Update CAUTION: FIXME Code with failing transformations (tasks) that update accumulator ( Map ) with TaskContext info. ==== [[example2]] Example: Custom Accumulator CAUTION: FIXME Improve the earlier example ==== [[example3]] Example: Distributed Stopwatch NOTE: This is almost a raw copy of org.apache.spark.ml.util.DistributedStopwatch.","title":"[source, scala]"},{"location":"accumulators/#source-scala_5","text":"class DistributedStopwatch(sc: SparkContext, val name: String) { val elapsedTime: Accumulator[Long] = sc.accumulator(0L, s\"DistributedStopwatch($name)\") override def elapsed(): Long = elapsedTime.value override protected def add(duration: Long): Unit = { elapsedTime += duration } } === [[i-want-more]] Further reading or watching http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf[Performance and Scalability of Broadcast in Spark]","title":"[source, scala]"},{"location":"architecture/","text":"= Spark Architecture Spark uses a master/worker architecture . There is a spark-driver.md[driver] that talks to a single coordinator called spark-master.md[master] that manages spark-workers.md[workers] in which executor:Executor.md[executors] run. .Spark architecture image::driver-sparkcontext-clustermanager-workers-executors.png[align=\"center\"] The driver and the executors run in their own Java processes. You can run them all on the same ( horizontal cluster ) or separate machines ( vertical cluster ) or in a mixed machine configuration. .Spark architecture in detail image::sparkapp-sparkcontext-master-slaves.png[align=\"center\"] Physical machines are called hosts or nodes .","title":"Architecture"},{"location":"barrier-execution-mode/","text":"= Barrier Execution Mode Barrier Execution Mode is...FIXME See https://jira.apache.org/jira/browse/SPARK-24374[SPIP : Barrier Execution Mode] and https://jira.apache.org/jira/browse/SPARK-24582[Design Doc]. NOTE: The barrier execution mode is experimental and it only handles limited scenarios. In case of a task failure, instead of only restarting the failed task, Spark will abort the entire stage and re-launch all tasks for this stage. Use < > transformation to mark the current stage as a < >. [[barrier]] [source, scala] barrier(): RDDBarrier[T] \u00b6 barrier simply creates a < > that comes with the barrier-aware < > transformation. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] mapPartitions is simply changes the regular < > transformation to create a rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:spark-rdd-MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. Task has a scheduler:Task.md#isBarrier[isBarrier] flag that says whether this task belongs to a barrier stage (default: false ). Spark must launch all the tasks at the same time for a < >. An RDD is in a < >, if at least one of its parent RDD(s), or itself, are mapped from an RDDBarrier . rdd:ShuffledRDD.md[ShuffledRDD] has the rdd:RDD.md#isBarrier[isBarrier] flag always disabled ( false ). rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the rdd:RDD.md#isBarrier_[isBarrier] flag enabled. rdd:spark-RDDBarrier.md#mapPartitions[RDDBarrier.mapPartitions] is the only transformation that creates a rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:spark-rdd-MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. == [[barrier-stage]] Barrier Stage Barrier Stage is a scheduler:Stage.md[stage] that...FIXME","title":"barrier-execution-mode"},{"location":"barrier-execution-mode/#barrier-rddbarriert","text":"barrier simply creates a < > that comes with the barrier-aware < > transformation. [[mapPartitions]] [source, scala] mapPartitions S: ClassTag : RDD[S] mapPartitions is simply changes the regular < > transformation to create a rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:spark-rdd-MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. Task has a scheduler:Task.md#isBarrier[isBarrier] flag that says whether this task belongs to a barrier stage (default: false ). Spark must launch all the tasks at the same time for a < >. An RDD is in a < >, if at least one of its parent RDD(s), or itself, are mapped from an RDDBarrier . rdd:ShuffledRDD.md[ShuffledRDD] has the rdd:RDD.md#isBarrier[isBarrier] flag always disabled ( false ). rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the rdd:RDD.md#isBarrier_[isBarrier] flag enabled. rdd:spark-RDDBarrier.md#mapPartitions[RDDBarrier.mapPartitions] is the only transformation that creates a rdd:spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] with the rdd:spark-rdd-MapPartitionsRDD.md#isFromBarrier[isFromBarrier] flag enabled. == [[barrier-stage]] Barrier Stage Barrier Stage is a scheduler:Stage.md[stage] that...FIXME","title":"barrier(): RDDBarrier[T]"},{"location":"configuration-properties/","text":"Spark Configuration Properties \u00b6 spark.ui.showConsoleProgress \u00b6 Enables ConsoleProgressBar Default: true spark.logLineage \u00b6 Default: false spark.shuffle.spill.initialMemoryThreshold \u00b6 Initial threshold for the size of an in-memory collection Default: 5MB Used by Spillable spark.shuffle.manager \u00b6 A fully-qualified class name or the alias of the ShuffleManager in a Spark application Default: sort Supported aliases: sort tungsten-sort Used when SparkEnv object is requested to create a \"base\" SparkEnv for a driver or an executor spark.shuffle.spill.numElementsForceSpillThreshold \u00b6 (internal) The maximum number of elements in memory before forcing the shuffle sorter to spill. Default: Integer.MAX_VALUE The default value is to never force the sorter to spill, until Spark reaches some limitations, like the max page size limitation for the pointer array in the sorter. Used when: ShuffleExternalSorter is created Spillable is created Spark SQL's SortBasedAggregator is requested for an UnsafeKVExternalSorter Spark SQL's ObjectAggregationMap is requested to dumpToExternalSorter Spark SQL's UnsafeExternalRowSorter is created Spark SQL's UnsafeFixedWidthAggregationMap is requested for an UnsafeKVExternalSorter spark.shuffle.file.buffer \u00b6 Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) Used when the following are created: BypassMergeSortShuffleWriter ShuffleExternalSorter UnsafeShuffleWriter ExternalAppendOnlyMap ExternalSorter spark.plugins \u00b6 A comma-separated list of class names implementing org.apache.spark.api.plugin.SparkPlugin to load into a Spark application. Default: (empty) Since: 3.0.0 Set when SparkContext is created spark.plugins.defaultList \u00b6 FIXME spark.app.id \u00b6 Unique identifier of a Spark application that Spark uses to uniquely identify metric sources . Default: TaskScheduler.applicationId() Set when SparkContext is created spark.extraListeners \u00b6 A comma-separated list of fully-qualified class names of SparkListener s (to be registered when SparkContext is created) Default: (empty) == [[properties]] Properties [cols=\"1m,1\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.blockManager.port a| [[spark.blockManager.port]][[BLOCK_MANAGER_PORT]] Port to use for block managers to listen on when a more specific setting is not provided (i.e. < > for the driver). Default: 0 In Spark on Kubernetes the default port is 7079 | spark.default.parallelism a| [[spark.default.parallelism]] Number of partitions to use for rdd:HashPartitioner.md[HashPartitioner] spark.default.parallelism corresponds to scheduler:SchedulerBackend.md#defaultParallelism[default parallelism] of a scheduler backend and is as follows: The number of threads for local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend]. the number of CPU cores in spark-mesos.md#defaultParallelism[Spark on Mesos] and defaults to 8 . Maximum of totalCoreCount and 2 in scheduler:CoarseGrainedSchedulerBackend.md#defaultParallelism[CoarseGrainedSchedulerBackend]. | spark.diskStore.subDirectories a| [[spark.diskStore.subDirectories]] Default: 64 | spark.driver.blockManager.port a| [[spark.driver.blockManager.port]][[DRIVER_BLOCK_MANAGER_PORT]] Port the storage:BlockManager.md[block manager] on the driver listens on Default: < > | spark.driver.maxResultSize a| [[maxResultSize]][[spark.driver.maxResultSize]][[MAX_RESULT_SIZE]] The maximum size of all results of the tasks in a TaskSet Default: 1g Used when: Executor is executor:Executor.md#maxResultSize[created] (and later for a executor:TaskRunner.md[]) TaskSetManager is scheduler:TaskSetManager.md#maxResultSize[created] (and later requested to scheduler:TaskSetManager.md#canFetchMoreResults[check available memory for task results]) | spark.executor.extraClassPath a| [[spark.executor.extraClassPath]][[EXECUTOR_CLASS_PATH]] User-defined class path for executors , i.e. URLs representing user-defined class path entries that are added to an executor's class path. URLs are separated by system-dependent path separator, i.e. : on Unix-like systems and ; on Microsoft Windows. Default: (empty) Used when: Spark Standalone's StandaloneSchedulerBackend is requested to spark-standalone:spark-standalone-StandaloneSchedulerBackend.md#start[start] (and creates a command for executor:CoarseGrainedExecutorBackend.md[]) Spark local's LocalSchedulerBackend is requested for the spark-local:spark-LocalSchedulerBackend.md#getUserClasspath[user-defined class path for executors] Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[create a command for CoarseGrainedExecutorBackend] Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to create a command for MesosExecutorBackend Spark on Kubernetes' BasicExecutorFeatureStep is requested to configurePod Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareEnvironment[prepareEnvironment] (for CoarseGrainedExecutorBackend ) | spark.executor.cores a| [[spark.executor.cores]] Number of cores of an executor:Executor.md[] | spark.executor.extraJavaOptions a| [[spark.executor.extraJavaOptions]] Extra Java options of an executor:Executor.md[] Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.extraLibraryPath a| [[spark.executor.extraLibraryPath]] Extra library paths separated by system-dependent path separator, i.e. : on Unix/MacOS systems and ; on Microsoft Windows Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.uri a| [[spark.executor.uri]] Equivalent to SPARK_EXECUTOR_URI | spark.executor.logs.rolling.time.interval a| [[spark.executor.logs.rolling.time.interval]] | spark.executor.logs.rolling.strategy a| [[spark.executor.logs.rolling.strategy]] | spark.executor.logs.rolling.maxRetainedFiles a| [[spark.executor.logs.rolling.maxRetainedFiles]] | spark.executor.logs.rolling.maxSize a| [[spark.executor.logs.rolling.maxSize]] | spark.executor.id a| [[spark.executor.id]] | spark.executor.heartbeatInterval a| [[spark.executor.heartbeatInterval]] Interval after which an executor:Executor.md[] reports heartbeat and metrics for active tasks to the driver Default: 10s Refer to executor:Executor.md#heartbeats-and-active-task-metrics[Sending heartbeats and partial metrics for active tasks] | spark.executor.heartbeat.maxFailures a| [[spark.executor.heartbeat.maxFailures]] Number of times an executor:Executor.md[] will try to send heartbeats to the driver before it gives up and exits (with exit code 56 ). Default: 60 NOTE: Introduced in https://issues.apache.org/jira/browse/SPARK-13522[SPARK-13522 Executor should kill itself when it's unable to heartbeat to the driver more than N times]. | spark.executor.instances a| [[spark.executor.instances]] Number of executor:Executor.md[] in use Default: 0 | spark.task.maxDirectResultSize a| [[spark.task.maxDirectResultSize]] Default: 1048576B | spark.executor.userClassPathFirst a| [[spark.executor.userClassPathFirst]] Flag to control whether to load classes in user jars before those in Spark jars Default: false | spark.executor.memory a| [[spark.executor.memory]] Amount of memory to use for an executor:Executor.md[] Default: 1g Equivalent to ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. Refer to executor:Executor.md#memory[Executor Memory -- spark.executor.memory or SPARK_EXECUTOR_MEMORY settings] | spark.executor.port a| [[spark.executor.port]] | spark.launcher.port a| [[spark.launcher.port]] | spark.launcher.secret a| [[spark.launcher.secret]] | spark.locality.wait a| [[spark.locality.wait]] For locality-aware delay scheduling for PROCESS_LOCAL , NODE_LOCAL , and RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocalities] when locality-specific setting is not set. Default: 3s | spark.locality.wait.node a| [[spark.locality.wait.node]] Scheduling delay for NODE_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.process a| [[spark.locality.wait.process]] Scheduling delay for PROCESS_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.rack a| [[spark.locality.wait.rack]] Scheduling delay for RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.logging.exceptionPrintInterval a| [[spark.logging.exceptionPrintInterval]] How frequently to reprint duplicate exceptions in full (in millis). Default: 10000 | spark.master a| [[spark.master]] Master URL to connect a Spark application to | spark.scheduler.allocation.file a| [[spark.scheduler.allocation.file]] Path to the configuration file of < > Default: fairscheduler.xml (on a Spark application's class path) | spark.scheduler.executorTaskBlacklistTime a| [[spark.scheduler.executorTaskBlacklistTime]] How long to wait before a task can be re-launched on the executor where it once failed. It is to prevent repeated task failures due to executor failures. Default: 0L | spark.scheduler.mode a| [[spark.scheduler.mode]][[SCHEDULER_MODE_PROPERTY]] Scheduling Mode of the scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl], i.e. case-insensitive name of the spark-scheduler-SchedulingMode.md[scheduling mode] that TaskSchedulerImpl uses to choose between the < > for task scheduling (of tasks of jobs submitted for execution to the same SparkContext ) Default: FIFO Supported values: FAIR for fair sharing (of cluster resources) FIFO (default) for queueing jobs one after another Task scheduling is an algorithm that is used to assign cluster resources (CPU cores and memory) to tasks (that are part of jobs with one or more stages). Fair sharing allows for executing tasks of different jobs at the same time (that were all submitted to the same SparkContext ). In FIFO scheduling mode a single SparkContext can submit a single job for execution only (regardless of how many cluster resources the job really use which could lead to a inefficient utilization of cluster resources and a longer execution of the Spark application overall). Scheduling mode is particularly useful in multi-tenant environments in which a single SparkContext could be shared across different users (to make a cluster resource utilization more efficient). TIP: Use web UI to know the current scheduling mode (e.g. < > tab as part of Spark Properties and < > tab as Scheduling Mode ). | spark.starvation.timeout a| [[spark.starvation.timeout]] Threshold above which Spark warns a user that an initial TaskSet may be starved Default: 15s | spark.storage.exceptionOnPinLeak a| [[spark.storage.exceptionOnPinLeak]] | spark.task.cpus a| [[spark.task.cpus]][[CPUS_PER_TASK]] The number of CPU cores used to schedule ( allocate for ) a task Default: 1 Used when: ExecutorAllocationManager is < > TaskSchedulerImpl is scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[created] AppStatusListener is requested to core:AppStatusListener.md#onEnvironmentUpdate[handle an SparkListenerEnvironmentUpdate event] LocalityPreferredContainerPlacementStrategy is requested to numExecutorsPending | spark.task.maxFailures a| [[spark.task.maxFailures]] The number of individual task failures before giving up on the entire scheduler:TaskSet.md[TaskSet] and the job afterwards Default: 1 in spark-local:spark-local.md[local] maxFailures in spark-local:spark-local.md#masterURL[local-with-retries] 4 in spark-cluster.md[cluster mode] | spark.unsafe.exceptionOnMemoryLeak a| [[spark.unsafe.exceptionOnMemoryLeak]] |=== == [[spark.memory.offHeap.size]][[MEMORY_OFFHEAP_SIZE]] spark.memory.offHeap.size Maximum memory (in bytes) for off-heap memory allocation. Default: 0 This setting has no impact on heap memory usage, so if your executors' total memory consumption must fit within some hard limit then be sure to shrink your JVM heap size accordingly. Must be set to a positive value when < > is enabled ( true ). Must not be negative == [[spark.memory.storageFraction]] spark.memory.storageFraction Fraction of the memory to use for off-heap storage region. Default: 0.5 == [[spark.memory.fraction]] spark.memory.fraction spark.memory.fraction is the fraction of JVM heap space used for execution and storage. Default: 0.6 == [[spark.memory.useLegacyMode]] spark.memory.useLegacyMode Controls the type of memory:MemoryManager.md[MemoryManager] to use. When enabled (i.e. true ) it is the legacy memory:StaticMemoryManager.md[StaticMemoryManager] while memory:UnifiedMemoryManager.md[UnifiedMemoryManager] otherwise (i.e. false ). Default: false == [[spark.memory.offHeap.enabled]] spark.memory.offHeap.enabled spark.memory.offHeap.enabled controls whether Spark will attempt to use off-heap memory for certain operations ( true ) or not ( false ). Default: false Tracks whether Tungsten memory will be allocated on the JVM heap or off-heap (using sun.misc.Unsafe ). If enabled, < > has to be memory:MemoryManager.md#tungstenMemoryMode[greater than 0]. Used when MemoryManager is requested for memory:MemoryManager.md#tungstenMemoryMode[tungstenMemoryMode]. == [[spark.shuffle.spill.batchSize]] spark.shuffle.spill.batchSize Size of object batches when reading or writing from serializers. Default: 10000 Used by shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] and shuffle:ExternalSorter.md[ExternalSorter] == [[spark.shuffle.mapOutput.dispatcher.numThreads]] spark.shuffle.mapOutput.dispatcher.numThreads Default: 8 == [[spark.shuffle.mapOutput.minSizeForBroadcast]] spark.shuffle.mapOutput.minSizeForBroadcast Size of serialized shuffle map output statuses when scheduler:MapOutputTrackerMaster.md#MessageLoop[MapOutputTrackerMaster] uses to determine whether to use a broadcast variable to send them to executors Default: 512k Must be below < > (to prevent sending an RPC message that is too large) == [[spark.rpc.message.maxSize]] spark.rpc.message.maxSize Maximum allowed message size for RPC communication (in MB unless specified) Default: 128 Generally only applies to map output size (serialized) information sent between executors and the driver. Increase this if you are running jobs with many thousands of map and reduce tasks and see messages about the RPC message size. == [[spark.shuffle.minNumPartitionsToHighlyCompress]] spark.shuffle.minNumPartitionsToHighlyCompress (internal) Minimum number of partitions (threshold) when MapStatus object creates a scheduler:MapStatus.md#HighlyCompressedMapStatus[HighlyCompressedMapStatus] (over scheduler:MapStatus.md#CompressedMapStatus[CompressedMapStatus]) when requested for scheduler:MapStatus.md#apply[one] (for shuffle:ShuffleWriter.md[ShuffleWriters]). Default: 2000 Must be a positive integer (above 0 ) == [[spark.shuffle.reduceLocality.enabled]] spark.shuffle.reduceLocality.enabled Enables locality preferences for reduce tasks Default: true When enabled ( true ), MapOutputTrackerMaster will scheduler:MapOutputTrackerMaster.md#getPreferredLocationsForShuffle[compute the preferred hosts] on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on. == [[spark.shuffle.sort.bypassMergeThreshold]] spark.shuffle.sort.bypassMergeThreshold Maximum number of reduce partitions below which shuffle:SortShuffleManager.md[SortShuffleManager] avoids merge-sorting data for no map-side aggregation Default: 200 == [[spark.shuffle.sort.initialBufferSize]] spark.shuffle.sort.initialBufferSize Initial buffer size for sorting Default: shuffle:UnsafeShuffleWriter.md#DEFAULT_INITIAL_SORT_BUFFER_SIZE[4096] Used exclusively when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#open[open] (and creates a shuffle:ShuffleExternalSorter.md[ShuffleExternalSorter]) == [[spark.shuffle.sync]] spark.shuffle.sync Controls whether DiskBlockObjectWriter should force outstanding writes to disk while storage:DiskBlockObjectWriter.md#commitAndGet[committing a single atomic block], i.e. all operating system buffers should synchronize with the disk to ensure that all changes to a file are in fact recorded in the storage. Default: false Used when BlockManager is requested for a storage:BlockManager.md#getDiskWriter[DiskBlockObjectWriter] == [[spark.shuffle.unsafe.file.output.buffer]] spark.shuffle.unsafe.file.output.buffer The file system for this buffer size after each partition is written in unsafe shuffle writer. In KiB unless otherwise specified. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) == [[spark.scheduler.revive.interval]] spark.scheduler.revive.interval Time (in ms) between resource offers revives Default: 1s == [[spark.scheduler.minRegisteredResourcesRatio]] spark.scheduler.minRegisteredResourcesRatio Minimum ratio of (registered resources / total expected resources) before submitting tasks Default: 0 == [[spark.scheduler.maxRegisteredResourcesWaitingTime]] spark.scheduler.maxRegisteredResourcesWaitingTime Time to wait for sufficient resources available Default: 30s == [[spark.file.transferTo]] spark.file.transferTo When enabled ( true ), copying data between two Java FileInputStreams uses Java FileChannels (Java NIO) to improve copy performance. Default: true == [[spark.shuffle.service.enabled]][[SHUFFLE_SERVICE_ENABLED]] spark.shuffle.service.enabled Controls whether to use the deploy:ExternalShuffleService.md[External Shuffle Service] Default: false When enabled ( true ), the driver registers itself with the shuffle service. == [[spark.shuffle.service.port]] spark.shuffle.service.port Default: 7337 == [[spark.shuffle.compress]] spark.shuffle.compress Controls whether to compress shuffle output when stored Default: true == [[spark.shuffle.unsafe.fastMergeEnabled]] spark.shuffle.unsafe.fastMergeEnabled Enables fast merge strategy for UnsafeShuffleWriter to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spill files]. Default: true == [[spark.rdd.compress]] spark.rdd.compress Controls whether to compress RDD partitions when stored serialized. Default: false == [[spark.shuffle.spill.compress]] spark.shuffle.spill.compress Controls whether to compress shuffle output temporarily spilled to disk. Default: true == [[spark.block.failures.beforeLocationRefresh]] spark.block.failures.beforeLocationRefresh Default: 5 == [[spark.io.encryption.enabled]] spark.io.encryption.enabled Controls whether to use IO encryption Default: false == [[spark.closure.serializer]] spark.closure.serializer serializer:Serializer.md[Serializer] Default: org.apache.spark.serializer.JavaSerializer == [[spark.serializer]] spark.serializer serializer:Serializer.md[Serializer] Default: org.apache.spark.serializer.JavaSerializer == [[spark.io.compression.codec]] spark.io.compression.codec The default io:CompressionCodec.md[CompressionCodec] Default: lz4 == [[spark.io.compression.lz4.blockSize]] spark.io.compression.lz4.blockSize The block size of the io:CompressionCodec.md#LZ4CompressionCodec[LZ4CompressionCodec] Default: 32k == [[spark.io.compression.snappy.blockSize]] spark.io.compression.snappy.blockSize The block size of the io:CompressionCodec.md#SnappyCompressionCodec[SnappyCompressionCodec] Default: 32k == [[spark.io.compression.zstd.bufferSize]] spark.io.compression.zstd.bufferSize The buffer size of the BufferedOutputStream of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 32k The buffer is used to avoid the overhead of excessive JNI calls while compressing or uncompressing small amount of data == [[spark.io.compression.zstd.level]] spark.io.compression.zstd.level The compression level of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 1 The default level is the fastest of all with reasonably high compression ratio == [[spark.buffer.size]] spark.buffer.size Default: 65536 == [[spark.cleaner.referenceTracking.cleanCheckpoints]] spark.cleaner.referenceTracking.cleanCheckpoints Enables cleaning checkpoint files when a checkpointed reference is out of scope Default: false == [[spark.cleaner.periodicGC.interval]] spark.cleaner.periodicGC.interval Controls how often to trigger a garbage collection Default: 30min == [[spark.cleaner.referenceTracking]] spark.cleaner.referenceTracking Controls whether to enable ContextCleaner Default: true == [[spark.cleaner.referenceTracking.blocking]] spark.cleaner.referenceTracking.blocking Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by < >) Default: true == [[spark.cleaner.referenceTracking.blocking.shuffle]] spark.cleaner.referenceTracking.blocking.shuffle Controls whether the cleaning thread should block on shuffle cleanup tasks. Default: false == [[spark.broadcast.blockSize]] spark.broadcast.blockSize The size of a block (in kB unless the unit is specified) Default: 4m Used when core:TorrentBroadcast.md#writeBlocks[ TorrentBroadcast stores brodcast blocks to BlockManager ] == [[spark.broadcast.compress]] spark.broadcast.compress Controls broadcast compression Default: true Used when core:TorrentBroadcast.md#creating-instance[ TorrentBroadcast is created] and later when core:TorrentBroadcast.md#writeBlocks[it stores broadcast blocks to BlockManager ]. Also in serializer:SerializerManager.md#settings[SerializerManager]. == [[spark.app.name]] spark.app.name Application Name Default: (undefined) == [[spark.rpc.lookupTimeout]] spark.rpc.lookupTimeout Timeout to use for the rpc:RpcEnv.md#defaultLookupTimeout[Default Endpoint Lookup Timeout] Default: 120s == [[spark.rpc.numRetries]] spark.rpc.numRetries Number of attempts to send a message to and receive a response from a remote endpoint. Default: 3 == [[spark.rpc.retry.wait]] spark.rpc.retry.wait Time to wait between retries. Default: 3s == [[spark.rpc.askTimeout]] spark.rpc.askTimeout Timeout for RPC ask calls Default: 120s == [[spark.network.timeout]] spark.network.timeout Network timeout to use for RPC remote endpoint lookup. Fallback for < >. Default: 120s == [[spark.speculation]] spark.speculation Enables ( true ) or disables ( false ) ROOT:speculative-execution-of-tasks.md[] Default: false == [[spark.speculation.interval]] spark.speculation.interval The time interval to use before checking for speculative tasks in ROOT:speculative-execution-of-tasks.md[]. Default: 100ms == [[spark.speculation.multiplier]] spark.speculation.multiplier Default: 1.5 == [[spark.speculation.quantile]] spark.speculation.quantile The percentage of tasks that has not finished yet at which to start speculation in ROOT:speculative-execution-of-tasks.md[]. Default: 0.75 == [[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Initial per-task memory size needed to store a block in memory. Default: 1024 * 1024 Must be at most the storage:MemoryStore.md#maxMemory[total amount of memory available for storage] Used when MemoryStore is requested to storage:MemoryStore.md#putIterator[putIterator] and storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes]","title":"Configuration Properties"},{"location":"configuration-properties/#spark-configuration-properties","text":"","title":"Spark Configuration Properties"},{"location":"configuration-properties/#sparkuishowconsoleprogress","text":"Enables ConsoleProgressBar Default: true","title":" spark.ui.showConsoleProgress"},{"location":"configuration-properties/#sparkloglineage","text":"Default: false","title":" spark.logLineage"},{"location":"configuration-properties/#sparkshufflespillinitialmemorythreshold","text":"Initial threshold for the size of an in-memory collection Default: 5MB Used by Spillable","title":" spark.shuffle.spill.initialMemoryThreshold"},{"location":"configuration-properties/#sparkshufflemanager","text":"A fully-qualified class name or the alias of the ShuffleManager in a Spark application Default: sort Supported aliases: sort tungsten-sort Used when SparkEnv object is requested to create a \"base\" SparkEnv for a driver or an executor","title":" spark.shuffle.manager"},{"location":"configuration-properties/#sparkshufflespillnumelementsforcespillthreshold","text":"(internal) The maximum number of elements in memory before forcing the shuffle sorter to spill. Default: Integer.MAX_VALUE The default value is to never force the sorter to spill, until Spark reaches some limitations, like the max page size limitation for the pointer array in the sorter. Used when: ShuffleExternalSorter is created Spillable is created Spark SQL's SortBasedAggregator is requested for an UnsafeKVExternalSorter Spark SQL's ObjectAggregationMap is requested to dumpToExternalSorter Spark SQL's UnsafeExternalRowSorter is created Spark SQL's UnsafeFixedWidthAggregationMap is requested for an UnsafeKVExternalSorter","title":" spark.shuffle.spill.numElementsForceSpillThreshold"},{"location":"configuration-properties/#sparkshufflefilebuffer","text":"Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) Used when the following are created: BypassMergeSortShuffleWriter ShuffleExternalSorter UnsafeShuffleWriter ExternalAppendOnlyMap ExternalSorter","title":" spark.shuffle.file.buffer"},{"location":"configuration-properties/#sparkplugins","text":"A comma-separated list of class names implementing org.apache.spark.api.plugin.SparkPlugin to load into a Spark application. Default: (empty) Since: 3.0.0 Set when SparkContext is created","title":" spark.plugins"},{"location":"configuration-properties/#sparkpluginsdefaultlist","text":"FIXME","title":" spark.plugins.defaultList"},{"location":"configuration-properties/#sparkappid","text":"Unique identifier of a Spark application that Spark uses to uniquely identify metric sources . Default: TaskScheduler.applicationId() Set when SparkContext is created","title":" spark.app.id"},{"location":"configuration-properties/#sparkextralisteners","text":"A comma-separated list of fully-qualified class names of SparkListener s (to be registered when SparkContext is created) Default: (empty) == [[properties]] Properties [cols=\"1m,1\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.blockManager.port a| [[spark.blockManager.port]][[BLOCK_MANAGER_PORT]] Port to use for block managers to listen on when a more specific setting is not provided (i.e. < > for the driver). Default: 0 In Spark on Kubernetes the default port is 7079 | spark.default.parallelism a| [[spark.default.parallelism]] Number of partitions to use for rdd:HashPartitioner.md[HashPartitioner] spark.default.parallelism corresponds to scheduler:SchedulerBackend.md#defaultParallelism[default parallelism] of a scheduler backend and is as follows: The number of threads for local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend]. the number of CPU cores in spark-mesos.md#defaultParallelism[Spark on Mesos] and defaults to 8 . Maximum of totalCoreCount and 2 in scheduler:CoarseGrainedSchedulerBackend.md#defaultParallelism[CoarseGrainedSchedulerBackend]. | spark.diskStore.subDirectories a| [[spark.diskStore.subDirectories]] Default: 64 | spark.driver.blockManager.port a| [[spark.driver.blockManager.port]][[DRIVER_BLOCK_MANAGER_PORT]] Port the storage:BlockManager.md[block manager] on the driver listens on Default: < > | spark.driver.maxResultSize a| [[maxResultSize]][[spark.driver.maxResultSize]][[MAX_RESULT_SIZE]] The maximum size of all results of the tasks in a TaskSet Default: 1g Used when: Executor is executor:Executor.md#maxResultSize[created] (and later for a executor:TaskRunner.md[]) TaskSetManager is scheduler:TaskSetManager.md#maxResultSize[created] (and later requested to scheduler:TaskSetManager.md#canFetchMoreResults[check available memory for task results]) | spark.executor.extraClassPath a| [[spark.executor.extraClassPath]][[EXECUTOR_CLASS_PATH]] User-defined class path for executors , i.e. URLs representing user-defined class path entries that are added to an executor's class path. URLs are separated by system-dependent path separator, i.e. : on Unix-like systems and ; on Microsoft Windows. Default: (empty) Used when: Spark Standalone's StandaloneSchedulerBackend is requested to spark-standalone:spark-standalone-StandaloneSchedulerBackend.md#start[start] (and creates a command for executor:CoarseGrainedExecutorBackend.md[]) Spark local's LocalSchedulerBackend is requested for the spark-local:spark-LocalSchedulerBackend.md#getUserClasspath[user-defined class path for executors] Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[create a command for CoarseGrainedExecutorBackend] Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to create a command for MesosExecutorBackend Spark on Kubernetes' BasicExecutorFeatureStep is requested to configurePod Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareEnvironment[prepareEnvironment] (for CoarseGrainedExecutorBackend ) | spark.executor.cores a| [[spark.executor.cores]] Number of cores of an executor:Executor.md[] | spark.executor.extraJavaOptions a| [[spark.executor.extraJavaOptions]] Extra Java options of an executor:Executor.md[] Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.extraLibraryPath a| [[spark.executor.extraLibraryPath]] Extra library paths separated by system-dependent path separator, i.e. : on Unix/MacOS systems and ; on Microsoft Windows Used when Spark on YARN's ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#prepareCommand[prepare the command to launch CoarseGrainedExecutorBackend in a YARN container] | spark.executor.uri a| [[spark.executor.uri]] Equivalent to SPARK_EXECUTOR_URI | spark.executor.logs.rolling.time.interval a| [[spark.executor.logs.rolling.time.interval]] | spark.executor.logs.rolling.strategy a| [[spark.executor.logs.rolling.strategy]] | spark.executor.logs.rolling.maxRetainedFiles a| [[spark.executor.logs.rolling.maxRetainedFiles]] | spark.executor.logs.rolling.maxSize a| [[spark.executor.logs.rolling.maxSize]] | spark.executor.id a| [[spark.executor.id]] | spark.executor.heartbeatInterval a| [[spark.executor.heartbeatInterval]] Interval after which an executor:Executor.md[] reports heartbeat and metrics for active tasks to the driver Default: 10s Refer to executor:Executor.md#heartbeats-and-active-task-metrics[Sending heartbeats and partial metrics for active tasks] | spark.executor.heartbeat.maxFailures a| [[spark.executor.heartbeat.maxFailures]] Number of times an executor:Executor.md[] will try to send heartbeats to the driver before it gives up and exits (with exit code 56 ). Default: 60 NOTE: Introduced in https://issues.apache.org/jira/browse/SPARK-13522[SPARK-13522 Executor should kill itself when it's unable to heartbeat to the driver more than N times]. | spark.executor.instances a| [[spark.executor.instances]] Number of executor:Executor.md[] in use Default: 0 | spark.task.maxDirectResultSize a| [[spark.task.maxDirectResultSize]] Default: 1048576B | spark.executor.userClassPathFirst a| [[spark.executor.userClassPathFirst]] Flag to control whether to load classes in user jars before those in Spark jars Default: false | spark.executor.memory a| [[spark.executor.memory]] Amount of memory to use for an executor:Executor.md[] Default: 1g Equivalent to ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. Refer to executor:Executor.md#memory[Executor Memory -- spark.executor.memory or SPARK_EXECUTOR_MEMORY settings] | spark.executor.port a| [[spark.executor.port]] | spark.launcher.port a| [[spark.launcher.port]] | spark.launcher.secret a| [[spark.launcher.secret]] | spark.locality.wait a| [[spark.locality.wait]] For locality-aware delay scheduling for PROCESS_LOCAL , NODE_LOCAL , and RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocalities] when locality-specific setting is not set. Default: 3s | spark.locality.wait.node a| [[spark.locality.wait.node]] Scheduling delay for NODE_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.process a| [[spark.locality.wait.process]] Scheduling delay for PROCESS_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.locality.wait.rack a| [[spark.locality.wait.rack]] Scheduling delay for RACK_LOCAL scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] Default: The value of < > | spark.logging.exceptionPrintInterval a| [[spark.logging.exceptionPrintInterval]] How frequently to reprint duplicate exceptions in full (in millis). Default: 10000 | spark.master a| [[spark.master]] Master URL to connect a Spark application to | spark.scheduler.allocation.file a| [[spark.scheduler.allocation.file]] Path to the configuration file of < > Default: fairscheduler.xml (on a Spark application's class path) | spark.scheduler.executorTaskBlacklistTime a| [[spark.scheduler.executorTaskBlacklistTime]] How long to wait before a task can be re-launched on the executor where it once failed. It is to prevent repeated task failures due to executor failures. Default: 0L | spark.scheduler.mode a| [[spark.scheduler.mode]][[SCHEDULER_MODE_PROPERTY]] Scheduling Mode of the scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl], i.e. case-insensitive name of the spark-scheduler-SchedulingMode.md[scheduling mode] that TaskSchedulerImpl uses to choose between the < > for task scheduling (of tasks of jobs submitted for execution to the same SparkContext ) Default: FIFO Supported values: FAIR for fair sharing (of cluster resources) FIFO (default) for queueing jobs one after another Task scheduling is an algorithm that is used to assign cluster resources (CPU cores and memory) to tasks (that are part of jobs with one or more stages). Fair sharing allows for executing tasks of different jobs at the same time (that were all submitted to the same SparkContext ). In FIFO scheduling mode a single SparkContext can submit a single job for execution only (regardless of how many cluster resources the job really use which could lead to a inefficient utilization of cluster resources and a longer execution of the Spark application overall). Scheduling mode is particularly useful in multi-tenant environments in which a single SparkContext could be shared across different users (to make a cluster resource utilization more efficient). TIP: Use web UI to know the current scheduling mode (e.g. < > tab as part of Spark Properties and < > tab as Scheduling Mode ). | spark.starvation.timeout a| [[spark.starvation.timeout]] Threshold above which Spark warns a user that an initial TaskSet may be starved Default: 15s | spark.storage.exceptionOnPinLeak a| [[spark.storage.exceptionOnPinLeak]] | spark.task.cpus a| [[spark.task.cpus]][[CPUS_PER_TASK]] The number of CPU cores used to schedule ( allocate for ) a task Default: 1 Used when: ExecutorAllocationManager is < > TaskSchedulerImpl is scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[created] AppStatusListener is requested to core:AppStatusListener.md#onEnvironmentUpdate[handle an SparkListenerEnvironmentUpdate event] LocalityPreferredContainerPlacementStrategy is requested to numExecutorsPending | spark.task.maxFailures a| [[spark.task.maxFailures]] The number of individual task failures before giving up on the entire scheduler:TaskSet.md[TaskSet] and the job afterwards Default: 1 in spark-local:spark-local.md[local] maxFailures in spark-local:spark-local.md#masterURL[local-with-retries] 4 in spark-cluster.md[cluster mode] | spark.unsafe.exceptionOnMemoryLeak a| [[spark.unsafe.exceptionOnMemoryLeak]] |=== == [[spark.memory.offHeap.size]][[MEMORY_OFFHEAP_SIZE]] spark.memory.offHeap.size Maximum memory (in bytes) for off-heap memory allocation. Default: 0 This setting has no impact on heap memory usage, so if your executors' total memory consumption must fit within some hard limit then be sure to shrink your JVM heap size accordingly. Must be set to a positive value when < > is enabled ( true ). Must not be negative == [[spark.memory.storageFraction]] spark.memory.storageFraction Fraction of the memory to use for off-heap storage region. Default: 0.5 == [[spark.memory.fraction]] spark.memory.fraction spark.memory.fraction is the fraction of JVM heap space used for execution and storage. Default: 0.6 == [[spark.memory.useLegacyMode]] spark.memory.useLegacyMode Controls the type of memory:MemoryManager.md[MemoryManager] to use. When enabled (i.e. true ) it is the legacy memory:StaticMemoryManager.md[StaticMemoryManager] while memory:UnifiedMemoryManager.md[UnifiedMemoryManager] otherwise (i.e. false ). Default: false == [[spark.memory.offHeap.enabled]] spark.memory.offHeap.enabled spark.memory.offHeap.enabled controls whether Spark will attempt to use off-heap memory for certain operations ( true ) or not ( false ). Default: false Tracks whether Tungsten memory will be allocated on the JVM heap or off-heap (using sun.misc.Unsafe ). If enabled, < > has to be memory:MemoryManager.md#tungstenMemoryMode[greater than 0]. Used when MemoryManager is requested for memory:MemoryManager.md#tungstenMemoryMode[tungstenMemoryMode]. == [[spark.shuffle.spill.batchSize]] spark.shuffle.spill.batchSize Size of object batches when reading or writing from serializers. Default: 10000 Used by shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] and shuffle:ExternalSorter.md[ExternalSorter] == [[spark.shuffle.mapOutput.dispatcher.numThreads]] spark.shuffle.mapOutput.dispatcher.numThreads Default: 8 == [[spark.shuffle.mapOutput.minSizeForBroadcast]] spark.shuffle.mapOutput.minSizeForBroadcast Size of serialized shuffle map output statuses when scheduler:MapOutputTrackerMaster.md#MessageLoop[MapOutputTrackerMaster] uses to determine whether to use a broadcast variable to send them to executors Default: 512k Must be below < > (to prevent sending an RPC message that is too large) == [[spark.rpc.message.maxSize]] spark.rpc.message.maxSize Maximum allowed message size for RPC communication (in MB unless specified) Default: 128 Generally only applies to map output size (serialized) information sent between executors and the driver. Increase this if you are running jobs with many thousands of map and reduce tasks and see messages about the RPC message size. == [[spark.shuffle.minNumPartitionsToHighlyCompress]] spark.shuffle.minNumPartitionsToHighlyCompress (internal) Minimum number of partitions (threshold) when MapStatus object creates a scheduler:MapStatus.md#HighlyCompressedMapStatus[HighlyCompressedMapStatus] (over scheduler:MapStatus.md#CompressedMapStatus[CompressedMapStatus]) when requested for scheduler:MapStatus.md#apply[one] (for shuffle:ShuffleWriter.md[ShuffleWriters]). Default: 2000 Must be a positive integer (above 0 ) == [[spark.shuffle.reduceLocality.enabled]] spark.shuffle.reduceLocality.enabled Enables locality preferences for reduce tasks Default: true When enabled ( true ), MapOutputTrackerMaster will scheduler:MapOutputTrackerMaster.md#getPreferredLocationsForShuffle[compute the preferred hosts] on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on. == [[spark.shuffle.sort.bypassMergeThreshold]] spark.shuffle.sort.bypassMergeThreshold Maximum number of reduce partitions below which shuffle:SortShuffleManager.md[SortShuffleManager] avoids merge-sorting data for no map-side aggregation Default: 200 == [[spark.shuffle.sort.initialBufferSize]] spark.shuffle.sort.initialBufferSize Initial buffer size for sorting Default: shuffle:UnsafeShuffleWriter.md#DEFAULT_INITIAL_SORT_BUFFER_SIZE[4096] Used exclusively when UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#open[open] (and creates a shuffle:ShuffleExternalSorter.md[ShuffleExternalSorter]) == [[spark.shuffle.sync]] spark.shuffle.sync Controls whether DiskBlockObjectWriter should force outstanding writes to disk while storage:DiskBlockObjectWriter.md#commitAndGet[committing a single atomic block], i.e. all operating system buffers should synchronize with the disk to ensure that all changes to a file are in fact recorded in the storage. Default: false Used when BlockManager is requested for a storage:BlockManager.md#getDiskWriter[DiskBlockObjectWriter] == [[spark.shuffle.unsafe.file.output.buffer]] spark.shuffle.unsafe.file.output.buffer The file system for this buffer size after each partition is written in unsafe shuffle writer. In KiB unless otherwise specified. Default: 32k Must be greater than 0 and less than or equal to 2097151 ( (Integer.MAX_VALUE - 15) / 1024 ) == [[spark.scheduler.revive.interval]] spark.scheduler.revive.interval Time (in ms) between resource offers revives Default: 1s == [[spark.scheduler.minRegisteredResourcesRatio]] spark.scheduler.minRegisteredResourcesRatio Minimum ratio of (registered resources / total expected resources) before submitting tasks Default: 0 == [[spark.scheduler.maxRegisteredResourcesWaitingTime]] spark.scheduler.maxRegisteredResourcesWaitingTime Time to wait for sufficient resources available Default: 30s == [[spark.file.transferTo]] spark.file.transferTo When enabled ( true ), copying data between two Java FileInputStreams uses Java FileChannels (Java NIO) to improve copy performance. Default: true == [[spark.shuffle.service.enabled]][[SHUFFLE_SERVICE_ENABLED]] spark.shuffle.service.enabled Controls whether to use the deploy:ExternalShuffleService.md[External Shuffle Service] Default: false When enabled ( true ), the driver registers itself with the shuffle service. == [[spark.shuffle.service.port]] spark.shuffle.service.port Default: 7337 == [[spark.shuffle.compress]] spark.shuffle.compress Controls whether to compress shuffle output when stored Default: true == [[spark.shuffle.unsafe.fastMergeEnabled]] spark.shuffle.unsafe.fastMergeEnabled Enables fast merge strategy for UnsafeShuffleWriter to shuffle:UnsafeShuffleWriter.md#mergeSpills[merge spill files]. Default: true == [[spark.rdd.compress]] spark.rdd.compress Controls whether to compress RDD partitions when stored serialized. Default: false == [[spark.shuffle.spill.compress]] spark.shuffle.spill.compress Controls whether to compress shuffle output temporarily spilled to disk. Default: true == [[spark.block.failures.beforeLocationRefresh]] spark.block.failures.beforeLocationRefresh Default: 5 == [[spark.io.encryption.enabled]] spark.io.encryption.enabled Controls whether to use IO encryption Default: false == [[spark.closure.serializer]] spark.closure.serializer serializer:Serializer.md[Serializer] Default: org.apache.spark.serializer.JavaSerializer == [[spark.serializer]] spark.serializer serializer:Serializer.md[Serializer] Default: org.apache.spark.serializer.JavaSerializer == [[spark.io.compression.codec]] spark.io.compression.codec The default io:CompressionCodec.md[CompressionCodec] Default: lz4 == [[spark.io.compression.lz4.blockSize]] spark.io.compression.lz4.blockSize The block size of the io:CompressionCodec.md#LZ4CompressionCodec[LZ4CompressionCodec] Default: 32k == [[spark.io.compression.snappy.blockSize]] spark.io.compression.snappy.blockSize The block size of the io:CompressionCodec.md#SnappyCompressionCodec[SnappyCompressionCodec] Default: 32k == [[spark.io.compression.zstd.bufferSize]] spark.io.compression.zstd.bufferSize The buffer size of the BufferedOutputStream of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 32k The buffer is used to avoid the overhead of excessive JNI calls while compressing or uncompressing small amount of data == [[spark.io.compression.zstd.level]] spark.io.compression.zstd.level The compression level of the io:CompressionCodec.md#ZStdCompressionCodec[ZStdCompressionCodec] Default: 1 The default level is the fastest of all with reasonably high compression ratio == [[spark.buffer.size]] spark.buffer.size Default: 65536 == [[spark.cleaner.referenceTracking.cleanCheckpoints]] spark.cleaner.referenceTracking.cleanCheckpoints Enables cleaning checkpoint files when a checkpointed reference is out of scope Default: false == [[spark.cleaner.periodicGC.interval]] spark.cleaner.periodicGC.interval Controls how often to trigger a garbage collection Default: 30min == [[spark.cleaner.referenceTracking]] spark.cleaner.referenceTracking Controls whether to enable ContextCleaner Default: true == [[spark.cleaner.referenceTracking.blocking]] spark.cleaner.referenceTracking.blocking Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by < >) Default: true == [[spark.cleaner.referenceTracking.blocking.shuffle]] spark.cleaner.referenceTracking.blocking.shuffle Controls whether the cleaning thread should block on shuffle cleanup tasks. Default: false == [[spark.broadcast.blockSize]] spark.broadcast.blockSize The size of a block (in kB unless the unit is specified) Default: 4m Used when core:TorrentBroadcast.md#writeBlocks[ TorrentBroadcast stores brodcast blocks to BlockManager ] == [[spark.broadcast.compress]] spark.broadcast.compress Controls broadcast compression Default: true Used when core:TorrentBroadcast.md#creating-instance[ TorrentBroadcast is created] and later when core:TorrentBroadcast.md#writeBlocks[it stores broadcast blocks to BlockManager ]. Also in serializer:SerializerManager.md#settings[SerializerManager]. == [[spark.app.name]] spark.app.name Application Name Default: (undefined) == [[spark.rpc.lookupTimeout]] spark.rpc.lookupTimeout Timeout to use for the rpc:RpcEnv.md#defaultLookupTimeout[Default Endpoint Lookup Timeout] Default: 120s == [[spark.rpc.numRetries]] spark.rpc.numRetries Number of attempts to send a message to and receive a response from a remote endpoint. Default: 3 == [[spark.rpc.retry.wait]] spark.rpc.retry.wait Time to wait between retries. Default: 3s == [[spark.rpc.askTimeout]] spark.rpc.askTimeout Timeout for RPC ask calls Default: 120s == [[spark.network.timeout]] spark.network.timeout Network timeout to use for RPC remote endpoint lookup. Fallback for < >. Default: 120s == [[spark.speculation]] spark.speculation Enables ( true ) or disables ( false ) ROOT:speculative-execution-of-tasks.md[] Default: false == [[spark.speculation.interval]] spark.speculation.interval The time interval to use before checking for speculative tasks in ROOT:speculative-execution-of-tasks.md[]. Default: 100ms == [[spark.speculation.multiplier]] spark.speculation.multiplier Default: 1.5 == [[spark.speculation.quantile]] spark.speculation.quantile The percentage of tasks that has not finished yet at which to start speculation in ROOT:speculative-execution-of-tasks.md[]. Default: 0.75 == [[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Initial per-task memory size needed to store a block in memory. Default: 1024 * 1024 Must be at most the storage:MemoryStore.md#maxMemory[total amount of memory available for storage] Used when MemoryStore is requested to storage:MemoryStore.md#putIterator[putIterator] and storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes]","title":" spark.extraListeners"},{"location":"data-locality/","text":"= Data Locality / Placement Spark relies on data locality , aka data placement or proximity to data source , that makes Spark jobs sensitive to where the data is located. It is therefore important to have yarn/README.md[Spark running on Hadoop YARN cluster] if the data comes from HDFS. In yarn/README.md[Spark on YARN] Spark tries to place tasks alongside HDFS blocks. With HDFS the Spark driver contacts NameNode about the DataNodes (ideally local) containing the various blocks of a file or directory as well as their locations (represented as InputSplits ), and then schedules the work to the SparkWorkers. Spark's compute nodes / workers should be running on storage nodes. Concept of locality-aware scheduling . Spark tries to execute tasks as close to the data as possible to minimize data transfer (over the wire). .Locality Level in the Spark UI image::sparkui-stages-locality-level.png[] There are the following task localities (consult https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskLocality$[org.apache.spark.scheduler.TaskLocality ] object): PROCESS_LOCAL NODE_LOCAL NO_PREF RACK_LOCAL ANY Task location can either be a host or a pair of a host and an executor.","title":"Data Locality"},{"location":"driver/","text":"Driver \u00b6 A Spark driver ( aka an application's driver process ) is a JVM process that hosts ROOT:SparkContext.md[SparkContext] for a Spark application. It is the master node in a Spark application. It is the cockpit of jobs and tasks execution (using scheduler:DAGScheduler.md[DAGScheduler] and scheduler:TaskScheduler.md[Task Scheduler]). It hosts spark-webui.md[Web UI] for the environment. .Driver with the services image::spark-driver.png[align=\"center\"] It splits a Spark application into tasks and schedules them to run on executors. A driver is where the task scheduler lives and spawns tasks across workers. A driver coordinates workers and overall execution of tasks. NOTE: spark-shell.md[Spark shell] is a Spark application and the driver. It creates a SparkContext that is available as sc . Driver requires the additional services (beside the common ones like shuffle:ShuffleManager.md[], memory:MemoryManager.md[], storage:BlockTransferService.md[], core:BroadcastManager.md[]): Listener Bus rpc:index.md[] scheduler:MapOutputTrackerMaster.md[] with the name MapOutputTracker storage:BlockManagerMaster.md[] with the name BlockManagerMaster metrics:spark-metrics-MetricsSystem.md[] with the name driver scheduler:OutputCommitCoordinator.md[] with the endpoint's name OutputCommitCoordinator CAUTION: FIXME Diagram of RpcEnv for a driver (and later executors). Perhaps it should be in the notes about RpcEnv? High-level control flow of work Your Spark application runs as long as the Spark driver. ** Once the driver terminates, so does your Spark application. Creates SparkContext , RDD 's, and executes transformations and actions Launches scheduler:Task.md[tasks] === [[driver-memory]] Driver's Memory It can be set first using spark-submit.md#command-line-options[spark-submit's --driver-memory ] command-line option or < > and falls back to spark-submit.md#environment-variables[SPARK_DRIVER_MEMORY] if not set earlier. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[driver-memory]] Driver's Cores It can be set first using spark-submit.md#driver-cores[spark-submit's --driver-cores ] command-line option for spark-deploy-mode.md#cluster[ cluster deploy mode]. NOTE: In spark-deploy-mode.md#client[ client deploy mode] the driver's memory corresponds to the memory of the JVM process the Spark application runs on. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_driver_blockManager_port]] spark.driver.blockManager.port | storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] | Port to use for the storage:BlockManager.md[BlockManager] on the driver. More precisely, spark.driver.blockManager.port is used when core:SparkEnv.md#NettyBlockTransferService[ NettyBlockTransferService is created] (while SparkEnv is created for the driver). | [[spark_driver_host]][[spark.driver.host]] spark.driver.host | localHostName | The address of the node where the driver runs on. Set when ROOT:SparkContext.md#creating-instance[ SparkContext is created] | [[spark_driver_port]][[spark.driver.port]] spark.driver.port | 0 | The port the driver listens to. It is first set to 0 in the driver when ROOT:SparkContext.md#creating-instance[SparkContext is initialized]. Set to the port of rpc:index.md[RpcEnv] of the driver (in < >) or when yarn/spark-yarn-applicationmaster.md#waitForSparkDriver[client-mode ApplicationMaster connected to the driver] (in Spark on YARN). | [[spark_driver_memory]] spark.driver.memory | 1g | The driver's memory size (in MiBs). Refer to < >. | [[spark_driver_cores]] spark.driver.cores | 1 | The number of CPU cores assigned to the driver in spark-deploy-mode.md#cluster[cluster deploy mode]. NOTE: When yarn/spark-yarn-client.md#creating-instance[Client is created] (for Spark on YARN in cluster mode only), it sets the number of cores for ApplicationManager using spark.driver.cores . Refer to < >. | [[spark_driver_extraLibraryPath]] spark.driver.extraLibraryPath | | | [[spark_driver_extraJavaOptions]] spark.driver.extraJavaOptions | | Additional JVM options for the driver. | [[spark.driver.appUIAddress]] spark.driver.appUIAddress spark.driver.appUIAddress is used exclusively in yarn/README.md[Spark on YARN]. It is set when yarn/spark-yarn-client-yarnclientschedulerbackend.md#start[YarnClientSchedulerBackend starts] to yarn/spark-yarn-applicationmaster.md#runExecutorLauncher[run ExecutorLauncher] (and yarn/spark-yarn-applicationmaster.md#registerAM[register ApplicationMaster] for the Spark application). | [[spark_driver_libraryPath]] spark.driver.libraryPath | | |=== ==== [[spark_driver_extraClassPath]] spark.driver.extraClassPath spark.driver.extraClassPath system property sets the additional classpath entries (e.g. jars and directories) that should be added to the driver's classpath in spark-deploy-mode.md#cluster[ cluster deploy mode]. [NOTE] \u00b6 For spark-deploy-mode.md#client[ client deploy mode] you can use a properties file or command line to set spark.driver.extraClassPath . Do not use ROOT:SparkConf.md[SparkConf] since it is too late for client deploy mode given the JVM has already been set up to start a Spark application. Refer to spark-class.md#buildSparkSubmitCommand[ buildSparkSubmitCommand Internal Method] for the very low-level details of how it is handled internally. \u00b6 spark.driver.extraClassPath uses a OS-specific path separator. NOTE: Use spark-submit 's spark-submit.md#driver-class-path[ --driver-class-path command-line option] on command line to override spark.driver.extraClassPath from a spark-properties.md#spark-defaults-conf[Spark properties file].","title":"Driver"},{"location":"driver/#driver","text":"A Spark driver ( aka an application's driver process ) is a JVM process that hosts ROOT:SparkContext.md[SparkContext] for a Spark application. It is the master node in a Spark application. It is the cockpit of jobs and tasks execution (using scheduler:DAGScheduler.md[DAGScheduler] and scheduler:TaskScheduler.md[Task Scheduler]). It hosts spark-webui.md[Web UI] for the environment. .Driver with the services image::spark-driver.png[align=\"center\"] It splits a Spark application into tasks and schedules them to run on executors. A driver is where the task scheduler lives and spawns tasks across workers. A driver coordinates workers and overall execution of tasks. NOTE: spark-shell.md[Spark shell] is a Spark application and the driver. It creates a SparkContext that is available as sc . Driver requires the additional services (beside the common ones like shuffle:ShuffleManager.md[], memory:MemoryManager.md[], storage:BlockTransferService.md[], core:BroadcastManager.md[]): Listener Bus rpc:index.md[] scheduler:MapOutputTrackerMaster.md[] with the name MapOutputTracker storage:BlockManagerMaster.md[] with the name BlockManagerMaster metrics:spark-metrics-MetricsSystem.md[] with the name driver scheduler:OutputCommitCoordinator.md[] with the endpoint's name OutputCommitCoordinator CAUTION: FIXME Diagram of RpcEnv for a driver (and later executors). Perhaps it should be in the notes about RpcEnv? High-level control flow of work Your Spark application runs as long as the Spark driver. ** Once the driver terminates, so does your Spark application. Creates SparkContext , RDD 's, and executes transformations and actions Launches scheduler:Task.md[tasks] === [[driver-memory]] Driver's Memory It can be set first using spark-submit.md#command-line-options[spark-submit's --driver-memory ] command-line option or < > and falls back to spark-submit.md#environment-variables[SPARK_DRIVER_MEMORY] if not set earlier. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[driver-memory]] Driver's Cores It can be set first using spark-submit.md#driver-cores[spark-submit's --driver-cores ] command-line option for spark-deploy-mode.md#cluster[ cluster deploy mode]. NOTE: In spark-deploy-mode.md#client[ client deploy mode] the driver's memory corresponds to the memory of the JVM process the Spark application runs on. NOTE: It is printed out to the standard error output in spark-submit.md#verbose-mode[spark-submit's verbose mode]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_driver_blockManager_port]] spark.driver.blockManager.port | storage:BlockManager.md#spark_blockManager_port[spark.blockManager.port] | Port to use for the storage:BlockManager.md[BlockManager] on the driver. More precisely, spark.driver.blockManager.port is used when core:SparkEnv.md#NettyBlockTransferService[ NettyBlockTransferService is created] (while SparkEnv is created for the driver). | [[spark_driver_host]][[spark.driver.host]] spark.driver.host | localHostName | The address of the node where the driver runs on. Set when ROOT:SparkContext.md#creating-instance[ SparkContext is created] | [[spark_driver_port]][[spark.driver.port]] spark.driver.port | 0 | The port the driver listens to. It is first set to 0 in the driver when ROOT:SparkContext.md#creating-instance[SparkContext is initialized]. Set to the port of rpc:index.md[RpcEnv] of the driver (in < >) or when yarn/spark-yarn-applicationmaster.md#waitForSparkDriver[client-mode ApplicationMaster connected to the driver] (in Spark on YARN). | [[spark_driver_memory]] spark.driver.memory | 1g | The driver's memory size (in MiBs). Refer to < >. | [[spark_driver_cores]] spark.driver.cores | 1 | The number of CPU cores assigned to the driver in spark-deploy-mode.md#cluster[cluster deploy mode]. NOTE: When yarn/spark-yarn-client.md#creating-instance[Client is created] (for Spark on YARN in cluster mode only), it sets the number of cores for ApplicationManager using spark.driver.cores . Refer to < >. | [[spark_driver_extraLibraryPath]] spark.driver.extraLibraryPath | | | [[spark_driver_extraJavaOptions]] spark.driver.extraJavaOptions | | Additional JVM options for the driver. | [[spark.driver.appUIAddress]] spark.driver.appUIAddress spark.driver.appUIAddress is used exclusively in yarn/README.md[Spark on YARN]. It is set when yarn/spark-yarn-client-yarnclientschedulerbackend.md#start[YarnClientSchedulerBackend starts] to yarn/spark-yarn-applicationmaster.md#runExecutorLauncher[run ExecutorLauncher] (and yarn/spark-yarn-applicationmaster.md#registerAM[register ApplicationMaster] for the Spark application). | [[spark_driver_libraryPath]] spark.driver.libraryPath | | |=== ==== [[spark_driver_extraClassPath]] spark.driver.extraClassPath spark.driver.extraClassPath system property sets the additional classpath entries (e.g. jars and directories) that should be added to the driver's classpath in spark-deploy-mode.md#cluster[ cluster deploy mode].","title":"Driver"},{"location":"driver/#note","text":"For spark-deploy-mode.md#client[ client deploy mode] you can use a properties file or command line to set spark.driver.extraClassPath . Do not use ROOT:SparkConf.md[SparkConf] since it is too late for client deploy mode given the JVM has already been set up to start a Spark application.","title":"[NOTE]"},{"location":"driver/#refer-to-spark-classmdbuildsparksubmitcommandbuildsparksubmitcommand-internal-method-for-the-very-low-level-details-of-how-it-is-handled-internally","text":"spark.driver.extraClassPath uses a OS-specific path separator. NOTE: Use spark-submit 's spark-submit.md#driver-class-path[ --driver-class-path command-line option] on command line to override spark.driver.extraClassPath from a spark-properties.md#spark-defaults-conf[Spark properties file].","title":"Refer to spark-class.md#buildSparkSubmitCommand[buildSparkSubmitCommand Internal Method] for the very low-level details of how it is handled internally."},{"location":"master/","text":"== Master A master is a running Spark instance that connects to a cluster manager for resources. The master acquires cluster nodes to run executors. CAUTION: FIXME Add it to the Spark architecture figure above.","title":"Master"},{"location":"overview/","text":"Apache Spark \u00b6 Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run rdd:index.md[(lazy) transformations] of these RDDs to some other form (shape), and finally perform rdd:index.md[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called rdd:index.md[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers spark-shell.md[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use spark-standalone.md[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via spark-streaming/spark-streaming.md[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , varia/spark-hadoop.md[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or spark-shell.md[the Spark shell], or the many Spark Application Frameworks leveraging the concept of rdd:index.md[RDD], i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (spark-mllib/spark-mllib.md[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using spark-shell.md[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any spark-cluster.md[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, spark-streaming/spark-streaming.md[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or spark-streaming/spark-streaming.md[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with spark-sql-thrift-server.md[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a scheduler:DAGScheduler.md[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can spark-rdd-caching.md[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was varia/spark-hadoop.md[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like varia/spark-hadoop.md[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so rdd:index.md[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Overview"},{"location":"overview/#apache-spark","text":"Apache Spark is an open-source distributed general-purpose cluster computing framework with (mostly) in-memory data processing engine that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich concise high-level APIs for the programming languages: Scala, Python, Java, R, and SQL. You could also describe Spark as a distributed, data processing engine for batch and streaming modes featuring SQL queries, graph processing, and machine learning. In contrast to Hadoop\u2019s two-stage disk-based MapReduce computation engine, Spark's multi-stage (mostly) in-memory computing engine allows for running most computations in memory, and hence most of the time provides better performance for certain applications, e.g. iterative algorithms or interactive data mining (read Spark officially sets a new record in large-scale sorting ). Spark aims at speed, ease of use, extensibility and interactive analytics. Spark is a distributed platform for executing complex multi-stage applications , like machine learning algorithms , and interactive ad hoc queries . Spark provides an efficient abstraction for in-memory cluster computing called Resilient Distributed Dataset . Using Spark Application Frameworks, Spark simplifies access to machine learning and predictive analytics at scale. Spark is mainly written in http://scala-lang.org/[Scala ], but provides developer API for languages like Java, Python, and R. If you have large amounts of data that requires low latency processing that a typical MapReduce program cannot provide, Spark is a viable alternative. Access any data type across any data source. Huge demand for storage and data processing. The Apache Spark project is an umbrella for https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] (with Datasets), https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[streaming ], http://spark.apache.org/mllib/[machine learning] (pipelines) and http://spark.apache.org/graphx/[graph ] processing engines built on top of the Spark Core. You can run them all in a single application using a consistent API. Spark runs locally as well as in clusters, on-premises or in cloud. It runs on top of Hadoop YARN, Apache Mesos, standalone or in the cloud (Amazon EC2 or IBM Bluemix). Apache Spark's https://jaceklaskowski.gitbooks.io/spark-structured-streaming/[Structured Streaming] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/[SQL ] programming models with MLlib and GraphX make it easier for developers and data scientists to build applications that exploit machine learning and graph analytics. At a high level, any Spark application creates RDDs out of some input, run rdd:index.md[(lazy) transformations] of these RDDs to some other form (shape), and finally perform rdd:index.md[actions] to collect or store data. Not much, huh? You can look at Spark from programmer's, data engineer's and administrator's point of view. And to be honest, all three types of people will spend quite a lot of their time with Spark to finally reach the point where they exploit all the available features. Programmers use language-specific APIs (and work at the level of RDDs using transformations and actions), data engineers use higher-level abstractions like DataFrames or Pipelines APIs or external tools (that connect to Spark), and finally it all can only be possible to run because administrators set up Spark clusters to deploy Spark applications to. It is Spark's goal to be a general-purpose computing platform with various specialized applications frameworks on top of a single unified engine. NOTE: When you hear \"Apache Spark\" it can be two things -- the Spark engine aka Spark Core or the Apache Spark open source project which is an \"umbrella\" term for Spark Core and the accompanying Spark Application Frameworks, i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX] that sit on top of Spark Core and the main data abstraction in Spark called rdd:index.md[RDD - Resilient Distributed Dataset]. == [[why-spark]] Why Spark Let's list a few of the many reasons for Spark. We are doing it first, and then comes the overview that lends a more technical helping hand. === Easy to Get Started Spark offers spark-shell.md[spark-shell] that makes for a very easy head start to writing and running Spark applications on the command line on your laptop. You could then use spark-standalone.md[Spark Standalone] built-in cluster manager to deploy your Spark applications to a production-grade cluster to run on a full dataset. === Unified Engine for Diverse Workloads As said by Matei Zaharia - the author of Apache Spark - in https://youtu.be/49Hr5xZyTEA[Introduction to AmpLab Spark Internals video] (quoting with few changes): One of the Spark project goals was to deliver a platform that supports a very wide array of diverse workflows - not only MapReduce batch jobs (there were available in Hadoop already at that time), but also iterative computations like graph algorithms or Machine Learning. And also different scales of workloads from sub-second interactive jobs to jobs that run for many hours. Spark combines batch, interactive, and streaming workloads under one rich concise API. Spark supports near real-time streaming workloads via spark-streaming/spark-streaming.md[Spark Streaming] application framework. ETL workloads and Analytics workloads are different, however Spark attempts to offer a unified platform for a wide variety of workloads. Graph and Machine Learning algorithms are iterative by nature and less saves to disk or transfers over network means better performance. There is also support for interactive workloads using Spark shell. You should watch the video https://youtu.be/SxAxAhn-BDU[What is Apache Spark?] by Mike Olson, Chief Strategy Officer and Co-Founder at Cloudera, who provides a very exceptional overview of Apache Spark, its rise in popularity in the open source community, and how Spark is primed to replace MapReduce as the general processing engine in Hadoop. === Leverages the Best in distributed batch data processing When you think about distributed batch data processing , varia/spark-hadoop.md[Hadoop] naturally comes to mind as a viable solution. Spark draws many ideas out of Hadoop MapReduce. They work together well - Spark on YARN and HDFS - while improving on the performance and simplicity of the distributed computing engine. For many, Spark is Hadoop++, i.e. MapReduce done in a better way. And it should not come as a surprise, without Hadoop MapReduce (its advances and deficiencies), Spark would not have been born at all. === RDD - Distributed Parallel Scala Collections As a Scala developer, you may find Spark's RDD API very similar (if not identical) to http://www.scala-lang.org/docu/files/collections-api/collections.html[Scala's Collections API]. It is also exposed in Java, Python and R (as well as SQL, i.e. SparkSQL, in a sense). So, when you have a need for distributed Collections API in Scala, Spark with RDD API should be a serious contender. === [[rich-standard-library]] Rich Standard Library Not only can you use map and reduce (as in Hadoop MapReduce jobs) in Spark, but also a vast array of other higher-level operators to ease your Spark queries and application development. It expanded on the available computation styles beyond the only map-and-reduce available in Hadoop MapReduce. === Unified development and deployment environment for all Regardless of the Spark tools you use - the Spark API for the many programming languages supported - Scala, Java, Python, R, or spark-shell.md[the Spark shell], or the many Spark Application Frameworks leveraging the concept of rdd:index.md[RDD], i.e. Spark SQL, spark-streaming/spark-streaming.md[Spark Streaming], spark-mllib/spark-mllib.md[Spark MLlib] and spark-graphx.md[Spark GraphX], you still use the same development and deployment environment to for large data sets to yield a result, be it a prediction (spark-mllib/spark-mllib.md[Spark MLlib]), a structured data queries (Spark SQL) or just a large distributed batch (Spark Core) or streaming (Spark Streaming) computation. It's also very productive of Spark that teams can exploit the different skills the team members have acquired so far. Data analysts, data scientists, Python programmers, or Java, or Scala, or R, can all use the same Spark platform using tailor-made API. It makes for bringing skilled people with their expertise in different programming languages together to a Spark project. === Interactive Exploration / Exploratory Analytics It is also called ad hoc queries . Using spark-shell.md[the Spark shell] you can execute computations to process large amount of data ( The Big Data ). It's all interactive and very useful to explore the data before final production release. Also, using the Spark shell you can access any spark-cluster.md[Spark cluster] as if it was your local machine. Just point the Spark shell to a 20-node of 10TB RAM memory in total (using --master ) and use all the components (and their abstractions) like Spark SQL, Spark MLlib, spark-streaming/spark-streaming.md[Spark Streaming], and Spark GraphX. Depending on your needs and skills, you may see a better fit for SQL vs programming APIs or apply machine learning algorithms (Spark MLlib) from data in graph data structures (Spark GraphX). === Single Environment Regardless of which programming language you are good at, be it Scala, Java, Python, R or SQL, you can use the same single clustered runtime environment for prototyping, ad hoc queries, and deploying your applications leveraging the many ingestion data points offered by the Spark platform. You can be as low-level as using RDD API directly or leverage higher-level APIs of Spark SQL (Datasets), Spark MLlib (ML Pipelines), Spark GraphX (Graphs) or spark-streaming/spark-streaming.md[Spark Streaming] (DStreams). Or use them all in a single application. The single programming model and execution engine for different kinds of workloads simplify development and deployment architectures. === Data Integration Toolkit with Rich Set of Supported Data Sources Spark can read from many types of data sources -- relational, NoSQL, file systems, etc. -- using many types of data formats - Parquet, Avro, CSV, JSON. Both, input and output data sources, allow programmers and data engineers use Spark as the platform with the large amount of data that is read from or saved to for processing, interactively (using Spark shell) or in applications. === Tools unavailable then, at your fingertips now As much and often as it's recommended http://c2.com/cgi/wiki?PickTheRightToolForTheJob[to pick the right tool for the job], it's not always feasible. Time, personal preference, operating system you work on are all factors to decide what is right at a time (and using a hammer can be a reasonable choice). Spark embraces many concepts in a single unified development and runtime environment. Machine learning that is so tool- and feature-rich in Python, e.g. SciKit library, can now be used by Scala developers (as Pipeline API in Spark MLlib or calling pipe() ). DataFrames from R are available in Scala, Java, Python, R APIs. Single node computations in machine learning algorithms are migrated to their distributed versions in Spark MLlib. This single platform gives plenty of opportunities for Python, Scala, Java, and R programmers as well as data engineers (SparkR) and scientists (using proprietary enterprise data warehouses with spark-sql-thrift-server.md[Thrift JDBC/ODBC Server] in Spark SQL). Mind the proverb https://en.wiktionary.org/wiki/if_all_you_have_is_a_hammer,_everything_looks_like_a_nail[if all you have is a hammer, everything looks like a nail], too. === Low-level Optimizations Apache Spark uses a scheduler:DAGScheduler.md[directed acyclic graph (DAG) of computation stages] (aka execution DAG ). It postpones any processing until really required for actions. Spark's lazy evaluation gives plenty of opportunities to induce low-level optimizations (so users have to know less to do more). Mind the proverb https://en.wiktionary.org/wiki/less_is_more[less is more]. === Excels at low-latency iterative workloads Spark supports diverse workloads, but successfully targets low-latency iterative ones. They are often used in Machine Learning and graph algorithms. Many Machine Learning algorithms require plenty of iterations before the result models get optimal, like logistic regression. The same applies to graph algorithms to traverse all the nodes and edges when needed. Such computations can increase their performance when the interim partial results are stored in memory or at very fast solid state drives. Spark can spark-rdd-caching.md[cache intermediate data in memory for faster model building and training]. Once the data is loaded to memory (as an initial step), reusing it multiple times incurs no performance slowdowns. Also, graph algorithms can traverse graphs one connection per iteration with the partial result in memory. Less disk access and network can make a huge difference when you need to process lots of data, esp. when it is a BIG Data. === ETL done easier Spark gives Extract, Transform and Load (ETL) a new look with the many programming languages supported - Scala, Java, Python (less likely R). You can use them all or pick the best for a problem. Scala in Spark, especially, makes for a much less boiler-plate code (comparing to other languages and approaches like MapReduce in Java). === [[unified-api]] Unified Concise High-Level API Spark offers a unified, concise, high-level APIs for batch analytics (RDD API), SQL queries (Dataset API), real-time analysis (DStream API), machine learning (ML Pipeline API) and graph processing (Graph API). Developers no longer have to learn many different processing engines and platforms, and let the time be spent on mastering framework APIs per use case (atop a single computation engine Spark). === Different kinds of data processing using unified API Spark offers three kinds of data processing using batch , interactive , and stream processing with the unified API and data structures. === Little to no disk use for better performance In the no-so-long-ago times, when the most prevalent distributed computing framework was varia/spark-hadoop.md[Hadoop MapReduce], you could reuse a data between computation (even partial ones!) only after you've written it to an external storage like varia/spark-hadoop.md[Hadoop Distributed Filesystem (HDFS)]. It can cost you a lot of time to compute even very basic multi-stage computations. It simply suffers from IO (and perhaps network) overhead. One of the many motivations to build Spark was to have a framework that is good at data reuse. Spark cuts it out in a way to keep as much data as possible in memory and keep it there until a job is finished. It doesn't matter how many stages belong to a job. What does matter is the available memory and how effective you are in using Spark API (so rdd:index.md[no shuffle occur]). The less network and disk IO, the better performance, and Spark tries hard to find ways to minimize both. === Fault Tolerance included Faults are not considered a special case in Spark, but obvious consequence of being a parallel and distributed system. Spark handles and recovers from faults by default without particularly complex logic to deal with them. === Small Codebase Invites Contributors Spark's design is fairly simple and the code that comes out of it is not huge comparing to the features it offers. The reasonably small codebase of Spark invites project contributors - programmers who extend the platform and fix bugs in a more steady pace. == [[i-want-more]] Further reading or watching (video) https://youtu.be/L029ZNBG7bk[Keynote : Spark 2.0 - Matei Zaharia, Apache Spark Creator and CTO of Databricks]","title":"Apache Spark"},{"location":"rdd-checkpointing/","text":"RDD Checkpointing \u00b6 RDD Checkpointing is a process of truncating rdd:spark-rdd-lineage.md[RDD lineage graph] and saving it to a reliable distributed (HDFS) or local file system. There are two types of checkpointing: < > - RDD checkpointing that saves the actual intermediate RDD data to a reliable distributed file system (e.g. Hadoop DFS) < > - RDD checkpointing that saves the data to a local file system It's up to a Spark application developer to decide when and how to checkpoint using RDD.checkpoint() method. Before checkpointing is used, a Spark developer has to set the checkpoint directory using SparkContext.setCheckpointDir(directory: String) method. == [[reliable-checkpointing]] Reliable Checkpointing You call SparkContext.setCheckpointDir(directory: String) to set the checkpoint directory - the directory where RDDs are checkpointed. The directory must be a HDFS path if running on a cluster. The reason is that the driver may attempt to reconstruct the checkpointed RDD from its own local file system, which is incorrect because the checkpoint files are actually on the executor machines. You mark an RDD for checkpointing by calling RDD.checkpoint() . The RDD will be saved to a file inside the checkpoint directory and all references to its parent RDDs will be removed. This function has to be called before any job has been executed on this RDD. NOTE: It is strongly recommended that a checkpointed RDD is persisted in memory, otherwise saving it on a file will require recomputation. When an action is called on a checkpointed RDD, the following INFO message is printed out in the logs: Done checkpointing RDD 5 to [path], new parent is RDD [id] == [[local-checkpointing]] Local Checkpointing rdd:RDD.md#localCheckpoint[localCheckpoint] allows to truncate rdd:spark-rdd-lineage.md[RDD lineage graph] while skipping the expensive step of replicating the materialized data to a reliable distributed file system. This is useful for RDDs with long lineages that need to be truncated periodically, e.g. GraphX. Local checkpointing trades fault-tolerance for performance. NOTE: The checkpoint directory set through SparkContext.setCheckpointDir is not used. == [[demo]] Demo [source,plaintext] \u00b6 val rdd = sc.parallelize(0 to 9) scala> rdd.checkpoint org.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext at org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1599) ... 49 elided sc.setCheckpointDir(\"/tmp/rdd-checkpoint\") // Creates a subdirectory for this SparkContext $ ls /tmp/rdd-checkpoint/ fc21e1d1-3cd9-4d51-880f-58d1dd07f783 // Mark the RDD to checkpoint at the earliest action rdd.checkpoint scala> println(rdd.getCheckpointFile) Some(file:/tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2) scala> println(ns.id) 2 scala> println(rdd.getNumPartitions) 16 rdd.count // Check out the checkpoint directory // You should find a directory for the checkpointed RDD, e.g. rdd-2 // The number of part-000* files is exactly the number of partitions $ ls -ltra /tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2/part-000* | wc -l 16","title":"RDD Checkpointing"},{"location":"rdd-checkpointing/#rdd-checkpointing","text":"RDD Checkpointing is a process of truncating rdd:spark-rdd-lineage.md[RDD lineage graph] and saving it to a reliable distributed (HDFS) or local file system. There are two types of checkpointing: < > - RDD checkpointing that saves the actual intermediate RDD data to a reliable distributed file system (e.g. Hadoop DFS) < > - RDD checkpointing that saves the data to a local file system It's up to a Spark application developer to decide when and how to checkpoint using RDD.checkpoint() method. Before checkpointing is used, a Spark developer has to set the checkpoint directory using SparkContext.setCheckpointDir(directory: String) method. == [[reliable-checkpointing]] Reliable Checkpointing You call SparkContext.setCheckpointDir(directory: String) to set the checkpoint directory - the directory where RDDs are checkpointed. The directory must be a HDFS path if running on a cluster. The reason is that the driver may attempt to reconstruct the checkpointed RDD from its own local file system, which is incorrect because the checkpoint files are actually on the executor machines. You mark an RDD for checkpointing by calling RDD.checkpoint() . The RDD will be saved to a file inside the checkpoint directory and all references to its parent RDDs will be removed. This function has to be called before any job has been executed on this RDD. NOTE: It is strongly recommended that a checkpointed RDD is persisted in memory, otherwise saving it on a file will require recomputation. When an action is called on a checkpointed RDD, the following INFO message is printed out in the logs: Done checkpointing RDD 5 to [path], new parent is RDD [id] == [[local-checkpointing]] Local Checkpointing rdd:RDD.md#localCheckpoint[localCheckpoint] allows to truncate rdd:spark-rdd-lineage.md[RDD lineage graph] while skipping the expensive step of replicating the materialized data to a reliable distributed file system. This is useful for RDDs with long lineages that need to be truncated periodically, e.g. GraphX. Local checkpointing trades fault-tolerance for performance. NOTE: The checkpoint directory set through SparkContext.setCheckpointDir is not used. == [[demo]] Demo","title":"RDD Checkpointing"},{"location":"rdd-checkpointing/#sourceplaintext","text":"val rdd = sc.parallelize(0 to 9) scala> rdd.checkpoint org.apache.spark.SparkException: Checkpoint directory has not been set in the SparkContext at org.apache.spark.rdd.RDD.checkpoint(RDD.scala:1599) ... 49 elided sc.setCheckpointDir(\"/tmp/rdd-checkpoint\") // Creates a subdirectory for this SparkContext $ ls /tmp/rdd-checkpoint/ fc21e1d1-3cd9-4d51-880f-58d1dd07f783 // Mark the RDD to checkpoint at the earliest action rdd.checkpoint scala> println(rdd.getCheckpointFile) Some(file:/tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2) scala> println(ns.id) 2 scala> println(rdd.getNumPartitions) 16 rdd.count // Check out the checkpoint directory // You should find a directory for the checkpointed RDD, e.g. rdd-2 // The number of part-000* files is exactly the number of partitions $ ls -ltra /tmp/rdd-checkpoint/fc21e1d1-3cd9-4d51-880f-58d1dd07f783/rdd-2/part-000* | wc -l 16","title":"[source,plaintext]"},{"location":"spark-HeartbeatReceiver/","text":"HeartbeatReceiver RPC Endpoint \u00b6 [[ENDPOINT_NAME]] HeartbeatReceiver is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] registered on the driver under the name HeartbeatReceiver . HeartbeatReceiver receives < > messages from executors that Spark uses as the mechanism to receive accumulator updates (with task metrics and a Spark application's accumulators) and scheduler:TaskScheduler.md#executorHeartbeatReceived[pass them along to TaskScheduler ]. NOTE: HeartbeatReceiver is registered immediately after a Spark application is started, i.e. when SparkContext is created. HeartbeatReceiver is a ROOT:SparkListener.md[] to get notified when < > to or < > in a Spark application. HeartbeatReceiver tracks executors (in < > registry) to handle < > and < > messages from executors that are assigned to the Spark application. [[messages]] .HeartbeatReceiver RPC Endpoint's Messages (in alphabetical order) [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Message | Description | < > | Posted when HeartbeatReceiver < > (to a Spark application). | < > | Posted when HeartbeatReceiver < > (with a Spark application). | < > | FIXME | < > | Posted when Executor executor:Executor.md#reportHeartBeat[informs that it is alive and reports task metrics]. | < > | Posted when SparkContext informs that TaskScheduler is available . |=== [[internal-registries]] .HeartbeatReceiver's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[executorLastSeen]] executorLastSeen | Executor ids and the timestamps of when the last heartbeat was received. | [[scheduler]] scheduler | scheduler:TaskScheduler.md[TaskScheduler] |=== [TIP] \u00b6 Enable DEBUG or TRACE logging levels for org.apache.spark.HeartbeatReceiver to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.HeartbeatReceiver=TRACE Refer to spark-logging.md[Logging]. \u00b6 === [[creating-instance]] Creating HeartbeatReceiver Instance HeartbeatReceiver takes the following when created: [[sc]] ROOT:SparkContext.md[] [[clock]] Clock HeartbeatReceiver ROOT:SparkContext.md#addSparkListener[registers itself as a SparkListener ]. HeartbeatReceiver initializes the < >. === [[onStart]] Starting HeartbeatReceiver RPC Endpoint -- onStart Method NOTE: onStart is part of the rpc:RpcEndpoint.md[RpcEndpoint Contract] When called, HeartbeatReceiver sends a blocking < > every < > on < >. === [[ExecutorRegistered]] ExecutorRegistered [source, scala] \u00b6 ExecutorRegistered(executorId: String) \u00b6 When received, HeartbeatReceiver registers the executorId executor and the current time (in < > internal registry). NOTE: HeartbeatReceiver uses the internal < > to know the current time. === [[ExecutorRemoved]] ExecutorRemoved [source, scala] \u00b6 ExecutorRemoved(executorId: String) \u00b6 When ExecutorRemoved arrives, HeartbeatReceiver removes executorId from < > internal registry. === [[ExpireDeadHosts]] ExpireDeadHosts [source, scala] \u00b6 ExpireDeadHosts \u00b6 When ExpireDeadHosts arrives the following TRACE is printed out to the logs: TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver. Each executor (in < > registry) is checked whether the time it was last seen is not longer than < >. For any such executor, the following WARN message is printed out to the logs: WARN HeartbeatReceiver: Removing executor [executorId] with no recent heartbeats: [time] ms exceeds timeout [timeout] ms scheduler:TaskScheduler.md#executorLost[TaskScheduler.executorLost] is called (with SlaveLost(\"Executor heartbeat timed out after [timeout] ms\" ). SparkContext.killAndReplaceExecutor is asynchronously called for the executor (i.e. on < >). The executor is removed from < >. === [[Heartbeat]] Heartbeat [source, scala] \u00b6 Heartbeat( executorId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId) When received, HeartbeatReceiver finds the executorId executor (in < > registry). When the executor is found, HeartbeatReceiver updates the time the heartbeat was received (in < >). NOTE: HeartbeatReceiver uses the internal < > to know the current time. HeartbeatReceiver then submits an asynchronous task to notify TaskScheduler that the scheduler:TaskScheduler.md#executorHeartbeatReceived[heartbeat was received from the executor] (using < > internal reference). HeartbeatReceiver posts a HeartbeatResponse back to the executor (with the response from TaskScheduler whether the executor has been registered already or not so it may eventually need to re-register). If however the executor was not found (in < > registry), i.e. the executor was not registered before, you should see the following DEBUG message in the logs and the response is to notify the executor to re-register. DEBUG Received heartbeat from unknown executor [executorId] In a very rare case, when < > is not yet assigned to HeartbeatReceiver , you should see the following WARN message in the logs and the response is to notify the executor to re-register. WARN Dropping [heartbeat] because TaskScheduler is not ready yet NOTE: < > can be unassigned when no < > has not been received yet. NOTE: Heartbeats messages are the mechanism of executor:Executor.md#heartbeats-and-active-task-metrics[executors to inform the Spark application that they are alive and update about the state of active tasks]. === [[TaskSchedulerIsSet]] TaskSchedulerIsSet [source, scala] \u00b6 TaskSchedulerIsSet \u00b6 When received, HeartbeatReceiver sets the internal reference to < >. NOTE: HeartbeatReceiver uses < > that is given when HeartbeatReceiver < >. === [[onExecutorAdded]] onExecutorAdded Method [source, scala] \u00b6 onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit \u00b6 onExecutorAdded simply < ExecutorRegistered message to itself>> (that in turn registers an executor). NOTE: onExecutorAdded is part of ROOT:SparkListener.md#onExecutorAdded[SparkListener contract] to announce that a new executor was registered with a Spark application. === [[addExecutor]] Sending ExecutorRegistered Message to Itself -- addExecutor Internal Method [source, scala] \u00b6 addExecutor(executorId: String): Option[Future[Boolean]] \u00b6 addExecutor sends a < > message (to register executorId executor). NOTE: addExecutor is used when HeartbeatReceiver < >. === [[onExecutorRemoved]] onExecutorRemoved Method [source, scala] \u00b6 onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit \u00b6 onExecutorRemoved simply passes the call to < > (that in turn unregisters an executor). NOTE: onExecutorRemoved is part of ROOT:SparkListener.md#onExecutorRemoved[SparkListener contract] to announce that an executor is no longer available for a Spark application. === [[removeExecutor]] Sending ExecutorRemoved Message to Itself -- removeExecutor Method [source, scala] \u00b6 removeExecutor(executorId: String): Option[Future[Boolean]] \u00b6 removeExecutor sends a < > message to itself (passing in executorId ). NOTE: removeExecutor is used when HeartbeatReceiver < >. === [[onStop]] Stopping HeartbeatReceiver RPC Endpoint -- onStop Method NOTE: onStop is part of the rpc:index.md#RpcEndpoint[RpcEndpoint Contract] When called, HeartbeatReceiver cancels the checking task (that sends a blocking < > every < > on < > - see < >) and shuts down < > and < > executors. === [[killExecutorThread]][[kill-executor-thread]] killExecutorThread -- Kill Executor Thread killExecutorThread is a daemon https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor ] with a single thread. The name of the thread pool is kill-executor-thread . NOTE: It is used to request SparkContext to kill the executor. === [[eventLoopThread]][[heartbeat-receiver-event-loop-thread]] eventLoopThread -- Heartbeat Receiver Event Loop Thread eventLoopThread is a daemon https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor ] with a single thread. The name of the thread pool is heartbeat-receiver-event-loop-thread . === [[expireDeadHosts]] expireDeadHosts Internal Method [source, scala] \u00b6 expireDeadHosts(): Unit \u00b6 CAUTION: FIXME NOTE: expireDeadHosts is used when HeartbeatReceiver < ExpireDeadHosts message>>. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark.storage.blockManagerTimeoutIntervalMs]] spark.storage.blockManagerTimeoutIntervalMs | 60s | | [[spark_storage_blockManagerSlaveTimeoutMs]] spark.storage.blockManagerSlaveTimeoutMs | 120s | | [[spark.network.timeout]] spark.network.timeout | < > | See rpc:index.md#spark.network.timeout[spark.network.timeout] in rpc:index.md[RPC Environment (RpcEnv)] | [[spark.network.timeoutInterval]] spark.network.timeoutInterval | < > | |===","title":"HeartbeatReceiver RPC Endpoint"},{"location":"spark-HeartbeatReceiver/#heartbeatreceiver-rpc-endpoint","text":"[[ENDPOINT_NAME]] HeartbeatReceiver is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] registered on the driver under the name HeartbeatReceiver . HeartbeatReceiver receives < > messages from executors that Spark uses as the mechanism to receive accumulator updates (with task metrics and a Spark application's accumulators) and scheduler:TaskScheduler.md#executorHeartbeatReceived[pass them along to TaskScheduler ]. NOTE: HeartbeatReceiver is registered immediately after a Spark application is started, i.e. when SparkContext is created. HeartbeatReceiver is a ROOT:SparkListener.md[] to get notified when < > to or < > in a Spark application. HeartbeatReceiver tracks executors (in < > registry) to handle < > and < > messages from executors that are assigned to the Spark application. [[messages]] .HeartbeatReceiver RPC Endpoint's Messages (in alphabetical order) [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Message | Description | < > | Posted when HeartbeatReceiver < > (to a Spark application). | < > | Posted when HeartbeatReceiver < > (with a Spark application). | < > | FIXME | < > | Posted when Executor executor:Executor.md#reportHeartBeat[informs that it is alive and reports task metrics]. | < > | Posted when SparkContext informs that TaskScheduler is available . |=== [[internal-registries]] .HeartbeatReceiver's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[executorLastSeen]] executorLastSeen | Executor ids and the timestamps of when the last heartbeat was received. | [[scheduler]] scheduler | scheduler:TaskScheduler.md[TaskScheduler] |===","title":"HeartbeatReceiver RPC Endpoint"},{"location":"spark-HeartbeatReceiver/#tip","text":"Enable DEBUG or TRACE logging levels for org.apache.spark.HeartbeatReceiver to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.HeartbeatReceiver=TRACE","title":"[TIP]"},{"location":"spark-HeartbeatReceiver/#refer-to-spark-loggingmdlogging","text":"=== [[creating-instance]] Creating HeartbeatReceiver Instance HeartbeatReceiver takes the following when created: [[sc]] ROOT:SparkContext.md[] [[clock]] Clock HeartbeatReceiver ROOT:SparkContext.md#addSparkListener[registers itself as a SparkListener ]. HeartbeatReceiver initializes the < >. === [[onStart]] Starting HeartbeatReceiver RPC Endpoint -- onStart Method NOTE: onStart is part of the rpc:RpcEndpoint.md[RpcEndpoint Contract] When called, HeartbeatReceiver sends a blocking < > every < > on < >. === [[ExecutorRegistered]] ExecutorRegistered","title":"Refer to spark-logging.md[Logging]."},{"location":"spark-HeartbeatReceiver/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#executorregisteredexecutorid-string","text":"When received, HeartbeatReceiver registers the executorId executor and the current time (in < > internal registry). NOTE: HeartbeatReceiver uses the internal < > to know the current time. === [[ExecutorRemoved]] ExecutorRemoved","title":"ExecutorRegistered(executorId: String)"},{"location":"spark-HeartbeatReceiver/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#executorremovedexecutorid-string","text":"When ExecutorRemoved arrives, HeartbeatReceiver removes executorId from < > internal registry. === [[ExpireDeadHosts]] ExpireDeadHosts","title":"ExecutorRemoved(executorId: String)"},{"location":"spark-HeartbeatReceiver/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#expiredeadhosts","text":"When ExpireDeadHosts arrives the following TRACE is printed out to the logs: TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver. Each executor (in < > registry) is checked whether the time it was last seen is not longer than < >. For any such executor, the following WARN message is printed out to the logs: WARN HeartbeatReceiver: Removing executor [executorId] with no recent heartbeats: [time] ms exceeds timeout [timeout] ms scheduler:TaskScheduler.md#executorLost[TaskScheduler.executorLost] is called (with SlaveLost(\"Executor heartbeat timed out after [timeout] ms\" ). SparkContext.killAndReplaceExecutor is asynchronously called for the executor (i.e. on < >). The executor is removed from < >. === [[Heartbeat]] Heartbeat","title":"ExpireDeadHosts"},{"location":"spark-HeartbeatReceiver/#source-scala_3","text":"Heartbeat( executorId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId) When received, HeartbeatReceiver finds the executorId executor (in < > registry). When the executor is found, HeartbeatReceiver updates the time the heartbeat was received (in < >). NOTE: HeartbeatReceiver uses the internal < > to know the current time. HeartbeatReceiver then submits an asynchronous task to notify TaskScheduler that the scheduler:TaskScheduler.md#executorHeartbeatReceived[heartbeat was received from the executor] (using < > internal reference). HeartbeatReceiver posts a HeartbeatResponse back to the executor (with the response from TaskScheduler whether the executor has been registered already or not so it may eventually need to re-register). If however the executor was not found (in < > registry), i.e. the executor was not registered before, you should see the following DEBUG message in the logs and the response is to notify the executor to re-register. DEBUG Received heartbeat from unknown executor [executorId] In a very rare case, when < > is not yet assigned to HeartbeatReceiver , you should see the following WARN message in the logs and the response is to notify the executor to re-register. WARN Dropping [heartbeat] because TaskScheduler is not ready yet NOTE: < > can be unassigned when no < > has not been received yet. NOTE: Heartbeats messages are the mechanism of executor:Executor.md#heartbeats-and-active-task-metrics[executors to inform the Spark application that they are alive and update about the state of active tasks]. === [[TaskSchedulerIsSet]] TaskSchedulerIsSet","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#taskschedulerisset","text":"When received, HeartbeatReceiver sets the internal reference to < >. NOTE: HeartbeatReceiver uses < > that is given when HeartbeatReceiver < >. === [[onExecutorAdded]] onExecutorAdded Method","title":"TaskSchedulerIsSet"},{"location":"spark-HeartbeatReceiver/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#onexecutoraddedexecutoradded-sparklistenerexecutoradded-unit","text":"onExecutorAdded simply < ExecutorRegistered message to itself>> (that in turn registers an executor). NOTE: onExecutorAdded is part of ROOT:SparkListener.md#onExecutorAdded[SparkListener contract] to announce that a new executor was registered with a Spark application. === [[addExecutor]] Sending ExecutorRegistered Message to Itself -- addExecutor Internal Method","title":"onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit"},{"location":"spark-HeartbeatReceiver/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#addexecutorexecutorid-string-optionfutureboolean","text":"addExecutor sends a < > message (to register executorId executor). NOTE: addExecutor is used when HeartbeatReceiver < >. === [[onExecutorRemoved]] onExecutorRemoved Method","title":"addExecutor(executorId: String): Option[Future[Boolean]]"},{"location":"spark-HeartbeatReceiver/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#onexecutorremovedexecutorremoved-sparklistenerexecutorremoved-unit","text":"onExecutorRemoved simply passes the call to < > (that in turn unregisters an executor). NOTE: onExecutorRemoved is part of ROOT:SparkListener.md#onExecutorRemoved[SparkListener contract] to announce that an executor is no longer available for a Spark application. === [[removeExecutor]] Sending ExecutorRemoved Message to Itself -- removeExecutor Method","title":"onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit"},{"location":"spark-HeartbeatReceiver/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#removeexecutorexecutorid-string-optionfutureboolean","text":"removeExecutor sends a < > message to itself (passing in executorId ). NOTE: removeExecutor is used when HeartbeatReceiver < >. === [[onStop]] Stopping HeartbeatReceiver RPC Endpoint -- onStop Method NOTE: onStop is part of the rpc:index.md#RpcEndpoint[RpcEndpoint Contract] When called, HeartbeatReceiver cancels the checking task (that sends a blocking < > every < > on < > - see < >) and shuts down < > and < > executors. === [[killExecutorThread]][[kill-executor-thread]] killExecutorThread -- Kill Executor Thread killExecutorThread is a daemon https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor ] with a single thread. The name of the thread pool is kill-executor-thread . NOTE: It is used to request SparkContext to kill the executor. === [[eventLoopThread]][[heartbeat-receiver-event-loop-thread]] eventLoopThread -- Heartbeat Receiver Event Loop Thread eventLoopThread is a daemon https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor ] with a single thread. The name of the thread pool is heartbeat-receiver-event-loop-thread . === [[expireDeadHosts]] expireDeadHosts Internal Method","title":"removeExecutor(executorId: String): Option[Future[Boolean]]"},{"location":"spark-HeartbeatReceiver/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-HeartbeatReceiver/#expiredeadhosts-unit","text":"CAUTION: FIXME NOTE: expireDeadHosts is used when HeartbeatReceiver < ExpireDeadHosts message>>. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark.storage.blockManagerTimeoutIntervalMs]] spark.storage.blockManagerTimeoutIntervalMs | 60s | | [[spark_storage_blockManagerSlaveTimeoutMs]] spark.storage.blockManagerSlaveTimeoutMs | 120s | | [[spark.network.timeout]] spark.network.timeout | < > | See rpc:index.md#spark.network.timeout[spark.network.timeout] in rpc:index.md[RPC Environment (RpcEnv)] | [[spark.network.timeoutInterval]] spark.network.timeoutInterval | < > | |===","title":"expireDeadHosts(): Unit"},{"location":"spark-building-from-sources/","text":"== Building Apache Spark from Sources You can download pre-packaged versions of Apache Spark from http://spark.apache.org/downloads.html[the project's web site]. The packages are built for a different Hadoop versions for Scala 2.11. NOTE: Since https://github.com/apache/spark/commit/289373b28cd2546165187de2e6a9185a1257b1e7[[SPARK-6363 ][BUILD] Make Scala 2.11 the default Scala version] the default version of Scala in Apache Spark is 2.11 . The build process for Scala 2.11 takes less than 15 minutes (on a decent machine like my shiny MacBook Pro with 8 cores and 16 GB RAM) and is so simple that it's unlikely to refuse the urge to do it yourself. You can use < > or < > as the build command. === [[sbt]] Using sbt as the build tool The build command with sbt as the build tool is as follows: ./build/sbt -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean assembly Using Java 8 to build Spark using sbt takes ca 10 minutes. \u279c spark git:(master) \u2717 ./build/sbt -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean assembly ... [success] Total time: 496 s, completed Dec 7, 2015 8:24:41 PM === [[profiles]] Build Profiles CAUTION: FIXME Describe yarn profile and others ==== [[hive-thriftserver]] hive-thriftserver Maven profile for Spark Thrift Server CAUTION: FIXME TIP: Read spark-sql-thrift-server.md[Thrift JDBC/ODBC Server -- Spark Thrift Server (STS)]. === [[maven]] Using Apache Maven as the build tool The build command with Apache Maven is as follows: $ ./build/mvn -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean install After a couple of minutes your freshly baked distro is ready to fly! I'm using Oracle Java 8 to build Spark. \u279c spark git:(master) \u2717 java -version java version \"1.8.0_102\" Java(TM) SE Runtime Environment (build 1.8.0_102-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode) \u279c spark git:(master) \u2717 ./build/mvn -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests clean install Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 Using `mvn` from path: /usr/local/bin/mvn Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 [INFO] Scanning for projects... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Build Order: [INFO] [INFO] Spark Project Parent POM [INFO] Spark Project Tags [INFO] Spark Project Sketch [INFO] Spark Project Networking [INFO] Spark Project Shuffle Streaming Service [INFO] Spark Project Unsafe [INFO] Spark Project Launcher [INFO] Spark Project Core [INFO] Spark Project GraphX [INFO] Spark Project Streaming [INFO] Spark Project Catalyst [INFO] Spark Project SQL [INFO] Spark Project ML Local Library [INFO] Spark Project ML Library [INFO] Spark Project Tools [INFO] Spark Project Hive [INFO] Spark Project REPL [INFO] Spark Project YARN Shuffle Service [INFO] Spark Project YARN [INFO] Spark Project Hive Thrift Server [INFO] Spark Project Assembly [INFO] Spark Project External Flume Sink [INFO] Spark Project External Flume [INFO] Spark Project External Flume Assembly [INFO] Spark Integration for Kafka 0.8 [INFO] Spark Project Examples [INFO] Spark Project External Kafka Assembly [INFO] Spark Integration for Kafka 0.10 [INFO] Spark Integration for Kafka 0.10 Assembly [INFO] Spark Project Java 8 Tests [INFO] [INFO] ------------------------------------------------------------------------ [INFO] Building Spark Project Parent POM 2.0.0-SNAPSHOT [INFO] ------------------------------------------------------------------------ ... [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM ........................... SUCCESS [ 4.186 s] [INFO] Spark Project Tags ................................. SUCCESS [ 4.893 s] [INFO] Spark Project Sketch ............................... SUCCESS [ 5.066 s] [INFO] Spark Project Networking ........................... SUCCESS [ 11.108 s] [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [ 7.051 s] [INFO] Spark Project Unsafe ............................... SUCCESS [ 7.650 s] [INFO] Spark Project Launcher ............................. SUCCESS [ 9.905 s] [INFO] Spark Project Core ................................. SUCCESS [02:09 min] [INFO] Spark Project GraphX ............................... SUCCESS [ 19.317 s] [INFO] Spark Project Streaming ............................ SUCCESS [ 42.077 s] [INFO] Spark Project Catalyst ............................. SUCCESS [01:32 min] [INFO] Spark Project SQL .................................. SUCCESS [01:47 min] [INFO] Spark Project ML Local Library ..................... SUCCESS [ 10.049 s] [INFO] Spark Project ML Library ........................... SUCCESS [01:36 min] [INFO] Spark Project Tools ................................ SUCCESS [ 3.520 s] [INFO] Spark Project Hive ................................. SUCCESS [ 52.528 s] [INFO] Spark Project REPL ................................. SUCCESS [ 7.243 s] [INFO] Spark Project YARN Shuffle Service ................. SUCCESS [ 7.898 s] [INFO] Spark Project YARN ................................. SUCCESS [ 15.380 s] [INFO] Spark Project Hive Thrift Server ................... SUCCESS [ 24.876 s] [INFO] Spark Project Assembly ............................. SUCCESS [ 2.971 s] [INFO] Spark Project External Flume Sink .................. SUCCESS [ 7.377 s] [INFO] Spark Project External Flume ....................... SUCCESS [ 10.752 s] [INFO] Spark Project External Flume Assembly .............. SUCCESS [ 1.695 s] [INFO] Spark Integration for Kafka 0.8 .................... SUCCESS [ 13.013 s] [INFO] Spark Project Examples ............................. SUCCESS [ 31.728 s] [INFO] Spark Project External Kafka Assembly .............. SUCCESS [ 3.472 s] [INFO] Spark Integration for Kafka 0.10 ................... SUCCESS [ 12.297 s] [INFO] Spark Integration for Kafka 0.10 Assembly .......... SUCCESS [ 3.789 s] [INFO] Spark Project Java 8 Tests ......................... SUCCESS [ 4.267 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 12:29 min [INFO] Finished at: 2016-07-07T22:29:56+02:00 [INFO] Final Memory: 110M/913M [INFO] ------------------------------------------------------------------------ Please note the messages that say the version of Spark ( Building Spark Project Parent POM 2.0.0-SNAPSHOT ), Scala version ( maven-clean-plugin:2.6.1:clean (default-clean) @ spark-parent_2.11 ) and the Spark modules built. The above command gives you the latest version of Apache Spark 2.0.0-SNAPSHOT built for Scala 2.11.8 (see https://github.com/apache/spark/blob/master/pom.xml#L2640-L2674[the configuration of scala-2.11 profile]). TIP: You can also know the version of Spark using ./bin/spark-shell --version . === [[make-distribution]] Making Distribution ./make-distribution.sh is the shell script to make a distribution. It uses the same profiles as for sbt and Maven. Use --tgz option to have a tar gz version of the Spark distribution. \u279c spark git:(master) \u2717 ./make-distribution.sh --tgz -Phadoop-2.7,yarn,mesos,hive,hive-thriftserver -DskipTests Once finished, you will have the distribution in the current directory, i.e. spark-2.0.0-SNAPSHOT-bin-2.7.2.tgz .","title":"Building from Sources"},{"location":"spark-debugging/","text":"== Debugging Spark === Using spark-shell and IntelliJ IDEA Start spark-shell with SPARK_SUBMIT_OPTS environment variable that configures the JVM's JDWP. SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" ./bin/spark-shell Attach IntelliJ IDEA to the JVM process using Run > Attach to Local Process menu. === Using sbt Use sbt -jvm-debug 5005 , connect to the remote JVM at the port 5005 using IntelliJ IDEA, place breakpoints on the desired lines of the source code of Spark. \u279c sparkme-app sbt -jvm-debug 5005 Listening for transport dt_socket at address: 5005 ... Run Spark context and the breakpoints get triggered. scala> val sc = new SparkContext(conf) 15/11/14 22:58:46 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT TIP: Read https://www.jetbrains.com/idea/help/debugging-2.html[Debugging ] chapter in IntelliJ IDEA 15.0 Help.","title":"spark-debugging"},{"location":"spark-deploy-mode/","text":"== Deploy Mode Deploy mode specifies the location of where spark-driver.md[driver] executes in the spark-deployment-environments.md[deployment environment]. Deploy mode can be one of the following options: client (default) - the driver runs on the machine that the Spark application was launched. cluster - the driver runs on a random node in a cluster. NOTE: cluster deploy mode is only available for spark-cluster.md[non-local cluster deployments]. You can control the deploy mode of a Spark application using spark-submit.md#deploy-mode[spark-submit's --deploy-mode command-line option] or < spark.submit.deployMode Spark property>>. NOTE: spark.submit.deployMode setting can be client or cluster . === [[client]] Client Deploy Mode CAUTION: FIXME === [[cluster]] Cluster Deploy Mode CAUTION: FIXME === [[spark.submit.deployMode]] spark.submit.deployMode spark.submit.deployMode (default: client ) can be client or cluster .","title":"Deploy Mode"},{"location":"spark-deployment-environments/","text":"Deployment Environments \u00b6 Spark Deployment Environments ( Run Modes ): local/spark-local.md[local] spark-cluster.md[clustered] ** spark-standalone.md[Spark Standalone] ** Spark on Apache Mesos ** yarn/README.md[Spark on Hadoop YARN] A Spark application is composed of the driver and executors that can run locally (on a single JVM) or using cluster resources (like CPU, RAM and disk that are managed by a cluster manager). NOTE: You can specify where to run the driver using the spark-deploy-mode.md[deploy mode] (using --deploy-mode option of spark-submit or spark.submit.deployMode Spark property). == [[master-urls]] Master URLs Spark supports the following master URLs (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2583-L2592[private object SparkMasterRegex]): local , local[N] and local[{asterisk}] for local/spark-local.md#masterURL[Spark local] local[N, maxRetries] for local/spark-local.md#masterURL[Spark local-with-retries] local-cluster[N, cores, memory] for simulating a Spark cluster of N executors (threads), cores CPUs and memory locally (aka Spark local-cluster ) spark://host:port,host1:port1,... for connecting to spark-standalone.md[Spark Standalone cluster(s)] mesos:// for spark-mesos/spark-mesos.md[Spark on Mesos cluster] yarn for yarn/README.md[Spark on YARN] You can specify the master URL of a Spark application as follows: spark-submit.md[spark-submit's --master command-line option], ROOT:SparkConf.md#spark.master[ spark.master Spark property], When creating a ROOT:SparkContext.md#getOrCreate[ SparkContext (using setMaster method)], When creating a spark-sql-sparksession-builder.md[ SparkSession (using master method of the builder interface)].","title":"Deployment Environments"},{"location":"spark-deployment-environments/#deployment-environments","text":"Spark Deployment Environments ( Run Modes ): local/spark-local.md[local] spark-cluster.md[clustered] ** spark-standalone.md[Spark Standalone] ** Spark on Apache Mesos ** yarn/README.md[Spark on Hadoop YARN] A Spark application is composed of the driver and executors that can run locally (on a single JVM) or using cluster resources (like CPU, RAM and disk that are managed by a cluster manager). NOTE: You can specify where to run the driver using the spark-deploy-mode.md[deploy mode] (using --deploy-mode option of spark-submit or spark.submit.deployMode Spark property). == [[master-urls]] Master URLs Spark supports the following master URLs (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2583-L2592[private object SparkMasterRegex]): local , local[N] and local[{asterisk}] for local/spark-local.md#masterURL[Spark local] local[N, maxRetries] for local/spark-local.md#masterURL[Spark local-with-retries] local-cluster[N, cores, memory] for simulating a Spark cluster of N executors (threads), cores CPUs and memory locally (aka Spark local-cluster ) spark://host:port,host1:port1,... for connecting to spark-standalone.md[Spark Standalone cluster(s)] mesos:// for spark-mesos/spark-mesos.md[Spark on Mesos cluster] yarn for yarn/README.md[Spark on YARN] You can specify the master URL of a Spark application as follows: spark-submit.md[spark-submit's --master command-line option], ROOT:SparkConf.md#spark.master[ spark.master Spark property], When creating a ROOT:SparkContext.md#getOrCreate[ SparkContext (using setMaster method)], When creating a spark-sql-sparksession-builder.md[ SparkSession (using master method of the builder interface)].","title":"Deployment Environments"},{"location":"spark-dynamic-allocation/","text":"Dynamic Allocation of Executors \u00b6 Dynamic Allocation (of Executors) (aka Elastic Scaling ) is a Spark feature that allows for adding or removing executor:Executor.md[Spark executors] dynamically to match the workload. Unlike the \"traditional\" static allocation where a Spark application reserves CPU and memory resources upfront (irrespective of how much it may eventually use), in dynamic allocation you get as much as needed and no more. It scales the number of executors up and down based on workload, i.e. idle executors are removed, and when there are pending tasks waiting for executors to be launched on, dynamic allocation requests them. Dynamic allocation is enabled using < > setting. When enabled, it is assumed that the deploy:ExternalShuffleService.md[External Shuffle Service] is also used (it is not by default as controlled by ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] property). ExecutorAllocationManager is responsible for dynamic allocation of executors. Dynamic allocation reports the current state using spark-service-ExecutorAllocationManagerSource.md[ ExecutorAllocationManager metric source]. Dynamic Allocation comes with the policy of scaling executors up and down as follows: Scale Up Policy requests new executors when there are pending tasks and increases the number of executors exponentially since executors start slow and Spark application may need slightly more. Scale Down Policy removes executors that have been idle for < > seconds. Dynamic allocation is available for all the currently-supported spark-cluster.md[cluster managers], i.e. Spark Standalone, Hadoop YARN and Apache Mesos. TIP: Read about deploy:ExternalShuffleService.md[Dynamic Allocation on Hadoop YARN]. TIP: Review the excellent slide deck http://www.slideshare.net/databricks/dynamic-allocation-in-spark[Dynamic Allocation in Spark] from Databricks. == [[isDynamicAllocationEnabled]] Is Dynamic Allocation Enabled? -- Utils.isDynamicAllocationEnabled Method [source, scala] \u00b6 isDynamicAllocationEnabled(conf: SparkConf): Boolean \u00b6 isDynamicAllocationEnabled returns true if all the following conditions hold: . < > is enabled (i.e. true ) . spark-cluster.md[Spark on cluster] is used (i.e. ROOT:configuration-properties.md#spark.master[spark.master] is non- local ) . < > is enabled (i.e. true ) Otherwise, isDynamicAllocationEnabled returns false . NOTE: isDynamicAllocationEnabled returns true , i.e. dynamic allocation is enabled, in local/spark-local.md[Spark local (pseudo-cluster)] for testing only (with < > enabled). NOTE: isDynamicAllocationEnabled is used when Spark calculates the initial number of executors for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] for yarn/README.md#getInitialTargetExecutorNumber[YARN], spark-standalone-StandaloneSchedulerBackend.md#start[Spark Standalone], and spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.md#executorLimitOption[Mesos]. It is also used for spark-streaming/spark-streaming-streamingcontext.md#validate[Spark Streaming]. [TIP] \u00b6 Enable WARN logging level for org.apache.spark.util.Utils logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.Utils=WARN Refer to spark-logging.md[Logging]. \u00b6 == [[programmable-dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers a ROOT:SparkContext.md#dynamic-allocation[developer API to scale executors up or down]. == [[getDynamicAllocationInitialExecutors]] Getting Initial Number of Executors for Dynamic Allocation -- Utils.getDynamicAllocationInitialExecutors Method [source, scala] \u00b6 getDynamicAllocationInitialExecutors(conf: SparkConf): Int \u00b6 getDynamicAllocationInitialExecutors first makes sure that < > is equal or greater than < >. NOTE: < > falls back to < > if not set. Why to print the WARN message to the logs? If not, you should see the following WARN message in the logs: [options=\"wrap\"] \u00b6 spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs. \u00b6 getDynamicAllocationInitialExecutors makes sure that executor:Executor.md#spark.executor.instances[spark.executor.instances] is greater than < >. NOTE: Both executor:Executor.md#spark.executor.instances[spark.executor.instances] and < > fall back to 0 when no defined explicitly. If not, you should see the following WARN message in the logs: [options=\"wrap\"] \u00b6 spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs. \u00b6 getDynamicAllocationInitialExecutors sets the initial number of executors to be the maximum of: < > < > executor:Executor.md#spark.executor.instances[spark.executor.instances] 0 You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 Using initial executors = [initialExecutors], max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances \u00b6 NOTE: getDynamicAllocationInitialExecutors is used when spark-ExecutorAllocationManager.md#initialNumExecutors[ ExecutorAllocationManager sets the initial number of executors] and yarn/spark-yarn-YarnSparkHadoopUtil.md#getInitialTargetExecutorNumber[in YARN to set initial target number of executors]. == [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark.dynamicAllocation.enabled]] spark.dynamicAllocation.enabled | false | Flag to enable ( true ) or disable ( false ) dynamic allocation. NOTE: executor:Executor.md#spark.executor.instances[spark.executor.instances] setting can be set using spark-submit.md#command-line-options[ --num-executors command-line option] of spark-submit.md[spark-submit]. | [[spark.dynamicAllocation.initialExecutors]] spark.dynamicAllocation.initialExecutors | < > | Initial number of executors for dynamic allocation. NOTE: < > warns when spark.dynamicAllocation.initialExecutors is less than < >. | [[spark.dynamicAllocation.minExecutors]] spark.dynamicAllocation.minExecutors | 0 | Minimum number of executors for dynamic allocation. spark-ExecutorAllocationManager.md#validateSettings[Must be positive and less than or equal to spark.dynamicAllocation.maxExecutors ]. | [[spark.dynamicAllocation.maxExecutors]] spark.dynamicAllocation.maxExecutors | Integer.MAX_VALUE | Maximum number of executors for dynamic allocation. spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 and greater than or equal to spark.dynamicAllocation.minExecutors ]. | [[spark.dynamicAllocation.schedulerBacklogTimeout]] spark.dynamicAllocation.schedulerBacklogTimeout | 1s | spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 ]. | [[spark.dynamicAllocation.sustainedSchedulerBacklogTimeout]] spark.dynamicAllocation.sustainedSchedulerBacklogTimeout | < >) | spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 ]. | [[spark.dynamicAllocation.executorIdleTimeout]] spark.dynamicAllocation.executorIdleTimeout | 60s | Time for how long an executor can be idle before it gets removed. spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 ]. | [[spark.dynamicAllocation.cachedExecutorIdleTimeout]] spark.dynamicAllocation.cachedExecutorIdleTimeout | Integer.MAX_VALUE | [[spark.dynamicAllocation.testing]] spark.dynamicAllocation.testing |=== === Future SPARK-4922 SPARK-4751 SPARK-7955","title":"Dynamic Allocation of Executors"},{"location":"spark-dynamic-allocation/#dynamic-allocation-of-executors","text":"Dynamic Allocation (of Executors) (aka Elastic Scaling ) is a Spark feature that allows for adding or removing executor:Executor.md[Spark executors] dynamically to match the workload. Unlike the \"traditional\" static allocation where a Spark application reserves CPU and memory resources upfront (irrespective of how much it may eventually use), in dynamic allocation you get as much as needed and no more. It scales the number of executors up and down based on workload, i.e. idle executors are removed, and when there are pending tasks waiting for executors to be launched on, dynamic allocation requests them. Dynamic allocation is enabled using < > setting. When enabled, it is assumed that the deploy:ExternalShuffleService.md[External Shuffle Service] is also used (it is not by default as controlled by ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] property). ExecutorAllocationManager is responsible for dynamic allocation of executors. Dynamic allocation reports the current state using spark-service-ExecutorAllocationManagerSource.md[ ExecutorAllocationManager metric source]. Dynamic Allocation comes with the policy of scaling executors up and down as follows: Scale Up Policy requests new executors when there are pending tasks and increases the number of executors exponentially since executors start slow and Spark application may need slightly more. Scale Down Policy removes executors that have been idle for < > seconds. Dynamic allocation is available for all the currently-supported spark-cluster.md[cluster managers], i.e. Spark Standalone, Hadoop YARN and Apache Mesos. TIP: Read about deploy:ExternalShuffleService.md[Dynamic Allocation on Hadoop YARN]. TIP: Review the excellent slide deck http://www.slideshare.net/databricks/dynamic-allocation-in-spark[Dynamic Allocation in Spark] from Databricks. == [[isDynamicAllocationEnabled]] Is Dynamic Allocation Enabled? -- Utils.isDynamicAllocationEnabled Method","title":"Dynamic Allocation of Executors"},{"location":"spark-dynamic-allocation/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-dynamic-allocation/#isdynamicallocationenabledconf-sparkconf-boolean","text":"isDynamicAllocationEnabled returns true if all the following conditions hold: . < > is enabled (i.e. true ) . spark-cluster.md[Spark on cluster] is used (i.e. ROOT:configuration-properties.md#spark.master[spark.master] is non- local ) . < > is enabled (i.e. true ) Otherwise, isDynamicAllocationEnabled returns false . NOTE: isDynamicAllocationEnabled returns true , i.e. dynamic allocation is enabled, in local/spark-local.md[Spark local (pseudo-cluster)] for testing only (with < > enabled). NOTE: isDynamicAllocationEnabled is used when Spark calculates the initial number of executors for scheduler:CoarseGrainedSchedulerBackend.md[coarse-grained scheduler backends] for yarn/README.md#getInitialTargetExecutorNumber[YARN], spark-standalone-StandaloneSchedulerBackend.md#start[Spark Standalone], and spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.md#executorLimitOption[Mesos]. It is also used for spark-streaming/spark-streaming-streamingcontext.md#validate[Spark Streaming].","title":"isDynamicAllocationEnabled(conf: SparkConf): Boolean"},{"location":"spark-dynamic-allocation/#tip","text":"Enable WARN logging level for org.apache.spark.util.Utils logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.util.Utils=WARN","title":"[TIP]"},{"location":"spark-dynamic-allocation/#refer-to-spark-loggingmdlogging","text":"== [[programmable-dynamic-allocation]] Programmable Dynamic Allocation SparkContext offers a ROOT:SparkContext.md#dynamic-allocation[developer API to scale executors up or down]. == [[getDynamicAllocationInitialExecutors]] Getting Initial Number of Executors for Dynamic Allocation -- Utils.getDynamicAllocationInitialExecutors Method","title":"Refer to spark-logging.md[Logging]."},{"location":"spark-dynamic-allocation/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-dynamic-allocation/#getdynamicallocationinitialexecutorsconf-sparkconf-int","text":"getDynamicAllocationInitialExecutors first makes sure that < > is equal or greater than < >. NOTE: < > falls back to < > if not set. Why to print the WARN message to the logs? If not, you should see the following WARN message in the logs:","title":"getDynamicAllocationInitialExecutors(conf: SparkConf): Int"},{"location":"spark-dynamic-allocation/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"spark-dynamic-allocation/#sparkdynamicallocationinitialexecutors-less-than-sparkdynamicallocationminexecutors-is-invalid-ignoring-its-setting-please-update-your-configs","text":"getDynamicAllocationInitialExecutors makes sure that executor:Executor.md#spark.executor.instances[spark.executor.instances] is greater than < >. NOTE: Both executor:Executor.md#spark.executor.instances[spark.executor.instances] and < > fall back to 0 when no defined explicitly. If not, you should see the following WARN message in the logs:","title":"spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs."},{"location":"spark-dynamic-allocation/#optionswrap_1","text":"","title":"[options=\"wrap\"]"},{"location":"spark-dynamic-allocation/#sparkexecutorinstances-less-than-sparkdynamicallocationminexecutors-is-invalid-ignoring-its-setting-please-update-your-configs","text":"getDynamicAllocationInitialExecutors sets the initial number of executors to be the maximum of: < > < > executor:Executor.md#spark.executor.instances[spark.executor.instances] 0 You should see the following INFO message in the logs:","title":"spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs."},{"location":"spark-dynamic-allocation/#optionswrap_2","text":"","title":"[options=\"wrap\"]"},{"location":"spark-dynamic-allocation/#using-initial-executors-initialexecutors-max-of-sparkdynamicallocationinitialexecutors-sparkdynamicallocationminexecutors-and-sparkexecutorinstances","text":"NOTE: getDynamicAllocationInitialExecutors is used when spark-ExecutorAllocationManager.md#initialNumExecutors[ ExecutorAllocationManager sets the initial number of executors] and yarn/spark-yarn-YarnSparkHadoopUtil.md#getInitialTargetExecutorNumber[in YARN to set initial target number of executors]. == [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark.dynamicAllocation.enabled]] spark.dynamicAllocation.enabled | false | Flag to enable ( true ) or disable ( false ) dynamic allocation. NOTE: executor:Executor.md#spark.executor.instances[spark.executor.instances] setting can be set using spark-submit.md#command-line-options[ --num-executors command-line option] of spark-submit.md[spark-submit]. | [[spark.dynamicAllocation.initialExecutors]] spark.dynamicAllocation.initialExecutors | < > | Initial number of executors for dynamic allocation. NOTE: < > warns when spark.dynamicAllocation.initialExecutors is less than < >. | [[spark.dynamicAllocation.minExecutors]] spark.dynamicAllocation.minExecutors | 0 | Minimum number of executors for dynamic allocation. spark-ExecutorAllocationManager.md#validateSettings[Must be positive and less than or equal to spark.dynamicAllocation.maxExecutors ]. | [[spark.dynamicAllocation.maxExecutors]] spark.dynamicAllocation.maxExecutors | Integer.MAX_VALUE | Maximum number of executors for dynamic allocation. spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 and greater than or equal to spark.dynamicAllocation.minExecutors ]. | [[spark.dynamicAllocation.schedulerBacklogTimeout]] spark.dynamicAllocation.schedulerBacklogTimeout | 1s | spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 ]. | [[spark.dynamicAllocation.sustainedSchedulerBacklogTimeout]] spark.dynamicAllocation.sustainedSchedulerBacklogTimeout | < >) | spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 ]. | [[spark.dynamicAllocation.executorIdleTimeout]] spark.dynamicAllocation.executorIdleTimeout | 60s | Time for how long an executor can be idle before it gets removed. spark-ExecutorAllocationManager.md#validateSettings[Must be greater than 0 ]. | [[spark.dynamicAllocation.cachedExecutorIdleTimeout]] spark.dynamicAllocation.cachedExecutorIdleTimeout | Integer.MAX_VALUE | [[spark.dynamicAllocation.testing]] spark.dynamicAllocation.testing |=== === Future SPARK-4922 SPARK-4751 SPARK-7955","title":"Using initial executors = [initialExecutors], max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances"},{"location":"spark-logging/","text":"Logging \u00b6 Spark uses log4j for logging. Logging Levels \u00b6 The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data) conf/log4j.properties \u00b6 You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point. Setting Default Log Level Programatically \u00b6 Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core . Setting Log Levels in Spark Applications \u00b6 In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF) sbt \u00b6 When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties Disabling Logging \u00b6 Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Logging"},{"location":"spark-logging/#logging","text":"Spark uses log4j for logging.","title":"Logging"},{"location":"spark-logging/#logging-levels","text":"The valid logging levels are log4j's Levels (from most specific to least): OFF (most specific, no logging) FATAL (most specific, little data) ERROR WARN INFO DEBUG TRACE (least specific, a lot of data) ALL (least specific, all data)","title":" Logging Levels"},{"location":"spark-logging/#conflog4jproperties","text":"You can set up the default logging for Spark shell in conf/log4j.properties . Use conf/log4j.properties.template as a starting point.","title":"conf/log4j.properties"},{"location":"spark-logging/#setting-default-log-level-programatically","text":"Refer to Setting Default Log Level Programatically in SparkContext -- Entry Point to Spark Core .","title":" Setting Default Log Level Programatically"},{"location":"spark-logging/#setting-log-levels-in-spark-applications","text":"In standalone Spark applications or while in Spark Shell session, use the following: import org.apache.log4j.{Level, Logger} Logger.getLogger(classOf[RackResolver]).getLevel Logger.getLogger(\"org\").setLevel(Level.OFF) Logger.getLogger(\"akka\").setLevel(Level.OFF)","title":" Setting Log Levels in Spark Applications"},{"location":"spark-logging/#sbt","text":"When running a Spark application from within sbt using run task, you can use the following build.sbt to configure logging levels: fork in run := true javaOptions in run ++= Seq( \"-Dlog4j.debug=true\", \"-Dlog4j.configuration=log4j.properties\") outputStrategy := Some(StdoutOutput) With the above configuration log4j.properties file should be on CLASSPATH which can be in src/main/resources directory (that is included in CLASSPATH by default). When run starts, you should see the following output in sbt: [spark-activator]> run [info] Running StreamingApp log4j: Trying to find [log4j.properties] using context classloader sun.misc.Launcher$AppClassLoader@1b6d3586. log4j: Using URL [file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties] for automatic log4j configuration. log4j: Reading configuration from URL file:/Users/jacek/dev/oss/spark-activator/target/scala-2.11/classes/log4j.properties","title":"sbt"},{"location":"spark-logging/#disabling-logging","text":"Use the following conf/log4j.properties to disable logging completely: log4j.logger.org=OFF","title":"Disabling Logging"},{"location":"spark-properties/","text":"== Spark Properties and spark-defaults.conf Properties File Spark properties are the means of tuning the execution environment of a Spark application. The default Spark properties file is < $SPARK_HOME/conf/spark-defaults.conf >> that could be overriden using spark-submit with the spark-submit.md#properties-file[--properties-file] command-line option. .Environment Variables [options=\"header\",width=\"100%\"] |=== | Environment Variable | Default Value | Description | SPARK_CONF_DIR | $\\{SPARK_HOME}/conf | Spark's configuration directory (with spark-defaults.conf ) |=== TIP: Read the official documentation of Apache Spark on http://spark.apache.org/docs/latest/configuration.html[Spark Configuration]. [[properties]] .Spark Application's Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Property Name | Default | Description | [[spark.local.dir]] spark.local.dir | /tmp | Comma-separated list of directories that are used as a temporary storage for \"scratch\" space, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. |=== === [[spark-defaults-conf]] spark-defaults.conf -- Default Spark Properties File spark-defaults.conf (under SPARK_CONF_DIR or $SPARK_HOME/conf ) is the default properties file with the Spark properties of your Spark applications. NOTE: spark-defaults.conf is loaded by spark-AbstractCommandBuilder.md#loadPropertiesFile[AbstractCommandBuilder's loadPropertiesFile internal method]. === [[getDefaultPropertiesFile]] Calculating Path of Default Spark Properties -- Utils.getDefaultPropertiesFile method [source, scala] \u00b6 getDefaultPropertiesFile(env: Map[String, String] = sys.env): String \u00b6 getDefaultPropertiesFile calculates the absolute path to spark-defaults.conf properties file that can be either in directory specified by SPARK_CONF_DIR environment variable or $SPARK_HOME/conf directory. NOTE: getDefaultPropertiesFile is part of private[spark] org.apache.spark.util.Utils object.","title":"Spark Properties"},{"location":"spark-properties/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-properties/#getdefaultpropertiesfileenv-mapstring-string-sysenv-string","text":"getDefaultPropertiesFile calculates the absolute path to spark-defaults.conf properties file that can be either in directory specified by SPARK_CONF_DIR environment variable or $SPARK_HOME/conf directory. NOTE: getDefaultPropertiesFile is part of private[spark] org.apache.spark.util.Utils object.","title":"getDefaultPropertiesFile(env: Map[String, String] = sys.env): String"},{"location":"spark-sparkcontext-SparkStatusTracker/","text":"SparkStatusTracker \u00b6 Creating Instance \u00b6 SparkStatusTracker takes the following when created: [[sc]] ROOT:SparkContext.md[] [[store]] core:AppStatusStore.md[] SparkStatusTracker is created for SparkContext .","title":"SparkStatusTracker"},{"location":"spark-sparkcontext-SparkStatusTracker/#sparkstatustracker","text":"","title":"SparkStatusTracker"},{"location":"spark-sparkcontext-SparkStatusTracker/#creating-instance","text":"SparkStatusTracker takes the following when created: [[sc]] ROOT:SparkContext.md[] [[store]] core:AppStatusStore.md[] SparkStatusTracker is created for SparkContext .","title":"Creating Instance"},{"location":"spark-sparkcontext-local-properties/","text":"== Local Properties -- Creating Logical Job Groups The purpose of local properties concept is to create logical groups of jobs by means of properties that (regardless of the threads used to submit the jobs) makes the separate jobs launched from different threads belong to a single logical group. You can < > that will affect Spark jobs submitted from a thread, such as the Spark fair scheduler pool. You can use your own custom properties. The properties are propagated through to worker tasks and can be accessed there via spark-TaskContext.md#getLocalProperty[TaskContext.getLocalProperty]. NOTE: Propagating local properties to workers starts when SparkContext is requested to ROOT:SparkContext.md#runJob[run] or ROOT:SparkContext.md#submitJob[submit] a Spark job that in turn scheduler:DAGScheduler.md#runJob[passes them along to DAGScheduler ]. NOTE: Local properties is used to spark-scheduler-FairSchedulableBuilder.md#spark.scheduler.pool[group jobs into pools in FAIR job scheduler by spark.scheduler.pool per-thread property] and in spark-sql-SQLExecution.md#withNewExecutionId[SQLExecution.withNewExecutionId Helper Methods] A common use case for the local property concept is to set a local property in a thread, say spark-scheduler-FairSchedulableBuilder.md[spark.scheduler.pool], after which all jobs submitted within the thread will be grouped, say into a pool by FAIR job scheduler. [source, scala] \u00b6 val rdd = sc.parallelize(0 to 9) sc.setLocalProperty(\"spark.scheduler.pool\", \"myPool\") // these two jobs (one per action) will run in the myPool pool rdd.count rdd.collect sc.setLocalProperty(\"spark.scheduler.pool\", null) // this job will run in the default pool rdd.count === [[localProperties]] Local Properties -- localProperties Property [source, scala] \u00b6 localProperties: InheritableThreadLocal[Properties] \u00b6 localProperties is a protected[spark] property of a ROOT:SparkContext.md[SparkContext] that are the properties through which you can create logical job groups. TIP: Read up on Java's https://docs.oracle.com/javase/8/docs/api/java/lang/InheritableThreadLocal.html[java.lang.InheritableThreadLocal ]. === [[setLocalProperty]] Setting Local Property -- setLocalProperty Method [source, scala] \u00b6 setLocalProperty(key: String, value: String): Unit \u00b6 setLocalProperty sets key local property to value . TIP: When value is null the key property is removed from < >. === [[getLocalProperty]] Getting Local Property -- getLocalProperty Method [source, scala] \u00b6 getLocalProperty(key: String): String \u00b6 getLocalProperty gets a local property by key in this thread. It returns null if key is missing. === [[getLocalProperties]] Getting Local Properties -- getLocalProperties Method [source, scala] \u00b6 getLocalProperties: Properties \u00b6 getLocalProperties is a private[spark] method that gives access to < >. === [[setLocalProperties]] setLocalProperties Method [source, scala] \u00b6 setLocalProperties(props: Properties): Unit \u00b6 setLocalProperties is a private[spark] method that sets props as < >.","title":"Local Properties"},{"location":"spark-sparkcontext-local-properties/#source-scala","text":"val rdd = sc.parallelize(0 to 9) sc.setLocalProperty(\"spark.scheduler.pool\", \"myPool\") // these two jobs (one per action) will run in the myPool pool rdd.count rdd.collect sc.setLocalProperty(\"spark.scheduler.pool\", null) // this job will run in the default pool rdd.count === [[localProperties]] Local Properties -- localProperties Property","title":"[source, scala]"},{"location":"spark-sparkcontext-local-properties/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sparkcontext-local-properties/#localproperties-inheritablethreadlocalproperties","text":"localProperties is a protected[spark] property of a ROOT:SparkContext.md[SparkContext] that are the properties through which you can create logical job groups. TIP: Read up on Java's https://docs.oracle.com/javase/8/docs/api/java/lang/InheritableThreadLocal.html[java.lang.InheritableThreadLocal ]. === [[setLocalProperty]] Setting Local Property -- setLocalProperty Method","title":"localProperties: InheritableThreadLocal[Properties]"},{"location":"spark-sparkcontext-local-properties/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sparkcontext-local-properties/#setlocalpropertykey-string-value-string-unit","text":"setLocalProperty sets key local property to value . TIP: When value is null the key property is removed from < >. === [[getLocalProperty]] Getting Local Property -- getLocalProperty Method","title":"setLocalProperty(key: String, value: String): Unit"},{"location":"spark-sparkcontext-local-properties/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sparkcontext-local-properties/#getlocalpropertykey-string-string","text":"getLocalProperty gets a local property by key in this thread. It returns null if key is missing. === [[getLocalProperties]] Getting Local Properties -- getLocalProperties Method","title":"getLocalProperty(key: String): String"},{"location":"spark-sparkcontext-local-properties/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sparkcontext-local-properties/#getlocalproperties-properties","text":"getLocalProperties is a private[spark] method that gives access to < >. === [[setLocalProperties]] setLocalProperties Method","title":"getLocalProperties: Properties"},{"location":"spark-sparkcontext-local-properties/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sparkcontext-local-properties/#setlocalpropertiesprops-properties-unit","text":"setLocalProperties is a private[spark] method that sets props as < >.","title":"setLocalProperties(props: Properties): Unit"},{"location":"spark-tips-and-tricks-access-private-members-spark-shell/","text":"== Access private members in Scala in Spark shell If you ever wanted to use private[spark] members in Spark using the Scala programming language, e.g. toy with org.apache.spark.scheduler.DAGScheduler or similar, you will have to use the following trick in Spark shell - use :paste -raw as described in https://issues.scala-lang.org/browse/SI-5299[REPL : support for package definition]. Open spark-shell and execute :paste -raw that allows you to enter any valid Scala code, including package . The following snippet shows how to access private[spark] member DAGScheduler.RESUBMIT_TIMEOUT : scala> :paste -raw // Entering paste mode (ctrl-D to finish) package org.apache.spark object spark { def test = { import org.apache.spark.scheduler._ println(DAGScheduler.RESUBMIT_TIMEOUT == 200) } } scala> spark.test true scala> sc.version res0: String = 1.6.0-SNAPSHOT","title":"Access private members in Scala in Spark shell"},{"location":"spark-tips-and-tricks-running-spark-windows/","text":"== Running Spark Applications on Windows Running Spark applications on Windows in general is no different than running it on other operating systems like Linux or macOS. NOTE: A Spark application could be spark-shell.md[spark-shell] or your own custom Spark application. What makes the huge difference between the operating systems is Hadoop that is used internally for file system access in Spark. You may run into few minor issues when you are on Windows due to the way Hadoop works with Windows' POSIX-incompatible NTFS filesystem. NOTE: You do not have to install Apache Hadoop to work with Spark or run Spark applications. TIP: Read the Apache Hadoop project's https://wiki.apache.org/hadoop/WindowsProblems[Problems running Hadoop on Windows]. Among the issues is the infamous java.io.IOException when running Spark Shell (below a stacktrace from Spark 2.0.2 on Windows 10 so the line numbers may be different in your case). 16/12/26 21:34:11 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries. at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379) at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394) at org.apache.hadoop.util.Shell.<clinit>(Shell.java:387) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:2327) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:365) at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:105) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.spark.util.Utils$.classForName(Utils.scala:228) at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:963) at org.apache.spark.repl.Main$.createSparkSession(Main.scala:91) [NOTE] \u00b6 You need to have Administrator rights on your laptop. All the following commands must be executed in a command-line window ( cmd ) ran as Administrator, i.e. using Run as administrator option while executing cmd . Read the official document in Microsoft TechNet -- ++ https://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspx++[Start a Command Prompt as an Administrator]. \u00b6 Download winutils.exe binary from https://github.com/steveloughran/winutils repository. NOTE: You should select the version of Hadoop the Spark distribution was compiled with, e.g. use hadoop-2.7.1 for Spark 2 ( https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe[here is the direct link to winutils.exe binary]). Save winutils.exe binary to a directory of your choice, e.g. c:\\hadoop\\bin . Set HADOOP_HOME to reflect the directory with winutils.exe (without bin ). set HADOOP_HOME=c:\\hadoop Set PATH environment variable to include %HADOOP_HOME%\\bin as follows: set PATH=%HADOOP_HOME%\\bin;%PATH% TIP: Define HADOOP_HOME and PATH environment variables in Control Panel so any Windows program would use them. Create C:\\tmp\\hive directory. [NOTE] \u00b6 c:\\tmp\\hive directory is the default value of https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.scratchdir [ hive.exec.scratchdir configuration property] in Hive 0.14.0 and later and Spark uses a custom build of Hive 1.2.1. You can change hive.exec.scratchdir configuration property to another directory as described in < hive.exec.scratchdir Configuration Property>> in this document. \u00b6 Execute the following command in cmd that you started using the option Run as administrator . winutils.exe chmod -R 777 C:\\tmp\\hive Check the permissions (that is one of the commands that are executed under the covers): winutils.exe ls -F C:\\tmp\\hive Open spark-shell and observe the output (perhaps with few WARN messages that you can simply disregard). As a verification step, execute the following line to display the content of a DataFrame : [source, scala] \u00b6 scala> spark.range(1).withColumn(\"status\", lit(\"All seems fine. Congratulations!\")).show(false) +---+--------------------------------+ |id |status | +---+--------------------------------+ |0 |All seems fine. Congratulations!| +---+--------------------------------+ [NOTE] \u00b6 Disregard WARN messages when you start spark-shell . They are harmless. 16/12/26 22:05:41 WARN General: Plugin (Bundle) \"org.datanucleus\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-core- 3.2.10.jar.\" 16/12/26 22:05:41 WARN General: Plugin (Bundle) \"org.datanucleus.api.jdo\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/C:/spark-2.0.2-bin- hadoop2.7/bin/../jars/datanucleus-api-jdo-3.2.6.jar.\" 16/12/26 22:05:41 WARN General: Plugin (Bundle) \"org.datanucleus.store.rdbms\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-rdbms-3.2.9.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/C:/spark-2.0.2-bin- hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar.\" \u00b6 If you see the above output, you're done. You should now be able to run Spark applications on your Windows. Congrats! === [[changing-hive.exec.scratchdir]] Changing hive.exec.scratchdir Configuration Property Create a hive-site.xml file with the following content: <configuration> <property> <name>hive.exec.scratchdir</name> <value>/tmp/mydir</value> <description>Scratch space for Hive jobs</description> </property> </configuration> Start a Spark application, e.g. spark-shell , with HADOOP_CONF_DIR environment variable set to the directory with hive-site.xml . HADOOP_CONF_DIR=conf ./bin/spark-shell","title":"Running Spark Applications on Windows"},{"location":"spark-tips-and-tricks-running-spark-windows/#note","text":"You need to have Administrator rights on your laptop. All the following commands must be executed in a command-line window ( cmd ) ran as Administrator, i.e. using Run as administrator option while executing cmd .","title":"[NOTE]"},{"location":"spark-tips-and-tricks-running-spark-windows/#read-the-official-document-in-microsoft-technet-httpstechnetmicrosoftcomen-uslibrarycc947813vws10aspxstart-a-command-prompt-as-an-administrator","text":"Download winutils.exe binary from https://github.com/steveloughran/winutils repository. NOTE: You should select the version of Hadoop the Spark distribution was compiled with, e.g. use hadoop-2.7.1 for Spark 2 ( https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe[here is the direct link to winutils.exe binary]). Save winutils.exe binary to a directory of your choice, e.g. c:\\hadoop\\bin . Set HADOOP_HOME to reflect the directory with winutils.exe (without bin ). set HADOOP_HOME=c:\\hadoop Set PATH environment variable to include %HADOOP_HOME%\\bin as follows: set PATH=%HADOOP_HOME%\\bin;%PATH% TIP: Define HADOOP_HOME and PATH environment variables in Control Panel so any Windows program would use them. Create C:\\tmp\\hive directory.","title":"Read the official document in Microsoft TechNet -- ++https://technet.microsoft.com/en-us/library/cc947813(v=ws.10).aspx++[Start a Command Prompt as an Administrator]."},{"location":"spark-tips-and-tricks-running-spark-windows/#note_1","text":"c:\\tmp\\hive directory is the default value of https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.exec.scratchdir [ hive.exec.scratchdir configuration property] in Hive 0.14.0 and later and Spark uses a custom build of Hive 1.2.1.","title":"[NOTE]"},{"location":"spark-tips-and-tricks-running-spark-windows/#you-can-change-hiveexecscratchdir-configuration-property-to-another-directory-as-described-in-wzxhzdk27-configuration-property-in-this-document","text":"Execute the following command in cmd that you started using the option Run as administrator . winutils.exe chmod -R 777 C:\\tmp\\hive Check the permissions (that is one of the commands that are executed under the covers): winutils.exe ls -F C:\\tmp\\hive Open spark-shell and observe the output (perhaps with few WARN messages that you can simply disregard). As a verification step, execute the following line to display the content of a DataFrame :","title":"You can change hive.exec.scratchdir configuration property to another directory as described in &lt;\u0002wzxhzdk:27\u0003 Configuration Property>&gt; in this document."},{"location":"spark-tips-and-tricks-running-spark-windows/#source-scala","text":"scala> spark.range(1).withColumn(\"status\", lit(\"All seems fine. Congratulations!\")).show(false) +---+--------------------------------+ |id |status | +---+--------------------------------+ |0 |All seems fine. Congratulations!| +---+--------------------------------+","title":"[source, scala]"},{"location":"spark-tips-and-tricks-running-spark-windows/#note_2","text":"Disregard WARN messages when you start spark-shell . They are harmless.","title":"[NOTE]"},{"location":"spark-tips-and-tricks-running-spark-windows/#161226-220541-warn-general-plugin-bundle-orgdatanucleus-is-already-registered-ensure-you-dont-have-multiple-jar-versions-of-the-same-plugin-in-the-classpath-the-url-filecspark-202-bin-hadoop27jarsdatanucleus-core-3210jar-is-already-registered-and-you-are-trying-to-register-an-identical-plugin-located-at-url-filecspark-202-bin-hadoop27binjarsdatanucleus-core-3210jar-161226-220541-warn-general-plugin-bundle-orgdatanucleusapijdo-is-already-registered-ensure-you-dont-have-multiple-jar-versions-of-the-same-plugin-in-the-classpath-the-url-filecspark-202-bin-hadoop27jarsdatanucleus-api-jdo-326jar-is-already-registered-and-you-are-trying-to-register-an-identical-plugin-located-at-url-filecspark-202-bin-hadoop27binjarsdatanucleus-api-jdo-326jar-161226-220541-warn-general-plugin-bundle-orgdatanucleusstorerdbms-is-already-registered-ensure-you-dont-have-multiple-jar-versions-of-the-same-plugin-in-the-classpath-the-url-filecspark-202-bin-hadoop27binjarsdatanucleus-rdbms-329jar-is-already-registered-and-you-are-trying-to-register-an-identical-plugin-located-at-url-filecspark-202-bin-hadoop27jarsdatanucleus-rdbms-329jar","text":"If you see the above output, you're done. You should now be able to run Spark applications on your Windows. Congrats! === [[changing-hive.exec.scratchdir]] Changing hive.exec.scratchdir Configuration Property Create a hive-site.xml file with the following content: <configuration> <property> <name>hive.exec.scratchdir</name> <value>/tmp/mydir</value> <description>Scratch space for Hive jobs</description> </property> </configuration> Start a Spark application, e.g. spark-shell , with HADOOP_CONF_DIR environment variable set to the directory with hive-site.xml . HADOOP_CONF_DIR=conf ./bin/spark-shell","title":"16/12/26 22:05:41 WARN General: Plugin (Bundle) &quot;org.datanucleus&quot; is already registered. Ensure you dont have multiple JAR versions of\nthe same plugin in the classpath. The URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar&quot; is already registered,\nand you are trying to register an identical plugin located at URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-core-\n3.2.10.jar.&quot;\n16/12/26 22:05:41 WARN General: Plugin (Bundle) &quot;org.datanucleus.api.jdo&quot; is already registered. Ensure you dont have multiple JAR\nversions of the same plugin in the classpath. The URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar&quot; is already\nregistered, and you are trying to register an identical plugin located at URL &quot;file:/C:/spark-2.0.2-bin-\nhadoop2.7/bin/../jars/datanucleus-api-jdo-3.2.6.jar.&quot;\n16/12/26 22:05:41 WARN General: Plugin (Bundle) &quot;org.datanucleus.store.rdbms&quot; is already registered. Ensure you dont have multiple JAR\nversions of the same plugin in the classpath. The URL &quot;file:/C:/spark-2.0.2-bin-hadoop2.7/bin/../jars/datanucleus-rdbms-3.2.9.jar&quot; is\nalready registered, and you are trying to register an identical plugin located at URL &quot;file:/C:/spark-2.0.2-bin-\nhadoop2.7/jars/datanucleus-rdbms-3.2.9.jar.&quot;\n"},{"location":"spark-tips-and-tricks-sparkexception-task-not-serializable/","text":"== org.apache.spark.SparkException: Task not serializable When you run into org.apache.spark.SparkException: Task not serializable exception, it means that you use a reference to an instance of a non-serializable class inside a transformation. See the following example: \u279c spark git:(master) \u2717 ./bin/spark-shell Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 1.6.0-SNAPSHOT /_/ Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66) Type in expressions to have them evaluated. Type :help for more information. scala> class NotSerializable(val num: Int) defined class NotSerializable scala> val notSerializable = new NotSerializable(10) notSerializable: NotSerializable = NotSerializable@2700f556 scala> sc.parallelize(0 to 10).map(_ => notSerializable.num).count org.apache.spark.SparkException: Task not serializable at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304) at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294) at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122) at org.apache.spark.SparkContext.clean(SparkContext.scala:2055) at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:318) at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:317) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111) at org.apache.spark.rdd.RDD.withScope(RDD.scala:310) at org.apache.spark.rdd.RDD.map(RDD.scala:317) ... 48 elided Caused by: java.io.NotSerializableException: NotSerializable Serialization stack: - object not serializable (class: NotSerializable, value: NotSerializable@2700f556) - field (class: $iw, name: notSerializable, type: class NotSerializable) - object (class $iw, $iw@10e542f3) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@729feae8) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@5fc3b20b) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@36dab184) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@5eb974) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@79c514e4) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@5aeaee3) - field (class: $iw, name: $iw, type: class $iw) - object (class $iw, $iw@2be9425f) - field (class: $line18.$read, name: $iw, type: class $iw) - object (class $line18.$read, $line18.$read@6311640d) - field (class: $iw, name: $line18$read, type: class $line18.$read) - object (class $iw, $iw@c9cd06e) - field (class: $iw, name: $outer, type: class $iw) - object (class $iw, $iw@6565691a) - field (class: $anonfun$1, name: $outer, type: class $iw) - object (class $anonfun$1, <function1>) at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40) at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47) at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101) at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301) ... 57 more === Further reading https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/troubleshooting/javaionotserializableexception.html[Job aborted due to stage failure: Task not serializable] https://issues.apache.org/jira/browse/SPARK-5307[Add utility to help with NotSerializableException debugging] http://stackoverflow.com/q/22592811/1305344[Task not serializable: java.io.NotSerializableException when calling function outside closure only on classes not objects]","title":"Task not serializable Exception"},{"location":"spark-tips-and-tricks/","text":"= Spark Tips and Tricks == [[SPARK_PRINT_LAUNCH_COMMAND]] Print Launch Command of Spark Scripts SPARK_PRINT_LAUNCH_COMMAND environment variable controls whether the Spark launch command is printed out to the standard error output, i.e. System.err , or not. Spark Command: [here comes the command] ======================================== All the Spark shell scripts use org.apache.spark.launcher.Main class internally that checks SPARK_PRINT_LAUNCH_COMMAND and when set (to any value) will print out the entire command line to launch it. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-1.6.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Dscala.usejavacp=true -Xms1g -Xmx1g org.apache.spark.deploy.SparkSubmit --master spark://localhost:7077 --class org.apache.spark.repl.Main --name Spark shell spark-shell ======================================== == Show Spark version in Spark shell In spark-shell, use sc.version or org.apache.spark.SPARK_VERSION to know the Spark version: scala> sc.version res0: String = 1.6.0-SNAPSHOT scala> org.apache.spark.SPARK_VERSION res1: String = 1.6.0-SNAPSHOT == Resolving local host name When you face networking issues when Spark can't resolve your local hostname or IP address, use the preferred SPARK_LOCAL_HOSTNAME environment variable as the custom host name or SPARK_LOCAL_IP as the custom IP that is going to be later resolved to a hostname. Spark checks them out before using http://docs.oracle.com/javase/8/docs/api/java/net/InetAddress.html#getLocalHost--[java.net.InetAddress.getLocalHost ()] (consult https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L759[org.apache.spark.util.Utils.findLocalInetAddress ()] method). You may see the following WARN messages in the logs when Spark finished the resolving process: WARN Your hostname, [hostname] resolves to a loopback address: [host-address]; using... WARN Set SPARK_LOCAL_IP if you need to bind to another address == [[spark-standalone-windows]] Starting standalone Master and workers on Windows 7 Windows 7 users can use spark-class.md[spark-class] to start spark-standalone.md[Spark Standalone] as there are no launch scripts for the Windows platform. $ ./bin/spark-class org.apache.spark.deploy.master.Master -h localhost $ ./bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077","title":"Spark Tips and Tricks"},{"location":"speculative-execution-of-tasks/","text":"Speculative Execution of Tasks \u00b6 Speculative tasks (also speculatable tasks or task strugglers ) are tasks that run slower than most (FIXME the setting) of the all tasks in a job. Speculative execution of tasks is a health-check procedure that checks for tasks to be speculated , i.e. running slower in a stage than the median of all successfully completed tasks in a taskset (FIXME the setting). Such slow tasks will be re-submitted to another worker. It will not stop the slow tasks, but run a new copy in parallel. The thread starts as TaskSchedulerImpl starts in ROOT:spark-cluster.md[clustered deployment modes] with ROOT:configuration-properties.md#spark.speculation[spark.speculation] enabled. It executes periodically every ROOT:configuration-properties.md#spark.speculation.interval[spark.speculation.interval] after the initial spark.speculation.interval passes. When enabled, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Starting speculative execution thread \u00b6 It works as scheduler:TaskSchedulerImpl.md#task-scheduler-speculation[ task-scheduler-speculation daemon thread pool] (using j.u.c.ScheduledThreadPoolExecutor with core pool size of 1). The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet. It uses checkSpeculatableTasks method that asks rootPool to check for speculatable tasks. If there are any, SchedulerBackend is called for scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. CAUTION: FIXME How does Spark handle repeated results of speculative tasks since there are copies launched?","title":"Speculative Execution of Tasks"},{"location":"speculative-execution-of-tasks/#speculative-execution-of-tasks","text":"Speculative tasks (also speculatable tasks or task strugglers ) are tasks that run slower than most (FIXME the setting) of the all tasks in a job. Speculative execution of tasks is a health-check procedure that checks for tasks to be speculated , i.e. running slower in a stage than the median of all successfully completed tasks in a taskset (FIXME the setting). Such slow tasks will be re-submitted to another worker. It will not stop the slow tasks, but run a new copy in parallel. The thread starts as TaskSchedulerImpl starts in ROOT:spark-cluster.md[clustered deployment modes] with ROOT:configuration-properties.md#spark.speculation[spark.speculation] enabled. It executes periodically every ROOT:configuration-properties.md#spark.speculation.interval[spark.speculation.interval] after the initial spark.speculation.interval passes. When enabled, you should see the following INFO message in the logs:","title":"Speculative Execution of Tasks"},{"location":"speculative-execution-of-tasks/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"speculative-execution-of-tasks/#starting-speculative-execution-thread","text":"It works as scheduler:TaskSchedulerImpl.md#task-scheduler-speculation[ task-scheduler-speculation daemon thread pool] (using j.u.c.ScheduledThreadPoolExecutor with core pool size of 1). The job with speculatable tasks should finish while speculative tasks are running, and it will leave these tasks running - no KILL command yet. It uses checkSpeculatableTasks method that asks rootPool to check for speculatable tasks. If there are any, SchedulerBackend is called for scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. CAUTION: FIXME How does Spark handle repeated results of speculative tasks since there are copies launched?","title":"Starting speculative execution thread"},{"location":"workers/","text":"== Workers Workers (aka slaves ) are running Spark instances where executors live to execute tasks. They are the compute nodes in Spark. CAUTION: FIXME Are workers perhaps part of Spark Standalone only? CAUTION: FIXME How many executors are spawned per worker? A worker receives serialized tasks that it runs in a thread pool. It hosts a local storage:BlockManager.md[Block Manager] that serves blocks to other workers in a Spark cluster. Workers communicate among themselves using their Block Manager instances. CAUTION: FIXME Diagram of a driver with workers as boxes. Explain task execution in Spark and understand Spark\u2019s underlying execution model. New vocabulary often faced in Spark UI ROOT:SparkContext.md[When you create SparkContext], each worker starts an executor. This is a separate process (JVM), and it loads your jar, too. The executors connect back to your driver program. Now the driver can send them commands, like flatMap , map and reduceByKey . When the driver quits, the executors shut down. A new process is not started for each step. A new process is started on each worker when the SparkContext is constructed. The executor deserializes the command (this is possible because it has loaded your jar), and executes it on a partition. Shortly speaking, an application in Spark is executed in three steps: Create RDD graph, i.e. DAG (directed acyclic graph) of RDDs to represent entire computation. Create stage graph, i.e. a DAG of stages that is a logical execution plan based on the RDD graph. Stages are created by breaking the RDD graph at shuffle boundaries. Based on the plan, schedule and execute tasks on workers. exercises/spark-examples-wordcount-spark-shell.md[In the WordCount example], the RDD graph is as follows: file -> lines -> words -> per-word count -> global word count -> output Based on this graph, two stages are created. The stage creation rule is based on the idea of pipelining as many rdd:index.md[narrow transformations] as possible. RDD operations with \"narrow\" dependencies, like map() and filter() , are pipelined together into one set of tasks in each stage. In the end, every stage will only have shuffle dependencies on other stages, and may compute multiple operations inside it. In the WordCount example, the narrow transformation finishes at per-word count. Therefore, you get two stages: file -> lines -> words -> per-word count global word count -> output Once stages are defined, Spark will generate scheduler:Task.md[tasks] from scheduler:Stage.md[stages]. The first stage will create scheduler:ShuffleMapTask.md[ShuffleMapTask]s with the last stage creating scheduler:ResultTask.md[ResultTask]s because in the last stage, one action operation is included to produce results. The number of tasks to be generated depends on how your files are distributed. Suppose that you have 3 three different files in three different nodes, the first stage will generate 3 tasks: one task per partition. Therefore, you should not map your steps to tasks directly. A task belongs to a stage, and is related to a partition. The number of tasks being generated in each stage will be equal to the number of partitions. === [[Cleanup]] Cleanup CAUTION: FIXME === [[settings]] Settings spark.worker.cleanup.enabled (default: false ) < > enabled.","title":"Workers"},{"location":"core/AppStatusListener/","text":"= AppStatusListener AppStatusListener is a ROOT:SparkListener.md[]. == [[creating-instance]] Creating Instance AppStatusListener takes the following to be created: [[kvstore]] core:ElementTrackingStore.md[] [[conf]] ROOT:SparkConf.md[] < > [[lastUpdateTime]] Optional lastUpdateTime (default: None ) AppStatusListener is created when: AppStatusStore is requested to core:AppStatusStore.md#createLiveStore[createLiveStore] (with the < > flag enabled) FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#rebuildAppStore[rebuildAppStore] (with the < > flag disabled) == [[live]] live Flag AppStatusListener is given a flag that indicates whether it is created for a live Spark application (for core:AppStatusStore.md[]) or when replaying Spark applications for spark-history-server:index.md[] (for spark-history-server:FsHistoryProvider.md[]). == [[event-handlers]] Event Handlers [width=\"100%\",cols=\"1m,1\",options=\"header\"] |=== | Event | Handler | SparkListenerApplicationStart | < > | SparkListenerApplicationEnd | < > | SparkListenerBlockManagerAdded | < > | SparkListenerBlockManagerRemoved | < > | SparkListenerBlockUpdated | < > | SparkListenerEnvironmentUpdate | < > | SparkListenerEvent | < > | SparkListenerExecutorAdded | < > | SparkListenerExecutorBlacklisted | < > | SparkListenerExecutorMetricsUpdate | < > | SparkListenerExecutorRemoved | < > | SparkListenerExecutorUnblacklisted | < > | SparkListenerJobStart | < > | SparkListenerJobEnd | < > | SparkListenerNodeBlacklisted | < > | SparkListenerNodeUnblacklisted | < > | SparkListenerStageCompleted | < > | SparkListenerStageSubmitted | < > | SparkListenerTaskEnd | < > | SparkListenerTaskGettingResult | < > | SparkListenerTaskStart | < > | SparkListenerUnpersistRDD | < > |=== == [[onStageSubmitted]] onStageSubmitted Method [source, scala] \u00b6 onStageSubmitted(event: SparkListenerStageSubmitted): Unit \u00b6 NOTE: onStageSubmitted is part of ROOT:SparkListener.md#onStageSubmitted[SparkListener Contract] to...FIXME. onStageSubmitted ...FIXME == [[update]] update Internal Method [source, scala] \u00b6 update(entity: LiveEntity, now: Long, last: Boolean = false): Unit \u00b6 update simply requests the LiveEntity to spark-core-LiveEntity.md#write[write] (with the < > as the store and the last flag as checkTriggers flag). NOTE: update is used in event handlers (i.e. onApplicationStart , onExecutorRemoved , onJobEnd , onStageSubmitted , onTaskEnd , onStageCompleted ), < >, < >, < > and < >. == [[flush]] flush Internal Method [source, scala] \u00b6 flush(): Unit \u00b6 flush ...FIXME NOTE: flush is used when...FIXME == [[maybeUpdate]] maybeUpdate Internal Method [source, scala] \u00b6 maybeUpdate(entity: LiveEntity, now: Long): Unit \u00b6 maybeUpdate ...FIXME NOTE: maybeUpdate is used when...FIXME == [[liveUpdate]] liveUpdate Internal Method [source, scala] \u00b6 liveUpdate(entity: LiveEntity, now: Long): Unit \u00b6 liveUpdate ...FIXME NOTE: liveUpdate is used when...FIXME == [[updateStreamBlock]] updateStreamBlock Internal Method [source, scala] \u00b6 updateStreamBlock(event: SparkListenerBlockUpdated, stream: StreamBlockId): Unit \u00b6 updateStreamBlock ...FIXME NOTE: updateStreamBlock is used exclusively when AppStatusListener is requested to < > (for a storage:BlockId.md#StreamBlockId[StreamBlockId]). == [[onBlockUpdated]] Intercepting SparkListenerBlockUpdated Events -- onBlockUpdated Handler Method [source, scala] \u00b6 onBlockUpdated(event: SparkListenerBlockUpdated): Unit \u00b6 NOTE: onBlockUpdated is part of ROOT:SparkListener.md#onBlockUpdated[SparkListener Contract] to...FIXME. onBlockUpdated simply dispatches to the following event-specific handlers (per storage:BlockId.md[] type): < > for storage:BlockId.md#RDDBlockId[RDDBlockIds] < > for storage:BlockId.md#StreamBlockId[StreamBlockIds] Ignores ( swallows ) the SparkListenerBlockUpdated event for the other types == [[updateRDDBlock]] updateRDDBlock Internal Method [source, scala] \u00b6 updateRDDBlock( event: SparkListenerBlockUpdated, block: RDDBlockId): Unit updateRDDBlock ...FIXME NOTE: updateRDDBlock is used exclusively when AppStatusListener is requested to < > (for a storage:BlockId.md#RDDBlockId[RDDBlockId]). == [[updateBroadcastBlock]] updateBroadcastBlock Internal Method [source, scala] \u00b6 updateBroadcastBlock( event: SparkListenerBlockUpdated, broadcast: BroadcastBlockId): Unit updateBroadcastBlock ...FIXME NOTE: updateBroadcastBlock is used...FIXME == [[internal-properties]] Internal Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appInfo | [[appInfo]] v1.ApplicationInfo | appSummary | [[appSummary]] AppSummary | liveUpdatePeriodNs | [[liveUpdatePeriodNs]] | coresPerTask | [[coresPerTask]] Default: 1 | liveRDDs | [[liveRDDs]] webui:spark-core-LiveRDD.md[LiveRDDs] by RDD ID | liveStages | [[liveStages]] LiveStages by (Int, Int) | liveTasks | [[liveTasks]] LiveTask by task ID | liveJobs | [[liveJobs]] LiveJob by job ID | liveExecutors | [[liveExecutors]] LiveExecutor by executor ID | pools | [[pools]] SchedulerPool by FIXME | activeExecutorCount | [[activeExecutorCount]] Number of active executors |===","title":"AppStatusListener"},{"location":"core/AppStatusListener/#source-scala","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#onstagesubmittedevent-sparklistenerstagesubmitted-unit","text":"NOTE: onStageSubmitted is part of ROOT:SparkListener.md#onStageSubmitted[SparkListener Contract] to...FIXME. onStageSubmitted ...FIXME == [[update]] update Internal Method","title":"onStageSubmitted(event: SparkListenerStageSubmitted): Unit"},{"location":"core/AppStatusListener/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#updateentity-liveentity-now-long-last-boolean-false-unit","text":"update simply requests the LiveEntity to spark-core-LiveEntity.md#write[write] (with the < > as the store and the last flag as checkTriggers flag). NOTE: update is used in event handlers (i.e. onApplicationStart , onExecutorRemoved , onJobEnd , onStageSubmitted , onTaskEnd , onStageCompleted ), < >, < >, < > and < >. == [[flush]] flush Internal Method","title":"update(entity: LiveEntity, now: Long, last: Boolean = false): Unit"},{"location":"core/AppStatusListener/#source-scala_2","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#flush-unit","text":"flush ...FIXME NOTE: flush is used when...FIXME == [[maybeUpdate]] maybeUpdate Internal Method","title":"flush(): Unit"},{"location":"core/AppStatusListener/#source-scala_3","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#maybeupdateentity-liveentity-now-long-unit","text":"maybeUpdate ...FIXME NOTE: maybeUpdate is used when...FIXME == [[liveUpdate]] liveUpdate Internal Method","title":"maybeUpdate(entity: LiveEntity, now: Long): Unit"},{"location":"core/AppStatusListener/#source-scala_4","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#liveupdateentity-liveentity-now-long-unit","text":"liveUpdate ...FIXME NOTE: liveUpdate is used when...FIXME == [[updateStreamBlock]] updateStreamBlock Internal Method","title":"liveUpdate(entity: LiveEntity, now: Long): Unit"},{"location":"core/AppStatusListener/#source-scala_5","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#updatestreamblockevent-sparklistenerblockupdated-stream-streamblockid-unit","text":"updateStreamBlock ...FIXME NOTE: updateStreamBlock is used exclusively when AppStatusListener is requested to < > (for a storage:BlockId.md#StreamBlockId[StreamBlockId]). == [[onBlockUpdated]] Intercepting SparkListenerBlockUpdated Events -- onBlockUpdated Handler Method","title":"updateStreamBlock(event: SparkListenerBlockUpdated, stream: StreamBlockId): Unit"},{"location":"core/AppStatusListener/#source-scala_6","text":"","title":"[source, scala]"},{"location":"core/AppStatusListener/#onblockupdatedevent-sparklistenerblockupdated-unit","text":"NOTE: onBlockUpdated is part of ROOT:SparkListener.md#onBlockUpdated[SparkListener Contract] to...FIXME. onBlockUpdated simply dispatches to the following event-specific handlers (per storage:BlockId.md[] type): < > for storage:BlockId.md#RDDBlockId[RDDBlockIds] < > for storage:BlockId.md#StreamBlockId[StreamBlockIds] Ignores ( swallows ) the SparkListenerBlockUpdated event for the other types == [[updateRDDBlock]] updateRDDBlock Internal Method","title":"onBlockUpdated(event: SparkListenerBlockUpdated): Unit"},{"location":"core/AppStatusListener/#source-scala_7","text":"updateRDDBlock( event: SparkListenerBlockUpdated, block: RDDBlockId): Unit updateRDDBlock ...FIXME NOTE: updateRDDBlock is used exclusively when AppStatusListener is requested to < > (for a storage:BlockId.md#RDDBlockId[RDDBlockId]). == [[updateBroadcastBlock]] updateBroadcastBlock Internal Method","title":"[source, scala]"},{"location":"core/AppStatusListener/#source-scala_8","text":"updateBroadcastBlock( event: SparkListenerBlockUpdated, broadcast: BroadcastBlockId): Unit updateBroadcastBlock ...FIXME NOTE: updateBroadcastBlock is used...FIXME == [[internal-properties]] Internal Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appInfo | [[appInfo]] v1.ApplicationInfo | appSummary | [[appSummary]] AppSummary | liveUpdatePeriodNs | [[liveUpdatePeriodNs]] | coresPerTask | [[coresPerTask]] Default: 1 | liveRDDs | [[liveRDDs]] webui:spark-core-LiveRDD.md[LiveRDDs] by RDD ID | liveStages | [[liveStages]] LiveStages by (Int, Int) | liveTasks | [[liveTasks]] LiveTask by task ID | liveJobs | [[liveJobs]] LiveJob by job ID | liveExecutors | [[liveExecutors]] LiveExecutor by executor ID | pools | [[pools]] SchedulerPool by FIXME | activeExecutorCount | [[activeExecutorCount]] Number of active executors |===","title":"[source, scala]"},{"location":"core/AppStatusStore/","text":"AppStatusStore \u00b6 AppStatusStore is available as ROOT:SparkContext.md#statusStore[SparkContext.statusStore] to other Spark services. Creating Instance \u00b6 AppStatusStore takes the following to be created: [[store]] core:KVStore.md[] [[listener]] Optional core:AppStatusListener.md[] (default: None ) AppStatusStore is created when: SparkContext is created (that triggers < >) FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[create a LoadedAppUI] == [[streamBlocksList]] streamBlocksList Method [source, scala] \u00b6 streamBlocksList(): Seq[StreamBlockData] \u00b6 streamBlocksList ...FIXME NOTE: streamBlocksList is used when...FIXME == [[activeStages]] activeStages Method [source, scala] \u00b6 activeStages(): Seq[v1.StageData] \u00b6 activeStages ...FIXME NOTE: activeStages is used when...FIXME == [[createLiveStore]] Creating Event Store [source, scala] \u00b6 createLiveStore( conf: SparkConf): AppStatusStore createLiveStore creates a fully-initialized AppStatusStore. Internally, createLiveStore creates a core:ElementTrackingStore.md[] (with a new core:InMemoryStore.md[] and the input ROOT:SparkConf.md[SparkConf]). createLiveStore creates a core:AppStatusListener.md[] (with the ElementTrackingStore created, the input SparkConf and the live flag enabled). In the end, createLiveStore creates an < > (with the ElementTrackingStore and AppStatusListener just created). createLiveStore is used when SparkContext is created. == [[close]] Closing AppStatusStore [source, scala] \u00b6 close(): Unit \u00b6 close simply requests < > to core:KVStore.md#close[close]. NOTE: close is used when...FIXME == [[rddList]] rddList Method [source, scala] \u00b6 rddList(cachedOnly: Boolean = true): Seq[v1.RDDStorageInfo] \u00b6 rddList requests the < > for a core:KVStore.md#view[view] over RDDStorageInfoWrapper entities. In the end, rddList takes RDDStorageInfos with at least one spark-webui-RDDStorageInfo.md#numCachedPartitions[partition cached] (when cachedOnly flag is on) or all RDDStorageInfos (when cachedOnly flag is off). NOTE: cachedOnly flag is on and therefore rddList gives RDDs cached only. [NOTE] \u00b6 rddList is used when: StoragePage is requested to spark-webui-StoragePage.md#render[render] itself AbstractApplicationResource is requested to handle spark-api-AbstractApplicationResource.md#storage_rdd[ storage/rdd] REST API * StagePagedTable is requested to makeDescription \u00b6","title":"AppStatusStore"},{"location":"core/AppStatusStore/#appstatusstore","text":"AppStatusStore is available as ROOT:SparkContext.md#statusStore[SparkContext.statusStore] to other Spark services.","title":"AppStatusStore"},{"location":"core/AppStatusStore/#creating-instance","text":"AppStatusStore takes the following to be created: [[store]] core:KVStore.md[] [[listener]] Optional core:AppStatusListener.md[] (default: None ) AppStatusStore is created when: SparkContext is created (that triggers < >) FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[create a LoadedAppUI] == [[streamBlocksList]] streamBlocksList Method","title":"Creating Instance"},{"location":"core/AppStatusStore/#source-scala","text":"","title":"[source, scala]"},{"location":"core/AppStatusStore/#streamblockslist-seqstreamblockdata","text":"streamBlocksList ...FIXME NOTE: streamBlocksList is used when...FIXME == [[activeStages]] activeStages Method","title":"streamBlocksList(): Seq[StreamBlockData]"},{"location":"core/AppStatusStore/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/AppStatusStore/#activestages-seqv1stagedata","text":"activeStages ...FIXME NOTE: activeStages is used when...FIXME == [[createLiveStore]] Creating Event Store","title":"activeStages(): Seq[v1.StageData]"},{"location":"core/AppStatusStore/#source-scala_2","text":"createLiveStore( conf: SparkConf): AppStatusStore createLiveStore creates a fully-initialized AppStatusStore. Internally, createLiveStore creates a core:ElementTrackingStore.md[] (with a new core:InMemoryStore.md[] and the input ROOT:SparkConf.md[SparkConf]). createLiveStore creates a core:AppStatusListener.md[] (with the ElementTrackingStore created, the input SparkConf and the live flag enabled). In the end, createLiveStore creates an < > (with the ElementTrackingStore and AppStatusListener just created). createLiveStore is used when SparkContext is created. == [[close]] Closing AppStatusStore","title":"[source, scala]"},{"location":"core/AppStatusStore/#source-scala_3","text":"","title":"[source, scala]"},{"location":"core/AppStatusStore/#close-unit","text":"close simply requests < > to core:KVStore.md#close[close]. NOTE: close is used when...FIXME == [[rddList]] rddList Method","title":"close(): Unit"},{"location":"core/AppStatusStore/#source-scala_4","text":"","title":"[source, scala]"},{"location":"core/AppStatusStore/#rddlistcachedonly-boolean-true-seqv1rddstorageinfo","text":"rddList requests the < > for a core:KVStore.md#view[view] over RDDStorageInfoWrapper entities. In the end, rddList takes RDDStorageInfos with at least one spark-webui-RDDStorageInfo.md#numCachedPartitions[partition cached] (when cachedOnly flag is on) or all RDDStorageInfos (when cachedOnly flag is off). NOTE: cachedOnly flag is on and therefore rddList gives RDDs cached only.","title":"rddList(cachedOnly: Boolean = true): Seq[v1.RDDStorageInfo]"},{"location":"core/AppStatusStore/#note","text":"rddList is used when: StoragePage is requested to spark-webui-StoragePage.md#render[render] itself AbstractApplicationResource is requested to handle spark-api-AbstractApplicationResource.md#storage_rdd[ storage/rdd] REST API","title":"[NOTE]"},{"location":"core/AppStatusStore/#stagepagedtable-is-requested-to-makedescription","text":"","title":"* StagePagedTable is requested to makeDescription"},{"location":"core/BlockFetchStarter/","text":"= BlockFetchStarter BlockFetchStarter is the < > of...FIXME...to < >. [[contract]] [[createAndStart]] [source, java] void createAndStart(String[] blockIds, BlockFetchingListener listener) throws IOException, InterruptedException; createAndStart is used when: ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is 0 ) NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is 0 ) RetryingBlockFetcher is requested to core:RetryingBlockFetcher.md#fetchAllOutstanding[fetchAllOutstanding]","title":"BlockFetchStarter"},{"location":"core/BlockFetchingListener/","text":"= BlockFetchingListener BlockFetchingListener is the < > of < > that want to be notified about < > and < >. BlockFetchingListener is used when: storage:ShuffleClient.md#fetchBlocks[ShuffleClient], storage:BlockTransferService.md#fetchBlocks[BlockTransferService], storage:NettyBlockTransferService.md#fetchBlocks[NettyBlockTransferService], and storage:ExternalShuffleClient.md#fetchBlocks[ExternalShuffleClient] are requested to fetch a sequence of blocks BlockFetchStarter is requested to core:BlockFetchStarter.md#createAndStart[createAndStart] core:RetryingBlockFetcher.md[] and storage:OneForOneBlockFetcher.md[] are created [[contract]] [source, java] package org.apache.spark.network.shuffle; interface BlockFetchingListener extends EventListener { void onBlockFetchSuccess(String blockId, ManagedBuffer data); void onBlockFetchFailure(String blockId, Throwable exception); } .BlockFetchingListener Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | onBlockFetchSuccess | [[onBlockFetchSuccess]] Used when...FIXME | onBlockFetchFailure | [[onBlockFetchFailure]] Used when...FIXME |=== [[implementations]] .BlockFetchingListeners [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | BlockFetchingListener | Description | core:RetryingBlockFetcher.md#RetryingBlockFetchListener[RetryingBlockFetchListener] | [[RetryingBlockFetchListener]] | \"Unnamed\" in storage:ShuffleBlockFetcherIterator.md#sendRequest[ShuffleBlockFetcherIterator] | [[ShuffleBlockFetcherIterator]] | \"Unnamed\" in storage:BlockTransferService.md#fetchBlockSync[BlockTransferService] | [[BlockTransferService]] |===","title":"BlockFetchingListener"},{"location":"core/BroadcastFactory/","text":"= BroadcastFactory BroadcastFactory is an < > for < > that core:BroadcastManager.md[BroadcastManager] uses for ROOT:Broadcast.md[]. NOTE: As of https://issues.apache.org/jira/browse/SPARK-12588[Spark 2.0], it is no longer possible to plug a custom BroadcastFactory in, and core:TorrentBroadcastFactory.md[TorrentBroadcastFactory] is the one and only known implementation. == [[contract]] Contract === [[initialize]] initialize Method [source,scala] \u00b6 initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit Used when BroadcastManager is BroadcastManager.md#creating-instance[created]. === [[newBroadcast]] newBroadcast Method [source,scala] \u00b6 newBroadcast T: ClassTag : Broadcast[T] Used when BroadcastManager is requested for a BroadcastManager.md#newBroadcast[new broadcast variable]. === [[stop]] stop Method [source,scala] \u00b6 stop(): Unit \u00b6 Used when BroadcastManager is requested to BroadcastManager.md#stop[stop]. === [[unbroadcast]] unbroadcast Method [source,scala] \u00b6 unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit Used when BroadcastManager is requested to BroadcastManager.md#unbroadcast[unbroadcast a broadcast variable]. == [[implementations]] Available BroadcastFactories core:TorrentBroadcastFactory.md[TorrentBroadcastFactory] is the default and only known BroadcastFactory in Apache Spark.","title":"BroadcastFactory"},{"location":"core/BroadcastFactory/#sourcescala","text":"initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit Used when BroadcastManager is BroadcastManager.md#creating-instance[created]. === [[newBroadcast]] newBroadcast Method","title":"[source,scala]"},{"location":"core/BroadcastFactory/#sourcescala_1","text":"newBroadcast T: ClassTag : Broadcast[T] Used when BroadcastManager is requested for a BroadcastManager.md#newBroadcast[new broadcast variable]. === [[stop]] stop Method","title":"[source,scala]"},{"location":"core/BroadcastFactory/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"core/BroadcastFactory/#stop-unit","text":"Used when BroadcastManager is requested to BroadcastManager.md#stop[stop]. === [[unbroadcast]] unbroadcast Method","title":"stop(): Unit"},{"location":"core/BroadcastFactory/#sourcescala_3","text":"unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit Used when BroadcastManager is requested to BroadcastManager.md#unbroadcast[unbroadcast a broadcast variable]. == [[implementations]] Available BroadcastFactories core:TorrentBroadcastFactory.md[TorrentBroadcastFactory] is the default and only known BroadcastFactory in Apache Spark.","title":"[source,scala]"},{"location":"core/BroadcastManager/","text":"BroadcastManager \u00b6 BroadcastManager is a Spark service to manage ROOT:Broadcast.md[]s in a Spark application. BroadcastManager, SparkEnv and BroadcastFactory BroadcastManager assigns < > to broadcast variables. BroadcastManager is used to create a scheduler:MapOutputTrackerMaster.md#BroadcastManager[MapOutputTrackerMaster] == [[creating-instance]] Creating Instance BroadcastManager takes the following to be created: < > flag [[conf]] ROOT:SparkConf.md[SparkConf] [[securityManager]] SecurityManager When created, BroadcastManager < >. BroadcastManager is created when SparkEnv is core:SparkEnv.md[created] (for the driver and executors and hence the need for the < > flag). == [[isDriver]] isDriver Flag BroadcastManager is given isDriver flag when < >. The isDriver flag indicates whether the initialization happens on the driver ( true ) or executors ( false ). BroadcastManager uses the flag when requested to < > for the < > to TorrentBroadcastFactory.md#initialize[initialize]. == [[broadcastFactory]] TorrentBroadcastFactory BroadcastManager manages a core:BroadcastFactory.md[BroadcastFactory]: It is created and initialized in < > It is stopped in < > (and that is all it does) BroadcastManager uses the BroadcastFactory when requested to < > and < >. == [[cachedValues]] cachedValues Registry [source,scala] \u00b6 cachedValues: ReferenceMap \u00b6 == [[nextBroadcastId]] Unique Identifiers of Broadcast Variables BroadcastManager tracks broadcast variables and controls their identifiers. Every < > is given a new and unique identifier. == [[initialize]][[initialized]] Initializing BroadcastManager [source, scala] \u00b6 initialize(): Unit \u00b6 initialize creates a < > and requests it to core:TorrentBroadcastFactory.md#initialize[initialize]. initialize turns initialized internal flag on to guard against multiple initializations. With the initialized flag already enabled, initialize does nothing. initialize is used once when BroadcastManager is < >. == [[stop]] Stopping BroadcastManager [source, scala] \u00b6 stop(): Unit \u00b6 stop requests the < > to core:BroadcastFactory.md#stop[stop]. == [[newBroadcast]] Creating Broadcast Variable [source, scala] \u00b6 newBroadcast T : Broadcast[T] newBroadcast requests the core:BroadcastFactory.md[current BroadcastFactory for a new broadcast variable]. The BroadcastFactory is created when < >. newBroadcast is used when: MapOutputTracker utility is used to scheduler:MapOutputTracker.md#serializeMapStatuses[serializeMapStatuses] SparkContext is requested for a ROOT:SparkContext.md#broadcast[new broadcast variable]","title":"BroadcastManager"},{"location":"core/BroadcastManager/#broadcastmanager","text":"BroadcastManager is a Spark service to manage ROOT:Broadcast.md[]s in a Spark application. BroadcastManager, SparkEnv and BroadcastFactory BroadcastManager assigns < > to broadcast variables. BroadcastManager is used to create a scheduler:MapOutputTrackerMaster.md#BroadcastManager[MapOutputTrackerMaster] == [[creating-instance]] Creating Instance BroadcastManager takes the following to be created: < > flag [[conf]] ROOT:SparkConf.md[SparkConf] [[securityManager]] SecurityManager When created, BroadcastManager < >. BroadcastManager is created when SparkEnv is core:SparkEnv.md[created] (for the driver and executors and hence the need for the < > flag). == [[isDriver]] isDriver Flag BroadcastManager is given isDriver flag when < >. The isDriver flag indicates whether the initialization happens on the driver ( true ) or executors ( false ). BroadcastManager uses the flag when requested to < > for the < > to TorrentBroadcastFactory.md#initialize[initialize]. == [[broadcastFactory]] TorrentBroadcastFactory BroadcastManager manages a core:BroadcastFactory.md[BroadcastFactory]: It is created and initialized in < > It is stopped in < > (and that is all it does) BroadcastManager uses the BroadcastFactory when requested to < > and < >. == [[cachedValues]] cachedValues Registry","title":"BroadcastManager"},{"location":"core/BroadcastManager/#sourcescala","text":"","title":"[source,scala]"},{"location":"core/BroadcastManager/#cachedvalues-referencemap","text":"== [[nextBroadcastId]] Unique Identifiers of Broadcast Variables BroadcastManager tracks broadcast variables and controls their identifiers. Every < > is given a new and unique identifier. == [[initialize]][[initialized]] Initializing BroadcastManager","title":"cachedValues: ReferenceMap"},{"location":"core/BroadcastManager/#source-scala","text":"","title":"[source, scala]"},{"location":"core/BroadcastManager/#initialize-unit","text":"initialize creates a < > and requests it to core:TorrentBroadcastFactory.md#initialize[initialize]. initialize turns initialized internal flag on to guard against multiple initializations. With the initialized flag already enabled, initialize does nothing. initialize is used once when BroadcastManager is < >. == [[stop]] Stopping BroadcastManager","title":"initialize(): Unit"},{"location":"core/BroadcastManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/BroadcastManager/#stop-unit","text":"stop requests the < > to core:BroadcastFactory.md#stop[stop]. == [[newBroadcast]] Creating Broadcast Variable","title":"stop(): Unit"},{"location":"core/BroadcastManager/#source-scala_2","text":"newBroadcast T : Broadcast[T] newBroadcast requests the core:BroadcastFactory.md[current BroadcastFactory for a new broadcast variable]. The BroadcastFactory is created when < >. newBroadcast is used when: MapOutputTracker utility is used to scheduler:MapOutputTracker.md#serializeMapStatuses[serializeMapStatuses] SparkContext is requested for a ROOT:SparkContext.md#broadcast[new broadcast variable]","title":"[source, scala]"},{"location":"core/CleanerListener/","text":"= CleanerListener CleanerListener is an abstraction of listeners that can be core:ContextCleaner.md#attachListener[registered with ContextCleaner] to be informed when < >, < >, < >, < > and < > are cleaned. == [[rddCleaned]] rddCleaned Callback Method [source, scala] \u00b6 rddCleaned( rddId: Int): Unit rddCleaned is used when...FIXME == [[broadcastCleaned]] broadcastCleaned Callback Method [source, scala] \u00b6 broadcastCleaned( broadcastId: Long): Unit broadcastCleaned is used when...FIXME == [[shuffleCleaned]] shuffleCleaned Callback Method [source, scala] \u00b6 shuffleCleaned( shuffleId: Int, blocking: Boolean): Unit shuffleCleaned is used when...FIXME == [[accumCleaned]] accumCleaned Callback Method [source, scala] \u00b6 accumCleaned( accId: Long): Unit accumCleaned is used when...FIXME == [[checkpointCleaned]] checkpointCleaned Callback Method [source, scala] \u00b6 checkpointCleaned( rddId: Long): Unit checkpointCleaned is used when...FIXME","title":"CleanerListener"},{"location":"core/CleanerListener/#source-scala","text":"rddCleaned( rddId: Int): Unit rddCleaned is used when...FIXME == [[broadcastCleaned]] broadcastCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_1","text":"broadcastCleaned( broadcastId: Long): Unit broadcastCleaned is used when...FIXME == [[shuffleCleaned]] shuffleCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_2","text":"shuffleCleaned( shuffleId: Int, blocking: Boolean): Unit shuffleCleaned is used when...FIXME == [[accumCleaned]] accumCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_3","text":"accumCleaned( accId: Long): Unit accumCleaned is used when...FIXME == [[checkpointCleaned]] checkpointCleaned Callback Method","title":"[source, scala]"},{"location":"core/CleanerListener/#source-scala_4","text":"checkpointCleaned( rddId: Long): Unit checkpointCleaned is used when...FIXME","title":"[source, scala]"},{"location":"core/ContextCleaner/","text":"ContextCleaner \u00b6 ContextCleaner is a Spark service that is responsible for < > ( cleanup ) of < >, < >, < >, < > and < > that is aimed at reducing the memory requirements of long-running data-heavy Spark applications. Creating Instance \u00b6 ContextCleaner takes the following to be created: [[sc]] ROOT:SparkContext.md[] ContextCleaner is created and requested to start when SparkContext is created with ROOT:configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[cleaningThread]] Spark Context Cleaner Cleaning Thread ContextCleaner uses a daemon thread Spark Context Cleaner to clean RDD, shuffle, and broadcast states. The Spark Context Cleaner thread is started when ContextCleaner is requested to < >. == [[listeners]][[attachListener]] CleanerListeners ContextCleaner allows attaching core:CleanerListener.md[CleanerListeners] to be informed when objects are cleaned using attachListener method. [source,scala] \u00b6 attachListener( listener: CleanerListener): Unit == [[doCleanupRDD]] doCleanupRDD Method [source, scala] \u00b6 doCleanupRDD( rddId: Int, blocking: Boolean): Unit doCleanupRDD...FIXME doCleanupRDD is used when ContextCleaner is requested to < > for a CleanRDD. == [[keepCleaning]] keepCleaning Internal Method [source, scala] \u00b6 keepCleaning(): Unit \u00b6 keepCleaning runs indefinitely until ContextCleaner is requested to < >. keepCleaning...FIXME keepCleaning prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Got cleaning task [task] \u00b6 keepCleaning is used in < > that is started once when ContextCleaner is requested to < >. == [[registerRDDCheckpointDataForCleanup]] registerRDDCheckpointDataForCleanup Method [source, scala] \u00b6 registerRDDCheckpointDataForCleanup T : Unit registerRDDCheckpointDataForCleanup...FIXME registerRDDCheckpointDataForCleanup is used when ContextCleaner is requested to < > (with ROOT:configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled). == [[registerBroadcastForCleanup]] registerBroadcastForCleanup Method [source, scala] \u00b6 registerBroadcastForCleanup T : Unit registerBroadcastForCleanup...FIXME registerBroadcastForCleanup is used when SparkContext is used to ROOT:SparkContext.md#broadcast[create a broadcast variable]. == [[registerRDDForCleanup]] registerRDDForCleanup Method [source, scala] \u00b6 registerRDDForCleanup( rdd: RDD[_]): Unit registerRDDForCleanup...FIXME registerRDDForCleanup is used for rdd:RDD.md#persist[RDD.persist] operation. == [[registerAccumulatorForCleanup]] registerAccumulatorForCleanup Method [source, scala] \u00b6 registerAccumulatorForCleanup( a: AccumulatorV2[_, _]): Unit registerAccumulatorForCleanup...FIXME registerAccumulatorForCleanup is used when AccumulatorV2 is requested to register. == [[stop]] Stopping ContextCleaner [source, scala] \u00b6 stop(): Unit \u00b6 stop...FIXME stop is used when SparkContext is requested to ROOT:SparkContext.md#stop[stop]. == [[start]] Starting ContextCleaner [source, scala] \u00b6 start(): Unit \u00b6 start starts the < > and an action to request the JVM garbage collector (using System.gc() ) on regular basis per ROOT:configuration-properties.md#spark.cleaner.periodicGC.interval[spark.cleaner.periodicGC.interval] configuration property. The action to request the JVM GC is scheduled on < >. start is used when SparkContext is created. == [[periodicGCService]] periodicGCService Single-Thread Executor Service periodicGCService is an internal single-thread {java-javadoc-url}/java/util/concurrent/ScheduledExecutorService.html[executor service] with the name context-cleaner-periodic-gc to request the JVM garbage collector. The periodic runs are started when < > and stopped when < >. == [[registerShuffleForCleanup]] Registering ShuffleDependency for Cleanup [source, scala] \u00b6 registerShuffleForCleanup( shuffleDependency: ShuffleDependency[_, _, _]): Unit registerShuffleForCleanup registers the given ShuffleDependency for cleanup. Internally, registerShuffleForCleanup simply executes < > for the given ShuffleDependency. registerShuffleForCleanup is used when ShuffleDependency is created. == [[registerForCleanup]] Registering Object Reference For Cleanup [source, scala] \u00b6 registerForCleanup( objectForCleanup: AnyRef, task: CleanupTask): Unit registerForCleanup adds the input objectForCleanup to the < > internal queue. Despite the widest-possible AnyRef type of the input objectForCleanup , the type is really CleanupTaskWeakReference which is a custom Java's {java-javadoc-url}/java/lang/ref/WeakReference.html[java.lang.ref.WeakReference]. registerForCleanup is used when ContextCleaner is requested to < >, < >, < >, < >, and < >. == [[doCleanupShuffle]] Shuffle Cleanup [source, scala] \u00b6 doCleanupShuffle( shuffleId: Int, blocking: Boolean): Unit doCleanupShuffle performs a shuffle cleanup which is to remove the shuffle from the current scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] and storage:BlockManagerMaster.md[BlockManagerMaster]. doCleanupShuffle also notifies core:CleanerListener.md[CleanerListeners]. Internally, when executed, doCleanupShuffle prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Cleaning shuffle [id] \u00b6 doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#mapOutputTracker[MapOutputTracker] to scheduler:MapOutputTracker.md#unregisterShuffle[unregister the given shuffle]. doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#blockManager[BlockManagerMaster] to storage:BlockManagerMaster.md#removeShuffle[remove the shuffle blocks] (for the given shuffleId). doCleanupShuffle informs all registered < > that core:CleanerListener.md#shuffleCleaned[shuffle was cleaned]. In the end, doCleanupShuffle prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Cleaned shuffle [id] \u00b6 In case of any exception, doCleanupShuffle prints out the following ERROR message to the logs and the exception itself: [source,plaintext] \u00b6 Error cleaning shuffle [id] \u00b6 doCleanupShuffle is used when ContextCleaner is requested to < > and (interestingly) while fitting an ALSModel (in Spark MLlib). == [[logging]] Logging Enable ALL logging level for org.apache.spark.ContextCleaner logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.ContextCleaner=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[referenceBuffer]] referenceBuffer === [[referenceQueue]] referenceQueue","title":"ContextCleaner"},{"location":"core/ContextCleaner/#contextcleaner","text":"ContextCleaner is a Spark service that is responsible for < > ( cleanup ) of < >, < >, < >, < > and < > that is aimed at reducing the memory requirements of long-running data-heavy Spark applications.","title":"ContextCleaner"},{"location":"core/ContextCleaner/#creating-instance","text":"ContextCleaner takes the following to be created: [[sc]] ROOT:SparkContext.md[] ContextCleaner is created and requested to start when SparkContext is created with ROOT:configuration-properties.md#spark.cleaner.referenceTracking[spark.cleaner.referenceTracking] configuration property enabled. == [[cleaningThread]] Spark Context Cleaner Cleaning Thread ContextCleaner uses a daemon thread Spark Context Cleaner to clean RDD, shuffle, and broadcast states. The Spark Context Cleaner thread is started when ContextCleaner is requested to < >. == [[listeners]][[attachListener]] CleanerListeners ContextCleaner allows attaching core:CleanerListener.md[CleanerListeners] to be informed when objects are cleaned using attachListener method.","title":"Creating Instance"},{"location":"core/ContextCleaner/#sourcescala","text":"attachListener( listener: CleanerListener): Unit == [[doCleanupRDD]] doCleanupRDD Method","title":"[source,scala]"},{"location":"core/ContextCleaner/#source-scala","text":"doCleanupRDD( rddId: Int, blocking: Boolean): Unit doCleanupRDD...FIXME doCleanupRDD is used when ContextCleaner is requested to < > for a CleanRDD. == [[keepCleaning]] keepCleaning Internal Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/ContextCleaner/#keepcleaning-unit","text":"keepCleaning runs indefinitely until ContextCleaner is requested to < >. keepCleaning...FIXME keepCleaning prints out the following DEBUG message to the logs:","title":"keepCleaning(): Unit"},{"location":"core/ContextCleaner/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#got-cleaning-task-task","text":"keepCleaning is used in < > that is started once when ContextCleaner is requested to < >. == [[registerRDDCheckpointDataForCleanup]] registerRDDCheckpointDataForCleanup Method","title":"Got cleaning task [task]"},{"location":"core/ContextCleaner/#source-scala_2","text":"registerRDDCheckpointDataForCleanup T : Unit registerRDDCheckpointDataForCleanup...FIXME registerRDDCheckpointDataForCleanup is used when ContextCleaner is requested to < > (with ROOT:configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled). == [[registerBroadcastForCleanup]] registerBroadcastForCleanup Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_3","text":"registerBroadcastForCleanup T : Unit registerBroadcastForCleanup...FIXME registerBroadcastForCleanup is used when SparkContext is used to ROOT:SparkContext.md#broadcast[create a broadcast variable]. == [[registerRDDForCleanup]] registerRDDForCleanup Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_4","text":"registerRDDForCleanup( rdd: RDD[_]): Unit registerRDDForCleanup...FIXME registerRDDForCleanup is used for rdd:RDD.md#persist[RDD.persist] operation. == [[registerAccumulatorForCleanup]] registerAccumulatorForCleanup Method","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_5","text":"registerAccumulatorForCleanup( a: AccumulatorV2[_, _]): Unit registerAccumulatorForCleanup...FIXME registerAccumulatorForCleanup is used when AccumulatorV2 is requested to register. == [[stop]] Stopping ContextCleaner","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_6","text":"","title":"[source, scala]"},{"location":"core/ContextCleaner/#stop-unit","text":"stop...FIXME stop is used when SparkContext is requested to ROOT:SparkContext.md#stop[stop]. == [[start]] Starting ContextCleaner","title":"stop(): Unit"},{"location":"core/ContextCleaner/#source-scala_7","text":"","title":"[source, scala]"},{"location":"core/ContextCleaner/#start-unit","text":"start starts the < > and an action to request the JVM garbage collector (using System.gc() ) on regular basis per ROOT:configuration-properties.md#spark.cleaner.periodicGC.interval[spark.cleaner.periodicGC.interval] configuration property. The action to request the JVM GC is scheduled on < >. start is used when SparkContext is created. == [[periodicGCService]] periodicGCService Single-Thread Executor Service periodicGCService is an internal single-thread {java-javadoc-url}/java/util/concurrent/ScheduledExecutorService.html[executor service] with the name context-cleaner-periodic-gc to request the JVM garbage collector. The periodic runs are started when < > and stopped when < >. == [[registerShuffleForCleanup]] Registering ShuffleDependency for Cleanup","title":"start(): Unit"},{"location":"core/ContextCleaner/#source-scala_8","text":"registerShuffleForCleanup( shuffleDependency: ShuffleDependency[_, _, _]): Unit registerShuffleForCleanup registers the given ShuffleDependency for cleanup. Internally, registerShuffleForCleanup simply executes < > for the given ShuffleDependency. registerShuffleForCleanup is used when ShuffleDependency is created. == [[registerForCleanup]] Registering Object Reference For Cleanup","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_9","text":"registerForCleanup( objectForCleanup: AnyRef, task: CleanupTask): Unit registerForCleanup adds the input objectForCleanup to the < > internal queue. Despite the widest-possible AnyRef type of the input objectForCleanup , the type is really CleanupTaskWeakReference which is a custom Java's {java-javadoc-url}/java/lang/ref/WeakReference.html[java.lang.ref.WeakReference]. registerForCleanup is used when ContextCleaner is requested to < >, < >, < >, < >, and < >. == [[doCleanupShuffle]] Shuffle Cleanup","title":"[source, scala]"},{"location":"core/ContextCleaner/#source-scala_10","text":"doCleanupShuffle( shuffleId: Int, blocking: Boolean): Unit doCleanupShuffle performs a shuffle cleanup which is to remove the shuffle from the current scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] and storage:BlockManagerMaster.md[BlockManagerMaster]. doCleanupShuffle also notifies core:CleanerListener.md[CleanerListeners]. Internally, when executed, doCleanupShuffle prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"core/ContextCleaner/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#cleaning-shuffle-id","text":"doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#mapOutputTracker[MapOutputTracker] to scheduler:MapOutputTracker.md#unregisterShuffle[unregister the given shuffle]. doCleanupShuffle uses core:SparkEnv.md[SparkEnv] to access the core:SparkEnv.md#blockManager[BlockManagerMaster] to storage:BlockManagerMaster.md#removeShuffle[remove the shuffle blocks] (for the given shuffleId). doCleanupShuffle informs all registered < > that core:CleanerListener.md#shuffleCleaned[shuffle was cleaned]. In the end, doCleanupShuffle prints out the following DEBUG message to the logs:","title":"Cleaning shuffle [id]"},{"location":"core/ContextCleaner/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#cleaned-shuffle-id","text":"In case of any exception, doCleanupShuffle prints out the following ERROR message to the logs and the exception itself:","title":"Cleaned shuffle [id]"},{"location":"core/ContextCleaner/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#error-cleaning-shuffle-id","text":"doCleanupShuffle is used when ContextCleaner is requested to < > and (interestingly) while fitting an ALSModel (in Spark MLlib). == [[logging]] Logging Enable ALL logging level for org.apache.spark.ContextCleaner logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Error cleaning shuffle [id]"},{"location":"core/ContextCleaner/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"core/ContextCleaner/#log4jloggerorgapachesparkcontextcleanerall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[referenceBuffer]] referenceBuffer === [[referenceQueue]] referenceQueue","title":"log4j.logger.org.apache.spark.ContextCleaner=ALL"},{"location":"core/ElementTrackingStore/","text":"= ElementTrackingStore ElementTrackingStore is a core:KVStore.md[]. == [[creating-instance]] Creating Instance ElementTrackingStore takes the following to be created: [[store]] core:KVStore.md[] [[conf]] ROOT:SparkConf.md[] ElementTrackingStore is created when...FIXME == [[write]] write Method [source, scala] \u00b6 write(value: Any, checkTriggers: Boolean): Unit \u00b6 NOTE: write is part of LINK#write[HERE Contract] to...FIXME. write ...FIXME NOTE: write is used when...FIXME == [[Trigger]] Trigger Trigger is...FIXME == [[internal-properties]] Internal Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | triggers | [[triggers]] < > per class Used when...FIXME | flushTriggers | [[flushTriggers]] Functions that...FIXME Used when...FIXME | executor | [[executor]] Java ExecutorService that...FIXME Used when...FIXME | stopped | [[stopped]] stopped flag to control...FIXME Used when...FIXME |===","title":"ElementTrackingStore"},{"location":"core/ElementTrackingStore/#source-scala","text":"","title":"[source, scala]"},{"location":"core/ElementTrackingStore/#writevalue-any-checktriggers-boolean-unit","text":"NOTE: write is part of LINK#write[HERE Contract] to...FIXME. write ...FIXME NOTE: write is used when...FIXME == [[Trigger]] Trigger Trigger is...FIXME == [[internal-properties]] Internal Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | triggers | [[triggers]] < > per class Used when...FIXME | flushTriggers | [[flushTriggers]] Functions that...FIXME Used when...FIXME | executor | [[executor]] Java ExecutorService that...FIXME Used when...FIXME | stopped | [[stopped]] stopped flag to control...FIXME Used when...FIXME |===","title":"write(value: Any, checkTriggers: Boolean): Unit"},{"location":"core/InMemoryStore/","text":"= InMemoryStore InMemoryStore is a core:KVStore.md[]. == [[creating-instance]] Creating Instance InMemoryStore takes no arguments when created. InMemoryStore is created when: FsHistoryProvider is spark-history-server:FsHistoryProvider.md#listing[created] and requested to spark-history-server:FsHistoryProvider.md#createInMemoryStore[createInMemoryStore] AppStatusStore utility is used to core:AppStatusStore.md#createLiveStore[create an AppStatusStore for a live Spark application]","title":"InMemoryStore"},{"location":"core/KVStore/","text":"= KVStore KVStore is an < > of < >. == [[contract]] Contract === [[count]] count [source,java] \u00b6 long count( Class<?> type) long count( Class<?> type, String index, Object indexedValue) === [[delete]] delete [source,java] \u00b6 void delete( Class<?> type, Object naturalKey) === [[getMetadata]] getMetadata [source,java] \u00b6 T getMetadata( Class klass) === [[read]] read [source,java] \u00b6 T read( Class klass, Object naturalKey) === [[removeAllByIndexValues]] removeAllByIndexValues [source,java] \u00b6 boolean removeAllByIndexValues( Class klass, String index, Collection<?> indexValues) === [[setMetadata]] setMetadata [source,java] \u00b6 void setMetadata( Object value) === [[view]] view [source,java] \u00b6 KVStoreView view( Class type) === [[write]] write [source,java] \u00b6 void write( Object value) == [[implementations]] KVStores [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KVStore | Description | core:ElementTrackingStore.md[] | [[ElementTrackingStore]] | core:InMemoryStore.md[] | [[InMemoryStore]] | core:LevelDB.md[] | [[LevelDB]] |===","title":"KVStore"},{"location":"core/KVStore/#sourcejava","text":"long count( Class<?> type) long count( Class<?> type, String index, Object indexedValue) === [[delete]] delete","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_1","text":"void delete( Class<?> type, Object naturalKey) === [[getMetadata]] getMetadata","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_2","text":"T getMetadata( Class klass) === [[read]] read","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_3","text":"T read( Class klass, Object naturalKey) === [[removeAllByIndexValues]] removeAllByIndexValues","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_4","text":"boolean removeAllByIndexValues( Class klass, String index, Collection<?> indexValues) === [[setMetadata]] setMetadata","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_5","text":"void setMetadata( Object value) === [[view]] view","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_6","text":"KVStoreView view( Class type) === [[write]] write","title":"[source,java]"},{"location":"core/KVStore/#sourcejava_7","text":"void write( Object value) == [[implementations]] KVStores [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KVStore | Description | core:ElementTrackingStore.md[] | [[ElementTrackingStore]] | core:InMemoryStore.md[] | [[InMemoryStore]] | core:LevelDB.md[] | [[LevelDB]] |===","title":"[source,java]"},{"location":"core/LevelDB/","text":"= LevelDB LevelDB is a core:KVStore.md[]. == [[creating-instance]] Creating Instance LevelDB takes the following to be created: [[path]] File [[serializer]] KVStoreSerializer LevelDB is created when KVUtils utility is used to open or create a LevelDB store (for spark-history-server:FsHistoryProvider.md[]).","title":"LevelDB"},{"location":"core/RetryingBlockFetcher/","text":"= RetryingBlockFetcher RetryingBlockFetcher is...FIXME RetryingBlockFetcher is < > and immediately < > when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) RetryingBlockFetcher uses a < > to core:BlockFetchStarter.md#createAndStart[createAndStart] when requested to < > and later < >. [[outstandingBlocksIds]] RetryingBlockFetcher uses outstandingBlocksIds internal registry of outstanding block IDs to fetch that is initially the < > when < >. At < >, RetryingBlockFetcher prints out the following INFO message to the logs (with the number of < >): Retrying fetch ([retryCount]/[maxRetries]) for [size] outstanding blocks after [retryWaitTime] ms On < > and < >, < > removes the block ID from < >. [[currentListener]] RetryingBlockFetcher uses a < > to remove block IDs from the < > internal registry. == [[creating-instance]] Creating RetryingBlockFetcher Instance RetryingBlockFetcher takes the following when created: [[conf]] network:TransportConf.md[] [[fetchStarter]] core:BlockFetchStarter.md[] [[blockIds]] Block IDs to fetch [[listener]] core:BlockFetchingListener.md[] == [[start]] Starting RetryingBlockFetcher -- start Method [source, java] \u00b6 void start() \u00b6 start simply < >. [NOTE] \u00b6 start is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) * ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default) \u00b6 == [[initiateRetry]] initiateRetry Internal Method [source, java] \u00b6 synchronized void initiateRetry() \u00b6 initiateRetry ...FIXME [NOTE] \u00b6 initiateRetry is used when: RetryingBlockFetcher is requested to < > * RetryingBlockFetchListener is requested to < > \u00b6 == [[fetchAllOutstanding]] fetchAllOutstanding Internal Method [source, java] \u00b6 void fetchAllOutstanding() \u00b6 fetchAllOutstanding requests < > to core:BlockFetchStarter.md#createAndStart[createAndStart] for the < >. NOTE: fetchAllOutstanding is used when RetryingBlockFetcher is requested to < > and < >. == [[RetryingBlockFetchListener]] RetryingBlockFetchListener RetryingBlockFetchListener is a core:BlockFetchingListener.md[] that < > uses to remove block IDs from the < > internal registry. === [[RetryingBlockFetchListener-onBlockFetchSuccess]] onBlockFetchSuccess Method [source, scala] \u00b6 void onBlockFetchSuccess(String blockId, ManagedBuffer data) \u00b6 NOTE: onBlockFetchSuccess is part of core:BlockFetchingListener.md#onBlockFetchSuccess[BlockFetchingListener Contract]. onBlockFetchSuccess ...FIXME === [[RetryingBlockFetchListener-onBlockFetchFailure]] onBlockFetchFailure Method [source, scala] \u00b6 void onBlockFetchFailure(String blockId, Throwable exception) \u00b6 NOTE: onBlockFetchFailure is part of core:BlockFetchingListener.md#onBlockFetchFailure[BlockFetchingListener Contract]. onBlockFetchFailure ...FIXME","title":"RetryingBlockFetcher"},{"location":"core/RetryingBlockFetcher/#source-java","text":"","title":"[source, java]"},{"location":"core/RetryingBlockFetcher/#void-start","text":"start simply < >.","title":"void start()"},{"location":"core/RetryingBlockFetcher/#note","text":"start is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default)","title":"[NOTE]"},{"location":"core/RetryingBlockFetcher/#externalshuffleclient-is-requested-to-storageexternalshuffleclientmdfetchblocksfetchblocks-when-networktransportconfmdiomaxretriesmaxioretries-is-greater-than-0-which-it-is-by-default","text":"== [[initiateRetry]] initiateRetry Internal Method","title":"* ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#fetchBlocks[fetchBlocks] (when network:TransportConf.md#io.maxRetries[maxIORetries] is greater than 0 which it is by default)"},{"location":"core/RetryingBlockFetcher/#source-java_1","text":"","title":"[source, java]"},{"location":"core/RetryingBlockFetcher/#synchronized-void-initiateretry","text":"initiateRetry ...FIXME","title":"synchronized void initiateRetry()"},{"location":"core/RetryingBlockFetcher/#note_1","text":"initiateRetry is used when: RetryingBlockFetcher is requested to < >","title":"[NOTE]"},{"location":"core/RetryingBlockFetcher/#retryingblockfetchlistener-is-requested-to","text":"== [[fetchAllOutstanding]] fetchAllOutstanding Internal Method","title":"* RetryingBlockFetchListener is requested to &lt;&gt;"},{"location":"core/RetryingBlockFetcher/#source-java_2","text":"","title":"[source, java]"},{"location":"core/RetryingBlockFetcher/#void-fetchalloutstanding","text":"fetchAllOutstanding requests < > to core:BlockFetchStarter.md#createAndStart[createAndStart] for the < >. NOTE: fetchAllOutstanding is used when RetryingBlockFetcher is requested to < > and < >. == [[RetryingBlockFetchListener]] RetryingBlockFetchListener RetryingBlockFetchListener is a core:BlockFetchingListener.md[] that < > uses to remove block IDs from the < > internal registry. === [[RetryingBlockFetchListener-onBlockFetchSuccess]] onBlockFetchSuccess Method","title":"void fetchAllOutstanding()"},{"location":"core/RetryingBlockFetcher/#source-scala","text":"","title":"[source, scala]"},{"location":"core/RetryingBlockFetcher/#void-onblockfetchsuccessstring-blockid-managedbuffer-data","text":"NOTE: onBlockFetchSuccess is part of core:BlockFetchingListener.md#onBlockFetchSuccess[BlockFetchingListener Contract]. onBlockFetchSuccess ...FIXME === [[RetryingBlockFetchListener-onBlockFetchFailure]] onBlockFetchFailure Method","title":"void onBlockFetchSuccess(String blockId, ManagedBuffer data)"},{"location":"core/RetryingBlockFetcher/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/RetryingBlockFetcher/#void-onblockfetchfailurestring-blockid-throwable-exception","text":"NOTE: onBlockFetchFailure is part of core:BlockFetchingListener.md#onBlockFetchFailure[BlockFetchingListener Contract]. onBlockFetchFailure ...FIXME","title":"void onBlockFetchFailure(String blockId, Throwable exception)"},{"location":"core/TorrentBroadcast/","text":"= TorrentBroadcast TorrentBroadcast is a ROOT:Broadcast.md[] that uses a BitTorrent-like protocol for broadcast blocks distribution. .TorrentBroadcast -- Broadcasting using BitTorrent image::sparkcontext-broadcast-bittorrent.png[align=\"center\"] When a ROOT:SparkContext.md#broadcast[broadcast variable is created (using SparkContext.broadcast )] on the driver, a < >. [source, scala] \u00b6 // On the driver val sc: SparkContext = ??? val anyScalaValue = ??? val b = sc.broadcast(anyScalaValue) // \u2190 TorrentBroadcast is created A broadcast variable is stored on the driver's storage:BlockManager.md[BlockManager] as a single value and separately as broadcast blocks (after it was < >). The broadcast block size is the value of core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property. .TorrentBroadcast puts broadcast and the chunks to driver's BlockManager image::sparkcontext-broadcast-bittorrent-newBroadcast.png[align=\"center\"] NOTE: TorrentBroadcast-based broadcast variables are created using core:TorrentBroadcastFactory.md[TorrentBroadcastFactory]. == [[creating-instance]] Creating Instance TorrentBroadcast takes the following to be created: [[obj]] Object (the value) to be broadcast [[id]] ID TorrentBroadcast is created when TorrentBroadcastFactory is requested for a core:TorrentBroadcastFactory.md#newBroadcast[new broadcast variable]. == [[_value]] Transient Lazy Broadcast Value [source, scala] \u00b6 _value: T \u00b6 TorrentBroadcast uses _value internal registry for the value that is < > on demand (and cached afterwards). _value is a @transient private lazy val and uses two Scala language features: It is not serialized when the TorrentBroadcast is serialized to be sent over the wire to executors (and has to be < > afterwards) It is lazily instantiated when first requested and cached afterwards == [[numBlocks]] numBlocks Internal Value TorrentBroadcast uses numBlocks internal value for the total number of blocks it contains. It is < > when TorrentBroadcast is < >. == [[getValue]] Getting Value of Broadcast Variable [source, scala] \u00b6 def getValue(): T \u00b6 getValue returns the <<_value, _value>>. getValue is part of the ROOT:Broadcast.md#getValue[Broadcast] abstraction. == [[broadcastId]] BroadcastBlockId TorrentBroadcast uses a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for...FIXME == [[readBroadcastBlock]] readBroadcastBlock Internal Method [source, scala] \u00b6 readBroadcastBlock(): T \u00b6 readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#broadcastManager[BroadcastManager] that is requested for BroadcastManager.md#cachedValues[cached broadcast values]. readBroadcastBlock looks up the < > in the cached broadcast values and returns it if found. If not found, readBroadcastBlock requests the SparkEnv for the core:SparkEnv.md#conf[SparkConf] and < >. readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#blockManager[BlockManager]. readBroadcastBlock requests the BlockManager for storage:BlockManager.md#getLocalValues[getLocalValues]. If the broadcast data was available locally, readBroadcastBlock < > for the broadcast and returns the value. If however the broadcast data was not found locally, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Started reading broadcast variable [id] \u00b6 readBroadcastBlock < > of the broadcast. You should see the following INFO message in the logs: [source,plaintext] \u00b6 Reading broadcast variable [id] took [usedTimeMs] \u00b6 readBroadcastBlock < ByteBuffer blocks>> NOTE: readBroadcastBlock uses the core:SparkEnv.md#serializer[current Serializer ] and the internal io:CompressionCodec.md[CompressionCodec] to bring all the blocks together as one single broadcast variable. readBroadcastBlock storage:BlockManager.md#putSingle[stores the broadcast variable with MEMORY_AND_DISK storage level to the local BlockManager ]. When storing the broadcast variable was unsuccessful, a SparkException is thrown. [source,plaintext] \u00b6 Failed to store [broadcastId] in BlockManager \u00b6 The broadcast variable is returned. readBroadcastBlock is used when TorrentBroadcast is requested for the <<_value, broadcast value>>. == [[setConf]] setConf Internal Method [source, scala] \u00b6 setConf( conf: SparkConf): Unit setConf uses the input conf ROOT:SparkConf.md[SparkConf] to set compression codec and the block size. Internally, setConf reads core:BroadcastManager.md#spark.broadcast.compress[spark.broadcast.compress] configuration property and if enabled (which it is by default) sets a io:CompressionCodec.md#createCodec[CompressionCodec] (as an internal compressionCodec property). setConf also reads core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property and sets the block size (as the internal blockSize property). setConf is executed when < > or < >. == [[writeBlocks]] Storing Broadcast and Blocks to BlockManager [source, scala] \u00b6 writeBlocks( value: T): Int writeBlocks stores the given value (that is the < >) and the blocks in storage:BlockManager.md[]. writeBlocks returns the < > (was divided into). Internally, writeBlocks uses the core:SparkEnv.md#get[SparkEnv] to access core:SparkEnv.md#blockManager[BlockManager]. writeBlocks requests the BlockManager to storage:BlockManager.md#putSingle[putSingle] (with MEMORY_AND_DISK storage level). writeBlocks < > the given value (of the < >, the system core:SparkEnv.md#serializer[Serializer], and the optional < >). For every block, writeBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > and piece[index] identifier, and requests the BlockManager to storage:BlockManager.md#putBytes[putBytes] (with MEMORY_AND_DISK_SER storage level). The entire broadcast value is stored in the local BlockManager with MEMORY_AND_DISK storage level whereas the blocks with MEMORY_AND_DISK_SER storage level. With < > writeBlocks...FIXME In case of an error while storing the value or the blocks, writeBlocks throws a SparkException: [source,plaintext] \u00b6 Failed to store [pieceId] of [broadcastId] in local BlockManager \u00b6 writeBlocks is used when TorrentBroadcast is < > for the < > internal registry (that happens on the driver only). == [[blockifyObject]] Chunking Broadcast Variable Into Blocks [source, scala] \u00b6 blockifyObject T : Array[ByteBuffer] blockifyObject divides (aka blockifies ) the input obj value into blocks ( ByteBuffer chunks). blockifyObject uses the given serializer:Serializer.md[] to write the value in a serialized format to a ChunkedByteBufferOutputStream of the given blockSize size with the optional io:CompressionCodec.md[CompressionCodec]. blockifyObject is used when TorrentBroadcast is requested to < >. == [[doUnpersist]] doUnpersist Method [source, scala] \u00b6 doUnpersist(blocking: Boolean): Unit \u00b6 doUnpersist < >. NOTE: doUnpersist is part of the ROOT:Broadcast.md#contract[ Broadcast Variable Contract] and is executed from < > method. == [[doDestroy]] doDestroy Method [source, scala] \u00b6 doDestroy(blocking: Boolean): Unit \u00b6 doDestroy < >, i.e. the driver and executors. NOTE: doDestroy is executed when ROOT:Broadcast.md#destroy-internal[ Broadcast removes the persisted data and metadata related to a broadcast variable]. == [[unpersist]] unpersist Utility [source, scala] \u00b6 unpersist( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unpersist removes all broadcast blocks from executors and, with the given removeFromDriver flag enabled, from the driver. When executed, unpersist prints out the following DEBUG message in the logs: [source,plaintext] \u00b6 Unpersisting TorrentBroadcast [id] \u00b6 unpersist requests storage:BlockManagerMaster.md#removeBroadcast[ BlockManagerMaster to remove the id broadcast]. NOTE: unpersist uses core:SparkEnv.md#blockManager[ SparkEnv to get the BlockManagerMaster ] (through blockManager property). unpersist is used when: TorrentBroadcast is requested to < > and < > TorrentBroadcastFactory is requested to TorrentBroadcastFactory.md#unbroadcast[unbroadcast] == [[readBlocks]] Reading Broadcast Blocks [source, scala] \u00b6 readBlocks(): Array[BlockData] \u00b6 readBlocks creates a local array of storage:BlockData.md[]s for < > elements (that is later modified and returned). readBlocks uses the core:SparkEnv.md[] to access core:SparkEnv.md#blockManager[BlockManager] (that is later used to fetch local or remote blocks). For every block (randomly-chosen by block ID between 0 and < >), readBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > (of the broadcast variable) and the chunk identified by the piece prefix followed by the ID. readBlocks prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Reading piece [pieceId] of [broadcastId] \u00b6 readBlocks first tries to look up the piece locally by requesting the BlockManager to storage:BlockManager.md#getLocalBytes[getLocalBytes] and, if found, stores the reference in the local block array (for the piece ID) and < > for the chunk. If not found locally, readBlocks requests the BlockManager to storage:BlockManager.md#getRemoteBytes[getRemoteBytes]. readBlocks...FIXME readBlocks throws a SparkException for blocks neither available locally nor remotely: [source,plaintext] \u00b6 Failed to get [pieceId] of [broadcastId] \u00b6 readBlocks is used when TorrentBroadcast is requested to < >. == [[unBlockifyObject]] unBlockifyObject Utility [source, scala] \u00b6 unBlockifyObject T: ClassTag : T unBlockifyObject...FIXME unBlockifyObject is used when TorrentBroadcast is requested to < >. == [[releaseLock]] releaseLock Internal Method [source, scala] \u00b6 releaseLock( blockId: BlockId): Unit releaseLock...FIXME releaseLock is used when TorrentBroadcast is requested to < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.broadcast.TorrentBroadcast logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.broadcast.TorrentBroadcast=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"TorrentBroadcast"},{"location":"core/TorrentBroadcast/#source-scala","text":"// On the driver val sc: SparkContext = ??? val anyScalaValue = ??? val b = sc.broadcast(anyScalaValue) // \u2190 TorrentBroadcast is created A broadcast variable is stored on the driver's storage:BlockManager.md[BlockManager] as a single value and separately as broadcast blocks (after it was < >). The broadcast block size is the value of core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property. .TorrentBroadcast puts broadcast and the chunks to driver's BlockManager image::sparkcontext-broadcast-bittorrent-newBroadcast.png[align=\"center\"] NOTE: TorrentBroadcast-based broadcast variables are created using core:TorrentBroadcastFactory.md[TorrentBroadcastFactory]. == [[creating-instance]] Creating Instance TorrentBroadcast takes the following to be created: [[obj]] Object (the value) to be broadcast [[id]] ID TorrentBroadcast is created when TorrentBroadcastFactory is requested for a core:TorrentBroadcastFactory.md#newBroadcast[new broadcast variable]. == [[_value]] Transient Lazy Broadcast Value","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_1","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#_value-t","text":"TorrentBroadcast uses _value internal registry for the value that is < > on demand (and cached afterwards). _value is a @transient private lazy val and uses two Scala language features: It is not serialized when the TorrentBroadcast is serialized to be sent over the wire to executors (and has to be < > afterwards) It is lazily instantiated when first requested and cached afterwards == [[numBlocks]] numBlocks Internal Value TorrentBroadcast uses numBlocks internal value for the total number of blocks it contains. It is < > when TorrentBroadcast is < >. == [[getValue]] Getting Value of Broadcast Variable","title":"_value: T"},{"location":"core/TorrentBroadcast/#source-scala_2","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#def-getvalue-t","text":"getValue returns the <<_value, _value>>. getValue is part of the ROOT:Broadcast.md#getValue[Broadcast] abstraction. == [[broadcastId]] BroadcastBlockId TorrentBroadcast uses a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for...FIXME == [[readBroadcastBlock]] readBroadcastBlock Internal Method","title":"def getValue(): T"},{"location":"core/TorrentBroadcast/#source-scala_3","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#readbroadcastblock-t","text":"readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#broadcastManager[BroadcastManager] that is requested for BroadcastManager.md#cachedValues[cached broadcast values]. readBroadcastBlock looks up the < > in the cached broadcast values and returns it if found. If not found, readBroadcastBlock requests the SparkEnv for the core:SparkEnv.md#conf[SparkConf] and < >. readBroadcastBlock SparkEnv.md#get[uses the SparkEnv] to access SparkEnv.md#blockManager[BlockManager]. readBroadcastBlock requests the BlockManager for storage:BlockManager.md#getLocalValues[getLocalValues]. If the broadcast data was available locally, readBroadcastBlock < > for the broadcast and returns the value. If however the broadcast data was not found locally, you should see the following INFO message in the logs:","title":"readBroadcastBlock(): T"},{"location":"core/TorrentBroadcast/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#started-reading-broadcast-variable-id","text":"readBroadcastBlock < > of the broadcast. You should see the following INFO message in the logs:","title":"Started reading broadcast variable [id]"},{"location":"core/TorrentBroadcast/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#reading-broadcast-variable-id-took-usedtimems","text":"readBroadcastBlock < ByteBuffer blocks>> NOTE: readBroadcastBlock uses the core:SparkEnv.md#serializer[current Serializer ] and the internal io:CompressionCodec.md[CompressionCodec] to bring all the blocks together as one single broadcast variable. readBroadcastBlock storage:BlockManager.md#putSingle[stores the broadcast variable with MEMORY_AND_DISK storage level to the local BlockManager ]. When storing the broadcast variable was unsuccessful, a SparkException is thrown.","title":"Reading broadcast variable [id] took [usedTimeMs]"},{"location":"core/TorrentBroadcast/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#failed-to-store-broadcastid-in-blockmanager","text":"The broadcast variable is returned. readBroadcastBlock is used when TorrentBroadcast is requested for the <<_value, broadcast value>>. == [[setConf]] setConf Internal Method","title":"Failed to store [broadcastId] in BlockManager"},{"location":"core/TorrentBroadcast/#source-scala_4","text":"setConf( conf: SparkConf): Unit setConf uses the input conf ROOT:SparkConf.md[SparkConf] to set compression codec and the block size. Internally, setConf reads core:BroadcastManager.md#spark.broadcast.compress[spark.broadcast.compress] configuration property and if enabled (which it is by default) sets a io:CompressionCodec.md#createCodec[CompressionCodec] (as an internal compressionCodec property). setConf also reads core:BroadcastManager.md#spark_broadcast_blockSize[spark.broadcast.blockSize] Spark property and sets the block size (as the internal blockSize property). setConf is executed when < > or < >. == [[writeBlocks]] Storing Broadcast and Blocks to BlockManager","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_5","text":"writeBlocks( value: T): Int writeBlocks stores the given value (that is the < >) and the blocks in storage:BlockManager.md[]. writeBlocks returns the < > (was divided into). Internally, writeBlocks uses the core:SparkEnv.md#get[SparkEnv] to access core:SparkEnv.md#blockManager[BlockManager]. writeBlocks requests the BlockManager to storage:BlockManager.md#putSingle[putSingle] (with MEMORY_AND_DISK storage level). writeBlocks < > the given value (of the < >, the system core:SparkEnv.md#serializer[Serializer], and the optional < >). For every block, writeBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > and piece[index] identifier, and requests the BlockManager to storage:BlockManager.md#putBytes[putBytes] (with MEMORY_AND_DISK_SER storage level). The entire broadcast value is stored in the local BlockManager with MEMORY_AND_DISK storage level whereas the blocks with MEMORY_AND_DISK_SER storage level. With < > writeBlocks...FIXME In case of an error while storing the value or the blocks, writeBlocks throws a SparkException:","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#failed-to-store-pieceid-of-broadcastid-in-local-blockmanager","text":"writeBlocks is used when TorrentBroadcast is < > for the < > internal registry (that happens on the driver only). == [[blockifyObject]] Chunking Broadcast Variable Into Blocks","title":"Failed to store [pieceId] of [broadcastId] in local BlockManager"},{"location":"core/TorrentBroadcast/#source-scala_6","text":"blockifyObject T : Array[ByteBuffer] blockifyObject divides (aka blockifies ) the input obj value into blocks ( ByteBuffer chunks). blockifyObject uses the given serializer:Serializer.md[] to write the value in a serialized format to a ChunkedByteBufferOutputStream of the given blockSize size with the optional io:CompressionCodec.md[CompressionCodec]. blockifyObject is used when TorrentBroadcast is requested to < >. == [[doUnpersist]] doUnpersist Method","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_7","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#dounpersistblocking-boolean-unit","text":"doUnpersist < >. NOTE: doUnpersist is part of the ROOT:Broadcast.md#contract[ Broadcast Variable Contract] and is executed from < > method. == [[doDestroy]] doDestroy Method","title":"doUnpersist(blocking: Boolean): Unit"},{"location":"core/TorrentBroadcast/#source-scala_8","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#dodestroyblocking-boolean-unit","text":"doDestroy < >, i.e. the driver and executors. NOTE: doDestroy is executed when ROOT:Broadcast.md#destroy-internal[ Broadcast removes the persisted data and metadata related to a broadcast variable]. == [[unpersist]] unpersist Utility","title":"doDestroy(blocking: Boolean): Unit"},{"location":"core/TorrentBroadcast/#source-scala_9","text":"unpersist( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unpersist removes all broadcast blocks from executors and, with the given removeFromDriver flag enabled, from the driver. When executed, unpersist prints out the following DEBUG message in the logs:","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#unpersisting-torrentbroadcast-id","text":"unpersist requests storage:BlockManagerMaster.md#removeBroadcast[ BlockManagerMaster to remove the id broadcast]. NOTE: unpersist uses core:SparkEnv.md#blockManager[ SparkEnv to get the BlockManagerMaster ] (through blockManager property). unpersist is used when: TorrentBroadcast is requested to < > and < > TorrentBroadcastFactory is requested to TorrentBroadcastFactory.md#unbroadcast[unbroadcast] == [[readBlocks]] Reading Broadcast Blocks","title":"Unpersisting TorrentBroadcast [id]"},{"location":"core/TorrentBroadcast/#source-scala_10","text":"","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#readblocks-arrayblockdata","text":"readBlocks creates a local array of storage:BlockData.md[]s for < > elements (that is later modified and returned). readBlocks uses the core:SparkEnv.md[] to access core:SparkEnv.md#blockManager[BlockManager] (that is later used to fetch local or remote blocks). For every block (randomly-chosen by block ID between 0 and < >), readBlocks creates a storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] for the < > (of the broadcast variable) and the chunk identified by the piece prefix followed by the ID. readBlocks prints out the following DEBUG message to the logs:","title":"readBlocks(): Array[BlockData]"},{"location":"core/TorrentBroadcast/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#reading-piece-pieceid-of-broadcastid","text":"readBlocks first tries to look up the piece locally by requesting the BlockManager to storage:BlockManager.md#getLocalBytes[getLocalBytes] and, if found, stores the reference in the local block array (for the piece ID) and < > for the chunk. If not found locally, readBlocks requests the BlockManager to storage:BlockManager.md#getRemoteBytes[getRemoteBytes]. readBlocks...FIXME readBlocks throws a SparkException for blocks neither available locally nor remotely:","title":"Reading piece [pieceId] of [broadcastId]"},{"location":"core/TorrentBroadcast/#sourceplaintext_6","text":"","title":"[source,plaintext]"},{"location":"core/TorrentBroadcast/#failed-to-get-pieceid-of-broadcastid","text":"readBlocks is used when TorrentBroadcast is requested to < >. == [[unBlockifyObject]] unBlockifyObject Utility","title":"Failed to get [pieceId] of [broadcastId]"},{"location":"core/TorrentBroadcast/#source-scala_11","text":"unBlockifyObject T: ClassTag : T unBlockifyObject...FIXME unBlockifyObject is used when TorrentBroadcast is requested to < >. == [[releaseLock]] releaseLock Internal Method","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source-scala_12","text":"releaseLock( blockId: BlockId): Unit releaseLock...FIXME releaseLock is used when TorrentBroadcast is requested to < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.broadcast.TorrentBroadcast logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"core/TorrentBroadcast/#source","text":"","title":"[source]"},{"location":"core/TorrentBroadcast/#log4jloggerorgapachesparkbroadcasttorrentbroadcastall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.broadcast.TorrentBroadcast=ALL"},{"location":"core/TorrentBroadcastFactory/","text":"= TorrentBroadcastFactory TorrentBroadcastFactory is a core:BroadcastFactory.md[BroadcastFactory] of core:TorrentBroadcast.md[TorrentBroadcast]s (for BitTorrent-like ROOT:Broadcast.md[]s). NOTE: As of https://issues.apache.org/jira/browse/SPARK-12588[Spark 2.0] TorrentBroadcastFactory is is the one and only known core:BroadcastFactory.md[BroadcastFactory]. == [[creating-instance]] Creating Instance TorrentBroadcastFactory takes no arguments to be created. TorrentBroadcastFactory is created for BroadcastManager.md#broadcastFactory[BroadcastManager]. == [[newBroadcast]] Creating Broadcast Variable (TorrentBroadcast) [source,scala] \u00b6 newBroadcast T: ClassTag : Broadcast[T] newBroadcast creates a core:TorrentBroadcast.md[] (for the given value_ and id and ignoring the isLocal flag). newBroadcast is part of the BroadcastFactory.md#newBroadcast[BroadcastFactory] abstraction. == [[unbroadcast]] Unbroadcasting Broadcast Variable [source,scala] \u00b6 unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unbroadcast core:TorrentBroadcast.md#unpersist[removes all persisted state associated with the TorrentBroadcast] (by the given id). unbroadcast is part of the BroadcastFactory.md#unbroadcast[BroadcastFactory] abstraction. == [[initialize]] Initializing TorrentBroadcastFactory [source,scala] \u00b6 initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit initialize does nothing. initialize is part of the BroadcastFactory.md#initialize[BroadcastFactory] abstraction. == [[stop]] Stopping TorrentBroadcastFactory [source,scala] \u00b6 stop(): Unit \u00b6 stop does nothing. stop is part of the BroadcastFactory.md#stop[BroadcastFactory] abstraction.","title":"TorrentBroadcastFactory"},{"location":"core/TorrentBroadcastFactory/#sourcescala","text":"newBroadcast T: ClassTag : Broadcast[T] newBroadcast creates a core:TorrentBroadcast.md[] (for the given value_ and id and ignoring the isLocal flag). newBroadcast is part of the BroadcastFactory.md#newBroadcast[BroadcastFactory] abstraction. == [[unbroadcast]] Unbroadcasting Broadcast Variable","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#sourcescala_1","text":"unbroadcast( id: Long, removeFromDriver: Boolean, blocking: Boolean): Unit unbroadcast core:TorrentBroadcast.md#unpersist[removes all persisted state associated with the TorrentBroadcast] (by the given id). unbroadcast is part of the BroadcastFactory.md#unbroadcast[BroadcastFactory] abstraction. == [[initialize]] Initializing TorrentBroadcastFactory","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#sourcescala_2","text":"initialize( isDriver: Boolean, conf: SparkConf, securityMgr: SecurityManager): Unit initialize does nothing. initialize is part of the BroadcastFactory.md#initialize[BroadcastFactory] abstraction. == [[stop]] Stopping TorrentBroadcastFactory","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#sourcescala_3","text":"","title":"[source,scala]"},{"location":"core/TorrentBroadcastFactory/#stop-unit","text":"stop does nothing. stop is part of the BroadcastFactory.md#stop[BroadcastFactory] abstraction.","title":"stop(): Unit"},{"location":"demo/diskblockmanager-and-block-data/","text":"= Demo: DiskBlockManager and Block Data The demo shows how Spark stores data blocks on local disk (using storage:DiskBlockManager.md[DiskBlockManager] and storage:DiskStore.md[DiskStore] among the services). == Configure Local Directories Spark uses spark.local.dir configuration property for one or more local directories to store data blocks. Start spark-shell with the property set to a directory of your choice (say local-dirs ). Use one directory for easier monitoring. $SPARK_HOME/bin/spark-shell --conf spark.local.dir=local-dirs When started, Spark will create a proper directory layout. You are interested in blockmgr-[uuid] directory. == \"Create\" Data Blocks Execute the following Spark application that forces persisting ( caching ) data to disk. import org.apache.spark.storage.StorageLevel spark.range(2).persist(StorageLevel.DISK_ONLY).count == Observe Block Files Go to the blockmgr-[uuid] directory and observe the block files. There should be a few. Do you know how many and why? $ tree local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ local-dirs/blockmgr-b7167b5a-ae8d-404b-8de2-1a0fb101fe00/ \u251c\u2500\u2500 00 \u251c\u2500\u2500 04 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.data \u251c\u2500\u2500 06 \u251c\u2500\u2500 08 \u2502 \u2514\u2500\u2500 shuffle_0_8_0.index ... \u251c\u2500\u2500 37 \u2502 \u2514\u2500\u2500 shuffle_0_7_0.index \u251c\u2500\u2500 38 \u2502 \u2514\u2500\u2500 shuffle_0_4_0.data \u251c\u2500\u2500 39 \u2502 \u2514\u2500\u2500 shuffle_0_9_0.index \u2514\u2500\u2500 3a \u2514\u2500\u2500 shuffle_0_6_0.data 47 directories, 48 files == Use web UI Open http://localhost:4040 and switch to Storage tab (at http://localhost:4040/storage/ ). You should see one RDD cached. .Storage tab in web UI image::demo-DiskBlockManager-and-Block-Data-webui-storage.png[align=\"center\"] Click the link in RDD Name column and review the information. == Enable Logging Enable ALL logging level for storage:DiskStore.md#logging[org.apache.spark.storage.DiskStore] and storage:DiskBlockManager.md#logging[org.apache.spark.storage.DiskBlockManager] loggers to have an even deeper insight on the block storage internals. log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL log4j.logger.org.apache.spark.storage.DiskStore=ALL","title":"DiskBlockManager and Block Data"},{"location":"deploy/ExternalShuffleBlockHandler/","text":"= ExternalShuffleBlockHandler ExternalShuffleBlockHandler is a network:RpcHandler.md[]. When created, ExternalShuffleBlockHandler requires a network:OneForOneStreamManager.md[] and network:TransportConf.md[] with a registeredExecutorFile to create a ExternalShuffleBlockResolver . It < BlockTransferMessage messages>>: < > and < >. == [[handleMessage]] Handling Messages [source, java] \u00b6 handleMessage( BlockTransferMessage msgObj, TransportClient client, RpcResponseCallback callback) handleMessage handles two types of BlockTransferMessage messages: < > < > For any other BlockTransferMessage message it throws a UnsupportedOperationException : Unexpected message: [msgObj] == [[OpenBlocks]] OpenBlocks [source, java] \u00b6 OpenBlocks( String appId, String execId, String[] blockIds) When OpenBlocks is received, < > authorizes the client . CAUTION: FIXME checkAuth ? It then < > for each block id in blockIds (using < >). Finally, it network:OneForOneStreamManager.md#registerStream[registers a stream] and does callback.onSuccess with a serialized byte buffer (for the streamId and the number of blocks in msg ). CAUTION: FIXME callback.onSuccess ? You should see the following TRACE message in the logs: Registered streamId [streamId] with [length] buffers for client [clientId] from host [remoteAddress] == [[RegisterExecutor]] RegisterExecutor [source, java] \u00b6 RegisterExecutor( String appId, String execId, ExecutorShuffleInfo executorInfo) RegisterExecutor...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.shuffle.ExternalShuffleBlockHandler logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockHandler=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"ExternalShuffleBlockHandler"},{"location":"deploy/ExternalShuffleBlockHandler/#source-java","text":"handleMessage( BlockTransferMessage msgObj, TransportClient client, RpcResponseCallback callback) handleMessage handles two types of BlockTransferMessage messages: < > < > For any other BlockTransferMessage message it throws a UnsupportedOperationException : Unexpected message: [msgObj] == [[OpenBlocks]] OpenBlocks","title":"[source, java]"},{"location":"deploy/ExternalShuffleBlockHandler/#source-java_1","text":"OpenBlocks( String appId, String execId, String[] blockIds) When OpenBlocks is received, < > authorizes the client . CAUTION: FIXME checkAuth ? It then < > for each block id in blockIds (using < >). Finally, it network:OneForOneStreamManager.md#registerStream[registers a stream] and does callback.onSuccess with a serialized byte buffer (for the streamId and the number of blocks in msg ). CAUTION: FIXME callback.onSuccess ? You should see the following TRACE message in the logs: Registered streamId [streamId] with [length] buffers for client [clientId] from host [remoteAddress] == [[RegisterExecutor]] RegisterExecutor","title":"[source, java]"},{"location":"deploy/ExternalShuffleBlockHandler/#source-java_2","text":"RegisterExecutor( String appId, String execId, ExecutorShuffleInfo executorInfo) RegisterExecutor...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.shuffle.ExternalShuffleBlockHandler logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, java]"},{"location":"deploy/ExternalShuffleBlockHandler/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleBlockHandler/#log4jloggerorgapachesparknetworkshuffleexternalshuffleblockhandlerall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockHandler=ALL"},{"location":"deploy/ExternalShuffleBlockResolver/","text":"= ExternalShuffleBlockResolver ExternalShuffleBlockResolver is...FIXME == [[getBlockData]] getBlockData Method [source, java] \u00b6 ManagedBuffer getBlockData( String appId, String execId, String blockId) getBlockData parses blockId (in the format of shuffle_[shuffleId]\\_[mapId]_[reduceId] ) and returns the FileSegmentManagedBuffer that corresponds to shuffle_[shuffleId]_[mapId]_0.data . getBlockData splits blockId to 4 parts using _ (underscore). It works exclusively with shuffle block ids with the other three parts being shuffleId , mapId , and reduceId . It looks up an executor (i.e. a ExecutorShuffleInfo in executors private registry) for appId and execId to search for a storage:BlockDataManager.md#ManagedBuffer[ManagedBuffer]. The ManagedBuffer is indexed using a binary file shuffle_[shuffleId]\\_[mapId]_0.index (that contains offset and length of the buffer) with a data file being shuffle_[shuffleId]_[mapId]_0.data (that is returned as FileSegmentManagedBuffer ). It throws a IllegalArgumentException for block ids with less than four parts: Unexpected block id format: [blockId] or for non- shuffle block ids: Expected shuffle block id, got: [blockId] It throws a RuntimeException when no ExecutorShuffleInfo could be found. Executor is not registered (appId=[appId], execId=[execId])\" == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.shuffle.ExternalShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockResolver=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"ExternalShuffleBlockResolver"},{"location":"deploy/ExternalShuffleBlockResolver/#source-java","text":"ManagedBuffer getBlockData( String appId, String execId, String blockId) getBlockData parses blockId (in the format of shuffle_[shuffleId]\\_[mapId]_[reduceId] ) and returns the FileSegmentManagedBuffer that corresponds to shuffle_[shuffleId]_[mapId]_0.data . getBlockData splits blockId to 4 parts using _ (underscore). It works exclusively with shuffle block ids with the other three parts being shuffleId , mapId , and reduceId . It looks up an executor (i.e. a ExecutorShuffleInfo in executors private registry) for appId and execId to search for a storage:BlockDataManager.md#ManagedBuffer[ManagedBuffer]. The ManagedBuffer is indexed using a binary file shuffle_[shuffleId]\\_[mapId]_0.index (that contains offset and length of the buffer) with a data file being shuffle_[shuffleId]_[mapId]_0.data (that is returned as FileSegmentManagedBuffer ). It throws a IllegalArgumentException for block ids with less than four parts: Unexpected block id format: [blockId] or for non- shuffle block ids: Expected shuffle block id, got: [blockId] It throws a RuntimeException when no ExecutorShuffleInfo could be found. Executor is not registered (appId=[appId], execId=[execId])\" == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.shuffle.ExternalShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, java]"},{"location":"deploy/ExternalShuffleBlockResolver/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleBlockResolver/#log4jloggerorgapachesparknetworkshuffleexternalshuffleblockresolverall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.network.shuffle.ExternalShuffleBlockResolver=ALL"},{"location":"deploy/ExternalShuffleService/","text":"= ExternalShuffleService :navtitle: External Shuffle Service ExternalShuffleService is a Spark service that can serve shuffle blocks from outside an executor:Executor.md[Executor] process. It runs as a standalone application and manages shuffle output files so they are available for executors at all time. As the shuffle output files are managed externally to the executors it offers an uninterrupted access to the shuffle output files regardless of executors being killed or down. You start ExternalShuffleService using < start-shuffle-service.sh shell script>> and enable its use by the driver and executors using ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled]. There is a custom external shuffle service for Spark on YARN -- spark-on-yarn:spark-yarn-YarnShuffleService.md[YarnShuffleService]. == [[start-script]] start-shuffle-service.sh Shell Script start-shuffle-service.sh start-shuffle-service.sh shell script allows you to launch ExternalShuffleService. The script is under sbin directory. When executed, it runs sbin/spark-config.sh and bin/load-spark-env.sh shell scripts. It then executes sbin/spark-daemon.sh with start command and the parameters: org.apache.spark.deploy.ExternalShuffleService and 1 . [options=\"wrap\"] \u00b6 $ ./sbin/start-shuffle-service.sh starting org.apache.spark.deploy.ExternalShuffleService, logging to ...logs/spark-jacek-org.apache.spark.deploy.ExternalShuffleService-1-japila.local.out $ tail -f ...logs/spark-jacek-org.apache.spark.deploy.ExternalShuffleService-1-japila.local.out Spark Command: /Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/jars/* -Xmx1g org.apache.spark.deploy.ExternalShuffleService ======================================== Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 16/06/07 08:02:02 INFO ExternalShuffleService: Started daemon with process name: 42918@japila.local 16/06/07 08:02:03 INFO ExternalShuffleService: Starting shuffle service on port 7337 with useSasl = false You can also use tools:spark-class.md[spark-class] to launch ExternalShuffleService. [source,plaintext] \u00b6 spark-class org.apache.spark.deploy.ExternalShuffleService \u00b6 == [[main]] Launching ExternalShuffleService Standalone Application When started, it executes Utils.initDaemon(log) . CAUTION: FIXME Utils.initDaemon(log) ? See spark-submit. It loads default Spark properties and creates a SecurityManager . It sets ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] to true (as < >). A ExternalShuffleService is < > and < >. A shutdown hook is registered so when ExternalShuffleService is shut down, it prints the following INFO message to the logs and the < > method is executed. [source,plaintext] \u00b6 Shutting down shuffle service. \u00b6 You should see the following INFO message in the logs: [source,plaintext] \u00b6 Registered executor [AppExecId] with [executorInfo] \u00b6 You should also see the following messages when a SparkContext is closed: [source,plaintext] \u00b6 Application [appId] removed, cleanupLocalDirs = [cleanupLocalDirs] Cleaning up executor [AppExecId]'s [executor.localDirs.length] local dirs Successfully cleaned up directory: [localDir] == [[creating-instance]] Creating Instance ExternalShuffleService requires a ROOT:SparkConf.md[SparkConf] and spark-security.md[SecurityManager]. When created, it reads ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property (disabled by default) and < > (defaults to 7337 ) configuration settings. It also checks whether authentication is enabled. CAUTION: FIXME Review securityManager.isAuthenticationEnabled() ExternalShuffleService creates a network:TransportConf.md[] (as transportConf ). It creates a deploy:ExternalShuffleBlockHandler.md[] (as blockHandler ) and TransportContext (as transportContext ). CAUTION: FIXME TransportContext? No internal TransportServer (as server ) is created. == [[start]] Starting ExternalShuffleService [source, scala] \u00b6 start(): Unit \u00b6 start starts an ExternalShuffleService. When start is executed, you should see the following INFO message in the logs: [source,plaintext] \u00b6 Starting shuffle service on port [port] with useSasl = [useSasl] \u00b6 If useSasl is enabled, a SaslServerBootstrap is created. CAUTION: FIXME SaslServerBootstrap? The internal server reference (a TransportServer ) is created (which will attempt to bind to port ). NOTE: port is < spark.shuffle.service.port or defaults to 7337 when ExternalShuffleService is created>>. == [[stop]] Stopping ExternalShuffleService [source, scala] \u00b6 stop(): Unit \u00b6 stop closes the internal server reference and clears it (i.e. sets it to null ). == [[logging]] Logging Enable ALL logging level for org.apache.spark.deploy.ExternalShuffleService logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.deploy.ExternalShuffleService=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"ExternalShuffleService"},{"location":"deploy/ExternalShuffleService/#optionswrap","text":"$ ./sbin/start-shuffle-service.sh starting org.apache.spark.deploy.ExternalShuffleService, logging to ...logs/spark-jacek-org.apache.spark.deploy.ExternalShuffleService-1-japila.local.out $ tail -f ...logs/spark-jacek-org.apache.spark.deploy.ExternalShuffleService-1-japila.local.out Spark Command: /Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/jars/* -Xmx1g org.apache.spark.deploy.ExternalShuffleService ======================================== Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties 16/06/07 08:02:02 INFO ExternalShuffleService: Started daemon with process name: 42918@japila.local 16/06/07 08:02:03 INFO ExternalShuffleService: Starting shuffle service on port 7337 with useSasl = false You can also use tools:spark-class.md[spark-class] to launch ExternalShuffleService.","title":"[options=\"wrap\"]"},{"location":"deploy/ExternalShuffleService/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleService/#spark-class-orgapachesparkdeployexternalshuffleservice","text":"== [[main]] Launching ExternalShuffleService Standalone Application When started, it executes Utils.initDaemon(log) . CAUTION: FIXME Utils.initDaemon(log) ? See spark-submit. It loads default Spark properties and creates a SecurityManager . It sets ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] to true (as < >). A ExternalShuffleService is < > and < >. A shutdown hook is registered so when ExternalShuffleService is shut down, it prints the following INFO message to the logs and the < > method is executed.","title":"spark-class org.apache.spark.deploy.ExternalShuffleService"},{"location":"deploy/ExternalShuffleService/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleService/#shutting-down-shuffle-service","text":"You should see the following INFO message in the logs:","title":"Shutting down shuffle service."},{"location":"deploy/ExternalShuffleService/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleService/#registered-executor-appexecid-with-executorinfo","text":"You should also see the following messages when a SparkContext is closed:","title":"Registered executor [AppExecId] with [executorInfo]"},{"location":"deploy/ExternalShuffleService/#sourceplaintext_3","text":"Application [appId] removed, cleanupLocalDirs = [cleanupLocalDirs] Cleaning up executor [AppExecId]'s [executor.localDirs.length] local dirs Successfully cleaned up directory: [localDir] == [[creating-instance]] Creating Instance ExternalShuffleService requires a ROOT:SparkConf.md[SparkConf] and spark-security.md[SecurityManager]. When created, it reads ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property (disabled by default) and < > (defaults to 7337 ) configuration settings. It also checks whether authentication is enabled. CAUTION: FIXME Review securityManager.isAuthenticationEnabled() ExternalShuffleService creates a network:TransportConf.md[] (as transportConf ). It creates a deploy:ExternalShuffleBlockHandler.md[] (as blockHandler ) and TransportContext (as transportContext ). CAUTION: FIXME TransportContext? No internal TransportServer (as server ) is created. == [[start]] Starting ExternalShuffleService","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleService/#source-scala","text":"","title":"[source, scala]"},{"location":"deploy/ExternalShuffleService/#start-unit","text":"start starts an ExternalShuffleService. When start is executed, you should see the following INFO message in the logs:","title":"start(): Unit"},{"location":"deploy/ExternalShuffleService/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleService/#starting-shuffle-service-on-port-port-with-usesasl-usesasl","text":"If useSasl is enabled, a SaslServerBootstrap is created. CAUTION: FIXME SaslServerBootstrap? The internal server reference (a TransportServer ) is created (which will attempt to bind to port ). NOTE: port is < spark.shuffle.service.port or defaults to 7337 when ExternalShuffleService is created>>. == [[stop]] Stopping ExternalShuffleService","title":"Starting shuffle service on port [port] with useSasl = [useSasl]"},{"location":"deploy/ExternalShuffleService/#source-scala_1","text":"","title":"[source, scala]"},{"location":"deploy/ExternalShuffleService/#stop-unit","text":"stop closes the internal server reference and clears it (i.e. sets it to null ). == [[logging]] Logging Enable ALL logging level for org.apache.spark.deploy.ExternalShuffleService logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"stop(): Unit"},{"location":"deploy/ExternalShuffleService/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"deploy/ExternalShuffleService/#log4jloggerorgapachesparkdeployexternalshuffleserviceall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.deploy.ExternalShuffleService=ALL"},{"location":"executor/CoarseGrainedExecutorBackend/","text":"= CoarseGrainedExecutorBackend CoarseGrainedExecutorBackend is an executor:ExecutorBackend.md[] that controls the lifecycle of a single < > and sends < > to the driver. .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] CoarseGrainedExecutorBackend is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that < > (before accepting < >) and < >. CoarseGrainedExecutorBackend is started in a resource container (as a < >). When < >, CoarseGrainedExecutorBackend < > to communicate with the driver (i.e. with scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[]). .CoarseGrainedExecutorBackend Communicates with Driver's CoarseGrainedSchedulerBackend Endpoint image::CoarseGrainedExecutorBackend.png[align=\"center\"] When < >, CoarseGrainedExecutorBackend immediately connects to the owning scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] to inform that it is ready to launch tasks. [[messages]] .CoarseGrainedExecutorBackend's RPC Messages [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Message | Description < > | < > | Forwards launch task requests from the driver to the single managed coarse-grained < >. | < > | Creates the single managed < >. Sent exclusively when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[receives RegisterExecutor ]. < > < > < > |=== == [[LaunchTask]] Forwarding Launch Task Request to Executor (from Driver) -- LaunchTask Message Handler [source, scala] \u00b6 LaunchTask(data: SerializableBuffer) extends CoarseGrainedClusterMessage \u00b6 NOTE: CoarseGrainedExecutorBackend acts as a proxy between the driver and the managed single < > and merely re-packages LaunchTask payload (as serialized data ) to pass it along for execution. LaunchTask first spark-scheduler-TaskDescription.md#decode[decodes TaskDescription from data ]. You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Got assigned task [id] LaunchTask then executor:Executor.md#launchTask[launches the task on the executor] (passing itself as the owning executor:ExecutorBackend.md[] and decoded scheduler:spark-scheduler-TaskDescription.md[TaskDescription]). If < > is not available, LaunchTask < > with the error code 1 and ExecutorLossReason with the following message: Received LaunchTask command but executor was null NOTE: LaunchTask is sent when CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks] (one LaunchTask per task). == [[statusUpdate]] Sending Task Status Updates to Driver -- statusUpdate Method [source, scala] \u00b6 statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit \u00b6 NOTE: statusUpdate is part of executor:ExecutorBackend.md#statusUpdate[ExecutorBackend Contract] to send task status updates to a scheduler (on the driver). statusUpdate creates a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StatusUpdate[StatusUpdate] (with the input taskId , state , and data together with the < >) and sends it to the < > (if connected already). .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] When no < > is available, you should see the following WARN message in the logs: WARN Drop [msg] because has not yet connected to driver == [[driverURL]] Driver's URL The driver's URL is of the format spark://[RpcEndpoint name]@[hostname]:[port] , e.g. spark://CoarseGrainedScheduler@192.168.1.6:64859 . == [[main]] Launching CoarseGrainedExecutorBackend Standalone Application (in Resource Container) CoarseGrainedExecutorBackend is a standalone application (i.e. comes with main entry method) that parses < > and < > to communicate with the driver. [[command-line-arguments]] .CoarseGrainedExecutorBackend Command-Line Arguments [cols=\"1,^1,2\",options=\"header\",width=\"100%\"] |=== | Argument | Required? | Description | [[driver-url]] --driver-url | yes | Driver's URL. See < > | [[executor-id]] --executor-id | yes | Executor id | [[hostname]] --hostname | yes | Host name | [[cores]] --cores | yes | Number of cores (that must be greater than 0 ). | [[app-id]] --app-id | yes | Application id | [[worker-url]] --worker-url | no | Worker's URL, e.g. spark://Worker@192.168.1.6:64557 NOTE: --worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone] to enforce fate-sharing with the worker. | [[user-class-path]] --user-class-path | no | User-defined class path entry which can be an URL or path to a resource (often a jar file) to be added to CLASSPATH; can be specified multiple times. |=== When executed with unrecognized command-line arguments or required arguments are missing, main shows the usage help and exits (with exit status 1 ). [source] \u00b6 $ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend Usage: CoarseGrainedExecutorBackend [options] Options are: --driver-url --executor-id --hostname --cores --app-id --worker-url --user-class-path main is used when: (Spark Standalone) StandaloneSchedulerBackend is requested to spark-standalone:spark-standalone-StandaloneSchedulerBackend.md#start[start] (Spark on YARN) ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#run[start] (in a YARN resource container). (Spark on Mesos) MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[launch Spark executors] == [[run]] Starting CoarseGrainedExecutorBackend [source, scala] \u00b6 run( driverUrl: String, executorId: String, hostname: String, cores: Int, appId: String, workerUrl: Option[String], userClassPath: scala.Seq[URL]): Unit When executed, run executes Utils.initDaemon(log) . CAUTION: FIXME What does initDaemon do? NOTE: run spark-SparkHadoopUtil.md#runAsSparkUser[runs itself with a Hadoop UserGroupInformation ] (as a thread local variable distributed to child threads for authenticating HDFS and YARN calls). NOTE: run expects a clear hostname with no : included (for a port perhaps). [[run-driverPropsFetcher]] run uses executor:Executor.md#spark_executor_port[spark.executor.port] Spark property (or 0 if not set) for the port to rpc:index.md#create[create a RpcEnv ] called driverPropsFetcher (together with the input hostname and clientMode enabled). run rpc:index.md#setupEndpointRefByURI[resolves RpcEndpointRef for the input driverUrl ] and requests SparkAppConfig (by posting a blocking RetrieveSparkAppConfig ). IMPORTANT: This is the first moment when CoarseGrainedExecutorBackend initiates communication with the driver available at driverUrl through RpcEnv . run uses SparkAppConfig to get the driver's sparkProperties and adds ROOT:SparkConf.md#spark.app.id[spark.app.id] Spark property with the value of the input appId . run rpc:index.md#shutdown[shuts driverPropsFetcher RPC Endpoint down]. run creates a ROOT:SparkConf.md[SparkConf] using the Spark properties fetched from the driver, i.e. with the ROOT:SparkConf.md#isExecutorStartupConf[executor-related Spark settings] if they ROOT:SparkConf.md#setIfMissing[were missing] and the ROOT:SparkConf.md#set[rest unconditionally]. If yarn/spark-yarn-settings.md#spark.yarn.credentials.file[spark.yarn.credentials.file] Spark property is defined in SparkConf , you should see the following INFO message in the logs: INFO Will periodically update credentials from: [spark.yarn.credentials.file] run spark-SparkHadoopUtil.md#startCredentialUpdater[requests the current SparkHadoopUtil to start start the credential updater]. NOTE: run uses spark-SparkHadoopUtil.md#get[SparkHadoopUtil.get] to access the current SparkHadoopUtil . run core:SparkEnv.md#createExecutorEnv[creates SparkEnv for executors] (with the input executorId , hostname and cores , and isLocal disabled). IMPORTANT: This is the moment when SparkEnv gets created with all the executor services. run rpc:index.md#setupEndpoint[sets up an RPC endpoint] with the name Executor and < > as the endpoint. (only in Spark Standalone) If the optional input workerUrl was defined, run sets up an RPC endpoint with the name WorkerWatcher and WorkerWatcher RPC endpoint. [NOTE] \u00b6 The optional input workerUrl is defined only when < --worker-url command-line argument>> was used to < >. --worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone]. \u00b6 run 's main thread is blocked until rpc:index.md#awaitTermination[ RpcEnv terminates] and only the RPC endpoints process RPC messages. Once RpcEnv has terminated, run spark-SparkHadoopUtil.md#stopCredentialUpdater[stops the credential updater]. CAUTION: FIXME Think of the place for Utils.initDaemon , Utils.getProcessName et al. run is used when CoarseGrainedExecutorBackend standalone application is < >. == [[creating-instance]] Creating CoarseGrainedExecutorBackend Instance CoarseGrainedExecutorBackend takes the following when created: . [[rpcEnv]] rpc:index.md[RpcEnv] . driverUrl . [[executorId]] executorId . hostname . cores . userClassPath . core:SparkEnv.md[SparkEnv] NOTE: driverUrl , executorId , hostname , cores and userClassPath correspond to CoarseGrainedExecutorBackend standalone application's < >. CoarseGrainedExecutorBackend initializes the < >. NOTE: CoarseGrainedExecutorBackend is created (to act as an RPC endpoint) when < Executor RPC endpoint is registered>>. == [[onStart]] Registering with Driver -- onStart Method [source, scala] \u00b6 onStart(): Unit \u00b6 NOTE: onStart is part of rpc:RpcEndpoint.md#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages. When executed, you should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Connecting to driver: [driverUrl] NOTE: < > is given when < >. onStart then rpc:index.md#asyncSetupEndpointRefByURI[takes the RpcEndpointRef of the driver asynchronously] and initializes the internal < > property. onStart sends a blocking scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[RegisterExecutor] message immediately (with < >, rpc:RpcEndpointRef.md[RpcEndpointRef] to itself, < >, < > and < >). In case of failures, onStart < > with the error code 1 and the reason (and no notification to the driver): Cannot register with driver: [driverUrl] == [[RegisteredExecutor]] Creating Single Managed Executor -- RegisteredExecutor Message Handler [source, scala] \u00b6 RegisteredExecutor extends CoarseGrainedClusterMessage with RegisterExecutorResponse When RegisteredExecutor is received, you should see the following INFO in the logs: INFO CoarseGrainedExecutorBackend: Successfully registered with driver CoarseGrainedExecutorBackend executor:Executor.md#creating-instance[creates a Executor ] (with isLocal disabled) that becomes the single managed < >. NOTE: CoarseGrainedExecutorBackend uses executorId , hostname , env , userClassPath to create the Executor that are specified when CoarseGrainedExecutorBackend < >. If creating the Executor fails with a non-fatal exception, RegisteredExecutor < > with the reason: Unable to create executor due to [message] NOTE: RegisteredExecutor is sent exclusively when CoarseGrainedSchedulerBackend RPC Endpoint scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RegisterExecutor[receives a RegisterExecutor ] (that is sent right before CoarseGrainedExecutorBackend RPC Endpoint < > which happens when CoarseGrainedExecutorBackend < >). == [[RegisterExecutorFailed]] RegisterExecutorFailed [source, scala] \u00b6 RegisterExecutorFailed(message) \u00b6 When a RegisterExecutorFailed message arrives, the following ERROR is printed out to the logs: ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message] CoarseGrainedExecutorBackend then exits with the exit code 1 . == [[KillTask]] Killing Tasks -- KillTask Message Handler KillTask(taskId, _, interruptThread) message kills a task (calls Executor.killTask ). If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits: ERROR Received KillTask command but executor was null == [[StopExecutor]] StopExecutor Handler [source, scala] \u00b6 case object StopExecutor extends CoarseGrainedClusterMessage When StopExecutor is received, the handler turns < > internal flag on. You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown In the end, the handler sends a < > message to itself. NOTE: StopExecutor message is sent when CoarseGrainedSchedulerBackend RPC Endpoint (aka DriverEndpoint ) processes scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StopExecutors[StopExecutors] or scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RemoveExecutor[RemoveExecutor] messages. == [[Shutdown]] Shutdown Handler [source, scala] \u00b6 case object Shutdown extends CoarseGrainedClusterMessage Shutdown turns < > internal flag on and starts the CoarseGrainedExecutorBackend-stop-executor thread that executor:Executor.md#stop[stops the owned Executor ] (using < > reference). NOTE: Shutdown message is sent exclusively when < StopExecutor >>. == [[exitExecutor]] Terminating CoarseGrainedExecutorBackend (and Notifying Driver with RemoveExecutor) -- exitExecutor Method [source, scala] \u00b6 exitExecutor( code: Int, reason: String, throwable: Throwable = null, notifyDriver: Boolean = true): Unit When exitExecutor is executed, you should see the following ERROR message in the logs (followed by throwable if available): ERROR Executor self-exiting due to : [reason] If notifyDriver is enabled (it is by default) exitExecutor informs the < > that the executor should be removed (by sending a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RemoveExecutor[blocking RemoveExecutor message] with < > and a ExecutorLossReason with the input reason ). You may see the following WARN message in the logs when the notification fails. Unable to notify the driver due to [message] In the end, exitExecutor terminates the CoarseGrainedExecutorBackend JVM process with the status code . NOTE: exitExecutor uses Java's https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#exit-int-[System.exit ] and initiates JVM's shutdown sequence (and executing all registered shutdown hooks). [NOTE] \u00b6 exitExecutor is used when: CoarseGrainedExecutorBackend fails to < >, < > or < > no < > has been created before < > or < > task requests * < >. \u00b6 == [[onDisconnected]] onDisconnected Callback CAUTION: FIXME == [[start]] start Method CAUTION: FIXME == [[stop]] stop Method CAUTION: FIXME == [[requestTotalExecutors]] requestTotalExecutors CAUTION: FIXME == [[extractLogUrls]] Extracting Log URLs -- extractLogUrls Method CAUTION: FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.CoarseGrainedExecutorBackend logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[ser]] SerializerInstance serializer:SerializerInstance.md[SerializerInstance] Initialized when < >. NOTE: CoarseGrainedExecutorBackend uses the input env to core:SparkEnv.md#closureSerializer[access closureSerializer ]. === [[driver]] Driver RpcEndpointRef rpc:RpcEndpointRef.md[RpcEndpointRef] of the driver === [[stopping]] stopping Flag Enabled when CoarseGrainedExecutorBackend gets notified to < > or < >. Default: false Used when CoarseGrainedExecutorBackend RPC Endpoint gets notified that < >. === [[executor]] Executor Single managed coarse-grained executor:Executor.md#coarse-grained-executor[Executor] managed exclusively by the CoarseGrainedExecutorBackend to forward < > and < > task requests to from the driver. Initialized after CoarseGrainedExecutorBackend < CoarseGrainedSchedulerBackend >> and stopped when CoarseGrainedExecutorBackend gets requested to < >.","title":"CoarseGrainedExecutorBackend"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#launchtaskdata-serializablebuffer-extends-coarsegrainedclustermessage","text":"NOTE: CoarseGrainedExecutorBackend acts as a proxy between the driver and the managed single < > and merely re-packages LaunchTask payload (as serialized data ) to pass it along for execution. LaunchTask first spark-scheduler-TaskDescription.md#decode[decodes TaskDescription from data ]. You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Got assigned task [id] LaunchTask then executor:Executor.md#launchTask[launches the task on the executor] (passing itself as the owning executor:ExecutorBackend.md[] and decoded scheduler:spark-scheduler-TaskDescription.md[TaskDescription]). If < > is not available, LaunchTask < > with the error code 1 and ExecutorLossReason with the following message: Received LaunchTask command but executor was null NOTE: LaunchTask is sent when CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks] (one LaunchTask per task). == [[statusUpdate]] Sending Task Status Updates to Driver -- statusUpdate Method","title":"LaunchTask(data: SerializableBuffer) extends CoarseGrainedClusterMessage"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#statusupdatetaskid-long-state-taskstate-data-bytebuffer-unit","text":"NOTE: statusUpdate is part of executor:ExecutorBackend.md#statusUpdate[ExecutorBackend Contract] to send task status updates to a scheduler (on the driver). statusUpdate creates a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StatusUpdate[StatusUpdate] (with the input taskId , state , and data together with the < >) and sends it to the < > (if connected already). .CoarseGrainedExecutorBackend Sending Task Status Updates to Driver's CoarseGrainedScheduler Endpoint image::CoarseGrainedExecutorBackend-statusUpdate.png[align=\"center\"] When no < > is available, you should see the following WARN message in the logs: WARN Drop [msg] because has not yet connected to driver == [[driverURL]] Driver's URL The driver's URL is of the format spark://[RpcEndpoint name]@[hostname]:[port] , e.g. spark://CoarseGrainedScheduler@192.168.1.6:64859 . == [[main]] Launching CoarseGrainedExecutorBackend Standalone Application (in Resource Container) CoarseGrainedExecutorBackend is a standalone application (i.e. comes with main entry method) that parses < > and < > to communicate with the driver. [[command-line-arguments]] .CoarseGrainedExecutorBackend Command-Line Arguments [cols=\"1,^1,2\",options=\"header\",width=\"100%\"] |=== | Argument | Required? | Description | [[driver-url]] --driver-url | yes | Driver's URL. See < > | [[executor-id]] --executor-id | yes | Executor id | [[hostname]] --hostname | yes | Host name | [[cores]] --cores | yes | Number of cores (that must be greater than 0 ). | [[app-id]] --app-id | yes | Application id | [[worker-url]] --worker-url | no | Worker's URL, e.g. spark://Worker@192.168.1.6:64557 NOTE: --worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone] to enforce fate-sharing with the worker. | [[user-class-path]] --user-class-path | no | User-defined class path entry which can be an URL or path to a resource (often a jar file) to be added to CLASSPATH; can be specified multiple times. |=== When executed with unrecognized command-line arguments or required arguments are missing, main shows the usage help and exits (with exit status 1 ).","title":"statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit"},{"location":"executor/CoarseGrainedExecutorBackend/#source","text":"$ ./bin/spark-class org.apache.spark.executor.CoarseGrainedExecutorBackend Usage: CoarseGrainedExecutorBackend [options] Options are: --driver-url --executor-id --hostname --cores --app-id --worker-url --user-class-path main is used when: (Spark Standalone) StandaloneSchedulerBackend is requested to spark-standalone:spark-standalone-StandaloneSchedulerBackend.md#start[start] (Spark on YARN) ExecutorRunnable is requested to spark-on-yarn:spark-yarn-ExecutorRunnable.md#run[start] (in a YARN resource container). (Spark on Mesos) MesosCoarseGrainedSchedulerBackend is requested to spark-on-mesos:spark-mesos-MesosCoarseGrainedSchedulerBackend.md#createCommand[launch Spark executors] == [[run]] Starting CoarseGrainedExecutorBackend","title":"[source]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_2","text":"run( driverUrl: String, executorId: String, hostname: String, cores: Int, appId: String, workerUrl: Option[String], userClassPath: scala.Seq[URL]): Unit When executed, run executes Utils.initDaemon(log) . CAUTION: FIXME What does initDaemon do? NOTE: run spark-SparkHadoopUtil.md#runAsSparkUser[runs itself with a Hadoop UserGroupInformation ] (as a thread local variable distributed to child threads for authenticating HDFS and YARN calls). NOTE: run expects a clear hostname with no : included (for a port perhaps). [[run-driverPropsFetcher]] run uses executor:Executor.md#spark_executor_port[spark.executor.port] Spark property (or 0 if not set) for the port to rpc:index.md#create[create a RpcEnv ] called driverPropsFetcher (together with the input hostname and clientMode enabled). run rpc:index.md#setupEndpointRefByURI[resolves RpcEndpointRef for the input driverUrl ] and requests SparkAppConfig (by posting a blocking RetrieveSparkAppConfig ). IMPORTANT: This is the first moment when CoarseGrainedExecutorBackend initiates communication with the driver available at driverUrl through RpcEnv . run uses SparkAppConfig to get the driver's sparkProperties and adds ROOT:SparkConf.md#spark.app.id[spark.app.id] Spark property with the value of the input appId . run rpc:index.md#shutdown[shuts driverPropsFetcher RPC Endpoint down]. run creates a ROOT:SparkConf.md[SparkConf] using the Spark properties fetched from the driver, i.e. with the ROOT:SparkConf.md#isExecutorStartupConf[executor-related Spark settings] if they ROOT:SparkConf.md#setIfMissing[were missing] and the ROOT:SparkConf.md#set[rest unconditionally]. If yarn/spark-yarn-settings.md#spark.yarn.credentials.file[spark.yarn.credentials.file] Spark property is defined in SparkConf , you should see the following INFO message in the logs: INFO Will periodically update credentials from: [spark.yarn.credentials.file] run spark-SparkHadoopUtil.md#startCredentialUpdater[requests the current SparkHadoopUtil to start start the credential updater]. NOTE: run uses spark-SparkHadoopUtil.md#get[SparkHadoopUtil.get] to access the current SparkHadoopUtil . run core:SparkEnv.md#createExecutorEnv[creates SparkEnv for executors] (with the input executorId , hostname and cores , and isLocal disabled). IMPORTANT: This is the moment when SparkEnv gets created with all the executor services. run rpc:index.md#setupEndpoint[sets up an RPC endpoint] with the name Executor and < > as the endpoint. (only in Spark Standalone) If the optional input workerUrl was defined, run sets up an RPC endpoint with the name WorkerWatcher and WorkerWatcher RPC endpoint.","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#note","text":"The optional input workerUrl is defined only when < --worker-url command-line argument>> was used to < >.","title":"[NOTE]"},{"location":"executor/CoarseGrainedExecutorBackend/#-worker-url-is-only-used-in-spark-standalone-standaloneschedulerbackendmdspark-standalone","text":"run 's main thread is blocked until rpc:index.md#awaitTermination[ RpcEnv terminates] and only the RPC endpoints process RPC messages. Once RpcEnv has terminated, run spark-SparkHadoopUtil.md#stopCredentialUpdater[stops the credential updater]. CAUTION: FIXME Think of the place for Utils.initDaemon , Utils.getProcessName et al. run is used when CoarseGrainedExecutorBackend standalone application is < >. == [[creating-instance]] Creating CoarseGrainedExecutorBackend Instance CoarseGrainedExecutorBackend takes the following when created: . [[rpcEnv]] rpc:index.md[RpcEnv] . driverUrl . [[executorId]] executorId . hostname . cores . userClassPath . core:SparkEnv.md[SparkEnv] NOTE: driverUrl , executorId , hostname , cores and userClassPath correspond to CoarseGrainedExecutorBackend standalone application's < >. CoarseGrainedExecutorBackend initializes the < >. NOTE: CoarseGrainedExecutorBackend is created (to act as an RPC endpoint) when < Executor RPC endpoint is registered>>. == [[onStart]] Registering with Driver -- onStart Method","title":"--worker-url is only used in spark-standalone-StandaloneSchedulerBackend.md[Spark Standalone]."},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#onstart-unit","text":"NOTE: onStart is part of rpc:RpcEndpoint.md#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages. When executed, you should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Connecting to driver: [driverUrl] NOTE: < > is given when < >. onStart then rpc:index.md#asyncSetupEndpointRefByURI[takes the RpcEndpointRef of the driver asynchronously] and initializes the internal < > property. onStart sends a blocking scheduler:CoarseGrainedSchedulerBackend.md#RegisterExecutor[RegisterExecutor] message immediately (with < >, rpc:RpcEndpointRef.md[RpcEndpointRef] to itself, < >, < > and < >). In case of failures, onStart < > with the error code 1 and the reason (and no notification to the driver): Cannot register with driver: [driverUrl] == [[RegisteredExecutor]] Creating Single Managed Executor -- RegisteredExecutor Message Handler","title":"onStart(): Unit"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_4","text":"RegisteredExecutor extends CoarseGrainedClusterMessage with RegisterExecutorResponse When RegisteredExecutor is received, you should see the following INFO in the logs: INFO CoarseGrainedExecutorBackend: Successfully registered with driver CoarseGrainedExecutorBackend executor:Executor.md#creating-instance[creates a Executor ] (with isLocal disabled) that becomes the single managed < >. NOTE: CoarseGrainedExecutorBackend uses executorId , hostname , env , userClassPath to create the Executor that are specified when CoarseGrainedExecutorBackend < >. If creating the Executor fails with a non-fatal exception, RegisteredExecutor < > with the reason: Unable to create executor due to [message] NOTE: RegisteredExecutor is sent exclusively when CoarseGrainedSchedulerBackend RPC Endpoint scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RegisterExecutor[receives a RegisterExecutor ] (that is sent right before CoarseGrainedExecutorBackend RPC Endpoint < > which happens when CoarseGrainedExecutorBackend < >). == [[RegisterExecutorFailed]] RegisterExecutorFailed","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#registerexecutorfailedmessage","text":"When a RegisterExecutorFailed message arrives, the following ERROR is printed out to the logs: ERROR CoarseGrainedExecutorBackend: Slave registration failed: [message] CoarseGrainedExecutorBackend then exits with the exit code 1 . == [[KillTask]] Killing Tasks -- KillTask Message Handler KillTask(taskId, _, interruptThread) message kills a task (calls Executor.killTask ). If an executor has not been initialized yet (FIXME: why?), the following ERROR message is printed out to the logs and CoarseGrainedExecutorBackend exits: ERROR Received KillTask command but executor was null == [[StopExecutor]] StopExecutor Handler","title":"RegisterExecutorFailed(message)"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_6","text":"case object StopExecutor extends CoarseGrainedClusterMessage When StopExecutor is received, the handler turns < > internal flag on. You should see the following INFO message in the logs: INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown In the end, the handler sends a < > message to itself. NOTE: StopExecutor message is sent when CoarseGrainedSchedulerBackend RPC Endpoint (aka DriverEndpoint ) processes scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StopExecutors[StopExecutors] or scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RemoveExecutor[RemoveExecutor] messages. == [[Shutdown]] Shutdown Handler","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_7","text":"case object Shutdown extends CoarseGrainedClusterMessage Shutdown turns < > internal flag on and starts the CoarseGrainedExecutorBackend-stop-executor thread that executor:Executor.md#stop[stops the owned Executor ] (using < > reference). NOTE: Shutdown message is sent exclusively when < StopExecutor >>. == [[exitExecutor]] Terminating CoarseGrainedExecutorBackend (and Notifying Driver with RemoveExecutor) -- exitExecutor Method","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#source-scala_8","text":"exitExecutor( code: Int, reason: String, throwable: Throwable = null, notifyDriver: Boolean = true): Unit When exitExecutor is executed, you should see the following ERROR message in the logs (followed by throwable if available): ERROR Executor self-exiting due to : [reason] If notifyDriver is enabled (it is by default) exitExecutor informs the < > that the executor should be removed (by sending a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RemoveExecutor[blocking RemoveExecutor message] with < > and a ExecutorLossReason with the input reason ). You may see the following WARN message in the logs when the notification fails. Unable to notify the driver due to [message] In the end, exitExecutor terminates the CoarseGrainedExecutorBackend JVM process with the status code . NOTE: exitExecutor uses Java's https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#exit-int-[System.exit ] and initiates JVM's shutdown sequence (and executing all registered shutdown hooks).","title":"[source, scala]"},{"location":"executor/CoarseGrainedExecutorBackend/#note_1","text":"exitExecutor is used when: CoarseGrainedExecutorBackend fails to < >, < > or < > no < > has been created before < > or < > task requests","title":"[NOTE]"},{"location":"executor/CoarseGrainedExecutorBackend/#_1","text":"== [[onDisconnected]] onDisconnected Callback CAUTION: FIXME == [[start]] start Method CAUTION: FIXME == [[stop]] stop Method CAUTION: FIXME == [[requestTotalExecutors]] requestTotalExecutors CAUTION: FIXME == [[extractLogUrls]] Extracting Log URLs -- extractLogUrls Method CAUTION: FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.CoarseGrainedExecutorBackend logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"* &lt;&gt;."},{"location":"executor/CoarseGrainedExecutorBackend/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"executor/CoarseGrainedExecutorBackend/#log4jloggerorgapachesparkexecutorcoarsegrainedexecutorbackendall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[ser]] SerializerInstance serializer:SerializerInstance.md[SerializerInstance] Initialized when < >. NOTE: CoarseGrainedExecutorBackend uses the input env to core:SparkEnv.md#closureSerializer[access closureSerializer ]. === [[driver]] Driver RpcEndpointRef rpc:RpcEndpointRef.md[RpcEndpointRef] of the driver === [[stopping]] stopping Flag Enabled when CoarseGrainedExecutorBackend gets notified to < > or < >. Default: false Used when CoarseGrainedExecutorBackend RPC Endpoint gets notified that < >. === [[executor]] Executor Single managed coarse-grained executor:Executor.md#coarse-grained-executor[Executor] managed exclusively by the CoarseGrainedExecutorBackend to forward < > and < > task requests to from the driver. Initialized after CoarseGrainedExecutorBackend < CoarseGrainedSchedulerBackend >> and stopped when CoarseGrainedExecutorBackend gets requested to < >.","title":"log4j.logger.org.apache.spark.executor.CoarseGrainedExecutorBackend=ALL"},{"location":"executor/Executor/","text":"Executor \u00b6 Executor is a process that is used for executing scheduler:Task.md[tasks]. Executor typically runs for the entire lifetime of a Spark application which is called static allocation of executors (but you could also opt in for ROOT:spark-dynamic-allocation.md[dynamic allocation]). Executors are managed by executor:ExecutorBackend.md[executor backends]. Executors < > to the < > on the driver. .HeartbeatReceiver's Heartbeat Message Handler image::HeartbeatReceiver-Heartbeat.png[align=\"center\"] Executors provide in-memory storage for RDDs that are cached in Spark applications (via storage:BlockManager.md[]). When started, an executor first registers itself with the driver that establishes a communication channel directly to the driver to accept tasks for execution. .Launching tasks on executor using TaskRunners image::executor-taskrunner-executorbackend.png[align=\"center\"] Executor offers are described by executor id and the host on which an executor runs (see < > in this document). Executors can run multiple tasks over its lifetime, both in parallel and sequentially. They track executor:TaskRunner.md[running tasks] (by their task ids in < > internal registry). Consult < > section. Executors use a < > for < >. Executors send < > (and heartbeats) using the < >. It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster. Executors are described by their id , hostname , environment (as SparkEnv ), and classpath (and, less importantly, and more for internal optimization, whether they run in spark-local:index.md[local] or ROOT:spark-cluster.md[cluster] mode). == [[creating-instance]] Creating Instance Executor takes the following to be created: [[executorId]] Executor ID [[executorHostname]] Host name [[env]] core:SparkEnv.md[] < > < > [[uncaughtExceptionHandler]] Java's UncaughtExceptionHandler (default: SparkUncaughtExceptionHandler ) When created, Executor prints out the following INFO messages to the logs: Starting executor ID [executorId] on host [executorHostname] (only for < >) Executor sets SparkUncaughtExceptionHandler as the default handler invoked when a thread abruptly terminates due to an uncaught exception. (only for < >) Executor requests the core:SparkEnv.md#blockManager[BlockManager] to storage:BlockManager.md#initialize[initialize] (with the ROOT:SparkConf.md#getAppId[Spark application id] of the core:SparkEnv.md#conf[SparkConf]). [[creating-instance-BlockManager-shuffleMetricsSource]] (only for < >) Executor requests the core:SparkEnv.md#metricsSystem[MetricsSystem] to metrics:spark-metrics-MetricsSystem.md#registerSource[register] the < > and storage:BlockManager.md#shuffleMetricsSource[shuffleMetricsSource] of the core:SparkEnv.md#blockManager[BlockManager]. Executor uses SparkEnv to access the core:SparkEnv.md#metricsSystem[MetricsSystem] and core:SparkEnv.md#blockManager[BlockManager]. Executor < > (optionally with < >) and requests the system Serializer to serializer:Serializer.md#setDefaultClassLoader[use as the default classloader] (for deserializing tasks). Executor < >. Executor is created when: CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[receives RegisteredExecutor message] (Spark on Mesos) MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#registered[registered] spark-local:spark-LocalEndpoint.md[LocalEndpoint] is created == [[isLocal]] isLocal Flag Executor is given a isLocal flag when created. This is how the executor knows whether it runs in local or cluster mode. It is disabled by default. The flag is turned on for spark-local:index.md[Spark local] (via spark-local:spark-LocalEndpoint.md[LocalEndpoint]). == [[userClassPath]] User-Defined Jars Executor is given user-defined jars when created. There are no jars defined by default. The jars are specified using ROOT:configuration-properties.md#spark.executor.extraClassPath[spark.executor.extraClassPath] configuration property (via executor:CoarseGrainedExecutorBackend.md#main[--user-class-path] command-line option of CoarseGrainedExecutorBackend). == [[runningTasks]] Running Tasks Executor tracks running tasks in a registry of executor:TaskRunner.md[TaskRunners] per task ID. == [[heartbeatReceiverRef]] HeartbeatReceiver RPC Endpoint Reference rpc:RpcEndpointRef.md[RPC endpoint reference] to ROOT:spark-HeartbeatReceiver.md[HeartbeatReceiver] on the ROOT:spark-driver.md[driver]. Set when Executor < >. Used exclusively when Executor < > (that happens every < > interval). == [[updateDependencies]] updateDependencies Method [source, scala] \u00b6 updateDependencies( newFiles: Map[String, Long], newJars: Map[String, Long]): Unit updateDependencies...FIXME updateDependencies is used when TaskRunner is requested to executor:TaskRunner.md#run[start] (and run a task). == [[launchTask]] Launching Task [source, scala] \u00b6 launchTask( context: ExecutorBackend, taskDescription: TaskDescription): Unit launchTask simply creates a executor:TaskRunner.md[] (with the given executor:ExecutorBackend.md[] and the scheduler:spark-scheduler-TaskDescription.md[TaskDescription]) and adds it to the < > internal registry. In the end, launchTask requests the < > to execute the TaskRunner (sometime in the future). .Launching tasks on executor using TaskRunners image::executor-taskrunner-executorbackend.png[align=\"center\"] launchTask is used when: CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] LocalEndpoint RPC endpoint (of spark-local:spark-LocalSchedulerBackend.md#[LocalSchedulerBackend]) is requested to spark-local:spark-LocalEndpoint.md#reviveOffers[reviveOffers] MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launchTask] == [[heartbeater]] Heartbeat Sender Thread heartbeater is a daemon {java-javadoc-url}/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor] with a single thread. The name of the thread pool is driver-heartbeater . == [[coarse-grained-executor]] Coarse-Grained Executors Coarse-grained executors are executors that use executor:CoarseGrainedExecutorBackend.md[] for task scheduling. == [[resource-offers]] Resource Offers Read scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] in TaskSchedulerImpl and scheduler:TaskSetManager.md#resourceOffers[resourceOffer] in TaskSetManager. == [[threadPool]] Executor task launch worker Thread Pool Executor uses threadPool daemon cached thread pool with the name Executor task launch worker-[ID] (with ID being the task id) for < >. threadPool is created when < > and shut down when < >. == [[memory]] Executor Memory You can control the amount of memory per executor using ROOT:configuration-properties.md#spark.executor.memory[spark.executor.memory] configuration property. It sets the available memory equally for all executors per application. The amount of memory per executor is looked up when ROOT:SparkContext.md#creating-instance[SparkContext is created]. You can change the assigned memory per executor per node in spark-standalone:index.md[standalone cluster] using ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. You can find the value displayed as Memory per Node in spark-standalone:spark-standalone-Master.md[web UI for standalone Master] (as depicted in the figure below). .Memory per Node in Spark Standalone's web UI image::spark-standalone-webui-memory-per-node.png[align=\"center\"] The above figure shows the result of running tools:spark-shell.md[Spark shell] with the amount of memory per executor defined explicitly (on command line), i.e. ./bin/spark-shell --master spark://localhost:7077 -c spark.executor.memory=2g == [[metrics]] Metrics Every executor registers its own executor:ExecutorSource.md[] to metrics:spark-metrics-MetricsSystem.md#report[report metrics]. == [[stop]] Stopping Executor [source, scala] \u00b6 stop(): Unit \u00b6 stop requests core:SparkEnv.md#metricsSystem[MetricsSystem] for a metrics:spark-metrics-MetricsSystem.md#report[report]. stop shuts < > down (and waits at most 10 seconds). stop shuts < > down. (only when < >) stop core:SparkEnv.md#stop[requests SparkEnv to stop]. stop is used when executor:CoarseGrainedExecutorBackend.md#Shutdown[CoarseGrainedExecutorBackend] and spark-local:spark-LocalEndpoint.md#StopExecutor[LocalEndpoint] are requested to stop their managed executors. == [[computeTotalGcTime]] computeTotalGcTime Method [source, scala] \u00b6 computeTotalGcTime(): Long \u00b6 computeTotalGcTime...FIXME computeTotalGcTime is used when: TaskRunner is requested to executor:TaskRunner.md#collectAccumulatorsAndResetStatusOnFailure[collectAccumulatorsAndResetStatusOnFailure] and executor:TaskRunner.md#run[run] Executor is requested to < > == [[createClassLoader]] createClassLoader Method [source, scala] \u00b6 createClassLoader(): MutableURLClassLoader \u00b6 createClassLoader...FIXME createClassLoader is used when...FIXME == [[addReplClassLoaderIfNeeded]] addReplClassLoaderIfNeeded Method [source, scala] \u00b6 addReplClassLoaderIfNeeded( parent: ClassLoader): ClassLoader addReplClassLoaderIfNeeded...FIXME addReplClassLoaderIfNeeded is used when...FIXME == [[reportHeartBeat]] Heartbeating With Partial Metrics For Active Tasks To Driver [source, scala] \u00b6 reportHeartBeat(): Unit \u00b6 reportHeartBeat collects executor:TaskRunner.md[TaskRunners] for < > (aka active tasks ) with their executor:TaskRunner.md#task[tasks] deserialized (i.e. either ready for execution or already started). executor:TaskRunner.md[] has TaskRunner.md#task[task] deserialized when it executor:TaskRunner.md#run[runs the task]. For every running task, reportHeartBeat takes its scheduler:Task.md#metrics[TaskMetrics] and: Requests executor:TaskMetrics.md#mergeShuffleReadMetrics[ShuffleRead metrics to be merged] executor:TaskMetrics.md#setJvmGCTime[Sets jvmGCTime metrics] reportHeartBeat then records the latest values of executor:TaskMetrics.md#accumulators[internal and external accumulators] for every task. NOTE: Internal accumulators are a task's metrics while external accumulators are a Spark application's accumulators that a user has created. reportHeartBeat sends a blocking ROOT:spark-HeartbeatReceiver.md#Heartbeat[Heartbeat] message to < HeartbeatReceiver endpoint>> (running on the driver). reportHeartBeat uses the value of ROOT:configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval] configuration property for the RPC timeout. NOTE: A Heartbeat message contains the executor identifier, the accumulator updates, and the identifier of the storage:BlockManager.md[]. If the response (from < HeartbeatReceiver endpoint>>) is to re-register the BlockManager , you should see the following INFO message in the logs and reportHeartBeat requests the BlockManager to storage:BlockManager.md#reregister[re-register] (which will register the blocks the BlockManager manages with the driver). [source,plaintext] \u00b6 Told to re-register on heartbeat \u00b6 HeartbeatResponse requests the BlockManager to re-register when either scheduler:TaskScheduler.md#executorHeartbeatReceived[TaskScheduler] or ROOT:spark-HeartbeatReceiver.md#Heartbeat[HeartbeatReceiver] know nothing about the executor. When posting the Heartbeat was successful, reportHeartBeat resets < > internal counter. In case of a non-fatal exception, you should see the following WARN message in the logs (followed by the stack trace). Issue communicating with driver in heartbeater Every failure reportHeartBeat increments < > up to ROOT:configuration-properties.md#spark.executor.heartbeat.maxFailures[spark.executor.heartbeat.maxFailures] configuration property. When the heartbeat failures reaches the maximum, you should see the following ERROR message in the logs and the executor terminates with the error code: 56 . Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times reportHeartBeat is used when Executor is requested to < > (that happens every ROOT:configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval]). == [[startDriverHeartbeater]][[heartbeats-and-active-task-metrics]] Sending Heartbeats and Active Tasks Metrics Executors keep sending < > to the driver every < > (defaults to 10s with some random initial delay so the heartbeats from different executors do not pile up on the driver). .Executors use HeartbeatReceiver endpoint to report task metrics image::executor-heartbeatReceiver-endpoint.png[align=\"center\"] An executor sends heartbeats using the < >. .HeartbeatReceiver's Heartbeat Message Handler image::HeartbeatReceiver-Heartbeat.png[align=\"center\"] For each scheduler:Task.md[task] in executor:TaskRunner.md[] (in < > internal registry), the task's metrics are computed (i.e. mergeShuffleReadMetrics and setJvmGCTime ) that become part of the heartbeat (with accumulators). NOTE: Executors track the executor:TaskRunner.md[] that run scheduler:Task.md[tasks]. A executor:TaskRunner.md#run[task might not be assigned to a TaskRunner yet] when the executor sends a heartbeat. A blocking ROOT:spark-HeartbeatReceiver.md#Heartbeat[Heartbeat] message that holds the executor id, all accumulator updates (per task id), and storage:BlockManagerId.md[] is sent to ROOT:spark-HeartbeatReceiver.md[HeartbeatReceiver RPC endpoint] (with < > timeout). If the response ROOT:spark-HeartbeatReceiver.md#Heartbeat[requests to reregister BlockManager], you should see the following INFO message in the logs: Told to re-register on heartbeat BlockManager is requested to storage:BlockManager.md#reregister[reregister]. The internal < > counter is reset (i.e. becomes 0 ). If there are any issues with communicating with the driver, you should see the following WARN message in the logs: [source,plaintext] \u00b6 Issue communicating with driver in heartbeater \u00b6 The internal < > is incremented and checked to be less than the < > (i.e. spark.executor.heartbeat.maxFailures Spark property). If the number is greater, the following ERROR is printed out to the logs: Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times The executor exits (using System.exit and exit code 56). == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.executor.Executor=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[executorSource]] ExecutorSource executor:ExecutorSource.md[] === [[heartbeatFailures]] heartbeatFailures === [[maxDirectResultSize]] maxDirectResultSize === [[maxResultSize]] maxResultSize Used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and creates a serialized ByteBuffer result that is a IndirectTaskResult )","title":"Executor"},{"location":"executor/Executor/#executor","text":"Executor is a process that is used for executing scheduler:Task.md[tasks]. Executor typically runs for the entire lifetime of a Spark application which is called static allocation of executors (but you could also opt in for ROOT:spark-dynamic-allocation.md[dynamic allocation]). Executors are managed by executor:ExecutorBackend.md[executor backends]. Executors < > to the < > on the driver. .HeartbeatReceiver's Heartbeat Message Handler image::HeartbeatReceiver-Heartbeat.png[align=\"center\"] Executors provide in-memory storage for RDDs that are cached in Spark applications (via storage:BlockManager.md[]). When started, an executor first registers itself with the driver that establishes a communication channel directly to the driver to accept tasks for execution. .Launching tasks on executor using TaskRunners image::executor-taskrunner-executorbackend.png[align=\"center\"] Executor offers are described by executor id and the host on which an executor runs (see < > in this document). Executors can run multiple tasks over its lifetime, both in parallel and sequentially. They track executor:TaskRunner.md[running tasks] (by their task ids in < > internal registry). Consult < > section. Executors use a < > for < >. Executors send < > (and heartbeats) using the < >. It is recommended to have as many executors as data nodes and as many cores as you can get from the cluster. Executors are described by their id , hostname , environment (as SparkEnv ), and classpath (and, less importantly, and more for internal optimization, whether they run in spark-local:index.md[local] or ROOT:spark-cluster.md[cluster] mode). == [[creating-instance]] Creating Instance Executor takes the following to be created: [[executorId]] Executor ID [[executorHostname]] Host name [[env]] core:SparkEnv.md[] < > < > [[uncaughtExceptionHandler]] Java's UncaughtExceptionHandler (default: SparkUncaughtExceptionHandler ) When created, Executor prints out the following INFO messages to the logs: Starting executor ID [executorId] on host [executorHostname] (only for < >) Executor sets SparkUncaughtExceptionHandler as the default handler invoked when a thread abruptly terminates due to an uncaught exception. (only for < >) Executor requests the core:SparkEnv.md#blockManager[BlockManager] to storage:BlockManager.md#initialize[initialize] (with the ROOT:SparkConf.md#getAppId[Spark application id] of the core:SparkEnv.md#conf[SparkConf]). [[creating-instance-BlockManager-shuffleMetricsSource]] (only for < >) Executor requests the core:SparkEnv.md#metricsSystem[MetricsSystem] to metrics:spark-metrics-MetricsSystem.md#registerSource[register] the < > and storage:BlockManager.md#shuffleMetricsSource[shuffleMetricsSource] of the core:SparkEnv.md#blockManager[BlockManager]. Executor uses SparkEnv to access the core:SparkEnv.md#metricsSystem[MetricsSystem] and core:SparkEnv.md#blockManager[BlockManager]. Executor < > (optionally with < >) and requests the system Serializer to serializer:Serializer.md#setDefaultClassLoader[use as the default classloader] (for deserializing tasks). Executor < >. Executor is created when: CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[receives RegisteredExecutor message] (Spark on Mesos) MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#registered[registered] spark-local:spark-LocalEndpoint.md[LocalEndpoint] is created == [[isLocal]] isLocal Flag Executor is given a isLocal flag when created. This is how the executor knows whether it runs in local or cluster mode. It is disabled by default. The flag is turned on for spark-local:index.md[Spark local] (via spark-local:spark-LocalEndpoint.md[LocalEndpoint]). == [[userClassPath]] User-Defined Jars Executor is given user-defined jars when created. There are no jars defined by default. The jars are specified using ROOT:configuration-properties.md#spark.executor.extraClassPath[spark.executor.extraClassPath] configuration property (via executor:CoarseGrainedExecutorBackend.md#main[--user-class-path] command-line option of CoarseGrainedExecutorBackend). == [[runningTasks]] Running Tasks Executor tracks running tasks in a registry of executor:TaskRunner.md[TaskRunners] per task ID. == [[heartbeatReceiverRef]] HeartbeatReceiver RPC Endpoint Reference rpc:RpcEndpointRef.md[RPC endpoint reference] to ROOT:spark-HeartbeatReceiver.md[HeartbeatReceiver] on the ROOT:spark-driver.md[driver]. Set when Executor < >. Used exclusively when Executor < > (that happens every < > interval). == [[updateDependencies]] updateDependencies Method","title":"Executor"},{"location":"executor/Executor/#source-scala","text":"updateDependencies( newFiles: Map[String, Long], newJars: Map[String, Long]): Unit updateDependencies...FIXME updateDependencies is used when TaskRunner is requested to executor:TaskRunner.md#run[start] (and run a task). == [[launchTask]] Launching Task","title":"[source, scala]"},{"location":"executor/Executor/#source-scala_1","text":"launchTask( context: ExecutorBackend, taskDescription: TaskDescription): Unit launchTask simply creates a executor:TaskRunner.md[] (with the given executor:ExecutorBackend.md[] and the scheduler:spark-scheduler-TaskDescription.md[TaskDescription]) and adds it to the < > internal registry. In the end, launchTask requests the < > to execute the TaskRunner (sometime in the future). .Launching tasks on executor using TaskRunners image::executor-taskrunner-executorbackend.png[align=\"center\"] launchTask is used when: CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] LocalEndpoint RPC endpoint (of spark-local:spark-LocalSchedulerBackend.md#[LocalSchedulerBackend]) is requested to spark-local:spark-LocalEndpoint.md#reviveOffers[reviveOffers] MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launchTask] == [[heartbeater]] Heartbeat Sender Thread heartbeater is a daemon {java-javadoc-url}/java/util/concurrent/ScheduledThreadPoolExecutor.html[ScheduledThreadPoolExecutor] with a single thread. The name of the thread pool is driver-heartbeater . == [[coarse-grained-executor]] Coarse-Grained Executors Coarse-grained executors are executors that use executor:CoarseGrainedExecutorBackend.md[] for task scheduling. == [[resource-offers]] Resource Offers Read scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] in TaskSchedulerImpl and scheduler:TaskSetManager.md#resourceOffers[resourceOffer] in TaskSetManager. == [[threadPool]] Executor task launch worker Thread Pool Executor uses threadPool daemon cached thread pool with the name Executor task launch worker-[ID] (with ID being the task id) for < >. threadPool is created when < > and shut down when < >. == [[memory]] Executor Memory You can control the amount of memory per executor using ROOT:configuration-properties.md#spark.executor.memory[spark.executor.memory] configuration property. It sets the available memory equally for all executors per application. The amount of memory per executor is looked up when ROOT:SparkContext.md#creating-instance[SparkContext is created]. You can change the assigned memory per executor per node in spark-standalone:index.md[standalone cluster] using ROOT:SparkContext.md#environment-variables[SPARK_EXECUTOR_MEMORY] environment variable. You can find the value displayed as Memory per Node in spark-standalone:spark-standalone-Master.md[web UI for standalone Master] (as depicted in the figure below). .Memory per Node in Spark Standalone's web UI image::spark-standalone-webui-memory-per-node.png[align=\"center\"] The above figure shows the result of running tools:spark-shell.md[Spark shell] with the amount of memory per executor defined explicitly (on command line), i.e. ./bin/spark-shell --master spark://localhost:7077 -c spark.executor.memory=2g == [[metrics]] Metrics Every executor registers its own executor:ExecutorSource.md[] to metrics:spark-metrics-MetricsSystem.md#report[report metrics]. == [[stop]] Stopping Executor","title":"[source, scala]"},{"location":"executor/Executor/#source-scala_2","text":"","title":"[source, scala]"},{"location":"executor/Executor/#stop-unit","text":"stop requests core:SparkEnv.md#metricsSystem[MetricsSystem] for a metrics:spark-metrics-MetricsSystem.md#report[report]. stop shuts < > down (and waits at most 10 seconds). stop shuts < > down. (only when < >) stop core:SparkEnv.md#stop[requests SparkEnv to stop]. stop is used when executor:CoarseGrainedExecutorBackend.md#Shutdown[CoarseGrainedExecutorBackend] and spark-local:spark-LocalEndpoint.md#StopExecutor[LocalEndpoint] are requested to stop their managed executors. == [[computeTotalGcTime]] computeTotalGcTime Method","title":"stop(): Unit"},{"location":"executor/Executor/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/Executor/#computetotalgctime-long","text":"computeTotalGcTime...FIXME computeTotalGcTime is used when: TaskRunner is requested to executor:TaskRunner.md#collectAccumulatorsAndResetStatusOnFailure[collectAccumulatorsAndResetStatusOnFailure] and executor:TaskRunner.md#run[run] Executor is requested to < > == [[createClassLoader]] createClassLoader Method","title":"computeTotalGcTime(): Long"},{"location":"executor/Executor/#source-scala_4","text":"","title":"[source, scala]"},{"location":"executor/Executor/#createclassloader-mutableurlclassloader","text":"createClassLoader...FIXME createClassLoader is used when...FIXME == [[addReplClassLoaderIfNeeded]] addReplClassLoaderIfNeeded Method","title":"createClassLoader(): MutableURLClassLoader"},{"location":"executor/Executor/#source-scala_5","text":"addReplClassLoaderIfNeeded( parent: ClassLoader): ClassLoader addReplClassLoaderIfNeeded...FIXME addReplClassLoaderIfNeeded is used when...FIXME == [[reportHeartBeat]] Heartbeating With Partial Metrics For Active Tasks To Driver","title":"[source, scala]"},{"location":"executor/Executor/#source-scala_6","text":"","title":"[source, scala]"},{"location":"executor/Executor/#reportheartbeat-unit","text":"reportHeartBeat collects executor:TaskRunner.md[TaskRunners] for < > (aka active tasks ) with their executor:TaskRunner.md#task[tasks] deserialized (i.e. either ready for execution or already started). executor:TaskRunner.md[] has TaskRunner.md#task[task] deserialized when it executor:TaskRunner.md#run[runs the task]. For every running task, reportHeartBeat takes its scheduler:Task.md#metrics[TaskMetrics] and: Requests executor:TaskMetrics.md#mergeShuffleReadMetrics[ShuffleRead metrics to be merged] executor:TaskMetrics.md#setJvmGCTime[Sets jvmGCTime metrics] reportHeartBeat then records the latest values of executor:TaskMetrics.md#accumulators[internal and external accumulators] for every task. NOTE: Internal accumulators are a task's metrics while external accumulators are a Spark application's accumulators that a user has created. reportHeartBeat sends a blocking ROOT:spark-HeartbeatReceiver.md#Heartbeat[Heartbeat] message to < HeartbeatReceiver endpoint>> (running on the driver). reportHeartBeat uses the value of ROOT:configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval] configuration property for the RPC timeout. NOTE: A Heartbeat message contains the executor identifier, the accumulator updates, and the identifier of the storage:BlockManager.md[]. If the response (from < HeartbeatReceiver endpoint>>) is to re-register the BlockManager , you should see the following INFO message in the logs and reportHeartBeat requests the BlockManager to storage:BlockManager.md#reregister[re-register] (which will register the blocks the BlockManager manages with the driver).","title":"reportHeartBeat(): Unit"},{"location":"executor/Executor/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"executor/Executor/#told-to-re-register-on-heartbeat","text":"HeartbeatResponse requests the BlockManager to re-register when either scheduler:TaskScheduler.md#executorHeartbeatReceived[TaskScheduler] or ROOT:spark-HeartbeatReceiver.md#Heartbeat[HeartbeatReceiver] know nothing about the executor. When posting the Heartbeat was successful, reportHeartBeat resets < > internal counter. In case of a non-fatal exception, you should see the following WARN message in the logs (followed by the stack trace). Issue communicating with driver in heartbeater Every failure reportHeartBeat increments < > up to ROOT:configuration-properties.md#spark.executor.heartbeat.maxFailures[spark.executor.heartbeat.maxFailures] configuration property. When the heartbeat failures reaches the maximum, you should see the following ERROR message in the logs and the executor terminates with the error code: 56 . Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times reportHeartBeat is used when Executor is requested to < > (that happens every ROOT:configuration-properties.md#spark.executor.heartbeatInterval[spark.executor.heartbeatInterval]). == [[startDriverHeartbeater]][[heartbeats-and-active-task-metrics]] Sending Heartbeats and Active Tasks Metrics Executors keep sending < > to the driver every < > (defaults to 10s with some random initial delay so the heartbeats from different executors do not pile up on the driver). .Executors use HeartbeatReceiver endpoint to report task metrics image::executor-heartbeatReceiver-endpoint.png[align=\"center\"] An executor sends heartbeats using the < >. .HeartbeatReceiver's Heartbeat Message Handler image::HeartbeatReceiver-Heartbeat.png[align=\"center\"] For each scheduler:Task.md[task] in executor:TaskRunner.md[] (in < > internal registry), the task's metrics are computed (i.e. mergeShuffleReadMetrics and setJvmGCTime ) that become part of the heartbeat (with accumulators). NOTE: Executors track the executor:TaskRunner.md[] that run scheduler:Task.md[tasks]. A executor:TaskRunner.md#run[task might not be assigned to a TaskRunner yet] when the executor sends a heartbeat. A blocking ROOT:spark-HeartbeatReceiver.md#Heartbeat[Heartbeat] message that holds the executor id, all accumulator updates (per task id), and storage:BlockManagerId.md[] is sent to ROOT:spark-HeartbeatReceiver.md[HeartbeatReceiver RPC endpoint] (with < > timeout). If the response ROOT:spark-HeartbeatReceiver.md#Heartbeat[requests to reregister BlockManager], you should see the following INFO message in the logs: Told to re-register on heartbeat BlockManager is requested to storage:BlockManager.md#reregister[reregister]. The internal < > counter is reset (i.e. becomes 0 ). If there are any issues with communicating with the driver, you should see the following WARN message in the logs:","title":"Told to re-register on heartbeat"},{"location":"executor/Executor/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"executor/Executor/#issue-communicating-with-driver-in-heartbeater","text":"The internal < > is incremented and checked to be less than the < > (i.e. spark.executor.heartbeat.maxFailures Spark property). If the number is greater, the following ERROR is printed out to the logs: Exit as unable to send heartbeats to driver more than [HEARTBEAT_MAX_FAILURES] times The executor exits (using System.exit and exit code 56). == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Issue communicating with driver in heartbeater"},{"location":"executor/Executor/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"executor/Executor/#log4jloggerorgapachesparkexecutorexecutorall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[executorSource]] ExecutorSource executor:ExecutorSource.md[] === [[heartbeatFailures]] heartbeatFailures === [[maxDirectResultSize]] maxDirectResultSize === [[maxResultSize]] maxResultSize Used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and creates a serialized ByteBuffer result that is a IndirectTaskResult )","title":"log4j.logger.org.apache.spark.executor.Executor=ALL"},{"location":"executor/ExecutorBackend/","text":"= ExecutorBackend ExecutorBackend is a < > that executor:TaskRunner.md[TaskRunners] use to < > to a scheduler. .ExecutorBackend receives notifications from TaskRunners image::ExecutorBackend.png[align=\"center\"] NOTE: TaskRunner manages a single individual scheduler:Task.md[task] and is managed by an executor:Executor.md#launchTask[ Executor to launch a task]. It is effectively a bridge between the driver and an executor, i.e. there are two endpoints running. There are three concrete executor backends: executor:CoarseGrainedExecutorBackend.md[] spark-local:spark-LocalSchedulerBackend.md[] (for spark-local:index.md[Spark local]) spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md[] == [[contract]] ExecutorBackend Contract === [[statusUpdate]] statusUpdate Method [source, scala] \u00b6 statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit Used when TaskRunner is requested to executor:TaskRunner.md#run[run a task] (to send task status updates).","title":"ExecutorBackend"},{"location":"executor/ExecutorBackend/#source-scala","text":"statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit Used when TaskRunner is requested to executor:TaskRunner.md#run[run a task] (to send task status updates).","title":"[source, scala]"},{"location":"executor/ExecutorSource/","text":"= ExecutorSource ExecutorSource is a spark-metrics-Source.md[metrics source] of an executor:Executor.md[]. It uses an executor's executor:Executor.md#threadPool[threadPool] for calculating the gauges. NOTE: Every executor has its own separate ExecutorSource that is registered when executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[ CoarseGrainedExecutorBackend receives a RegisteredExecutor ]. The name of a ExecutorSource is executor . .ExecutorSource in JConsole (using Spark Standalone) image::spark-executorsource-jconsole.png[align=\"center\"] .ExecutorSource Gauges [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Gauge | Description | threadpool.activeTasks | Approximate number of threads that are actively executing tasks. Uses http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getActiveCount ()]. | threadpool.completeTasks | Approximate total number of tasks that have completed execution. Uses http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getCompletedTaskCount ()]. | threadpool.currentPool_size | Current number of threads in the pool. Uses http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getPoolSize ()]. | threadpool.maxPool_size | Maximum allowed number of threads that have ever simultaneously been in the pool Uses http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[ThreadPoolExecutor.getMaximumPoolSize ()]. | filesystem.hdfs.read_bytes | Uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics ()] and getBytesRead() . | filesystem.hdfs.write_bytes | Uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics ()] and getBytesWritten() . | filesystem.hdfs.read_ops | Uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics ()] and getReadOps() | filesystem.hdfs.largeRead_ops | Uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics ()] and getLargeReadOps() . | filesystem.hdfs.write_ops | Uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem.getAllStatistics ()] and getWriteOps() . | filesystem.file.read_bytes | The same as hdfs but for file scheme. | filesystem.file.write_bytes | The same as hdfs but for file scheme. | filesystem.file.read_ops | The same as hdfs but for file scheme. | filesystem.file.largeRead_ops | The same as hdfs but for file scheme. | filesystem.file.write_ops | The same as hdfs but for file scheme. |===","title":"ExecutorSource"},{"location":"executor/ShuffleReadMetrics/","text":"= ShuffleReadMetrics ShuffleReadMetrics is...FIXME","title":"ShuffleReadMetrics"},{"location":"executor/ShuffleWriteMetrics/","text":"= ShuffleWriteMetrics ShuffleWriteMetrics is a < > that represents task metrics about writing shuffle data. ShuffleWriteMetrics tracks the following task metrics: < > < > < > NOTE: ROOT:spark-accumulators.md[Accumulators] allow tasks (running on executors) to communicate with the driver. [[accumulators]] .ShuffleWriteMetrics's Accumulators [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_bytesWritten]] _bytesWritten | Accumulator to track how many shuffle bytes were written in a shuffle task. Used when ShuffleWriteMetrics is requested the < > and to < > or < > it. NOTE: _bytesWritten is available as internal.metrics.shuffle.write.bytesWritten (internally shuffleWrite.BYTES_WRITTEN ) in executor:TaskMetrics.md[TaskMetrics]. | [[_writeTime]] _writeTime | Accumulator to track shuffle write time (as 64-bit integer) of a shuffle task. Used when ShuffleWriteMetrics is requested the < > and to < >. NOTE: _writeTime is available as internal.metrics.shuffle.write.writeTime (internally shuffleWrite.WRITE_TIME ) in executor:TaskMetrics.md[TaskMetrics]. | [[_recordsWritten]] _recordsWritten | Accumulator to track how many shuffle records were written in a shuffle task. Used when ShuffleWriteMetrics is requested the < > and to < > or < > it. NOTE: _recordsWritten is available as internal.metrics.shuffle.write.recordsWritten (internally shuffleWrite.RECORDS_WRITTEN ) in executor:TaskMetrics.md[TaskMetrics]. |=== == [[decRecordsWritten]] decRecordsWritten Method CAUTION: FIXME == [[decBytesWritten]] decBytesWritten Method CAUTION: FIXME == [[writeTime]] writeTime Method CAUTION: FIXME == [[recordsWritten]] recordsWritten Method CAUTION: FIXME == [[bytesWritten]] Returning Number of Shuffle Bytes Written -- bytesWritten Method [source, scala] \u00b6 bytesWritten: Long \u00b6 bytesWritten represents the shuffle bytes written metrics of a shuffle task. Internally, bytesWritten returns the sum of <<_bytesWritten, _bytesWritten>> internal accumulator. [NOTE] \u00b6 bytesWritten is used when: ShuffleWriteMetricsUIData is created In < > spark-SparkListener-StatsReportListener.md#onStageCompleted[ StatsReportListener intercepts stage completed events] to show shuffle bytes written shuffle:ShuffleExternalSorter.md#writeSortedFile[ ShuffleExternalSorter does writeSortedFile ] (to incDiskBytesSpilled ) spark-history-server:JsonProtocol.md#taskMetricsToJson[ JsonProtocol converts ShuffleWriteMetrics to JSON] spark-webui-executors-ExecutorsListener.md#onTaskEnd[ ExecutorsListener intercepts task end events] to update executor metrics 7. spark-webui-JobProgressListener.md#updateAggregateMetrics[ JobProgressListener updates stage and executor metrics] \u00b6 == [[incBytesWritten]] Incrementing Shuffle Bytes Written Metrics -- incBytesWritten Method [source, scala] \u00b6 incBytesWritten(v: Long): Unit \u00b6 incBytesWritten simply adds v to <<_bytesWritten, _bytesWritten>> internal accumulator. [NOTE] \u00b6 incBytesWritten is used when: shuffle:UnsafeShuffleWriter.md#mergeSpills[ UnsafeShuffleWriter does mergeSpills ] storage:DiskBlockObjectWriter.md#updateBytesWritten[ DiskBlockObjectWriter does updateBytesWritten ] spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON] ==== == [[incWriteTime]] Incrementing Shuffle Write Time Metrics -- incWriteTime Method [source, scala] \u00b6 incWriteTime(v: Long): Unit \u00b6 incWriteTime simply adds v to <<_writeTime, _writeTime>> internal accumulator. [NOTE] \u00b6 incWriteTime is used when: shuffle:SortShuffleWriter.md#stop[ SortShuffleWriter stops]. BypassMergeSortShuffleWriter shuffle:BypassMergeSortShuffleWriter.md#write[writes records] (i.e. when it initializes DiskBlockObjectWriter partition writers) and later when shuffle:BypassMergeSortShuffleWriter.md#writePartitionedFile[concatenates per-partition files into a single file]. shuffle:UnsafeShuffleWriter.md#mergeSpillsWithTransferTo[ UnsafeShuffleWriter does mergeSpillsWithTransferTo ]. storage:DiskBlockObjectWriter.md#commitAndGet[ DiskBlockObjectWriter does commitAndGet ] (but only when syncWrites flag is enabled that forces outstanding writes to disk). spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON] 6. TimeTrackingOutputStream does its operation (after all it is an output stream to track shuffle write time). \u00b6 == [[incRecordsWritten]] Incrementing Shuffle Records Written Metrics -- incRecordsWritten Method [source, scala] \u00b6 incRecordsWritten(v: Long): Unit \u00b6 incRecordsWritten simply adds v to <<_recordsWritten, _recordsWritten>> internal accumulator. [NOTE] \u00b6 incRecordsWritten is used when: shuffle:ShuffleExternalSorter.md#writeSortedFile[ ShuffleExternalSorter does writeSortedFile ] storage:DiskBlockObjectWriter.md#recordWritten[ DiskBlockObjectWriter does recordWritten ] spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON] ====","title":"ShuffleWriteMetrics"},{"location":"executor/ShuffleWriteMetrics/#source-scala","text":"","title":"[source, scala]"},{"location":"executor/ShuffleWriteMetrics/#byteswritten-long","text":"bytesWritten represents the shuffle bytes written metrics of a shuffle task. Internally, bytesWritten returns the sum of <<_bytesWritten, _bytesWritten>> internal accumulator.","title":"bytesWritten: Long"},{"location":"executor/ShuffleWriteMetrics/#note","text":"bytesWritten is used when: ShuffleWriteMetricsUIData is created In < > spark-SparkListener-StatsReportListener.md#onStageCompleted[ StatsReportListener intercepts stage completed events] to show shuffle bytes written shuffle:ShuffleExternalSorter.md#writeSortedFile[ ShuffleExternalSorter does writeSortedFile ] (to incDiskBytesSpilled ) spark-history-server:JsonProtocol.md#taskMetricsToJson[ JsonProtocol converts ShuffleWriteMetrics to JSON] spark-webui-executors-ExecutorsListener.md#onTaskEnd[ ExecutorsListener intercepts task end events] to update executor metrics","title":"[NOTE]"},{"location":"executor/ShuffleWriteMetrics/#7-spark-webui-jobprogresslistenermdupdateaggregatemetricsjobprogresslistener-updates-stage-and-executor-metrics","text":"== [[incBytesWritten]] Incrementing Shuffle Bytes Written Metrics -- incBytesWritten Method","title":"7. spark-webui-JobProgressListener.md#updateAggregateMetrics[JobProgressListener updates stage and executor metrics]"},{"location":"executor/ShuffleWriteMetrics/#source-scala_1","text":"","title":"[source, scala]"},{"location":"executor/ShuffleWriteMetrics/#incbyteswrittenv-long-unit","text":"incBytesWritten simply adds v to <<_bytesWritten, _bytesWritten>> internal accumulator.","title":"incBytesWritten(v: Long): Unit"},{"location":"executor/ShuffleWriteMetrics/#note_1","text":"incBytesWritten is used when: shuffle:UnsafeShuffleWriter.md#mergeSpills[ UnsafeShuffleWriter does mergeSpills ] storage:DiskBlockObjectWriter.md#updateBytesWritten[ DiskBlockObjectWriter does updateBytesWritten ] spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON] ==== == [[incWriteTime]] Incrementing Shuffle Write Time Metrics -- incWriteTime Method","title":"[NOTE]"},{"location":"executor/ShuffleWriteMetrics/#source-scala_2","text":"","title":"[source, scala]"},{"location":"executor/ShuffleWriteMetrics/#incwritetimev-long-unit","text":"incWriteTime simply adds v to <<_writeTime, _writeTime>> internal accumulator.","title":"incWriteTime(v: Long): Unit"},{"location":"executor/ShuffleWriteMetrics/#note_2","text":"incWriteTime is used when: shuffle:SortShuffleWriter.md#stop[ SortShuffleWriter stops]. BypassMergeSortShuffleWriter shuffle:BypassMergeSortShuffleWriter.md#write[writes records] (i.e. when it initializes DiskBlockObjectWriter partition writers) and later when shuffle:BypassMergeSortShuffleWriter.md#writePartitionedFile[concatenates per-partition files into a single file]. shuffle:UnsafeShuffleWriter.md#mergeSpillsWithTransferTo[ UnsafeShuffleWriter does mergeSpillsWithTransferTo ]. storage:DiskBlockObjectWriter.md#commitAndGet[ DiskBlockObjectWriter does commitAndGet ] (but only when syncWrites flag is enabled that forces outstanding writes to disk). spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON]","title":"[NOTE]"},{"location":"executor/ShuffleWriteMetrics/#6-timetrackingoutputstream-does-its-operation-after-all-it-is-an-output-stream-to-track-shuffle-write-time","text":"== [[incRecordsWritten]] Incrementing Shuffle Records Written Metrics -- incRecordsWritten Method","title":"6. TimeTrackingOutputStream does its operation (after all it is an output stream to track shuffle write time)."},{"location":"executor/ShuffleWriteMetrics/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/ShuffleWriteMetrics/#increcordswrittenv-long-unit","text":"incRecordsWritten simply adds v to <<_recordsWritten, _recordsWritten>> internal accumulator.","title":"incRecordsWritten(v: Long): Unit"},{"location":"executor/ShuffleWriteMetrics/#note_3","text":"incRecordsWritten is used when: shuffle:ShuffleExternalSorter.md#writeSortedFile[ ShuffleExternalSorter does writeSortedFile ] storage:DiskBlockObjectWriter.md#recordWritten[ DiskBlockObjectWriter does recordWritten ] spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON] ====","title":"[NOTE]"},{"location":"executor/TaskMetrics/","text":"TaskMetrics \u00b6 TaskMetrics is a < > (as < >) tracked during execution of a scheduler:Task.md[Task]. TaskMetrics is < > when: Stage is requested to scheduler:Stage.md#makeNewStageAttempt[create a new stage attempt] (when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit the missing tasks of a stage]) TaskMetrics utility is requested to < >, < >, and < > [[creating-instance]] TaskMetrics takes no arguments to be created. TaskMetrics is available using < >. TIP: Use ROOT:SparkListener.md#onTaskEnd[SparkListener.onTaskEnd] to intercept ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] events to access the < > of a task that has finished successfully. TIP: Use < > for summary statistics at runtime (after a stage completes). TIP: Use spark-history-server:EventLoggingListener.md[EventLoggingListener] for post-execution (history) statistics. TaskMetrics uses spark-accumulators.md[accumulators] to represent the metrics and offers \"increment\" methods to increment them. NOTE: The local values of the accumulators for a scheduler:Task.md[task] (as accumulated while the scheduler:Task.md#run[task runs]) are sent from the executor to the driver when the task completes (and < DAGScheduler re-creates TaskMetrics>>). [[metrics]] .Metrics [cols=\"1,1,1,2\",options=\"header\",width=\"100%\"] |=== | Property | Name | Type | Description | [[_memoryBytesSpilled]] _memoryBytesSpilled | internal.metrics.memoryBytesSpilled | LongAccumulator | Used in < >, < > | [[_updatedBlockStatuses]] _updatedBlockStatuses | internal.metrics.updatedBlockStatuses | CollectionAccumulator[(BlockId, BlockStatus)] | Used in < >, < BlockStatus for a Block >>, < > |=== [[internal-registries]] .TaskMetrics's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[nameToAccums]] nameToAccums | Internal spark-accumulators.md[accumulators] indexed by their names. Used when TaskMetrics < AccumulatorV2s >>, ...FIXME NOTE: nameToAccums is a transient and lazy value. | [[internalAccums]] internalAccums | Collection of internal spark-accumulators.md[AccumulatorV2] objects. Used when...FIXME NOTE: internalAccums is a transient and lazy value. | [[externalAccums]] externalAccums | Collection of external spark-accumulators.md[AccumulatorV2] objects. Used when TaskMetrics < AccumulatorV2s >>, ...FIXME NOTE: externalAccums is a transient and lazy value. |=== == [[accumulators]] accumulators Method CAUTION: FIXME == [[mergeShuffleReadMetrics]] mergeShuffleReadMetrics Method CAUTION: FIXME == [[memoryBytesSpilled]] memoryBytesSpilled Method CAUTION: FIXME == [[updatedBlockStatuses]] updatedBlockStatuses Method CAUTION: FIXME == [[setExecutorCpuTime]] setExecutorCpuTime Method CAUTION: FIXME == [[setResultSerializationTime]] setResultSerializationTime Method CAUTION: FIXME == [[setJvmGCTime]] setJvmGCTime Method CAUTION: FIXME == [[setExecutorRunTime]] setExecutorRunTime Method CAUTION: FIXME == [[setExecutorDeserializeCpuTime]] setExecutorDeserializeCpuTime Method CAUTION: FIXME == [[setExecutorDeserializeTime]] setExecutorDeserializeTime Method CAUTION: FIXME == [[setUpdatedBlockStatuses]] setUpdatedBlockStatuses Method CAUTION: FIXME == [[fromAccumulators]] Re-Creating TaskMetrics From AccumulatorV2s -- fromAccumulators Method [source, scala] \u00b6 fromAccumulators(accums: Seq[AccumulatorV2[_, _]]): TaskMetrics \u00b6 fromAccumulators creates a new TaskMetrics and registers accums as internal and external task metrics (using < > internal registry). Internally, fromAccumulators creates a new TaskMetrics. It then splits accums into internal and external task metrics collections (using < > internal registry). For every internal task metrics, fromAccumulators finds the metrics in < > internal registry (of the new TaskMetrics instance), copies spark-accumulators.md#metadata[metadata], and spark-accumulators.md#merge[merges state]. In the end, fromAccumulators < >. NOTE: fromAccumulators is used exclusively when scheduler:DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler gets notified that a task has finished] (and re-creates TaskMetrics). == [[incMemoryBytesSpilled]] Increasing Memory Bytes Spilled -- incMemoryBytesSpilled Method [source, scala] \u00b6 incMemoryBytesSpilled(v: Long): Unit \u00b6 incMemoryBytesSpilled adds v to <<_memoryBytesSpilled, _memoryBytesSpilled>> task metrics. [NOTE] \u00b6 incMemoryBytesSpilled is used when: rdd:Aggregator.md#updateMetrics[ Aggregator updates task metrics] CoGroupedRDD is requested to compute a partition shuffle:BlockStoreShuffleReader.md#read[ BlockStoreShuffleReader reads combined key-value records for a reduce task] shuffle:ShuffleExternalSorter.md#spill[ ShuffleExternalSorter frees execution memory by spilling to disk] shuffle:ExternalSorter.md#writePartitionedFile[ ExternalSorter writes the records into a temporary partitioned file in the disk store] UnsafeExternalSorter spills current records due to memory pressure SpillableIterator spills records to disk 8. spark-history-server:JsonProtocol.md#taskMetricsFromJson[ JsonProtocol creates TaskMetrics from JSON] \u00b6 == [[incUpdatedBlockStatuses]] Recording Updated BlockStatus For Block -- incUpdatedBlockStatuses Method [source, scala] \u00b6 incUpdatedBlockStatuses(v: (BlockId, BlockStatus)): Unit \u00b6 incUpdatedBlockStatuses adds v in <<_updatedBlockStatuses, _updatedBlockStatuses>> internal registry. NOTE: incUpdatedBlockStatuses is used exclusively when storage:BlockManager.md#addUpdatedBlockStatusToTaskMetrics[ BlockManager does addUpdatedBlockStatusToTaskMetrics ]. == [[register]] Registering Internal Accumulators -- register Method [source, scala] \u00b6 register(sc: SparkContext): Unit \u00b6 register spark-accumulators.md#register[registers the internal accumulators] (from < > internal registry) with countFailedValues enabled ( true ). NOTE: register is used exclusively when scheduler:Stage.md#makeNewStageAttempt[ Stage is requested for its new attempt]. == [[empty]] empty Factory Method [source, scala] \u00b6 empty: TaskMetrics \u00b6 empty ...FIXME [NOTE] \u00b6 empty is used when: TaskContextImpl is < > TaskMetrics utility is requested to < > * JsonProtocol utility is requested to spark-history-server:JsonProtocol.md#taskMetricsFromJson[taskMetricsFromJson] \u00b6 == [[registered]] registered Factory Method [source, scala] \u00b6 registered: TaskMetrics \u00b6 registered ...FIXME NOTE: registered is used exclusively when Task is scheduler:Task.md#serializedTaskMetrics[created]. == [[fromAccumulatorInfos]] fromAccumulatorInfos Factory Method [source, scala] \u00b6 fromAccumulatorInfos(infos: Seq[AccumulableInfo]): TaskMetrics \u00b6 fromAccumulatorInfos ...FIXME NOTE: fromAccumulatorInfos is used exclusively when AppStatusListener is requested to core:AppStatusListener.md#onExecutorMetricsUpdate[onExecutorMetricsUpdate] (for spark-history-server:index.md[Spark History Server] only). == [[fromAccumulators]] fromAccumulators Factory Method [source, scala] \u00b6 fromAccumulators(accums: Seq[AccumulatorV2[_, _]]): TaskMetrics \u00b6 fromAccumulators ...FIXME NOTE: fromAccumulators is used exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#postTaskEnd[postTaskEnd].","title":"TaskMetrics"},{"location":"executor/TaskMetrics/#taskmetrics","text":"TaskMetrics is a < > (as < >) tracked during execution of a scheduler:Task.md[Task]. TaskMetrics is < > when: Stage is requested to scheduler:Stage.md#makeNewStageAttempt[create a new stage attempt] (when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit the missing tasks of a stage]) TaskMetrics utility is requested to < >, < >, and < > [[creating-instance]] TaskMetrics takes no arguments to be created. TaskMetrics is available using < >. TIP: Use ROOT:SparkListener.md#onTaskEnd[SparkListener.onTaskEnd] to intercept ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] events to access the < > of a task that has finished successfully. TIP: Use < > for summary statistics at runtime (after a stage completes). TIP: Use spark-history-server:EventLoggingListener.md[EventLoggingListener] for post-execution (history) statistics. TaskMetrics uses spark-accumulators.md[accumulators] to represent the metrics and offers \"increment\" methods to increment them. NOTE: The local values of the accumulators for a scheduler:Task.md[task] (as accumulated while the scheduler:Task.md#run[task runs]) are sent from the executor to the driver when the task completes (and < DAGScheduler re-creates TaskMetrics>>). [[metrics]] .Metrics [cols=\"1,1,1,2\",options=\"header\",width=\"100%\"] |=== | Property | Name | Type | Description | [[_memoryBytesSpilled]] _memoryBytesSpilled | internal.metrics.memoryBytesSpilled | LongAccumulator | Used in < >, < > | [[_updatedBlockStatuses]] _updatedBlockStatuses | internal.metrics.updatedBlockStatuses | CollectionAccumulator[(BlockId, BlockStatus)] | Used in < >, < BlockStatus for a Block >>, < > |=== [[internal-registries]] .TaskMetrics's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[nameToAccums]] nameToAccums | Internal spark-accumulators.md[accumulators] indexed by their names. Used when TaskMetrics < AccumulatorV2s >>, ...FIXME NOTE: nameToAccums is a transient and lazy value. | [[internalAccums]] internalAccums | Collection of internal spark-accumulators.md[AccumulatorV2] objects. Used when...FIXME NOTE: internalAccums is a transient and lazy value. | [[externalAccums]] externalAccums | Collection of external spark-accumulators.md[AccumulatorV2] objects. Used when TaskMetrics < AccumulatorV2s >>, ...FIXME NOTE: externalAccums is a transient and lazy value. |=== == [[accumulators]] accumulators Method CAUTION: FIXME == [[mergeShuffleReadMetrics]] mergeShuffleReadMetrics Method CAUTION: FIXME == [[memoryBytesSpilled]] memoryBytesSpilled Method CAUTION: FIXME == [[updatedBlockStatuses]] updatedBlockStatuses Method CAUTION: FIXME == [[setExecutorCpuTime]] setExecutorCpuTime Method CAUTION: FIXME == [[setResultSerializationTime]] setResultSerializationTime Method CAUTION: FIXME == [[setJvmGCTime]] setJvmGCTime Method CAUTION: FIXME == [[setExecutorRunTime]] setExecutorRunTime Method CAUTION: FIXME == [[setExecutorDeserializeCpuTime]] setExecutorDeserializeCpuTime Method CAUTION: FIXME == [[setExecutorDeserializeTime]] setExecutorDeserializeTime Method CAUTION: FIXME == [[setUpdatedBlockStatuses]] setUpdatedBlockStatuses Method CAUTION: FIXME == [[fromAccumulators]] Re-Creating TaskMetrics From AccumulatorV2s -- fromAccumulators Method","title":"TaskMetrics"},{"location":"executor/TaskMetrics/#source-scala","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#fromaccumulatorsaccums-seqaccumulatorv2_-_-taskmetrics","text":"fromAccumulators creates a new TaskMetrics and registers accums as internal and external task metrics (using < > internal registry). Internally, fromAccumulators creates a new TaskMetrics. It then splits accums into internal and external task metrics collections (using < > internal registry). For every internal task metrics, fromAccumulators finds the metrics in < > internal registry (of the new TaskMetrics instance), copies spark-accumulators.md#metadata[metadata], and spark-accumulators.md#merge[merges state]. In the end, fromAccumulators < >. NOTE: fromAccumulators is used exclusively when scheduler:DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler gets notified that a task has finished] (and re-creates TaskMetrics). == [[incMemoryBytesSpilled]] Increasing Memory Bytes Spilled -- incMemoryBytesSpilled Method","title":"fromAccumulators(accums: Seq[AccumulatorV2[_, _]]): TaskMetrics"},{"location":"executor/TaskMetrics/#source-scala_1","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#incmemorybytesspilledv-long-unit","text":"incMemoryBytesSpilled adds v to <<_memoryBytesSpilled, _memoryBytesSpilled>> task metrics.","title":"incMemoryBytesSpilled(v: Long): Unit"},{"location":"executor/TaskMetrics/#note","text":"incMemoryBytesSpilled is used when: rdd:Aggregator.md#updateMetrics[ Aggregator updates task metrics] CoGroupedRDD is requested to compute a partition shuffle:BlockStoreShuffleReader.md#read[ BlockStoreShuffleReader reads combined key-value records for a reduce task] shuffle:ShuffleExternalSorter.md#spill[ ShuffleExternalSorter frees execution memory by spilling to disk] shuffle:ExternalSorter.md#writePartitionedFile[ ExternalSorter writes the records into a temporary partitioned file in the disk store] UnsafeExternalSorter spills current records due to memory pressure SpillableIterator spills records to disk","title":"[NOTE]"},{"location":"executor/TaskMetrics/#8-spark-history-serverjsonprotocolmdtaskmetricsfromjsonjsonprotocol-creates-taskmetrics-from-json","text":"== [[incUpdatedBlockStatuses]] Recording Updated BlockStatus For Block -- incUpdatedBlockStatuses Method","title":"8. spark-history-server:JsonProtocol.md#taskMetricsFromJson[JsonProtocol creates TaskMetrics from JSON]"},{"location":"executor/TaskMetrics/#source-scala_2","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#incupdatedblockstatusesv-blockid-blockstatus-unit","text":"incUpdatedBlockStatuses adds v in <<_updatedBlockStatuses, _updatedBlockStatuses>> internal registry. NOTE: incUpdatedBlockStatuses is used exclusively when storage:BlockManager.md#addUpdatedBlockStatusToTaskMetrics[ BlockManager does addUpdatedBlockStatusToTaskMetrics ]. == [[register]] Registering Internal Accumulators -- register Method","title":"incUpdatedBlockStatuses(v: (BlockId, BlockStatus)): Unit"},{"location":"executor/TaskMetrics/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#registersc-sparkcontext-unit","text":"register spark-accumulators.md#register[registers the internal accumulators] (from < > internal registry) with countFailedValues enabled ( true ). NOTE: register is used exclusively when scheduler:Stage.md#makeNewStageAttempt[ Stage is requested for its new attempt]. == [[empty]] empty Factory Method","title":"register(sc: SparkContext): Unit"},{"location":"executor/TaskMetrics/#source-scala_4","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#empty-taskmetrics","text":"empty ...FIXME","title":"empty: TaskMetrics"},{"location":"executor/TaskMetrics/#note_1","text":"empty is used when: TaskContextImpl is < > TaskMetrics utility is requested to < >","title":"[NOTE]"},{"location":"executor/TaskMetrics/#jsonprotocol-utility-is-requested-to-spark-history-serverjsonprotocolmdtaskmetricsfromjsontaskmetricsfromjson","text":"== [[registered]] registered Factory Method","title":"* JsonProtocol utility is requested to spark-history-server:JsonProtocol.md#taskMetricsFromJson[taskMetricsFromJson]"},{"location":"executor/TaskMetrics/#source-scala_5","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#registered-taskmetrics","text":"registered ...FIXME NOTE: registered is used exclusively when Task is scheduler:Task.md#serializedTaskMetrics[created]. == [[fromAccumulatorInfos]] fromAccumulatorInfos Factory Method","title":"registered: TaskMetrics"},{"location":"executor/TaskMetrics/#source-scala_6","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#fromaccumulatorinfosinfos-seqaccumulableinfo-taskmetrics","text":"fromAccumulatorInfos ...FIXME NOTE: fromAccumulatorInfos is used exclusively when AppStatusListener is requested to core:AppStatusListener.md#onExecutorMetricsUpdate[onExecutorMetricsUpdate] (for spark-history-server:index.md[Spark History Server] only). == [[fromAccumulators]] fromAccumulators Factory Method","title":"fromAccumulatorInfos(infos: Seq[AccumulableInfo]): TaskMetrics"},{"location":"executor/TaskMetrics/#source-scala_7","text":"","title":"[source, scala]"},{"location":"executor/TaskMetrics/#fromaccumulatorsaccums-seqaccumulatorv2_-_-taskmetrics_1","text":"fromAccumulators ...FIXME NOTE: fromAccumulators is used exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#postTaskEnd[postTaskEnd].","title":"fromAccumulators(accums: Seq[AccumulatorV2[_, _]]): TaskMetrics"},{"location":"executor/TaskRunner/","text":"= TaskRunner TaskRunner is a thread of execution (a {java-javadoc-url}/java/lang/Runnable.html[java.lang.Runnable]) of a < >. .Executor creates and runs TaskRunner image::TaskRunner.png[align=\"center\"] Once < >, TaskRunner can be < > and optionally < > (which starts and kills a given < >, respectively). TaskRunner is an internal class of the executor:Executor.md[] class and has access to its internals (properties and methods). == [[creating-instance]] Creating Instance TaskRunner takes the following to be created: [[execBackend]] executor:ExecutorBackend.md[] (that manages the parent executor:Executor.md[]) [[taskDescription]] scheduler:spark-scheduler-TaskDescription.md[TaskDescription] TaskRunner is created when Executor is requested to executor:Executor.md#launchTask[launch a task]. == [[threadName]] Thread Name TaskRunner uses Executor task launch worker for task [taskId] as the thread name. == [[run]] Running Task [source, scala] \u00b6 run(): Unit \u00b6 When executed, run initializes < > as the current thread identifier (using Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#getId--++[Thread ]) run then sets the name of the current thread as < > (using Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#setName-java.lang.String-++[Thread ]). run memory:TaskMemoryManager.md#creating-instance[creates a TaskMemoryManager ] (using the current memory:MemoryManager.md[MemoryManager] and < >). NOTE: run uses core:SparkEnv.md#memoryManager[ SparkEnv to access the current MemoryManager ]. run starts tracking the time to deserialize a task. run sets the current thread's context classloader (with < >). run serializer:Serializer.md#newInstance[creates a closure Serializer ]. NOTE: run uses SparkEnv core:SparkEnv.md#closureSerializer[to access the current closure Serializer ]. You should see the following INFO message in the logs: Running [taskName] (TID [taskId]) run executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend ] that < > is in TaskState.RUNNING state. NOTE: run uses ExecutorBackend that was specified when TaskRunner < >. run < startGCTime >>. run < >. NOTE: run uses spark-scheduler-TaskDescription.md[TaskDescription] that is specified when TaskRunner < >. run serializer:SerializerInstance.md#deserialize[deserializes the task] (using the context class loader) and sets its localProperties and TaskMemoryManager . run sets the < > internal reference to hold the deserialized task. NOTE: run uses TaskDescription spark-scheduler-TaskDescription.md#serializedTask[to access serialized task]. If < > flag is enabled, run throws a TaskKilledException . You should see the following DEBUG message in the logs: Task [taskId]'s epoch is [task.epoch] run scheduler:MapOutputTracker.md#updateEpoch[notifies MapOutputTracker about the epoch of the task]. NOTE: run uses core:SparkEnv.md#mapOutputTracker[ SparkEnv to access the current MapOutputTracker ]. run records the current time as the task's start time (as taskStart ). run scheduler:Task.md#run[runs the task] (with taskAttemptId as < >, attemptNumber from TaskDescription , and metricsSystem as the current metrics:spark-metrics-MetricsSystem.md[MetricsSystem]). NOTE: run uses core:SparkEnv.md#metricsSystem[ SparkEnv to access the current MetricsSystem ]. NOTE: The task runs inside a \"monitored\" block (i.e. try-finally block) to detect any memory and lock leaks after the task's run finishes regardless of the final outcome - the computed value or an exception thrown. After the task's run has finished (inside the \"finally\" block of the \"monitored\" block), run BlockManager.md#releaseAllLocksForTask[requests BlockManager to release all locks of the task] (for the task's < >). The locks are later used for lock leak detection. run then memory:TaskMemoryManager.md#cleanUpAllAllocatedMemory[requests TaskMemoryManager to clean up allocated memory] (that helps finding memory leaks). If run detects memory leak of the managed memory (i.e. the memory freed is greater than 0 ) and ROOT:configuration-properties.md#spark.unsafe.exceptionOnMemoryLeak[spark.unsafe.exceptionOnMemoryLeak] Spark property is enabled (it is not by default) and no exception was reported while the task ran, run reports a SparkException : Managed memory leak detected; size = [freedMemory] bytes, TID = [taskId] Otherwise, if ROOT:configuration-properties.md#spark.unsafe.exceptionOnMemoryLeak[spark.unsafe.exceptionOnMemoryLeak] is disabled, you should see the following ERROR message in the logs instead: Managed memory leak detected; size = [freedMemory] bytes, TID = [taskId] NOTE: If run detects a memory leak, it leads to a SparkException or ERROR message in the logs. If run detects lock leaking (i.e. the number of locks released) and ROOT:configuration-properties.md#spark.storage.exceptionOnPinLeak[spark.storage.exceptionOnPinLeak] configuration property is enabled (it is not by default) and no exception was reported while the task ran, run reports a SparkException : [releasedLocks] block locks were not released by TID = [taskId]: [releasedLocks separated by comma] Otherwise, if ROOT:configuration-properties.md#spark.storage.exceptionOnPinLeak[spark.storage.exceptionOnPinLeak] is disabled or the task reported an exception, you should see the following INFO message in the logs instead: [releasedLocks] block locks were not released by TID = [taskId]: [releasedLocks separated by comma] NOTE: If run detects any lock leak, it leads to a SparkException or INFO message in the logs. Rigth after the \"monitored\" block, run records the current time as the task's finish time (as taskFinish ). If the scheduler:Task.md#kill[task was killed] (while it was running), run reports a TaskKilledException (and the TaskRunner exits). run serializer:Serializer.md#newInstance[creates a Serializer ] and serializer:Serializer.md#serialize[serializes the task's result]. run measures the time to serialize the result. NOTE: run uses SparkEnv core:SparkEnv.md#serializer[to access the current Serializer ]. SparkEnv was specified when executor:Executor.md#creating-instance[the owning Executor was created]. IMPORTANT: This is when TaskExecutor serializes the computed value of a task to be sent back to the driver. run records the scheduler:Task.md#metrics[task metrics]: executor:TaskMetrics.md#setExecutorDeserializeTime[executorDeserializeTime] executor:TaskMetrics.md#setExecutorDeserializeCpuTime[executorDeserializeCpuTime] executor:TaskMetrics.md#setExecutorRunTime[executorRunTime] executor:TaskMetrics.md#setExecutorCpuTime[executorCpuTime] executor:TaskMetrics.md#setJvmGCTime[jvmGCTime] executor:TaskMetrics.md#setResultSerializationTime[resultSerializationTime] run scheduler:Task.md#collectAccumulatorUpdates[collects the latest values of internal and external accumulators used in the task]. run creates a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult] (with the serialized result and the latest values of accumulators). run serializer:Serializer.md#serialize[serializes the DirectTaskResult ] and gets the byte buffer's limit. NOTE: A serialized DirectTaskResult is Java's https://docs.oracle.com/javase/8/docs/api/java/nio/ByteBuffer.html[java.nio.ByteBuffer ]. run selects the proper serialized version of the result before executor:ExecutorBackend.md#statusUpdate[sending it to ExecutorBackend ]. run branches off based on the serialized DirectTaskResult byte buffer's limit. When executor:Executor.md#maxResultSize[maxResultSize] is greater than 0 and the serialized DirectTaskResult buffer limit exceeds it, the following WARN message is displayed in the logs: Finished [taskName] (TID [taskId]). Result is larger than maxResultSize ([resultSize] > [maxResultSize]), dropping it. TIP: Read about ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize]. $ ./bin/spark-shell -c spark.driver.maxResultSize=1m scala> sc.version res0: String = 2.0.0-SNAPSHOT scala> sc.getConf.get(\"spark.driver.maxResultSize\") res1: String = 1m scala> sc.range(0, 1024 * 1024 + 10, 1).collect WARN Executor: Finished task 4.0 in stage 0.0 (TID 4). Result is larger than maxResultSize (1031.4 KB > 1024.0 KB), dropping it. ... ERROR TaskSetManager: Total size of serialized results of 1 tasks (1031.4 KB) is bigger than spark.driver.maxResultSize (1024.0 KB) ... org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1 tasks (1031.4 KB) is bigger than spark.driver.maxResultSize (1024.0 KB) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1448) ... In this case, run creates a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] (with a TaskResultBlockId for the task's < > and resultSize ) and serializer:Serializer.md#serialize[serializes it]. [[run-result-sent-via-blockmanager]] When maxResultSize is not positive or resultSize is smaller than maxResultSize but greater than executor:Executor.md#maxDirectResultSize[maxDirectResultSize], run creates a TaskResultBlockId for the task's < > and BlockManager.md#putBytes[stores the serialized DirectTaskResult in BlockManager ] (as the TaskResultBlockId with MEMORY_AND_DISK_SER storage level). You should see the following INFO message in the logs: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent via BlockManager) In this case, run creates a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] (with a TaskResultBlockId for the task's < > and resultSize ) and serializer:Serializer.md#serialize[serializes it]. NOTE: The difference between the two above cases is that the result is dropped or stored in BlockManager with MEMORY_AND_DISK_SER storage level. When the two cases above do not hold, you should see the following INFO message in the logs: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver run uses the serialized DirectTaskResult byte buffer as the final serializedResult . NOTE: The final serializedResult is either a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] (possibly with the block stored in BlockManager ) or a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult]. run executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend ] that < > is in TaskState.FINISHED state with the serialized result and removes < > from the owning executor's executor:Executor.md#runningTasks[ runningTasks] registry. NOTE: run uses ExecutorBackend that is specified when TaskRunner < >. NOTE: TaskRunner is Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[Runnable ] and the contract requires that once a TaskRunner has completed execution it must not be restarted. When run catches a exception while executing the task, run acts according to its type (as presented in the following \"run's Exception Cases\" table and the following sections linked from the table). .run's Exception Cases, TaskState and Serialized ByteBuffer [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Exception Type | TaskState | Serialized ByteBuffer | < > | FAILED | TaskFailedReason | < > | KILLED | TaskKilled | < > | KILLED | TaskKilled | < > | FAILED | TaskFailedReason | < > | FAILED | ExceptionFailure |=== run is part of {java-javadoc-url}/java/lang/Runnable.html[java.lang.Runnable] contract. === [[run-FetchFailedException]] FetchFailedException When shuffle:FetchFailedException.md[FetchFailedException] is reported while running a task, run < >. run shuffle:FetchFailedException.md#toTaskFailedReason[requests FetchFailedException for the TaskFailedReason ], serializes it and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized reason). NOTE: ExecutorBackend was specified when < >. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the failure reason. The Serializer was created before run ran the task. === [[run-TaskKilledException]] TaskKilledException When TaskKilledException is reported while running a task, you should see the following INFO message in the logs: Executor killed [taskName] (TID [taskId]), reason: [reason] run then < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). === [[run-InterruptedException]] InterruptedException (with Task Killed) When InterruptedException is reported while running a task, and the task has been killed, you should see the following INFO message in the logs: Executor interrupted and killed [taskName] (TID [taskId]), reason: [killReason] run then < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). NOTE: The difference between this InterruptedException and < > is the INFO message in the logs. === [[run-CommitDeniedException]] CommitDeniedException When CommitDeniedException is reported while running a task, run < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized TaskKilled object). NOTE: The difference between this CommitDeniedException and < > is just the reason being sent to ExecutorBackend . === [[run-Throwable]] Throwable When run catches a Throwable , you should see the following ERROR message in the logs (followed by the exception). Exception in [taskName] (TID [taskId]) run then records the following task metrics (only when < > is available): executor:TaskMetrics.md#setExecutorRunTime[executorRunTime] executor:TaskMetrics.md#setJvmGCTime[jvmGCTime] run then scheduler:Task.md#collectAccumulatorUpdates[collects the latest values of internal and external accumulators] (with taskFailed flag enabled to inform that the collection is for a failed task). Otherwise, when < > is not available, the accumulator collection is empty. run converts the task accumulators to collection of AccumulableInfo , creates a ExceptionFailure (with the accumulators), and serializer:Serializer.md#serialize[serializes them]. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the ExceptionFailure . CAUTION: FIXME Why does run create new ExceptionFailure(t, accUpdates).withAccums(accums) , i.e. accumulators occur twice in the object. run < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and the serialized ExceptionFailure ). run may also trigger SparkUncaughtExceptionHandler.uncaughtException(t) if this is a fatal error. NOTE: The difference between this most Throwable case and other FAILED cases (i.e. < > and < >) is just the serialized ExceptionFailure vs a reason being sent to ExecutorBackend , respectively. == [[kill]] Killing Task [source, scala] \u00b6 kill( interruptThread: Boolean, reason: String): Unit kill marks the TaskRunner as < > and scheduler:Task.md#kill[kills the task] (if available and not < > already). NOTE: kill passes the input interruptThread on to the task itself while killing it. When executed, you should see the following INFO message in the logs: Executor is trying to kill [taskName] (TID [taskId]), reason: [reason] NOTE: < > flag is checked periodically in < > to stop executing the task. Once killed, the task will eventually stop. == [[collectAccumulatorsAndResetStatusOnFailure]] collectAccumulatorsAndResetStatusOnFailure Method [source, scala] \u00b6 collectAccumulatorsAndResetStatusOnFailure( taskStartTime: Long): (Seq[AccumulatorV2[_, _]], Seq[AccumulableInfo]) collectAccumulatorsAndResetStatusOnFailure...FIXME collectAccumulatorsAndResetStatusOnFailure is used when TaskRunner is requested to < >. == [[hasFetchFailure]] hasFetchFailure Method [source, scala] \u00b6 hasFetchFailure: Boolean \u00b6 hasFetchFailure...FIXME hasFetchFailure is used when TaskRunner is requested to < >. == [[setTaskFinishedAndClearInterruptStatus]] setTaskFinishedAndClearInterruptStatus Method [source, scala] \u00b6 setTaskFinishedAndClearInterruptStatus(): Unit \u00b6 setTaskFinishedAndClearInterruptStatus...FIXME setTaskFinishedAndClearInterruptStatus is used when TaskRunner is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside (since TaskRunner is an internal class of Executor). Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.executor.Executor=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[finished]][[isFinished]] finished Flag finished flag says whether the < > has finished ( true ) or not ( false ) Default: false Enabled ( true ) after TaskRunner has been requested to < > Used when TaskRunner is requested to < > === [[reasonIfKilled]] reasonIfKilled Reason to < > (and avoid < >) Default: (empty) ( None ) === [[startGCTime]] startGCTime Timestamp Timestamp (which is really the executor:Executor.md#computeTotalGcTime[total amount of time this Executor JVM process has already spent in garbage collection]) that is used to mark the GC \"zero\" time (when < >) and then compute the JVM GC time metric when: TaskRunner is requested to < > and < > Executor is requested to executor:Executor.md#reportHeartBeat[reportHeartBeat] === [[task]] Task Deserialized scheduler:Task.md[task] to execute Used when: TaskRunner is requested to < >, < >, < >, < > Executor is requested to executor:Executor.md#reportHeartBeat[reportHeartBeat] === [[taskId]] Task Id The < > (of the < >) Used when: TaskRunner is requested to < > (to create a memory:TaskMemoryManager.md[TaskMemoryManager] and serialize a IndirectTaskResult for a large task result) and < > the task and for the < > Executor is requested to executor:Executor.md#reportHeartBeat[reportHeartBeat] === [[taskName]] Task Name The < > (of the < >) that is used exclusively for < > purposes when TaskRunner is requested to < > and < > the task === [[threadId]][[getThreadId]] Thread Id Current thread ID Default: -1 Set immediately when TaskRunner is requested to < > and used exclusively when TaskReaper is requested for the thread info of the current thread (aka thread dump )","title":"TaskRunner"},{"location":"executor/TaskRunner/#source-scala","text":"","title":"[source, scala]"},{"location":"executor/TaskRunner/#run-unit","text":"When executed, run initializes < > as the current thread identifier (using Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#getId--++[Thread ]) run then sets the name of the current thread as < > (using Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#setName-java.lang.String-++[Thread ]). run memory:TaskMemoryManager.md#creating-instance[creates a TaskMemoryManager ] (using the current memory:MemoryManager.md[MemoryManager] and < >). NOTE: run uses core:SparkEnv.md#memoryManager[ SparkEnv to access the current MemoryManager ]. run starts tracking the time to deserialize a task. run sets the current thread's context classloader (with < >). run serializer:Serializer.md#newInstance[creates a closure Serializer ]. NOTE: run uses SparkEnv core:SparkEnv.md#closureSerializer[to access the current closure Serializer ]. You should see the following INFO message in the logs: Running [taskName] (TID [taskId]) run executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend ] that < > is in TaskState.RUNNING state. NOTE: run uses ExecutorBackend that was specified when TaskRunner < >. run < startGCTime >>. run < >. NOTE: run uses spark-scheduler-TaskDescription.md[TaskDescription] that is specified when TaskRunner < >. run serializer:SerializerInstance.md#deserialize[deserializes the task] (using the context class loader) and sets its localProperties and TaskMemoryManager . run sets the < > internal reference to hold the deserialized task. NOTE: run uses TaskDescription spark-scheduler-TaskDescription.md#serializedTask[to access serialized task]. If < > flag is enabled, run throws a TaskKilledException . You should see the following DEBUG message in the logs: Task [taskId]'s epoch is [task.epoch] run scheduler:MapOutputTracker.md#updateEpoch[notifies MapOutputTracker about the epoch of the task]. NOTE: run uses core:SparkEnv.md#mapOutputTracker[ SparkEnv to access the current MapOutputTracker ]. run records the current time as the task's start time (as taskStart ). run scheduler:Task.md#run[runs the task] (with taskAttemptId as < >, attemptNumber from TaskDescription , and metricsSystem as the current metrics:spark-metrics-MetricsSystem.md[MetricsSystem]). NOTE: run uses core:SparkEnv.md#metricsSystem[ SparkEnv to access the current MetricsSystem ]. NOTE: The task runs inside a \"monitored\" block (i.e. try-finally block) to detect any memory and lock leaks after the task's run finishes regardless of the final outcome - the computed value or an exception thrown. After the task's run has finished (inside the \"finally\" block of the \"monitored\" block), run BlockManager.md#releaseAllLocksForTask[requests BlockManager to release all locks of the task] (for the task's < >). The locks are later used for lock leak detection. run then memory:TaskMemoryManager.md#cleanUpAllAllocatedMemory[requests TaskMemoryManager to clean up allocated memory] (that helps finding memory leaks). If run detects memory leak of the managed memory (i.e. the memory freed is greater than 0 ) and ROOT:configuration-properties.md#spark.unsafe.exceptionOnMemoryLeak[spark.unsafe.exceptionOnMemoryLeak] Spark property is enabled (it is not by default) and no exception was reported while the task ran, run reports a SparkException : Managed memory leak detected; size = [freedMemory] bytes, TID = [taskId] Otherwise, if ROOT:configuration-properties.md#spark.unsafe.exceptionOnMemoryLeak[spark.unsafe.exceptionOnMemoryLeak] is disabled, you should see the following ERROR message in the logs instead: Managed memory leak detected; size = [freedMemory] bytes, TID = [taskId] NOTE: If run detects a memory leak, it leads to a SparkException or ERROR message in the logs. If run detects lock leaking (i.e. the number of locks released) and ROOT:configuration-properties.md#spark.storage.exceptionOnPinLeak[spark.storage.exceptionOnPinLeak] configuration property is enabled (it is not by default) and no exception was reported while the task ran, run reports a SparkException : [releasedLocks] block locks were not released by TID = [taskId]: [releasedLocks separated by comma] Otherwise, if ROOT:configuration-properties.md#spark.storage.exceptionOnPinLeak[spark.storage.exceptionOnPinLeak] is disabled or the task reported an exception, you should see the following INFO message in the logs instead: [releasedLocks] block locks were not released by TID = [taskId]: [releasedLocks separated by comma] NOTE: If run detects any lock leak, it leads to a SparkException or INFO message in the logs. Rigth after the \"monitored\" block, run records the current time as the task's finish time (as taskFinish ). If the scheduler:Task.md#kill[task was killed] (while it was running), run reports a TaskKilledException (and the TaskRunner exits). run serializer:Serializer.md#newInstance[creates a Serializer ] and serializer:Serializer.md#serialize[serializes the task's result]. run measures the time to serialize the result. NOTE: run uses SparkEnv core:SparkEnv.md#serializer[to access the current Serializer ]. SparkEnv was specified when executor:Executor.md#creating-instance[the owning Executor was created]. IMPORTANT: This is when TaskExecutor serializes the computed value of a task to be sent back to the driver. run records the scheduler:Task.md#metrics[task metrics]: executor:TaskMetrics.md#setExecutorDeserializeTime[executorDeserializeTime] executor:TaskMetrics.md#setExecutorDeserializeCpuTime[executorDeserializeCpuTime] executor:TaskMetrics.md#setExecutorRunTime[executorRunTime] executor:TaskMetrics.md#setExecutorCpuTime[executorCpuTime] executor:TaskMetrics.md#setJvmGCTime[jvmGCTime] executor:TaskMetrics.md#setResultSerializationTime[resultSerializationTime] run scheduler:Task.md#collectAccumulatorUpdates[collects the latest values of internal and external accumulators used in the task]. run creates a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult] (with the serialized result and the latest values of accumulators). run serializer:Serializer.md#serialize[serializes the DirectTaskResult ] and gets the byte buffer's limit. NOTE: A serialized DirectTaskResult is Java's https://docs.oracle.com/javase/8/docs/api/java/nio/ByteBuffer.html[java.nio.ByteBuffer ]. run selects the proper serialized version of the result before executor:ExecutorBackend.md#statusUpdate[sending it to ExecutorBackend ]. run branches off based on the serialized DirectTaskResult byte buffer's limit. When executor:Executor.md#maxResultSize[maxResultSize] is greater than 0 and the serialized DirectTaskResult buffer limit exceeds it, the following WARN message is displayed in the logs: Finished [taskName] (TID [taskId]). Result is larger than maxResultSize ([resultSize] > [maxResultSize]), dropping it. TIP: Read about ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize]. $ ./bin/spark-shell -c spark.driver.maxResultSize=1m scala> sc.version res0: String = 2.0.0-SNAPSHOT scala> sc.getConf.get(\"spark.driver.maxResultSize\") res1: String = 1m scala> sc.range(0, 1024 * 1024 + 10, 1).collect WARN Executor: Finished task 4.0 in stage 0.0 (TID 4). Result is larger than maxResultSize (1031.4 KB > 1024.0 KB), dropping it. ... ERROR TaskSetManager: Total size of serialized results of 1 tasks (1031.4 KB) is bigger than spark.driver.maxResultSize (1024.0 KB) ... org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1 tasks (1031.4 KB) is bigger than spark.driver.maxResultSize (1024.0 KB) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1448) ... In this case, run creates a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] (with a TaskResultBlockId for the task's < > and resultSize ) and serializer:Serializer.md#serialize[serializes it]. [[run-result-sent-via-blockmanager]] When maxResultSize is not positive or resultSize is smaller than maxResultSize but greater than executor:Executor.md#maxDirectResultSize[maxDirectResultSize], run creates a TaskResultBlockId for the task's < > and BlockManager.md#putBytes[stores the serialized DirectTaskResult in BlockManager ] (as the TaskResultBlockId with MEMORY_AND_DISK_SER storage level). You should see the following INFO message in the logs: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent via BlockManager) In this case, run creates a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] (with a TaskResultBlockId for the task's < > and resultSize ) and serializer:Serializer.md#serialize[serializes it]. NOTE: The difference between the two above cases is that the result is dropped or stored in BlockManager with MEMORY_AND_DISK_SER storage level. When the two cases above do not hold, you should see the following INFO message in the logs: Finished [taskName] (TID [taskId]). [resultSize] bytes result sent to driver run uses the serialized DirectTaskResult byte buffer as the final serializedResult . NOTE: The final serializedResult is either a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] (possibly with the block stored in BlockManager ) or a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult]. run executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend ] that < > is in TaskState.FINISHED state with the serialized result and removes < > from the owning executor's executor:Executor.md#runningTasks[ runningTasks] registry. NOTE: run uses ExecutorBackend that is specified when TaskRunner < >. NOTE: TaskRunner is Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[Runnable ] and the contract requires that once a TaskRunner has completed execution it must not be restarted. When run catches a exception while executing the task, run acts according to its type (as presented in the following \"run's Exception Cases\" table and the following sections linked from the table). .run's Exception Cases, TaskState and Serialized ByteBuffer [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Exception Type | TaskState | Serialized ByteBuffer | < > | FAILED | TaskFailedReason | < > | KILLED | TaskKilled | < > | KILLED | TaskKilled | < > | FAILED | TaskFailedReason | < > | FAILED | ExceptionFailure |=== run is part of {java-javadoc-url}/java/lang/Runnable.html[java.lang.Runnable] contract. === [[run-FetchFailedException]] FetchFailedException When shuffle:FetchFailedException.md[FetchFailedException] is reported while running a task, run < >. run shuffle:FetchFailedException.md#toTaskFailedReason[requests FetchFailedException for the TaskFailedReason ], serializes it and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized reason). NOTE: ExecutorBackend was specified when < >. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the failure reason. The Serializer was created before run ran the task. === [[run-TaskKilledException]] TaskKilledException When TaskKilledException is reported while running a task, you should see the following INFO message in the logs: Executor killed [taskName] (TID [taskId]), reason: [reason] run then < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). === [[run-InterruptedException]] InterruptedException (with Task Killed) When InterruptedException is reported while running a task, and the task has been killed, you should see the following INFO message in the logs: Executor interrupted and killed [taskName] (TID [taskId]), reason: [killReason] run then < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has been killed] (with < >, TaskState.KILLED , and a serialized TaskKilled object). NOTE: The difference between this InterruptedException and < > is the INFO message in the logs. === [[run-CommitDeniedException]] CommitDeniedException When CommitDeniedException is reported while running a task, run < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and a serialized TaskKilled object). NOTE: The difference between this CommitDeniedException and < > is just the reason being sent to ExecutorBackend . === [[run-Throwable]] Throwable When run catches a Throwable , you should see the following ERROR message in the logs (followed by the exception). Exception in [taskName] (TID [taskId]) run then records the following task metrics (only when < > is available): executor:TaskMetrics.md#setExecutorRunTime[executorRunTime] executor:TaskMetrics.md#setJvmGCTime[jvmGCTime] run then scheduler:Task.md#collectAccumulatorUpdates[collects the latest values of internal and external accumulators] (with taskFailed flag enabled to inform that the collection is for a failed task). Otherwise, when < > is not available, the accumulator collection is empty. run converts the task accumulators to collection of AccumulableInfo , creates a ExceptionFailure (with the accumulators), and serializer:Serializer.md#serialize[serializes them]. NOTE: run uses a closure serializer:Serializer.md[Serializer] to serialize the ExceptionFailure . CAUTION: FIXME Why does run create new ExceptionFailure(t, accUpdates).withAccums(accums) , i.e. accumulators occur twice in the object. run < > and executor:ExecutorBackend.md#statusUpdate[notifies ExecutorBackend that the task has failed] (with < >, TaskState.FAILED , and the serialized ExceptionFailure ). run may also trigger SparkUncaughtExceptionHandler.uncaughtException(t) if this is a fatal error. NOTE: The difference between this most Throwable case and other FAILED cases (i.e. < > and < >) is just the serialized ExceptionFailure vs a reason being sent to ExecutorBackend , respectively. == [[kill]] Killing Task","title":"run(): Unit"},{"location":"executor/TaskRunner/#source-scala_1","text":"kill( interruptThread: Boolean, reason: String): Unit kill marks the TaskRunner as < > and scheduler:Task.md#kill[kills the task] (if available and not < > already). NOTE: kill passes the input interruptThread on to the task itself while killing it. When executed, you should see the following INFO message in the logs: Executor is trying to kill [taskName] (TID [taskId]), reason: [reason] NOTE: < > flag is checked periodically in < > to stop executing the task. Once killed, the task will eventually stop. == [[collectAccumulatorsAndResetStatusOnFailure]] collectAccumulatorsAndResetStatusOnFailure Method","title":"[source, scala]"},{"location":"executor/TaskRunner/#source-scala_2","text":"collectAccumulatorsAndResetStatusOnFailure( taskStartTime: Long): (Seq[AccumulatorV2[_, _]], Seq[AccumulableInfo]) collectAccumulatorsAndResetStatusOnFailure...FIXME collectAccumulatorsAndResetStatusOnFailure is used when TaskRunner is requested to < >. == [[hasFetchFailure]] hasFetchFailure Method","title":"[source, scala]"},{"location":"executor/TaskRunner/#source-scala_3","text":"","title":"[source, scala]"},{"location":"executor/TaskRunner/#hasfetchfailure-boolean","text":"hasFetchFailure...FIXME hasFetchFailure is used when TaskRunner is requested to < >. == [[setTaskFinishedAndClearInterruptStatus]] setTaskFinishedAndClearInterruptStatus Method","title":"hasFetchFailure: Boolean"},{"location":"executor/TaskRunner/#source-scala_4","text":"","title":"[source, scala]"},{"location":"executor/TaskRunner/#settaskfinishedandclearinterruptstatus-unit","text":"setTaskFinishedAndClearInterruptStatus...FIXME setTaskFinishedAndClearInterruptStatus is used when TaskRunner is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.executor.Executor logger to see what happens inside (since TaskRunner is an internal class of Executor). Add the following line to conf/log4j.properties :","title":"setTaskFinishedAndClearInterruptStatus(): Unit"},{"location":"executor/TaskRunner/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"executor/TaskRunner/#log4jloggerorgapachesparkexecutorexecutorall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[finished]][[isFinished]] finished Flag finished flag says whether the < > has finished ( true ) or not ( false ) Default: false Enabled ( true ) after TaskRunner has been requested to < > Used when TaskRunner is requested to < > === [[reasonIfKilled]] reasonIfKilled Reason to < > (and avoid < >) Default: (empty) ( None ) === [[startGCTime]] startGCTime Timestamp Timestamp (which is really the executor:Executor.md#computeTotalGcTime[total amount of time this Executor JVM process has already spent in garbage collection]) that is used to mark the GC \"zero\" time (when < >) and then compute the JVM GC time metric when: TaskRunner is requested to < > and < > Executor is requested to executor:Executor.md#reportHeartBeat[reportHeartBeat] === [[task]] Task Deserialized scheduler:Task.md[task] to execute Used when: TaskRunner is requested to < >, < >, < >, < > Executor is requested to executor:Executor.md#reportHeartBeat[reportHeartBeat] === [[taskId]] Task Id The < > (of the < >) Used when: TaskRunner is requested to < > (to create a memory:TaskMemoryManager.md[TaskMemoryManager] and serialize a IndirectTaskResult for a large task result) and < > the task and for the < > Executor is requested to executor:Executor.md#reportHeartBeat[reportHeartBeat] === [[taskName]] Task Name The < > (of the < >) that is used exclusively for < > purposes when TaskRunner is requested to < > and < > the task === [[threadId]][[getThreadId]] Thread Id Current thread ID Default: -1 Set immediately when TaskRunner is requested to < > and used exclusively when TaskReaper is requested for the thread info of the current thread (aka thread dump )","title":"log4j.logger.org.apache.spark.executor.Executor=ALL"},{"location":"exercises/spark-examples-wordcount-spark-shell/","text":"== WordCount using Spark shell It is like any introductory big data example should somehow demonstrate how to count words in distributed fashion. In the following example you're going to count the words in README.md file that sits in your Spark distribution and save the result under README.count directory. You're going to use spark-shell.md[the Spark shell] for the example. Execute spark-shell . [source,scala] \u00b6 val lines = sc.textFile(\"README.md\") // <1> val words = lines.flatMap(_.split(\"\\s+\")) // <2> val wc = words.map(w => (w, 1)).reduceByKey(_ + _) // <3> wc.saveAsTextFile(\"README.count\") // <4> \u00b6 <1> Read the text file - refer to spark-io.md[Using Input and Output (I/O)]. <2> Split each line into words and flatten the result. <3> Map each word into a pair and count them by word (key). <4> Save the result into text files - one per partition. After you have executed the example, see the contents of the README.count directory: $ ls -lt README.count total 16 -rw-r--r-- 1 jacek staff 0 9 pa\u017a 13:36 _SUCCESS -rw-r--r-- 1 jacek staff 1963 9 pa\u017a 13:36 part-00000 -rw-r--r-- 1 jacek staff 1663 9 pa\u017a 13:36 part-00001 The files part-0000x contain the pairs of word and the count. $ cat README.count/part-00000 (package,1) (this,1) (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1) (Because,1) (Python,2) (cluster.,1) (its,1) ([run,1) ... === Further (self-)development Please read the questions and give answers first before looking at the link given. Why are there two files under the directory? How could you have only one? How to filter out words by name? How to count words? Please refer to the chapter spark-rdd-partitions.md[Partitions] to find some of the answers.","title":"WordCount using Spark shell"},{"location":"exercises/spark-examples-wordcount-spark-shell/#sourcescala","text":"val lines = sc.textFile(\"README.md\") // <1> val words = lines.flatMap(_.split(\"\\s+\")) // <2> val wc = words.map(w => (w, 1)).reduceByKey(_ + _) // <3>","title":"[source,scala]"},{"location":"exercises/spark-examples-wordcount-spark-shell/#wcsaveastextfilereadmecount-4","text":"<1> Read the text file - refer to spark-io.md[Using Input and Output (I/O)]. <2> Split each line into words and flatten the result. <3> Map each word into a pair and count them by word (key). <4> Save the result into text files - one per partition. After you have executed the example, see the contents of the README.count directory: $ ls -lt README.count total 16 -rw-r--r-- 1 jacek staff 0 9 pa\u017a 13:36 _SUCCESS -rw-r--r-- 1 jacek staff 1963 9 pa\u017a 13:36 part-00000 -rw-r--r-- 1 jacek staff 1663 9 pa\u017a 13:36 part-00001 The files part-0000x contain the pairs of word and the count. $ cat README.count/part-00000 (package,1) (this,1) (Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1) (Because,1) (Python,2) (cluster.,1) (its,1) ([run,1) ... === Further (self-)development Please read the questions and give answers first before looking at the link given. Why are there two files under the directory? How could you have only one? How to filter out words by name? How to count words? Please refer to the chapter spark-rdd-partitions.md[Partitions] to find some of the answers.","title":"wc.saveAsTextFile(\"README.count\")                  // &lt;4&gt;"},{"location":"exercises/spark-exercise-custom-rdd/","text":"== Developing Custom RDD CAUTION: FIXME","title":"Developing Custom RDD"},{"location":"exercises/spark-exercise-custom-scheduler-listener/","text":"== Exercise: Developing Custom SparkListener to monitor DAGScheduler in Scala The example shows how to develop a custom Spark Listener. You should read ROOT:SparkListener.md[] first to understand the motivation for the example. === Requirements https://www.jetbrains.com/idea/[IntelliJ IDEA] (or eventually http://www.scala-sbt.org/[sbt ] alone if you're adventurous). Access to Internet to download Apache Spark's dependencies. === Setting up Scala project using IntelliJ IDEA Create a new project custom-spark-listener . Add the following line to build.sbt (the main configuration file for the sbt project) that adds the dependency on Apache Spark. libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.1\" build.sbt should look as follows: [source, scala] \u00b6 name := \"custom-spark-listener\" organization := \"pl.jaceklaskowski.spark\" version := \"1.0\" scalaVersion := \"2.11.8\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.1\" \u00b6 === Custom Listener - pl.jaceklaskowski.spark.CustomSparkListener Create a Scala class -- CustomSparkListener -- for your custom SparkListener . It should be under src/main/scala directory (create one if it does not exist). The aim of the class is to intercept scheduler events about jobs being started and tasks completed. [source,scala] \u00b6 package pl.jaceklaskowski.spark import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListener, SparkListenerJobStart} class CustomSparkListener extends SparkListener { override def onJobStart(jobStart: SparkListenerJobStart) { println(s\"Job started with ${jobStart.stageInfos.size} stages: $jobStart\") } override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { println(s\"Stage ${stageCompleted.stageInfo.stageId} completed with ${stageCompleted.stageInfo.numTasks} tasks.\") } } === Creating deployable package Package the custom Spark listener. Execute sbt package command in the custom-spark-listener project's main directory. $ sbt package [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins [info] Loading project definition from /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project/}custom-spark-listener-build... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to custom-spark-listener (in build file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/) [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/}custom-spark-listener... [info] Resolving jline#jline;2.12.1 ... [info] Done updating. [info] Compiling 1 Scala source to /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/classes... [info] Packaging /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/custom-spark-listener_2.11-1.0.jar ... [info] Done packaging. [success] Total time: 8 s, completed Oct 27, 2016 11:23:50 AM You should find the result jar file with the custom scheduler listener ready under target/scala-2.11 directory, e.g. target/scala-2.11/custom-spark-listener_2.11-1.0.jar . === Activating Custom Listener in Spark shell Start ../spark-shell.md[spark-shell] with additional configurations for the extra custom listener and the jar that includes the class. $ spark-shell \\ --conf spark.logConf=true \\ --conf spark.extraListeners=pl.jaceklaskowski.spark.CustomSparkListener \\ --driver-class-path target/scala-2.11/custom-spark-listener_2.11-1.0.jar Create a ../spark-sql-Dataset.md#implicits[Dataset] and execute an action like show to start a job as follows: scala> spark.read.text(\"README.md\").count [CustomSparkListener] Job started with 2 stages: SparkListenerJobStart(1,1473946006715,WrappedArray(org.apache.spark.scheduler.StageInfo@71515592, org.apache.spark.scheduler.StageInfo@6852819d),{spark.rdd.scope.noOverride=true, spark.rdd.scope={\"id\":\"14\",\"name\":\"collect\"}, spark.sql.execution.id=2}) [CustomSparkListener] Stage 1 completed with 1 tasks. [CustomSparkListener] Stage 2 completed with 1 tasks. res0: Long = 7 The lines with [CustomSparkListener] came from your custom Spark listener. Congratulations! The exercise's over. === BONUS Activating Custom Listener in Spark Application TIP: Read ROOT:SparkContext.md#addSparkListener[Registering SparkListener]. === Questions What are the pros and cons of using the command line version vs inside a Spark application?","title":"Developing Custom SparkListener to monitor DAGScheduler in Scala"},{"location":"exercises/spark-exercise-custom-scheduler-listener/#source-scala","text":"name := \"custom-spark-listener\" organization := \"pl.jaceklaskowski.spark\" version := \"1.0\" scalaVersion := \"2.11.8\"","title":"[source, scala]"},{"location":"exercises/spark-exercise-custom-scheduler-listener/#librarydependencies-orgapachespark-spark-core-201","text":"=== Custom Listener - pl.jaceklaskowski.spark.CustomSparkListener Create a Scala class -- CustomSparkListener -- for your custom SparkListener . It should be under src/main/scala directory (create one if it does not exist). The aim of the class is to intercept scheduler events about jobs being started and tasks completed.","title":"libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.1\""},{"location":"exercises/spark-exercise-custom-scheduler-listener/#sourcescala","text":"package pl.jaceklaskowski.spark import org.apache.spark.scheduler.{SparkListenerStageCompleted, SparkListener, SparkListenerJobStart} class CustomSparkListener extends SparkListener { override def onJobStart(jobStart: SparkListenerJobStart) { println(s\"Job started with ${jobStart.stageInfos.size} stages: $jobStart\") } override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { println(s\"Stage ${stageCompleted.stageInfo.stageId} completed with ${stageCompleted.stageInfo.numTasks} tasks.\") } } === Creating deployable package Package the custom Spark listener. Execute sbt package command in the custom-spark-listener project's main directory. $ sbt package [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins [info] Loading project definition from /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/project/}custom-spark-listener-build... [info] Resolving org.fusesource.jansi#jansi;1.4 ... [info] Done updating. [info] Set current project to custom-spark-listener (in build file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/) [info] Updating {file:/Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/}custom-spark-listener... [info] Resolving jline#jline;2.12.1 ... [info] Done updating. [info] Compiling 1 Scala source to /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/classes... [info] Packaging /Users/jacek/dev/workshops/spark-workshop/solutions/custom-spark-listener/target/scala-2.11/custom-spark-listener_2.11-1.0.jar ... [info] Done packaging. [success] Total time: 8 s, completed Oct 27, 2016 11:23:50 AM You should find the result jar file with the custom scheduler listener ready under target/scala-2.11 directory, e.g. target/scala-2.11/custom-spark-listener_2.11-1.0.jar . === Activating Custom Listener in Spark shell Start ../spark-shell.md[spark-shell] with additional configurations for the extra custom listener and the jar that includes the class. $ spark-shell \\ --conf spark.logConf=true \\ --conf spark.extraListeners=pl.jaceklaskowski.spark.CustomSparkListener \\ --driver-class-path target/scala-2.11/custom-spark-listener_2.11-1.0.jar Create a ../spark-sql-Dataset.md#implicits[Dataset] and execute an action like show to start a job as follows: scala> spark.read.text(\"README.md\").count [CustomSparkListener] Job started with 2 stages: SparkListenerJobStart(1,1473946006715,WrappedArray(org.apache.spark.scheduler.StageInfo@71515592, org.apache.spark.scheduler.StageInfo@6852819d),{spark.rdd.scope.noOverride=true, spark.rdd.scope={\"id\":\"14\",\"name\":\"collect\"}, spark.sql.execution.id=2}) [CustomSparkListener] Stage 1 completed with 1 tasks. [CustomSparkListener] Stage 2 completed with 1 tasks. res0: Long = 7 The lines with [CustomSparkListener] came from your custom Spark listener. Congratulations! The exercise's over. === BONUS Activating Custom Listener in Spark Application TIP: Read ROOT:SparkContext.md#addSparkListener[Registering SparkListener]. === Questions What are the pros and cons of using the command line version vs inside a Spark application?","title":"[source,scala]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/","text":"== Working with Datasets from JDBC Data Sources (and PostgreSQL) Start spark-shell with the JDBC driver for the database you want to use. In our case, it is PostgreSQL JDBC Driver. NOTE: Download the jar for PostgreSQL JDBC Driver 42.1.1 directly from the http://central.maven.org/maven2/org/postgresql/postgresql/42.1.1/postgresql-42.1.1.jar[Maven repository]. [TIP] \u00b6 Execute the command to have the jar downloaded into ~/.ivy2/jars directory by spark-shell itself: ./bin/spark-shell --packages org.postgresql:postgresql:42.1.1 The entire path to the driver file is then like /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar . You should see the following while spark-shell downloads the driver. Ivy Default Cache set to: /Users/jacek/.ivy2/cache The jars for the packages stored in: /Users/jacek/.ivy2/jars :: loading settings :: url = jar:file:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml org.postgresql#postgresql added as a dependency :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0 confs: [default] found org.postgresql#postgresql;42.1.1 in central downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.1.1/postgresql-42.1.1.jar ... [SUCCESSFUL ] org.postgresql#postgresql;42.1.1!postgresql.jar(bundle) (205ms) :: resolution report :: resolve 1887ms :: artifacts dl 207ms :: modules in use: org.postgresql#postgresql;42.1.1 from central in [default] --------------------------------------------------------------------- | | modules || artifacts | | conf | number| search|dwnlded|evicted|| number|dwnlded| --------------------------------------------------------------------- | default | 1 | 1 | 1 | 0 || 1 | 1 | --------------------------------------------------------------------- :: retrieving :: org.apache.spark#spark-submit-parent confs: [default] 1 artifacts copied, 0 already retrieved (695kB/8ms) \u00b6 Start ./bin/spark-shell with spark-submit.md#driver-class-path[--driver-class-path] command line option and the driver jar. SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell --driver-class-path /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar It will give you the proper setup for accessing PostgreSQL using the JDBC driver. Execute the following to access projects table in sparkdb . [source, scala] \u00b6 // that gives an one-partition Dataset val opts = Map( \"url\" -> \"jdbc:postgresql:sparkdb\", \"dbtable\" -> \"projects\") val df = spark. read. format(\"jdbc\"). options(opts). load NOTE: Use user and password options to specify the credentials if needed. [source, scala] \u00b6 // Note the number of partition (aka numPartitions) scala> df.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=1] [id#0,name#1,website#2] ReadSchema: struct scala> df.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ // use jdbc method with predicates to define partitions import java.util.Properties val df4parts = spark. read. jdbc( url = \"jdbc:postgresql:sparkdb\", table = \"projects\", predicates = Array(\"id=1\", \"id=2\", \"id=3\", \"id=4\"), connectionProperties = new Properties()) // Note the number of partitions (aka numPartitions) scala> df4parts.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=4] [id#16,name#17,website#18] ReadSchema: struct scala> df4parts.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ === Troubleshooting If things can go wrong, they sooner or later go wrong. Here is a list of possible issues and their solutions. ==== java.sql.SQLException: No suitable driver Ensure that the JDBC driver sits on the CLASSPATH. Use spark-submit.md#driver-class-path[--driver-class-path] as described above ( --packages or --jars do not work). scala> val df = spark. | read. | format(\"jdbc\"). | options(opts). | load java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:83) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:158) ... 52 elided === PostgreSQL Setup NOTE: I'm on Mac OS X so YMMV (aka Your Mileage May Vary ). Use the sections to have a properly configured PostgreSQL database. < > < > < > < > < > < > < > ==== [[installation]] Installation Install PostgreSQL as described in...TK CAUTION: This page serves as a cheatsheet for the author so he does not have to search Internet to find the installation steps. $ initdb /usr/local/var/postgres -E utf8 The files belonging to this database system will be owned by user \"jacek\". This user must also own the server process. The database cluster will be initialized with locale \"pl_pl.utf-8\". initdb: could not find suitable text search configuration for locale \"pl_pl.utf-8\" The default text search configuration will be set to \"simple\". Data page checksums are disabled. creating directory /usr/local/var/postgres ... ok creating subdirectories ... ok selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting dynamic shared memory implementation ... posix creating configuration files ... ok creating template1 database in /usr/local/var/postgres/base/1 ... ok initializing pg_authid ... ok initializing dependencies ... ok creating system views ... ok loading system objects' descriptions ... ok creating collations ... ok creating conversions ... ok creating dictionaries ... ok setting privileges on built-in objects ... ok creating information schema ... ok loading PL/pgSQL server-side language ... ok vacuuming database template1 ... ok copying template1 to template0 ... ok copying template1 to postgres ... ok syncing data to disk ... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /usr/local/var/postgres -l logfile start ==== [[starting-database-server]] Starting Database Server NOTE: Consult http://www.postgresql.org/docs/current/static/server-start.html[17.3 . Starting the Database Server] in the official documentation. [TIP] \u00b6 Enable all logs in PostgreSQL to see query statements. log_statement = 'all' Add log_statement = 'all' to /usr/local/var/postgres/postgresql.conf on Mac OS X with PostgreSQL installed using brew . \u00b6 Start the database server using pg_ctl . $ pg_ctl -D /usr/local/var/postgres -l logfile start server starting Alternatively, you can run the database server using postgres . $ postgres -D /usr/local/var/postgres ==== [[creating-database]] Create Database $ createdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-createdb.html[createdb ] in the official documentation. ==== Accessing Database Use psql sparkdb to access the database. $ psql sparkdb psql (9.6.2) Type \"help\" for help. sparkdb=# Execute SELECT version() to know the version of the database server you have connected to. sparkdb=# SELECT version(); version -------------------------------------------------------------------------------------------------------------- PostgreSQL 9.6.2 on x86_64-apple-darwin14.5.0, compiled by Apple LLVM version 7.0.2 (clang-700.1.81), 64-bit (1 row) Use \\h for help and \\q to leave a session. ==== Creating Table Create a table using CREATE TABLE command. CREATE TABLE projects ( id SERIAL PRIMARY KEY, name text, website text ); Insert rows to initialize the table with data. INSERT INTO projects (name, website) VALUES ('Apache Spark', 'http://spark.apache.org'); INSERT INTO projects (name, website) VALUES ('Apache Hive', 'http://hive.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Kafka', 'http://kafka.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Flink', 'http://flink.apache.org'); Execute select * from projects; to ensure that you have the following records in projects table: sparkdb=# select * from projects; id | name | website ----+--------------+------------------------- 1 | Apache Spark | http://spark.apache.org 2 | Apache Hive | http://hive.apache.org 3 | Apache Kafka | http://kafka.apache.org 4 | Apache Flink | http://flink.apache.org (4 rows) ==== Dropping Database $ dropdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-dropdb.html[dropdb ] in the official documentation. ==== Stopping Database Server pg_ctl -D /usr/local/var/postgres stop","title":"Working with Datasets from JDBC Data Sources (and PostgreSQL)"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#tip","text":"Execute the command to have the jar downloaded into ~/.ivy2/jars directory by spark-shell itself: ./bin/spark-shell --packages org.postgresql:postgresql:42.1.1 The entire path to the driver file is then like /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar . You should see the following while spark-shell downloads the driver.","title":"[TIP]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#ivy-default-cache-set-to-usersjacekivy2cache-the-jars-for-the-packages-stored-in-usersjacekivy2jars-loading-settings-url-jarfileusersjacekdevosssparkassemblytargetscala-211jarsivy-240jarorgapacheivycoresettingsivysettingsxml-orgpostgresqlpostgresql-added-as-a-dependency-resolving-dependencies-orgapachesparkspark-submit-parent10-confs-default-found-orgpostgresqlpostgresql4211-in-central-downloading-httpsrepo1mavenorgmaven2orgpostgresqlpostgresql4211postgresql-4211jar-successful-orgpostgresqlpostgresql4211postgresqljarbundle-205ms-resolution-report-resolve-1887ms-artifacts-dl-207ms-modules-in-use-orgpostgresqlpostgresql4211-from-central-in-default-modules-artifacts-conf-number-searchdwnldedevicted-numberdwnlded-default-1-1-1-0-1-1-retrieving-orgapachesparkspark-submit-parent-confs-default-1-artifacts-copied-0-already-retrieved-695kb8ms","text":"Start ./bin/spark-shell with spark-submit.md#driver-class-path[--driver-class-path] command line option and the driver jar. SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell --driver-class-path /Users/jacek/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar It will give you the proper setup for accessing PostgreSQL using the JDBC driver. Execute the following to access projects table in sparkdb .","title":"Ivy Default Cache set to: /Users/jacek/.ivy2/cache\nThe jars for the packages stored in: /Users/jacek/.ivy2/jars\n:: loading settings :: url = jar:file:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\norg.postgresql#postgresql added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n    confs: [default]\n    found org.postgresql#postgresql;42.1.1 in central\ndownloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.1.1/postgresql-42.1.1.jar ...\n    [SUCCESSFUL ] org.postgresql#postgresql;42.1.1!postgresql.jar(bundle) (205ms)\n:: resolution report :: resolve 1887ms :: artifacts dl 207ms\n    :: modules in use:\n    org.postgresql#postgresql;42.1.1 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n    confs: [default]\n    1 artifacts copied, 0 already retrieved (695kB/8ms)\n"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#source-scala","text":"// that gives an one-partition Dataset val opts = Map( \"url\" -> \"jdbc:postgresql:sparkdb\", \"dbtable\" -> \"projects\") val df = spark. read. format(\"jdbc\"). options(opts). load NOTE: Use user and password options to specify the credentials if needed.","title":"[source, scala]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#source-scala_1","text":"// Note the number of partition (aka numPartitions) scala> df.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=1] [id#0,name#1,website#2] ReadSchema: struct scala> df.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ // use jdbc method with predicates to define partitions import java.util.Properties val df4parts = spark. read. jdbc( url = \"jdbc:postgresql:sparkdb\", table = \"projects\", predicates = Array(\"id=1\", \"id=2\", \"id=3\", \"id=4\"), connectionProperties = new Properties()) // Note the number of partitions (aka numPartitions) scala> df4parts.explain == Physical Plan == *Scan JDBCRelation(projects) [numPartitions=4] [id#16,name#17,website#18] ReadSchema: struct scala> df4parts.show(truncate = false) +---+------------+-----------------------+ |id |name |website | +---+------------+-----------------------+ |1 |Apache Spark| http://spark.apache.org | |2 |Apache Hive | http://hive.apache.org | |3 |Apache Kafka| http://kafka.apache.org | |4 |Apache Flink| http://flink.apache.org | +---+------------+-----------------------+ === Troubleshooting If things can go wrong, they sooner or later go wrong. Here is a list of possible issues and their solutions. ==== java.sql.SQLException: No suitable driver Ensure that the JDBC driver sits on the CLASSPATH. Use spark-submit.md#driver-class-path[--driver-class-path] as described above ( --packages or --jars do not work). scala> val df = spark. | read. | format(\"jdbc\"). | options(opts). | load java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:84) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:83) at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:301) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:158) ... 52 elided === PostgreSQL Setup NOTE: I'm on Mac OS X so YMMV (aka Your Mileage May Vary ). Use the sections to have a properly configured PostgreSQL database. < > < > < > < > < > < > < > ==== [[installation]] Installation Install PostgreSQL as described in...TK CAUTION: This page serves as a cheatsheet for the author so he does not have to search Internet to find the installation steps. $ initdb /usr/local/var/postgres -E utf8 The files belonging to this database system will be owned by user \"jacek\". This user must also own the server process. The database cluster will be initialized with locale \"pl_pl.utf-8\". initdb: could not find suitable text search configuration for locale \"pl_pl.utf-8\" The default text search configuration will be set to \"simple\". Data page checksums are disabled. creating directory /usr/local/var/postgres ... ok creating subdirectories ... ok selecting default max_connections ... 100 selecting default shared_buffers ... 128MB selecting dynamic shared memory implementation ... posix creating configuration files ... ok creating template1 database in /usr/local/var/postgres/base/1 ... ok initializing pg_authid ... ok initializing dependencies ... ok creating system views ... ok loading system objects' descriptions ... ok creating collations ... ok creating conversions ... ok creating dictionaries ... ok setting privileges on built-in objects ... ok creating information schema ... ok loading PL/pgSQL server-side language ... ok vacuuming database template1 ... ok copying template1 to template0 ... ok copying template1 to postgres ... ok syncing data to disk ... ok WARNING: enabling \"trust\" authentication for local connections You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb. Success. You can now start the database server using: pg_ctl -D /usr/local/var/postgres -l logfile start ==== [[starting-database-server]] Starting Database Server NOTE: Consult http://www.postgresql.org/docs/current/static/server-start.html[17.3 . Starting the Database Server] in the official documentation.","title":"[source, scala]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#tip_1","text":"Enable all logs in PostgreSQL to see query statements. log_statement = 'all'","title":"[TIP]"},{"location":"exercises/spark-exercise-dataframe-jdbc-postgresql/#add-log_statement-all-to-usrlocalvarpostgrespostgresqlconf-on-mac-os-x-with-postgresql-installed-using-brew","text":"Start the database server using pg_ctl . $ pg_ctl -D /usr/local/var/postgres -l logfile start server starting Alternatively, you can run the database server using postgres . $ postgres -D /usr/local/var/postgres ==== [[creating-database]] Create Database $ createdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-createdb.html[createdb ] in the official documentation. ==== Accessing Database Use psql sparkdb to access the database. $ psql sparkdb psql (9.6.2) Type \"help\" for help. sparkdb=# Execute SELECT version() to know the version of the database server you have connected to. sparkdb=# SELECT version(); version -------------------------------------------------------------------------------------------------------------- PostgreSQL 9.6.2 on x86_64-apple-darwin14.5.0, compiled by Apple LLVM version 7.0.2 (clang-700.1.81), 64-bit (1 row) Use \\h for help and \\q to leave a session. ==== Creating Table Create a table using CREATE TABLE command. CREATE TABLE projects ( id SERIAL PRIMARY KEY, name text, website text ); Insert rows to initialize the table with data. INSERT INTO projects (name, website) VALUES ('Apache Spark', 'http://spark.apache.org'); INSERT INTO projects (name, website) VALUES ('Apache Hive', 'http://hive.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Kafka', 'http://kafka.apache.org'); INSERT INTO projects VALUES (DEFAULT, 'Apache Flink', 'http://flink.apache.org'); Execute select * from projects; to ensure that you have the following records in projects table: sparkdb=# select * from projects; id | name | website ----+--------------+------------------------- 1 | Apache Spark | http://spark.apache.org 2 | Apache Hive | http://hive.apache.org 3 | Apache Kafka | http://kafka.apache.org 4 | Apache Flink | http://flink.apache.org (4 rows) ==== Dropping Database $ dropdb sparkdb TIP: Consult http://www.postgresql.org/docs/current/static/app-dropdb.html[dropdb ] in the official documentation. ==== Stopping Database Server pg_ctl -D /usr/local/var/postgres stop","title":"Add log_statement = 'all' to /usr/local/var/postgres/postgresql.conf on Mac OS X with PostgreSQL installed using brew."},{"location":"exercises/spark-exercise-failing-stage/","text":"== Exercise: Causing Stage to Fail The example shows how Spark re-executes a stage in case of stage failure. === Recipe Start a Spark cluster, e.g. 1-node Hadoop YARN. start-yarn.sh // 2-stage job -- it _appears_ that a stage can be failed only when there is a shuffle sc.parallelize(0 to 3e3.toInt, 2).map(n => (n % 2, n)).groupByKey.count Use 2 executors at least so you can kill one and keep the application up and running (on one executor). YARN_CONF_DIR=hadoop-conf ./bin/spark-shell --master yarn \\ -c spark.shuffle.service.enabled=true \\ --num-executors 2","title":"Causing Stage to Fail"},{"location":"exercises/spark-exercise-pairrddfunctions-oneliners/","text":"== Exercise: One-liners using PairRDDFunctions This is a set of one-liners to give you a entry point into using rdd:PairRDDFunctions.md[PairRDDFunctions]. === Exercise How would you go about solving a requirement to pair elements of the same key and creating a new RDD out of the matched values? [source, scala] \u00b6 val users = Seq((1, \"user1\"), (1, \"user2\"), (2, \"user1\"), (2, \"user3\"), (3,\"user2\"), (3,\"user4\"), (3,\"user1\")) // Input RDD val us = sc.parallelize(users) // ...your code here // Desired output Seq(\"user1\",\"user2\"),(\"user1\",\"user3\"),(\"user1\",\"user4\"),(\"user2\",\"user4\"))","title":"One-liners using PairRDDFunctions"},{"location":"exercises/spark-exercise-pairrddfunctions-oneliners/#source-scala","text":"val users = Seq((1, \"user1\"), (1, \"user2\"), (2, \"user1\"), (2, \"user3\"), (3,\"user2\"), (3,\"user4\"), (3,\"user1\")) // Input RDD val us = sc.parallelize(users) // ...your code here // Desired output Seq(\"user1\",\"user2\"),(\"user1\",\"user3\"),(\"user1\",\"user4\"),(\"user2\",\"user4\"))","title":"[source, scala]"},{"location":"exercises/spark-exercise-standalone-master-ha/","text":"== Spark Standalone - Using ZooKeeper for High-Availability of Master TIP: Read ../spark-standalone-Master.md#recovery-mode[Recovery Mode] to know the theory. You're going to start two standalone Masters. You'll need 4 terminals (adjust addresses as needed): Start ZooKeeper. Create a configuration file ha.conf with the content as follows: spark.deploy.recoveryMode=ZOOKEEPER spark.deploy.zookeeper.url=<zookeeper_host>:2181 spark.deploy.zookeeper.dir=/spark Start the first standalone Master. ./sbin/start-master.sh -h localhost -p 7077 --webui-port 8080 --properties-file ha.conf Start the second standalone Master. NOTE: It is not possible to start another instance of standalone Master on the same machine using ./sbin/start-master.sh . The reason is that the script assumes one instance per machine only. We're going to change the script to make it possible. $ cp ./sbin/start-master{,-2}.sh $ grep \"CLASS 1\" ./sbin/start-master-2.sh \"$\\{SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS 1 \\ $ sed -i -e 's/CLASS 1/CLASS 2/' sbin/start-master-2.sh $ grep \"CLASS 1\" ./sbin/start-master-2.sh $ grep \"CLASS 2\" ./sbin/start-master-2.sh \"$\\{SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS 2 \\ $ ./sbin/start-master-2.sh -h localhost -p 17077 --webui-port 18080 --properties-file ha.conf You can check how many instances you're currently running using jps command as follows: $ jps -lm 5024 sun.tools.jps.Jps -lm 4994 org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080 -h localhost -p 17077 --webui-port 18080 --properties-file ha.conf 4808 org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080 -h localhost -p 7077 --webui-port 8080 --properties-file ha.conf 4778 org.apache.zookeeper.server.quorum.QuorumPeerMain config/zookeeper.properties Start a standalone Worker. ./sbin/start-slave.sh spark://localhost:7077,localhost:17077 Start Spark shell. ./bin/spark-shell --master spark://localhost:7077,localhost:17077 Wait till the Spark shell connects to an active standalone Master. Find out which standalone Master is active (there can only be one). Kill it. Observe how the other standalone Master takes over and lets the Spark shell register with itself. Check out the master's UI. Optionally, kill the worker, make sure it goes away instantly in the active master's logs.","title":"Spark Standalone - Using ZooKeeper for High-Availability of Master"},{"location":"exercises/spark-exercise-take-multiple-jobs/","text":"== Exercise: Learning Jobs and Partitions Using take Action The exercise aims for introducing take action and using spark-shell and web UI. It should introduce you to the concepts of partitions and jobs. The following snippet creates an RDD of 16 elements with 16 partitions. scala> val r1 = sc.parallelize(0 to 15, 16) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at <console>:18 scala> r1.partitions.size res63: Int = 16 scala> r1.foreachPartition(it => println(\">>> partition size: \" + it.size)) ... >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 ... // the machine has 8 cores ... // so first 8 tasks get executed immediately ... // with the others after a core is free to take on new tasks. >>> partition size: 1 ... >>> partition size: 1 ... >>> partition size: 1 ... >>> partition size: 1 >>> partition size: 1 ... >>> partition size: 1 >>> partition size: 1 >>> partition size: 1 All 16 partitions have one element. When you execute r1.take(1) only one job gets run since it is enough to compute one task on one partition. CAUTION: FIXME Snapshot from web UI - note the number of tasks However, when you execute r1.take(2) two jobs get run as the implementation assumes one job with one partition, and if the elements didn't total to the number of elements requested in take , quadruple the partitions to work on in the following jobs. CAUTION: FIXME Snapshot from web UI - note the number of tasks Can you guess how many jobs are run for r1.take(15) ? How many tasks per job? CAUTION: FIXME Snapshot from web UI - note the number of tasks Answer: 3.","title":"Learning Jobs and Partitions Using take Action"},{"location":"exercises/spark-first-app/","text":"== Your first Spark application (using Scala and sbt) This page gives you the exact steps to develop and run a complete Spark application using http://www.scala-lang.org/[Scala ] programming language and http://www.scala-sbt.org/[sbt ] as the build tool. [TIP] Refer to Quick Start's http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/quick-start.html#self-contained-applications[Self-Contained Applications] in the official documentation. The sample application called SparkMe App is...FIXME === Overview You're going to use http://www.scala-sbt.org/[sbt ] as the project build tool. It uses build.sbt for the project's description as well as the dependencies, i.e. the version of Apache Spark and others. The application's main code is under src/main/scala directory, in SparkMeApp.scala file. With the files in a directory, executing sbt package results in a package that can be deployed onto a Spark cluster using spark-submit . In this example, you're going to use Spark's local/spark-local.md[local mode]. === Project's build - build.sbt Any Scala project managed by sbt uses build.sbt as the central place for configuration, including project dependencies denoted as libraryDependencies . build.sbt name := \"SparkMe Project\" version := \"1.0\" organization := \"pl.japila\" scalaVersion := \"2.11.7\" libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.6.0-SNAPSHOT\" // <1> resolvers += Resolver.mavenLocal <1> Use the development version of Spark 1.6.0-SNAPSHOT === SparkMe Application The application uses a single command-line parameter (as args(0) ) that is the file to process. The file is read and the number of lines printed out. package pl.japila.spark import org.apache.spark.{SparkContext, SparkConf} object SparkMeApp { def main(args: Array[String]) { val conf = new SparkConf().setAppName(\"SparkMe Application\") val sc = new SparkContext(conf) val fileName = args(0) val lines = sc.textFile(fileName).cache val c = lines.count println(s\"There are $c lines in $fileName\") } } === sbt version - project/build.properties sbt (launcher) uses project/build.properties file to set (the real) sbt up sbt.version=0.13.9 TIP: With the file the build is more predictable as the version of sbt doesn't depend on the sbt launcher. === Packaging Application Execute sbt package to package the application. \u279c sparkme-app sbt package [info] Loading global plugins from /Users/jacek/.sbt/0.13/plugins [info] Loading project definition from /Users/jacek/dev/sandbox/sparkme-app/project [info] Set current project to SparkMe Project (in build file:/Users/jacek/dev/sandbox/sparkme-app/) [info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/sparkme-app/target/scala-2.11/classes... [info] Packaging /Users/jacek/dev/sandbox/sparkme-app/target/scala-2.11/sparkme-project_2.11-1.0.jar ... [info] Done packaging. [success] Total time: 3 s, completed Sep 23, 2015 12:47:52 AM The application uses only classes that comes with Spark so package is enough. In target/scala-2.11/sparkme-project_2.11-1.0.jar there is the final application ready for deployment. === Submitting Application to Spark (local) NOTE: The application is going to be deployed to local[*] . Change it to whatever cluster you have available (refer to spark-cluster.md[Running Spark in cluster]). spark-submit the SparkMe application and specify the file to process (as it is the only and required input parameter to the application), e.g. build.sbt of the project. NOTE: build.sbt is sbt's build definition and is only used as an input file for demonstration purposes. Any file is going to work fine. \u279c sparkme-app ~/dev/oss/spark/bin/spark-submit --master \"local[*]\" --class pl.japila.spark.SparkMeApp target/scala-2.11/sparkme-project_2.11-1.0.jar build.sbt Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties To adjust logging level use sc.setLogLevel(\"INFO\") 15/09/23 01:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 15/09/23 01:06:04 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set. There are 8 lines in build.sbt NOTE: Disregard the two above WARN log messages. You're done. Sincere congratulations!","title":"Your first complete Spark application (using Scala and sbt)"},{"location":"exercises/spark-hello-world-using-spark-shell/","text":"== Exercise: Spark's Hello World using Spark shell and Scala Run Spark shell and count the number of words in a file using MapReduce pattern. Use sc.textFile to read the file into memory Use RDD.flatMap for a mapper step Use reduceByKey for a reducer step","title":"Spark's Hello World using Spark shell and Scala"},{"location":"exercises/spark-sql-hive-orc-example/","text":"== Using Spark SQL to update data in Hive using ORC files The example has showed up on Spark's users mailing list. [CAUTION] \u00b6 FIXME Offer a complete working solution in Scala FIXME Load ORC files into dataframe ** val df = hiveContext.read.format(\"orc\").load(to/path) ==== Solution was to use Hive in ORC format with partitions: A table in Hive stored as an ORC file (using partitioning) Using SQLContext.sql to insert data into the table Using SQLContext.sql to periodically run ALTER TABLE...CONCATENATE to merge your many small files into larger files optimized for your HDFS block size ** Since the CONCATENATE command operates on files in place it is transparent to any downstream processing Hive solution is just to concatenate the files ** it does not alter or change records. ** it's possible to update data in Hive using ORC format ** With transactional tables in Hive together with insert, update, delete, it does the \"concatenate \" for you automatically in regularly intervals. Currently this works only with tables in orc.format (stored as orc) ** Alternatively, use Hbase with Phoenix as the SQL layer on top ** Hive was originally not designed for updates, because it was.purely warehouse focused, the most recent one can do updates, deletes etc in a transactional way. Criteria: spark-streaming/spark-streaming.md[Spark Streaming] jobs are receiving a lot of small events (avg 10kb) Events are stored to HDFS, e.g. for Pig jobs There are a lot of small files in HDFS (several millions)","title":"Using Spark SQL to update data in Hive using ORC files"},{"location":"exercises/spark-sql-hive-orc-example/#caution","text":"FIXME Offer a complete working solution in Scala FIXME Load ORC files into dataframe ** val df = hiveContext.read.format(\"orc\").load(to/path) ==== Solution was to use Hive in ORC format with partitions: A table in Hive stored as an ORC file (using partitioning) Using SQLContext.sql to insert data into the table Using SQLContext.sql to periodically run ALTER TABLE...CONCATENATE to merge your many small files into larger files optimized for your HDFS block size ** Since the CONCATENATE command operates on files in place it is transparent to any downstream processing Hive solution is just to concatenate the files ** it does not alter or change records. ** it's possible to update data in Hive using ORC format ** With transactional tables in Hive together with insert, update, delete, it does the \"concatenate \" for you automatically in regularly intervals. Currently this works only with tables in orc.format (stored as orc) ** Alternatively, use Hbase with Phoenix as the SQL layer on top ** Hive was originally not designed for updates, because it was.purely warehouse focused, the most recent one can do updates, deletes etc in a transactional way. Criteria: spark-streaming/spark-streaming.md[Spark Streaming] jobs are receiving a lot of small events (avg 10kb) Events are stored to HDFS, e.g. for Pig jobs There are a lot of small files in HDFS (several millions)","title":"[CAUTION]"},{"location":"history-server/","text":"= Spark History Server Spark History Server is the web UI of Spark applications with event log collection enabled (based on ROOT:configuration-properties.md#spark.eventLog.enabled[spark.eventLog.enabled] configuration property). .History Server's web UI image::spark-history-server-webui.png[align=\"center\"] Spark History Server is an extension of Spark's webui:index.md[web UI]. Spark History Server can be started using < > and stopped using < > shell scripts. Spark History Server supports custom spark-history-server:configuration-properties.md#HistoryServer[configuration properties] that can be defined using --properties-file [propertiesFile] command-line option. The properties file can have any valid spark. -prefixed Spark property. [source,plaintext] \u00b6 $ ./sbin/start-history-server.sh --properties-file history.properties \u00b6 If not specified explicitly, Spark History Server uses the default configuration file, i.e. ROOT:spark-properties.md#spark-defaults-conf[spark-defaults.conf]. Spark History Server can replay events from event log files recorded by EventLoggingListener.md[EventLoggingListener]. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.deploy.history logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history=INFO Refer to ROOT:spark-logging.md[Logging]. \u00b6 == [[start_history_server_sh]] Starting History Server -- start-history-server.sh Shell Script $SPARK_HOME/sbin/start-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to start a Spark History Server instance. [source,plaintext] \u00b6 $ ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to .../spark/logs/spark-jacek-org.apache.spark.deploy.history.HistoryServer-1-japila.out Internally, start-history-server.sh script starts HistoryServer.md[org.apache.spark.deploy.history.HistoryServer] standalone application (using spark-daemon.sh shell script). [source,plaintext] \u00b6 $ ./bin/spark-class org.apache.spark.deploy.history.HistoryServer \u00b6 TIP: Using the more explicit approach with spark-class to start Spark History Server could be easier to trace execution by seeing the logs printed out to the standard output and hence terminal directly. When started, start-history-server.sh prints out the following INFO message to the logs: Started daemon with process name: [processName] start-history-server.sh registers signal handlers (using SignalUtils ) for TERM , HUP , INT to log their execution: RECEIVED SIGNAL [signal] start-history-server.sh inits security if enabled (based on ROOT:configuration-properties.md#spark.history.kerberos.enabled[spark.history.kerberos.enabled] configuration property). CAUTION: FIXME Describe initSecurity start-history-server.sh creates a SecurityManager . start-history-server.sh creates a ApplicationHistoryProvider.md[ApplicationHistoryProvider] (based on ROOT:configuration-properties.md#spark.history.provider[spark.history.provider] configuration property). In the end, start-history-server.sh creates a HistoryServer.md[HistoryServer] and requests it to bind to the port (based on ROOT:configuration-properties.md#spark.history.ui.port[spark.history.ui.port] configuration property). [TIP] \u00b6 The host's IP can be specified using SPARK_LOCAL_IP environment variable (defaults to 0.0.0.0 ). \u00b6 start-history-server.sh prints out the following INFO message to the logs: Bound HistoryServer to [host], and started at [webUrl] start-history-server.sh registers a shutdown hook to call stop on the HistoryServer instance. TIP: Use < > shell script to to stop a running History Server. == [[stop_history_server_sh]] Stopping History Server -- stop-history-server.sh Shell Script $SPARK_HOME/sbin/stop-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to stop a running instance of Spark History Server. [source,plaintext] \u00b6 $ ./sbin/stop-history-server.sh stopping org.apache.spark.deploy.history.HistoryServer","title":"Spark History Server"},{"location":"history-server/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"history-server/#sbinstart-history-serversh-properties-file-historyproperties","text":"If not specified explicitly, Spark History Server uses the default configuration file, i.e. ROOT:spark-properties.md#spark-defaults-conf[spark-defaults.conf]. Spark History Server can replay events from event log files recorded by EventLoggingListener.md[EventLoggingListener]. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.deploy.history logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history=INFO","title":"$ ./sbin/start-history-server.sh --properties-file history.properties"},{"location":"history-server/#refer-to-rootspark-loggingmdlogging","text":"== [[start_history_server_sh]] Starting History Server -- start-history-server.sh Shell Script $SPARK_HOME/sbin/start-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to start a Spark History Server instance.","title":"Refer to ROOT:spark-logging.md[Logging]."},{"location":"history-server/#sourceplaintext_1","text":"$ ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to .../spark/logs/spark-jacek-org.apache.spark.deploy.history.HistoryServer-1-japila.out Internally, start-history-server.sh script starts HistoryServer.md[org.apache.spark.deploy.history.HistoryServer] standalone application (using spark-daemon.sh shell script).","title":"[source,plaintext]"},{"location":"history-server/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"history-server/#binspark-class-orgapachesparkdeployhistoryhistoryserver","text":"TIP: Using the more explicit approach with spark-class to start Spark History Server could be easier to trace execution by seeing the logs printed out to the standard output and hence terminal directly. When started, start-history-server.sh prints out the following INFO message to the logs: Started daemon with process name: [processName] start-history-server.sh registers signal handlers (using SignalUtils ) for TERM , HUP , INT to log their execution: RECEIVED SIGNAL [signal] start-history-server.sh inits security if enabled (based on ROOT:configuration-properties.md#spark.history.kerberos.enabled[spark.history.kerberos.enabled] configuration property). CAUTION: FIXME Describe initSecurity start-history-server.sh creates a SecurityManager . start-history-server.sh creates a ApplicationHistoryProvider.md[ApplicationHistoryProvider] (based on ROOT:configuration-properties.md#spark.history.provider[spark.history.provider] configuration property). In the end, start-history-server.sh creates a HistoryServer.md[HistoryServer] and requests it to bind to the port (based on ROOT:configuration-properties.md#spark.history.ui.port[spark.history.ui.port] configuration property).","title":"$ ./bin/spark-class org.apache.spark.deploy.history.HistoryServer"},{"location":"history-server/#tip","text":"","title":"[TIP]"},{"location":"history-server/#the-hosts-ip-can-be-specified-using-spark_local_ip-environment-variable-defaults-to-0000","text":"start-history-server.sh prints out the following INFO message to the logs: Bound HistoryServer to [host], and started at [webUrl] start-history-server.sh registers a shutdown hook to call stop on the HistoryServer instance. TIP: Use < > shell script to to stop a running History Server. == [[stop_history_server_sh]] Stopping History Server -- stop-history-server.sh Shell Script $SPARK_HOME/sbin/stop-history-server.sh shell script (where SPARK_HOME is the directory of your Spark installation) is used to stop a running instance of Spark History Server.","title":"The host's IP can be specified using SPARK_LOCAL_IP environment variable (defaults to 0.0.0.0)."},{"location":"history-server/#sourceplaintext_3","text":"$ ./sbin/stop-history-server.sh stopping org.apache.spark.deploy.history.HistoryServer","title":"[source,plaintext]"},{"location":"history-server/ApplicationCache/","text":"== [[ApplicationCache]] ApplicationCache ApplicationCache is...FIXME ApplicationCache is < > exclusively when HistoryServer is HistoryServer.md#appCache[created]. ApplicationCache uses https://github.com/google/guava/wiki/Release14[Google Guava 14.0.1] library for the internal < >. [[internal-registries]] .ApplicationCache's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appLoader | [[appLoader]] Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html[CacheLoader ] with a custom ++ https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html#load(K)++[load ] which is simply < >. Used when...FIXME | removalListener | [[removalListener]] | appCache a| [[appCache]] Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/LoadingCache.html[LoadingCache ] of CacheKey keys and CacheEntry entries Used when ApplicationCache is requested for the following: < > given appId and attemptId IDs FIXME (other uses) | metrics | [[metrics]] |=== === [[creating-instance]] Creating ApplicationCache Instance ApplicationCache takes the following when created: [[operations]] ApplicationCacheOperations.md[ApplicationCacheOperations] [[retainedApplications]] retainedApplications [[clock]] Clock ApplicationCache initializes the < >. === [[loadApplicationEntry]] loadApplicationEntry Internal Method [source, scala] \u00b6 loadApplicationEntry(appId: String, attemptId: Option[String]): CacheEntry \u00b6 loadApplicationEntry ...FIXME NOTE: loadApplicationEntry is used exclusively when ApplicationCache is requested to < >. === [[load]] Loading Cached Spark Application UI -- load Method [source, scala] \u00b6 load(key: CacheKey): CacheEntry \u00b6 NOTE: load is part of Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html[CacheLoader ] to retrieve a CacheEntry , based on a CacheKey , for < >. load simply relays to < > with the appId and attemptId of the input CacheKey . === [[get]] Requesting Cached UI of Spark Application (CacheEntry) -- get Method [source, scala] \u00b6 get(appId: String, attemptId: Option[String] = None): CacheEntry \u00b6 get ...FIXME NOTE: get is used exclusively when ApplicationCache is requested to < >. === [[withSparkUI]] Executing Closure While Holding Application's UI Read Lock -- withSparkUI Method [source, scala] \u00b6 withSparkUI T (fn: SparkUI => T): T \u00b6 withSparkUI ...FIXME NOTE: withSparkUI is used when HistoryServer is requested to HistoryServer.md#withSparkUI[withSparkUI] and HistoryServer.md#loadAppUi[loadAppUi].","title":"ApplicationCache"},{"location":"history-server/ApplicationCache/#source-scala","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#loadapplicationentryappid-string-attemptid-optionstring-cacheentry","text":"loadApplicationEntry ...FIXME NOTE: loadApplicationEntry is used exclusively when ApplicationCache is requested to < >. === [[load]] Loading Cached Spark Application UI -- load Method","title":"loadApplicationEntry(appId: String, attemptId: Option[String]): CacheEntry"},{"location":"history-server/ApplicationCache/#source-scala_1","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#loadkey-cachekey-cacheentry","text":"NOTE: load is part of Google Guava's https://google.github.io/guava/releases/14.0/api/docs/com/google/common/cache/CacheLoader.html[CacheLoader ] to retrieve a CacheEntry , based on a CacheKey , for < >. load simply relays to < > with the appId and attemptId of the input CacheKey . === [[get]] Requesting Cached UI of Spark Application (CacheEntry) -- get Method","title":"load(key: CacheKey): CacheEntry"},{"location":"history-server/ApplicationCache/#source-scala_2","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#getappid-string-attemptid-optionstring-none-cacheentry","text":"get ...FIXME NOTE: get is used exclusively when ApplicationCache is requested to < >. === [[withSparkUI]] Executing Closure While Holding Application's UI Read Lock -- withSparkUI Method","title":"get(appId: String, attemptId: Option[String] = None): CacheEntry"},{"location":"history-server/ApplicationCache/#source-scala_3","text":"","title":"[source, scala]"},{"location":"history-server/ApplicationCache/#withsparkuitfn-sparkui-t-t","text":"withSparkUI ...FIXME NOTE: withSparkUI is used when HistoryServer is requested to HistoryServer.md#withSparkUI[withSparkUI] and HistoryServer.md#loadAppUi[loadAppUi].","title":"withSparkUIT(fn: SparkUI =&gt; T): T"},{"location":"history-server/ApplicationCacheOperations/","text":"== [[ApplicationCacheOperations]] ApplicationCacheOperations ApplicationCacheOperations is the < > of...FIXME [[contract]] [source, scala] package org.apache.spark.deploy.history trait ApplicationCacheOperations { // only required methods that have no implementation // the others follow def getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI] def attachSparkUI( appId: String, attemptId: Option[String], ui: SparkUI, completed: Boolean): Unit def detachSparkUI(appId: String, attemptId: Option[String], ui: SparkUI): Unit } NOTE: ApplicationCacheOperations is a private[history] contract. .(Subset of) ApplicationCacheOperations Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | getAppUI | [[getAppUI]] spark-webui-SparkUI.md[SparkUI] (the UI of a Spark application) Used exclusively when ApplicationCache is requested for ApplicationCache.md#loadApplicationEntry[loadApplicationEntry] | attachSparkUI | [[attachSparkUI]] | detachSparkUI | [[detachSparkUI]] |=== [[implementations]] NOTE: HistoryServer.md[HistoryServer] is the one and only known implementation of < > in Apache Spark.","title":"ApplicationCacheOperations"},{"location":"history-server/ApplicationHistoryProvider/","text":"== [[ApplicationHistoryProvider]] ApplicationHistoryProvider ApplicationHistoryProvider is the < > of the < > of < >. [[contract]] [source, scala] package org.apache.spark.deploy.history abstract class ApplicationHistoryProvider { // only required methods that have no implementation // the others follow def getListing(): Iterator[ApplicationInfo] def getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI] def writeEventLogs(appId: String, attemptId: Option[String], zipStream: ZipOutputStream): Unit def getApplicationInfo(appId: String): Option[ApplicationInfo] } NOTE: ApplicationHistoryProvider is a private[history] contract. .(Subset of) ApplicationHistoryProvider Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | getListing | [[getListing]] | getAppUI | [[getAppUI]] spark-webui-SparkUI.md[SparkUI] (the UI of a Spark application) Used exclusively when HistoryServer is requested for the HistoryServer.md#getAppUI[UI of a Spark application] | writeEventLogs | [[writeEventLogs]] Writes events to a stream | getApplicationInfo | [[getApplicationInfo]] |=== ApplicationHistoryProvider is a Scala abstract class and cannot be created directly, but only as one of the < >. [[implementations]] NOTE: FsHistoryProvider.md[FsHistoryProvider] is the one and only known implementation of < > in Apache Spark.","title":"ApplicationHistoryProvider"},{"location":"history-server/EventLoggingListener/","text":"= EventLoggingListener EventLoggingListener is a ROOT:SparkListener.md[] that < > of a Spark application with event logging enabled (based on ROOT:configuration-properties.md#spark.eventLog.enabled[spark.eventLog.enabled] configuration property). EventLoggingListener supports custom spark-history-server:configuration-properties.md#EventLoggingListener[configuration properties]. EventLoggingListener writes out log files to a directory (based on ROOT:configuration-properties.md#spark.eventLog.dir[spark.eventLog.dir] configuration property). All ROOT:SparkListener.md[]s are logged (but ROOT:SparkListener.md#SparkListenerBlockUpdated[SparkListenerBlockUpdated] and ROOT:SparkListener.md#SparkListenerExecutorMetricsUpdate[SparkListenerExecutorMetricsUpdate]). TIP: Use index.md[Spark History Server] to view the event logs in a browser (similarly to webui:index.md[web UI] of a Spark application). [[inprogress-extension]][[IN_PROGRESS]] EventLoggingListener uses .inprogress file extension for in-flight event log files of active Spark applications. EventLoggingListener can compress events (based on ROOT:configuration-properties.md#spark.eventLog.compress[spark.eventLog.compress] configuration property). == [[creating-instance]] Creating Instance EventLoggingListener takes the following to be created: [[appId]] Application ID [[appAttemptId]] Application Attempt ID (optional) [[logBaseDir]] Log Directory [[sparkConf]] ROOT:SparkConf.md[SparkConf] [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] EventLoggingListener initializes the < >. NOTE: When initialized with no < >, EventLoggingListener uses SparkHadoopUtil utility to ROOT:spark-SparkHadoopUtil.md#newConfiguration[create a new one]. == [[logPath]] Event Log File [source, scala] \u00b6 logPath: String \u00b6 logPath is...FIXME NOTE: logPath is used when EventLoggingListener is requested to < > and < >. == [[start]] Starting EventLoggingListener [source, scala] \u00b6 start(): Unit \u00b6 start deletes the < > with the < > extension. The log file's working name is created based on appId with or without the compression codec used and appAttemptId , i.e. local-1461696754069 . It also uses .inprogress extension. If < >, you should see the WARN message: Event log [path] already exists. Overwriting... The working log .inprogress is attempted to be deleted. In case it could not be deleted, the following WARN message is printed out to the logs: Error deleting [path] The buffered output stream is created with metadata with Spark's version and SparkListenerLogStart class' name as the first line. {\"Event\":\"SparkListenerLogStart\",\"Spark Version\":\"2.0.0-SNAPSHOT\"} At this point, EventLoggingListener is ready for event logging and you should see the following INFO message in the logs: Logging events to [logPath] start throws an IllegalArgumentException when the < > is not a directory: Log directory [logBaseDir] is not a directory. start is used when SparkContext is created. == [[logEvent]] Logging Event (In JSON Format) [source, scala] \u00b6 logEvent( event: SparkListenerEvent, flushLogger: Boolean = false): Unit logEvent logs the given event as JSON. CAUTION: FIXME == [[stop]] Stopping EventLoggingListener [source, scala] \u00b6 stop(): Unit \u00b6 stop closes PrintWriter for the log file and renames the file to be without .inprogress extension. If the target log file exists (one without .inprogress extension), it overwrites the file if < > is enabled. You should see the following WARN message in the logs: Event log [target] already exists. Overwriting... If the target log file exists and overwrite is disabled, an java.io.IOException is thrown with the following message: Target log file already exists ([logPath]) NOTE: stop is executed while SparkContext is requested to stop . == [[getLogPath]] getLogPath Utility [source, scala] \u00b6 getLogPath( logBaseDir: URI, appId: String, appAttemptId: Option[String], compressionCodecName: Option[String] = None): String getLogPath ...FIXME NOTE: getLogPath is used when EventLoggingListener is < > (for the < >). == [[openEventLog]] openEventLog Utility [source, scala] \u00b6 openEventLog( log: Path, fs: FileSystem): InputStream openEventLog...FIXME openEventLog is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.EventLoggingListener logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.scheduler.EventLoggingListener=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[hadoopDataStream]] FSDataOutputStream Hadoop http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FSDataOutputStream.html[FSDataOutputStream ] (default: None ) Used when...FIXME === [[writer]] PrintWriter {java-javadoc-url}/java/io/PrintWriter.html[java.io.PrintWriter] for < > to the < >. Initialized when EventLoggingListener is requested to < > Used when EventLoggingListener is requested to < > Closed when EventLoggingListener is requested to < >","title":"EventLoggingListener"},{"location":"history-server/EventLoggingListener/#source-scala","text":"","title":"[source, scala]"},{"location":"history-server/EventLoggingListener/#logpath-string","text":"logPath is...FIXME NOTE: logPath is used when EventLoggingListener is requested to < > and < >. == [[start]] Starting EventLoggingListener","title":"logPath: String"},{"location":"history-server/EventLoggingListener/#source-scala_1","text":"","title":"[source, scala]"},{"location":"history-server/EventLoggingListener/#start-unit","text":"start deletes the < > with the < > extension. The log file's working name is created based on appId with or without the compression codec used and appAttemptId , i.e. local-1461696754069 . It also uses .inprogress extension. If < >, you should see the WARN message: Event log [path] already exists. Overwriting... The working log .inprogress is attempted to be deleted. In case it could not be deleted, the following WARN message is printed out to the logs: Error deleting [path] The buffered output stream is created with metadata with Spark's version and SparkListenerLogStart class' name as the first line. {\"Event\":\"SparkListenerLogStart\",\"Spark Version\":\"2.0.0-SNAPSHOT\"} At this point, EventLoggingListener is ready for event logging and you should see the following INFO message in the logs: Logging events to [logPath] start throws an IllegalArgumentException when the < > is not a directory: Log directory [logBaseDir] is not a directory. start is used when SparkContext is created. == [[logEvent]] Logging Event (In JSON Format)","title":"start(): Unit"},{"location":"history-server/EventLoggingListener/#source-scala_2","text":"logEvent( event: SparkListenerEvent, flushLogger: Boolean = false): Unit logEvent logs the given event as JSON. CAUTION: FIXME == [[stop]] Stopping EventLoggingListener","title":"[source, scala]"},{"location":"history-server/EventLoggingListener/#source-scala_3","text":"","title":"[source, scala]"},{"location":"history-server/EventLoggingListener/#stop-unit","text":"stop closes PrintWriter for the log file and renames the file to be without .inprogress extension. If the target log file exists (one without .inprogress extension), it overwrites the file if < > is enabled. You should see the following WARN message in the logs: Event log [target] already exists. Overwriting... If the target log file exists and overwrite is disabled, an java.io.IOException is thrown with the following message: Target log file already exists ([logPath]) NOTE: stop is executed while SparkContext is requested to stop . == [[getLogPath]] getLogPath Utility","title":"stop(): Unit"},{"location":"history-server/EventLoggingListener/#source-scala_4","text":"getLogPath( logBaseDir: URI, appId: String, appAttemptId: Option[String], compressionCodecName: Option[String] = None): String getLogPath ...FIXME NOTE: getLogPath is used when EventLoggingListener is < > (for the < >). == [[openEventLog]] openEventLog Utility","title":"[source, scala]"},{"location":"history-server/EventLoggingListener/#source-scala_5","text":"openEventLog( log: Path, fs: FileSystem): InputStream openEventLog...FIXME openEventLog is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.EventLoggingListener logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"history-server/EventLoggingListener/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"history-server/EventLoggingListener/#log4jloggerorgapachesparkschedulereventlogginglistenerall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[hadoopDataStream]] FSDataOutputStream Hadoop http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FSDataOutputStream.html[FSDataOutputStream ] (default: None ) Used when...FIXME === [[writer]] PrintWriter {java-javadoc-url}/java/io/PrintWriter.html[java.io.PrintWriter] for < > to the < >. Initialized when EventLoggingListener is requested to < > Used when EventLoggingListener is requested to < > Closed when EventLoggingListener is requested to < >","title":"log4j.logger.org.apache.spark.scheduler.EventLoggingListener=ALL"},{"location":"history-server/FsHistoryProvider/","text":"= FsHistoryProvider FsHistoryProvider is the default ApplicationHistoryProvider.md[ApplicationHistoryProvider] for index.md[Spark History Server]. FsHistoryProvider is < > exclusively when HistoryServer is HistoryServer.md#main[started] as a standalone application and spark.history.provider configuration property was not defined. [TIP] \u00b6 Enable INFO or DEBUG logging levels for org.apache.spark.deploy.history.FsHistoryProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.FsHistoryProvider=DEBUG Refer to spark-logging.md[Logging]. \u00b6 == [[rebuildAppStore]] rebuildAppStore Internal Method [source, scala] \u00b6 rebuildAppStore( store: KVStore, eventLog: FileStatus, lastUpdated: Long): Unit rebuildAppStore ...FIXME NOTE: rebuildAppStore is used when...FIXME == [[getAppUI]] getAppUI Method [source, scala] \u00b6 getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI] \u00b6 NOTE: getAppUI is part of ApplicationHistoryProvider.md#getAppUI[ApplicationHistoryProvider Contract] to...FIXME. getAppUI ...FIXME == [[creating-instance]] Creating FsHistoryProvider Instance FsHistoryProvider takes the following when created: [[conf]] ROOT:SparkConf.md[SparkConf] [[clock]] Clock (default: SystemClock ) FsHistoryProvider initializes the < >.","title":"FsHistoryProvider"},{"location":"history-server/FsHistoryProvider/#tip","text":"Enable INFO or DEBUG logging levels for org.apache.spark.deploy.history.FsHistoryProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.FsHistoryProvider=DEBUG","title":"[TIP]"},{"location":"history-server/FsHistoryProvider/#refer-to-spark-loggingmdlogging","text":"== [[rebuildAppStore]] rebuildAppStore Internal Method","title":"Refer to spark-logging.md[Logging]."},{"location":"history-server/FsHistoryProvider/#source-scala","text":"rebuildAppStore( store: KVStore, eventLog: FileStatus, lastUpdated: Long): Unit rebuildAppStore ...FIXME NOTE: rebuildAppStore is used when...FIXME == [[getAppUI]] getAppUI Method","title":"[source, scala]"},{"location":"history-server/FsHistoryProvider/#source-scala_1","text":"","title":"[source, scala]"},{"location":"history-server/FsHistoryProvider/#getappuiappid-string-attemptid-optionstring-optionloadedappui","text":"NOTE: getAppUI is part of ApplicationHistoryProvider.md#getAppUI[ApplicationHistoryProvider Contract] to...FIXME. getAppUI ...FIXME == [[creating-instance]] Creating FsHistoryProvider Instance FsHistoryProvider takes the following when created: [[conf]] ROOT:SparkConf.md[SparkConf] [[clock]] Clock (default: SystemClock ) FsHistoryProvider initializes the < >.","title":"getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI]"},{"location":"history-server/HistoryServer/","text":"= [[HistoryServer]] HistoryServer -- WebUI For Active And Completed Spark Applications HistoryServer is an extension of the webui:spark-webui-WebUI.md[web UI] for reviewing event logs of running (active) and completed Spark applications with event log collection enabled (based on ROOT:configuration-properties.md#spark.eventLog.enabled[spark.eventLog.enabled] configuration property). HistoryServer supports custom spark-history-server:configuration-properties.md#HistoryServer[configuration properties]. HistoryServer is < > when...FIXME HistoryServer uses the < > to handle requests to /* URI that < >. [[ApplicationCacheOperations]] HistoryServer is a ApplicationCacheOperations.md[ApplicationCacheOperations]. [[UIRoot]] HistoryServer is a rest-api:spark-api-UIRoot.md[UIRoot]. [[retainedApplications]] HistoryServer uses ROOT:configuration-properties.md#spark.history.retainedApplications[spark.history.retainedApplications] configuration property (default: 50 ) for...FIXME [[maxApplications]] HistoryServer uses ROOT:configuration-properties.md#spark.history.ui.maxApplications[spark.history.ui.maxApplications] configuration property (default: unbounded ) for...FIXME [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.deploy.history.HistoryServer logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServer=ALL Refer to ROOT:spark-logging.md[Logging]. \u00b6 == [[creating-instance]] Creating HistoryServer Instance HistoryServer takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] [[provider]] ApplicationHistoryProvider.md[ApplicationHistoryProvider] [[securityManager]] SecurityManager [[port]] Port number HistoryServer initializes the < >. While being created, HistoryServer is requested to < >. == [[initialize]] Initializing HistoryServer -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 NOTE: initialize is part of webui:spark-webui-WebUI.md#initialize[WebUI Contract] to initialize web components. initialize ...FIXME == [[attachSparkUI]] attachSparkUI Method [source, scala] \u00b6 attachSparkUI( appId: String, attemptId: Option[String], ui: SparkUI, completed: Boolean): Unit NOTE: attachSparkUI is part of ApplicationCacheOperations.md#attachSparkUI[ApplicationCacheOperations Contract] to...FIXME. attachSparkUI ...FIXME == [[main]] Launching HistoryServer Standalone Application -- main Method [source, scala] \u00b6 main(argStrings: Array[String]): Unit \u00b6 main ...FIXME == [[getAppUI]] Requesting Spark Application UI -- getAppUI Method [source, scala] \u00b6 getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI] \u00b6 NOTE: getAppUI is part of ApplicationCacheOperations.md#getAppUI[ApplicationCacheOperations Contract] to...FIXME. getAppUI ...FIXME == [[withSparkUI]] withSparkUI Method [source, scala] \u00b6 withSparkUI T (fn: SparkUI => T): T \u00b6 NOTE: withSparkUI is part of spark-api-UIRoot.md#withSparkUI[UIRoot Contract] to...FIXME. withSparkUI ...FIXME == [[loadAppUi]] loadAppUi Internal Method [source, scala] \u00b6 loadAppUi(appId: String, attemptId: Option[String]): Boolean \u00b6 loadAppUi ...FIXME NOTE: loadAppUi is used exclusively when HistoryServer is < >. == [[doGet]] doGet Method [source, scala] \u00b6 doGet(req: HttpServletRequest, res: HttpServletResponse): Unit \u00b6 NOTE: doGet is part of Java Servlet's https://docs.oracle.com/javaee/7/api/javax/servlet/http/HttpServlet.html[HttpServlet ] to handle HTTP GET requests. doGet ...FIXME NOTE: doGet is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | appCache a| [[appCache]] ApplicationCache.md[ApplicationCache] for this HistoryServer and < > Used when...FIXME | loaderServlet a| [[loaderServlet]] Java Servlets' https://docs.oracle.com/javaee/7/api/javax/servlet/http/HttpServlet.html[HttpServlet ] Used exclusively when HistoryServer is requested to < > (and spark-webui-WebUI.md#attachHandler[attaches the servlet to the web UI] to handle /* URI) |===","title":"HistoryServer"},{"location":"history-server/HistoryServer/#refer-to-rootspark-loggingmdlogging","text":"== [[creating-instance]] Creating HistoryServer Instance HistoryServer takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] [[provider]] ApplicationHistoryProvider.md[ApplicationHistoryProvider] [[securityManager]] SecurityManager [[port]] Port number HistoryServer initializes the < >. While being created, HistoryServer is requested to < >. == [[initialize]] Initializing HistoryServer -- initialize Method","title":"Refer to ROOT:spark-logging.md[Logging]."},{"location":"history-server/HistoryServer/#source-scala","text":"","title":"[source, scala]"},{"location":"history-server/HistoryServer/#initialize-unit","text":"NOTE: initialize is part of webui:spark-webui-WebUI.md#initialize[WebUI Contract] to initialize web components. initialize ...FIXME == [[attachSparkUI]] attachSparkUI Method","title":"initialize(): Unit"},{"location":"history-server/HistoryServer/#source-scala_1","text":"attachSparkUI( appId: String, attemptId: Option[String], ui: SparkUI, completed: Boolean): Unit NOTE: attachSparkUI is part of ApplicationCacheOperations.md#attachSparkUI[ApplicationCacheOperations Contract] to...FIXME. attachSparkUI ...FIXME == [[main]] Launching HistoryServer Standalone Application -- main Method","title":"[source, scala]"},{"location":"history-server/HistoryServer/#source-scala_2","text":"","title":"[source, scala]"},{"location":"history-server/HistoryServer/#mainargstrings-arraystring-unit","text":"main ...FIXME == [[getAppUI]] Requesting Spark Application UI -- getAppUI Method","title":"main(argStrings: Array[String]): Unit"},{"location":"history-server/HistoryServer/#source-scala_3","text":"","title":"[source, scala]"},{"location":"history-server/HistoryServer/#getappuiappid-string-attemptid-optionstring-optionloadedappui","text":"NOTE: getAppUI is part of ApplicationCacheOperations.md#getAppUI[ApplicationCacheOperations Contract] to...FIXME. getAppUI ...FIXME == [[withSparkUI]] withSparkUI Method","title":"getAppUI(appId: String, attemptId: Option[String]): Option[LoadedAppUI]"},{"location":"history-server/HistoryServer/#source-scala_4","text":"","title":"[source, scala]"},{"location":"history-server/HistoryServer/#withsparkuitfn-sparkui-t-t","text":"NOTE: withSparkUI is part of spark-api-UIRoot.md#withSparkUI[UIRoot Contract] to...FIXME. withSparkUI ...FIXME == [[loadAppUi]] loadAppUi Internal Method","title":"withSparkUIT(fn: SparkUI =&gt; T): T"},{"location":"history-server/HistoryServer/#source-scala_5","text":"","title":"[source, scala]"},{"location":"history-server/HistoryServer/#loadappuiappid-string-attemptid-optionstring-boolean","text":"loadAppUi ...FIXME NOTE: loadAppUi is used exclusively when HistoryServer is < >. == [[doGet]] doGet Method","title":"loadAppUi(appId: String, attemptId: Option[String]): Boolean"},{"location":"history-server/HistoryServer/#source-scala_6","text":"","title":"[source, scala]"},{"location":"history-server/HistoryServer/#dogetreq-httpservletrequest-res-httpservletresponse-unit","text":"NOTE: doGet is part of Java Servlet's https://docs.oracle.com/javaee/7/api/javax/servlet/http/HttpServlet.html[HttpServlet ] to handle HTTP GET requests. doGet ...FIXME NOTE: doGet is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | appCache a| [[appCache]] ApplicationCache.md[ApplicationCache] for this HistoryServer and < > Used when...FIXME | loaderServlet a| [[loaderServlet]] Java Servlets' https://docs.oracle.com/javaee/7/api/javax/servlet/http/HttpServlet.html[HttpServlet ] Used exclusively when HistoryServer is requested to < > (and spark-webui-WebUI.md#attachHandler[attaches the servlet to the web UI] to handle /* URI) |===","title":"doGet(req: HttpServletRequest, res: HttpServletResponse): Unit"},{"location":"history-server/HistoryServerArguments/","text":"== HistoryServerArguments HistoryServerArguments is the command-line parser for the index.md[History Server]. When HistoryServerArguments is executed with a single command-line parameter it is assumed to be the event logs directory. $ ./sbin/start-history-server.sh /tmp/spark-events This is however deprecated since Spark 1.1.0 and you should see the following WARN message in the logs: WARN HistoryServerArguments: Setting log directory through the command line is deprecated as of Spark 1.1.0. Please set this through spark.history.fs.logDirectory instead. The same WARN message shows up for --dir and -d command-line options. --properties-file [propertiesFile] command-line option specifies the file with the custom spark-properties.md[Spark properties]. NOTE: When not specified explicitly, History Server uses the default configuration file, i.e. spark-properties.md#spark-defaults-conf[spark-defaults.conf]. [TIP] \u00b6 Enable WARN logging level for org.apache.spark.deploy.history.HistoryServerArguments logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServerArguments=WARN Refer to spark-logging.md[Logging]. \u00b6","title":"HistoryServerArguments"},{"location":"history-server/HistoryServerArguments/#tip","text":"Enable WARN logging level for org.apache.spark.deploy.history.HistoryServerArguments logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.deploy.history.HistoryServerArguments=WARN","title":"[TIP]"},{"location":"history-server/HistoryServerArguments/#refer-to-spark-loggingmdlogging","text":"","title":"Refer to spark-logging.md[Logging]."},{"location":"history-server/JsonProtocol/","text":"= JsonProtocol Utility :navtitle: JsonProtocol JsonProtocol is an utility to convert SparkListenerEvents < > and < > JSON format. JsonProtocol is used by spark-history-server:EventLoggingListener.md[] and spark-history-server:ReplayListenerBus.md[] (to spark-history-server:EventLoggingListener.md#logEvent[log] and spark-history-server:ReplayListenerBus.md#replay[replay] events for spark-history-server:index.md[], respectively). == [[sparkEventFromJson]] sparkEventFromJson Utility [source,scala] \u00b6 sparkEventFromJson( json: JValue): SparkListenerEvent sparkEventFromJson...FIXME sparkEventFromJson is used when ReplayListenerBus is requested to spark-history-server:ReplayListenerBus.md#replay[replay]. == [[logStartToJson]] logStartToJson Utility [source,scala] \u00b6 logStartToJson( logStart: SparkListenerLogStart): JValue logStartToJson...FIXME logStartToJson is used when...FIXME == [[taskEndFromJson]] taskEndFromJson Utility [source,scala] \u00b6 taskEndFromJson( json: JValue): SparkListenerTaskEnd taskEndFromJson...FIXME taskEndFromJson is used when JsonProtocol utility is used to < >. == [[executorMetricsUpdateFromJson]] executorMetricsUpdateFromJson Utility [source,scala] \u00b6 executorMetricsUpdateFromJson( json: JValue): SparkListenerExecutorMetricsUpdate executorMetricsUpdateFromJson...FIXME executorMetricsUpdateFromJson is used when JsonProtocol utility is used to < >. == [[taskEndReasonFromJson]] taskEndReasonFromJson Utility [source,scala] \u00b6 taskEndReasonFromJson( json: JValue): TaskEndReason taskEndReasonFromJson...FIXME taskEndReasonFromJson is used when JsonProtocol utility is used to < >. == [[stageInfoFromJson]] stageInfoFromJson Utility [source,scala] \u00b6 stageInfoFromJson( json: JValue): StageInfo stageInfoFromJson...FIXME stageInfoFromJson is used when JsonProtocol utility is used to < >, < >, < >. == [[taskInfoFromJson]] taskInfoFromJson Utility [source,scala] \u00b6 taskInfoFromJson( json: JValue): TaskInfo taskInfoFromJson...FIXME taskInfoFromJson is used when JsonProtocol utility is used to < >, < >, < >. == [[accumulableInfoFromJson]] accumulableInfoFromJson Utility [source,scala] \u00b6 accumulableInfoFromJson( json: JValue): AccumulableInfo accumulableInfoFromJson...FIXME accumulableInfoFromJson is used when JsonProtocol utility is used to < >, < >, < >, < >. == [[accumValueFromJson]] accumValueFromJson Utility [source,scala] \u00b6 accumValueFromJson( name: Option[String], value: JValue): Any accumValueFromJson...FIXME accumValueFromJson is used when JsonProtocol utility is used to < >. == [[taskMetricsFromJson]] taskMetricsFromJson Utility [source,scala] \u00b6 taskMetricsFromJson( json: JValue): TaskMetrics taskMetricsFromJson...FIXME taskMetricsFromJson is used when JsonProtocol utility is used to < > and < >. == [[taskEndToJson]] taskEndToJson Utility [source,scala] \u00b6 taskEndToJson( taskEnd: SparkListenerTaskEnd): JValue taskEndToJson...FIXME taskEndToJson is used when JsonProtocol utility is used to < > ( serialize a SparkListenerEvent to JSON ). == [[taskMetricsToJson]] taskMetricsToJson Utility [source,scala] \u00b6 taskMetricsToJson( taskMetrics: TaskMetrics): JValue taskMetricsToJson...FIXME taskMetricsToJson is used when JsonProtocol utility is used to < >. == [[blockUpdateFromJson]] blockUpdateFromJson Utility [source,scala] \u00b6 blockUpdateFromJson( json: JValue): SparkListenerBlockUpdated blockUpdateFromJson...FIXME blockUpdateFromJson is used when JsonProtocol utility is used to < >. == [[blockUpdatedInfoFromJson]] blockUpdatedInfoFromJson Utility [source,scala] \u00b6 blockUpdatedInfoFromJson( json: JValue): BlockUpdatedInfo blockUpdatedInfoFromJson...FIXME blockUpdatedInfoFromJson is used when JsonProtocol utility is used to < >.","title":"JsonProtocol"},{"location":"history-server/JsonProtocol/#sourcescala","text":"sparkEventFromJson( json: JValue): SparkListenerEvent sparkEventFromJson...FIXME sparkEventFromJson is used when ReplayListenerBus is requested to spark-history-server:ReplayListenerBus.md#replay[replay]. == [[logStartToJson]] logStartToJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_1","text":"logStartToJson( logStart: SparkListenerLogStart): JValue logStartToJson...FIXME logStartToJson is used when...FIXME == [[taskEndFromJson]] taskEndFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_2","text":"taskEndFromJson( json: JValue): SparkListenerTaskEnd taskEndFromJson...FIXME taskEndFromJson is used when JsonProtocol utility is used to < >. == [[executorMetricsUpdateFromJson]] executorMetricsUpdateFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_3","text":"executorMetricsUpdateFromJson( json: JValue): SparkListenerExecutorMetricsUpdate executorMetricsUpdateFromJson...FIXME executorMetricsUpdateFromJson is used when JsonProtocol utility is used to < >. == [[taskEndReasonFromJson]] taskEndReasonFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_4","text":"taskEndReasonFromJson( json: JValue): TaskEndReason taskEndReasonFromJson...FIXME taskEndReasonFromJson is used when JsonProtocol utility is used to < >. == [[stageInfoFromJson]] stageInfoFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_5","text":"stageInfoFromJson( json: JValue): StageInfo stageInfoFromJson...FIXME stageInfoFromJson is used when JsonProtocol utility is used to < >, < >, < >. == [[taskInfoFromJson]] taskInfoFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_6","text":"taskInfoFromJson( json: JValue): TaskInfo taskInfoFromJson...FIXME taskInfoFromJson is used when JsonProtocol utility is used to < >, < >, < >. == [[accumulableInfoFromJson]] accumulableInfoFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_7","text":"accumulableInfoFromJson( json: JValue): AccumulableInfo accumulableInfoFromJson...FIXME accumulableInfoFromJson is used when JsonProtocol utility is used to < >, < >, < >, < >. == [[accumValueFromJson]] accumValueFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_8","text":"accumValueFromJson( name: Option[String], value: JValue): Any accumValueFromJson...FIXME accumValueFromJson is used when JsonProtocol utility is used to < >. == [[taskMetricsFromJson]] taskMetricsFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_9","text":"taskMetricsFromJson( json: JValue): TaskMetrics taskMetricsFromJson...FIXME taskMetricsFromJson is used when JsonProtocol utility is used to < > and < >. == [[taskEndToJson]] taskEndToJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_10","text":"taskEndToJson( taskEnd: SparkListenerTaskEnd): JValue taskEndToJson...FIXME taskEndToJson is used when JsonProtocol utility is used to < > ( serialize a SparkListenerEvent to JSON ). == [[taskMetricsToJson]] taskMetricsToJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_11","text":"taskMetricsToJson( taskMetrics: TaskMetrics): JValue taskMetricsToJson...FIXME taskMetricsToJson is used when JsonProtocol utility is used to < >. == [[blockUpdateFromJson]] blockUpdateFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_12","text":"blockUpdateFromJson( json: JValue): SparkListenerBlockUpdated blockUpdateFromJson...FIXME blockUpdateFromJson is used when JsonProtocol utility is used to < >. == [[blockUpdatedInfoFromJson]] blockUpdatedInfoFromJson Utility","title":"[source,scala]"},{"location":"history-server/JsonProtocol/#sourcescala_13","text":"blockUpdatedInfoFromJson( json: JValue): BlockUpdatedInfo blockUpdatedInfoFromJson...FIXME blockUpdatedInfoFromJson is used when JsonProtocol utility is used to < >.","title":"[source,scala]"},{"location":"history-server/ReplayListenerBus/","text":"= ReplayListenerBus ReplayListenerBus is a ROOT:spark-SparkListenerBus.md[] that can < SparkListenerEvent events>>. ReplayListenerBus is used by spark-history-server:FsHistoryProvider.md[]. == [[replay]] Replaying JSON-encoded SparkListenerEvents from Stream [source, scala] \u00b6 replay( logData: InputStream, sourceName: String, maybeTruncated: Boolean = false): Unit replay reads JSON-encoded ROOT:SparkListener.md#SparkListenerEvent[SparkListenerEvent] events from logData (one event per line) and posts them to all registered ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface listeners]. replay uses spark-history-server:JsonProtocol.md#sparkEventFromJson[ JsonProtocol to convert JSON-encoded events to SparkListenerEvent objects]. NOTE: replay uses jackson from http://json4s.org/[json4s ] library to parse the AST for JSON. When there is an exception parsing a JSON event, you may see the following WARN message in the logs (for the last line) or a JsonParseException . WARN Got JsonParseException from log file $sourceName at line [lineNumber], the file might not have finished writing cleanly. Any other non-IO exceptions end up with the following ERROR messages in the logs: ERROR Exception parsing Spark event log: [sourceName] ERROR Malformed line #[lineNumber]: [currentLine] NOTE: The sourceName input argument is only used for messages.","title":"ReplayListenerBus"},{"location":"history-server/ReplayListenerBus/#source-scala","text":"replay( logData: InputStream, sourceName: String, maybeTruncated: Boolean = false): Unit replay reads JSON-encoded ROOT:SparkListener.md#SparkListenerEvent[SparkListenerEvent] events from logData (one event per line) and posts them to all registered ROOT:SparkListener.md#SparkListenerInterface[SparkListenerInterface listeners]. replay uses spark-history-server:JsonProtocol.md#sparkEventFromJson[ JsonProtocol to convert JSON-encoded events to SparkListenerEvent objects]. NOTE: replay uses jackson from http://json4s.org/[json4s ] library to parse the AST for JSON. When there is an exception parsing a JSON event, you may see the following WARN message in the logs (for the last line) or a JsonParseException . WARN Got JsonParseException from log file $sourceName at line [lineNumber], the file might not have finished writing cleanly. Any other non-IO exceptions end up with the following ERROR messages in the logs: ERROR Exception parsing Spark event log: [sourceName] ERROR Malformed line #[lineNumber]: [currentLine] NOTE: The sourceName input argument is only used for messages.","title":"[source, scala]"},{"location":"history-server/SQLHistoryListener/","text":"== SQLHistoryListener SQLHistoryListener is a custom spark-sql-SQLListener.md[SQLListener] for index.md[History Server]. It attaches spark-sql-webui.md#creating-instance[SQL tab] to History Server's web UI only when the first spark-sql-SQLListener.md#SparkListenerSQLExecutionStart[SparkListenerSQLExecutionStart] arrives and shuts < > off. It also handles < >. NOTE: Support for SQL UI in History Server was added in SPARK-11206 Support SQL UI on the history server. CAUTION: FIXME Add the link to the JIRA. === [[onOtherEvent]] onOtherEvent [source, scala] \u00b6 onOtherEvent(event: SparkListenerEvent): Unit \u00b6 When SparkListenerSQLExecutionStart event comes, onOtherEvent attaches spark-sql-webui.md#creating-instance[SQL tab] to web UI and passes the call to the parent spark-sql-SQLListener.md[SQLListener]. === [[onTaskEnd]] onTaskEnd CAUTION: FIXME === [[creating-instance]] Creating SQLHistoryListener Instance SQLHistoryListener is created using a ( private[sql] ) SQLHistoryListenerFactory class (which is SparkHistoryListenerFactory ). The SQLHistoryListenerFactory class is registered when spark-webui-SparkUI.md#createHistoryUI[ SparkUI creates a web UI for History Server] as a Java service in META-INF/services/org.apache.spark.scheduler.SparkHistoryListenerFactory : org.apache.spark.sql.execution.ui.SQLHistoryListenerFactory NOTE: Loading the service uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-[ServiceLoader.load ] method. === [[onExecutorMetricsUpdate]] onExecutorMetricsUpdate onExecutorMetricsUpdate does nothing.","title":"SQLHistoryListener"},{"location":"history-server/SQLHistoryListener/#source-scala","text":"","title":"[source, scala]"},{"location":"history-server/SQLHistoryListener/#onothereventevent-sparklistenerevent-unit","text":"When SparkListenerSQLExecutionStart event comes, onOtherEvent attaches spark-sql-webui.md#creating-instance[SQL tab] to web UI and passes the call to the parent spark-sql-SQLListener.md[SQLListener]. === [[onTaskEnd]] onTaskEnd CAUTION: FIXME === [[creating-instance]] Creating SQLHistoryListener Instance SQLHistoryListener is created using a ( private[sql] ) SQLHistoryListenerFactory class (which is SparkHistoryListenerFactory ). The SQLHistoryListenerFactory class is registered when spark-webui-SparkUI.md#createHistoryUI[ SparkUI creates a web UI for History Server] as a Java service in META-INF/services/org.apache.spark.scheduler.SparkHistoryListenerFactory : org.apache.spark.sql.execution.ui.SQLHistoryListenerFactory NOTE: Loading the service uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-[ServiceLoader.load ] method. === [[onExecutorMetricsUpdate]] onExecutorMetricsUpdate onExecutorMetricsUpdate does nothing.","title":"onOtherEvent(event: SparkListenerEvent): Unit"},{"location":"history-server/configuration-properties/","text":"Configuration Properties \u00b6 The following contains the configuration properties of < > and < >. NOTE: EventLoggingListener.md[EventLoggingListener] is responsible for writing out JSON-encoded events of a Spark application to an event log file that HistoryServer.md[HistoryServer] can display in a web UI-based interface. spark.eventLog.dir \u00b6 Directory where Spark events are logged to (e.g. hdfs://namenode:8021/directory ) Default: /tmp/spark-events The directory must exist before SparkContext is created. [[EventLoggingListener]] .EventLoggingListener's Spark Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.eventLog.buffer.kb a| [[spark.eventLog.buffer.kb]] Size of the buffer to use when writing to output streams. Default: 100 | spark.eventLog.compress a| [[spark.eventLog.compress]] Whether to enable ( true ) or disable ( false ) event compression (using a io:CompressionCodec.md[CompressionCodec]) Default: false | spark.eventLog.enabled a| [[spark.eventLog.enabled]] Whether to enable ( true ) or disable ( false ) persisting Spark events. Default: false | spark.eventLog.logBlockUpdates.enabled a| [[spark.eventLog.logBlockUpdates.enabled]][[EVENT_LOG_BLOCK_UPDATES]] Whether EventLoggingListener.md[EventLoggingListener] should log RDD block updates ( true ) or not ( false ) Default: false | spark.eventLog.overwrite a| [[spark.eventLog.overwrite]] Whether to enable ( true ) or disable ( false ) deleting (or at least overwriting) an existing EventLoggingListener.md#inprogress[.inprogress] event log files Default: false | spark.eventLog.testing a| [[spark.eventLog.testing]] (internal) Whether to enable ( true ) or disable ( false ) adding JSON-encoded events to the internal loggedEvents array for testing Default: false |=== [[HistoryServer]] .HistoryServer's Spark Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.history.fs.logDirectory | [[spark.history.fs.logDirectory]] The directory for event log files. The directory has to exist before starting History Server. Default: file:/tmp/spark-events | spark.history.kerberos.enabled | [[spark.history.kerberos.enabled]] Whether to enable ( true ) or disable ( false ) security when working with HDFS with security enabled (Kerberos). Default: false | spark.history.kerberos.keytab | [[spark.history.kerberos.keytab]] Keytab to use for login to Kerberos. Required when spark.history.kerberos.enabled is enabled. Default: (empty) | spark.history.kerberos.principal | [[spark.history.kerberos.principal]] Kerberos principal. Required when spark.history.kerberos.enabled is enabled. Default: (empty) | spark.history.provider | [[spark.history.provider]] Fully-qualified class name of the ApplicationHistoryProvider.md[ApplicationHistoryProvider] Default: FsHistoryProvider.md[org.apache.spark.deploy.history.FsHistoryProvider] | spark.history.retainedApplications | [[spark.history.retainedApplications]] How many Spark applications HistoryServer.md#retainedApplications[HistoryServer] should retain Default: 50 | spark.history.ui.maxApplications | [[spark.history.ui.maxApplications]][[HISTORY_UI_MAX_APPS]] How many Spark applications HistoryServer.md#maxApplications[HistoryServer] should show in the UI Default: (unbounded) | spark.history.ui.port | [[spark.history.ui.port]][[HISTORY_SERVER_UI_PORT]] The port of History Server's web UI. Default: 18080 |===","title":"Configuration Properties"},{"location":"history-server/configuration-properties/#configuration-properties","text":"The following contains the configuration properties of < > and < >. NOTE: EventLoggingListener.md[EventLoggingListener] is responsible for writing out JSON-encoded events of a Spark application to an event log file that HistoryServer.md[HistoryServer] can display in a web UI-based interface.","title":"Configuration Properties"},{"location":"history-server/configuration-properties/#sparkeventlogdir","text":"Directory where Spark events are logged to (e.g. hdfs://namenode:8021/directory ) Default: /tmp/spark-events The directory must exist before SparkContext is created. [[EventLoggingListener]] .EventLoggingListener's Spark Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.eventLog.buffer.kb a| [[spark.eventLog.buffer.kb]] Size of the buffer to use when writing to output streams. Default: 100 | spark.eventLog.compress a| [[spark.eventLog.compress]] Whether to enable ( true ) or disable ( false ) event compression (using a io:CompressionCodec.md[CompressionCodec]) Default: false | spark.eventLog.enabled a| [[spark.eventLog.enabled]] Whether to enable ( true ) or disable ( false ) persisting Spark events. Default: false | spark.eventLog.logBlockUpdates.enabled a| [[spark.eventLog.logBlockUpdates.enabled]][[EVENT_LOG_BLOCK_UPDATES]] Whether EventLoggingListener.md[EventLoggingListener] should log RDD block updates ( true ) or not ( false ) Default: false | spark.eventLog.overwrite a| [[spark.eventLog.overwrite]] Whether to enable ( true ) or disable ( false ) deleting (or at least overwriting) an existing EventLoggingListener.md#inprogress[.inprogress] event log files Default: false | spark.eventLog.testing a| [[spark.eventLog.testing]] (internal) Whether to enable ( true ) or disable ( false ) adding JSON-encoded events to the internal loggedEvents array for testing Default: false |=== [[HistoryServer]] .HistoryServer's Spark Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.history.fs.logDirectory | [[spark.history.fs.logDirectory]] The directory for event log files. The directory has to exist before starting History Server. Default: file:/tmp/spark-events | spark.history.kerberos.enabled | [[spark.history.kerberos.enabled]] Whether to enable ( true ) or disable ( false ) security when working with HDFS with security enabled (Kerberos). Default: false | spark.history.kerberos.keytab | [[spark.history.kerberos.keytab]] Keytab to use for login to Kerberos. Required when spark.history.kerberos.enabled is enabled. Default: (empty) | spark.history.kerberos.principal | [[spark.history.kerberos.principal]] Kerberos principal. Required when spark.history.kerberos.enabled is enabled. Default: (empty) | spark.history.provider | [[spark.history.provider]] Fully-qualified class name of the ApplicationHistoryProvider.md[ApplicationHistoryProvider] Default: FsHistoryProvider.md[org.apache.spark.deploy.history.FsHistoryProvider] | spark.history.retainedApplications | [[spark.history.retainedApplications]] How many Spark applications HistoryServer.md#retainedApplications[HistoryServer] should retain Default: 50 | spark.history.ui.maxApplications | [[spark.history.ui.maxApplications]][[HISTORY_UI_MAX_APPS]] How many Spark applications HistoryServer.md#maxApplications[HistoryServer] should show in the UI Default: (unbounded) | spark.history.ui.port | [[spark.history.ui.port]][[HISTORY_SERVER_UI_PORT]] The port of History Server's web UI. Default: 18080 |===","title":" spark.eventLog.dir"},{"location":"kubernetes/ExecutorPodsLifecycleManager/","text":"ExecutorPodsLifecycleManager \u00b6 ExecutorPodsLifecycleManager is...FIXME Creating Instance \u00b6 ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend . Starting \u00b6 start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when...FIXME Handling State Changes in Executor Pods \u00b6 onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots ...FIXME","title":"ExecutorPodsLifecycleManager"},{"location":"kubernetes/ExecutorPodsLifecycleManager/#executorpodslifecyclemanager","text":"ExecutorPodsLifecycleManager is...FIXME","title":"ExecutorPodsLifecycleManager"},{"location":"kubernetes/ExecutorPodsLifecycleManager/#creating-instance","text":"ExecutorPodsLifecycleManager takes the following to be created: SparkConf KubernetesClient ExecutorPodsSnapshotsStore Guava Cache ExecutorPodsLifecycleManager is created when KubernetesClusterManager is requested for a SchedulerBackend .","title":"Creating Instance"},{"location":"kubernetes/ExecutorPodsLifecycleManager/#starting","text":"start ( schedulerBackend : KubernetesClusterSchedulerBackend ) : Unit start requests the ExecutorPodsSnapshotsStore to add a subscriber to intercept state changes in executor pods . start is used when...FIXME","title":" Starting"},{"location":"kubernetes/ExecutorPodsLifecycleManager/#handling-state-changes-in-executor-pods","text":"onNewSnapshots ( schedulerBackend : KubernetesClusterSchedulerBackend , snapshots : Seq [ ExecutorPodsSnapshot ]) : Unit onNewSnapshots ...FIXME","title":" Handling State Changes in Executor Pods"},{"location":"kubernetes/ExecutorPodsSnapshotsStore/","text":"ExecutorPodsSnapshotsStore \u00b6 ExecutorPodsSnapshotsStore is...FIXME","title":"ExecutorPodsSnapshotsStore"},{"location":"kubernetes/ExecutorPodsSnapshotsStore/#executorpodssnapshotsstore","text":"ExecutorPodsSnapshotsStore is...FIXME","title":"ExecutorPodsSnapshotsStore"},{"location":"kubernetes/KubernetesClusterManager/","text":"KubernetesClusterManager \u00b6 KubernetesClusterManager is an ExternalClusterManager for k8s master URLs. Creating TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler creates a TaskSchedulerImpl . createTaskScheduler is part of the ExternalClusterManager abstraction. Creating SchedulerBackend \u00b6 createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend ...FIXME createSchedulerBackend is part of the ExternalClusterManager abstraction. Initializing Scheduling Components \u00b6 initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize requests the given TaskSchedulerImpl to initialize with the given SchedulerBackend . initialize is part of the ExternalClusterManager abstraction.","title":"KubernetesClusterManager"},{"location":"kubernetes/KubernetesClusterManager/#kubernetesclustermanager","text":"KubernetesClusterManager is an ExternalClusterManager for k8s master URLs.","title":"KubernetesClusterManager"},{"location":"kubernetes/KubernetesClusterManager/#creating-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler createTaskScheduler creates a TaskSchedulerImpl . createTaskScheduler is part of the ExternalClusterManager abstraction.","title":" Creating TaskScheduler"},{"location":"kubernetes/KubernetesClusterManager/#creating-schedulerbackend","text":"createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend createSchedulerBackend ...FIXME createSchedulerBackend is part of the ExternalClusterManager abstraction.","title":" Creating SchedulerBackend"},{"location":"kubernetes/KubernetesClusterManager/#initializing-scheduling-components","text":"initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit initialize requests the given TaskSchedulerImpl to initialize with the given SchedulerBackend . initialize is part of the ExternalClusterManager abstraction.","title":" Initializing Scheduling Components"},{"location":"kubernetes/KubernetesClusterSchedulerBackend/","text":"KubernetesClusterSchedulerBackend \u00b6 KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend . Creating DriverEndpoint \u00b6 createDriverEndpoint () : DriverEndpoint createDriverEndpoint creates a KubernetesDriverEndpoint . createDriverEndpoint is part of the CoarseGrainedSchedulerBackend abstraction.","title":"KubernetesClusterSchedulerBackend"},{"location":"kubernetes/KubernetesClusterSchedulerBackend/#kubernetesclusterschedulerbackend","text":"KubernetesClusterSchedulerBackend is a CoarseGrainedSchedulerBackend .","title":"KubernetesClusterSchedulerBackend"},{"location":"kubernetes/KubernetesClusterSchedulerBackend/#creating-driverendpoint","text":"createDriverEndpoint () : DriverEndpoint createDriverEndpoint creates a KubernetesDriverEndpoint . createDriverEndpoint is part of the CoarseGrainedSchedulerBackend abstraction.","title":" Creating DriverEndpoint"},{"location":"kubernetes/KubernetesDriverEndpoint/","text":"KubernetesDriverEndpoint \u00b6 KubernetesDriverEndpoint is a DriverEndpoint .","title":"KubernetesDriverEndpoint"},{"location":"kubernetes/KubernetesDriverEndpoint/#kubernetesdriverendpoint","text":"KubernetesDriverEndpoint is a DriverEndpoint .","title":"KubernetesDriverEndpoint"},{"location":"kubernetes/configuration-properties/","text":"Configuration Properties of Kubernetes Cluster Manager \u00b6 spark.kubernetes.submitInDriver \u00b6 (internal) Default: false spark.kubernetes.driver.pod.name \u00b6 Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed using spark-submit in cluster mode spark.kubernetes.executor.podNamePrefix \u00b6 (internal) Prefix to use in front of the executor pod names Default: (undefined) spark.kubernetes.namespace \u00b6 The namespace that will be used for running the driver and executor pods. Default: default spark.kubernetes.executor.podTemplateFile \u00b6 File containing a template pod spec for executors Default: (undefined) spark.kubernetes.executor.podTemplateContainerName \u00b6 Container name to be used as a basis for executors in the given pod template Default: (undefined) spark.kubernetes.authenticate.driver.mounted \u00b6 FIXME spark.kubernetes.driver.master \u00b6 The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc spark.kubernetes.authenticate \u00b6 FIXME spark.kubernetes.executor.eventProcessingInterval \u00b6 Interval between successive inspection of executor events sent from the Kubernetes API Default: 1s spark.kubernetes.executor.deleteOnTermination \u00b6 When disabled ( false ), executor pods will not be deleted in case of failure or normal termination Default: true","title":"Configuration Properties"},{"location":"kubernetes/configuration-properties/#configuration-properties-of-kubernetes-cluster-manager","text":"","title":"Configuration Properties of Kubernetes Cluster Manager"},{"location":"kubernetes/configuration-properties/#sparkkubernetessubmitindriver","text":"(internal) Default: false","title":" spark.kubernetes.submitInDriver"},{"location":"kubernetes/configuration-properties/#sparkkubernetesdriverpodname","text":"Name of the driver pod Default: (undefined) Must be provided if a Spark application is deployed using spark-submit in cluster mode","title":" spark.kubernetes.driver.pod.name"},{"location":"kubernetes/configuration-properties/#sparkkubernetesexecutorpodnameprefix","text":"(internal) Prefix to use in front of the executor pod names Default: (undefined)","title":" spark.kubernetes.executor.podNamePrefix"},{"location":"kubernetes/configuration-properties/#sparkkubernetesnamespace","text":"The namespace that will be used for running the driver and executor pods. Default: default","title":" spark.kubernetes.namespace"},{"location":"kubernetes/configuration-properties/#sparkkubernetesexecutorpodtemplatefile","text":"File containing a template pod spec for executors Default: (undefined)","title":" spark.kubernetes.executor.podTemplateFile"},{"location":"kubernetes/configuration-properties/#sparkkubernetesexecutorpodtemplatecontainername","text":"Container name to be used as a basis for executors in the given pod template Default: (undefined)","title":" spark.kubernetes.executor.podTemplateContainerName"},{"location":"kubernetes/configuration-properties/#sparkkubernetesauthenticatedrivermounted","text":"FIXME","title":" spark.kubernetes.authenticate.driver.mounted"},{"location":"kubernetes/configuration-properties/#sparkkubernetesdrivermaster","text":"The internal Kubernetes master (API server) address to be used for driver to request executors. Default: https://kubernetes.default.svc","title":" spark.kubernetes.driver.master"},{"location":"kubernetes/configuration-properties/#sparkkubernetesauthenticate","text":"FIXME","title":" spark.kubernetes.authenticate"},{"location":"kubernetes/configuration-properties/#sparkkubernetesexecutoreventprocessinginterval","text":"Interval between successive inspection of executor events sent from the Kubernetes API Default: 1s","title":" spark.kubernetes.executor.eventProcessingInterval"},{"location":"kubernetes/configuration-properties/#sparkkubernetesexecutordeleteontermination","text":"When disabled ( false ), executor pods will not be deleted in case of failure or normal termination Default: true","title":" spark.kubernetes.executor.deleteOnTermination"},{"location":"local/","text":"Spark local \u00b6 Spark local is one of the available runtime environments in Apache Spark. It is the only available runtime with no need for a proper cluster manager (and hence many call it a pseudo-cluster , however such concept do exist in Spark and is a bit different). Spark local is used for the following master URLs (as specified using <<../SparkConf.md#, SparkConf.setMaster>> method or <<../configuration-properties.md#spark.master, spark.master>> configuration property): local (with exactly 1 CPU core) local[n] (with exactly n CPU cores) ++local[ ]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) local[n, m] (with exactly n CPU cores and m retries when a task fails) ++local[ , m]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) Internally, Spark local uses < > as the <<../SchedulerBackend.md#, SchedulerBackend>> and executor:ExecutorBackend.md[]. .Architecture of Spark local image::../diagrams/spark-local-architecture.png[align=\"center\"] In this non-distributed multi-threaded runtime environment, Spark spawns all the main execution components - the spark-driver.md[driver] and an executor:Executor.md[] - in the same single JVM. The default parallelism is the number of threads as specified in the < >. This is the only mode where a driver is used for execution (as it acts both as the driver and the only executor). The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications. This mode of operation is also called http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) a local version of Spark . SparkContext.isLocal returns true when Spark runs in local mode. scala> sc.isLocal res0: Boolean = true spark-shell.md[Spark shell] defaults to local mode with local[*] as the spark-deployment-environments.md#master-urls[the master URL]. scala> sc.master res0: String = local[*] Tasks are not re-executed on failure in local mode (unless < > is used). The scheduler:TaskScheduler.md[task scheduler] in local mode works with local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend] task scheduler backend. == [[masterURL]] Master URL You can run Spark in local mode using local , local[n] or the most general local[*] for spark-deployment-environments.md#master-urls[the master URL]. The URL says how many threads can be used in total: local uses 1 thread only. local[n] uses n threads. local[*] uses as many threads as the number of processors available to the Java virtual machine (it uses https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--[Runtime.getRuntime.availableProcessors ()] to know the number). NOTE: What happens when there are less cores than n in local[n] master URL? \"Breaks\" scheduling as Spark assumes more CPU cores available to execute tasks. [[local-with-retries]] local[N, maxFailures] (called local-with-retries ) with N being * or the number of threads to use (as explained above) and maxFailures being the value of <<../configuration-properties.md#spark.task.maxFailures, spark.task.maxFailures>> configuration property. == [[task-submission]] Task Submission a.k.a. reviveOffers .TaskSchedulerImpl.submitTasks in local mode image::taskscheduler-submitTasks-local-mode.png[align=\"center\"] When ReviveOffers or StatusUpdate messages are received, local/spark-LocalEndpoint.md[LocalEndpoint] places an offer to TaskSchedulerImpl (using TaskSchedulerImpl.resourceOffers ). If there is one or more tasks that match the offer, they are launched (using executor.launchTask method). The number of tasks to be launched is controlled by the number of threads as specified in < >. The executor uses threads to spawn the tasks.","title":"Spark Local"},{"location":"local/#spark-local","text":"Spark local is one of the available runtime environments in Apache Spark. It is the only available runtime with no need for a proper cluster manager (and hence many call it a pseudo-cluster , however such concept do exist in Spark and is a bit different). Spark local is used for the following master URLs (as specified using <<../SparkConf.md#, SparkConf.setMaster>> method or <<../configuration-properties.md#spark.master, spark.master>> configuration property): local (with exactly 1 CPU core) local[n] (with exactly n CPU cores) ++local[ ]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) local[n, m] (with exactly n CPU cores and m retries when a task fails) ++local[ , m]++* (with the total number of CPU cores that is the number of available CPU cores on the local machine) Internally, Spark local uses < > as the <<../SchedulerBackend.md#, SchedulerBackend>> and executor:ExecutorBackend.md[]. .Architecture of Spark local image::../diagrams/spark-local-architecture.png[align=\"center\"] In this non-distributed multi-threaded runtime environment, Spark spawns all the main execution components - the spark-driver.md[driver] and an executor:Executor.md[] - in the same single JVM. The default parallelism is the number of threads as specified in the < >. This is the only mode where a driver is used for execution (as it acts both as the driver and the only executor). The local mode is very convenient for testing, debugging or demonstration purposes as it requires no earlier setup to launch Spark applications. This mode of operation is also called http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark[Spark in-process] or (less commonly) a local version of Spark . SparkContext.isLocal returns true when Spark runs in local mode. scala> sc.isLocal res0: Boolean = true spark-shell.md[Spark shell] defaults to local mode with local[*] as the spark-deployment-environments.md#master-urls[the master URL]. scala> sc.master res0: String = local[*] Tasks are not re-executed on failure in local mode (unless < > is used). The scheduler:TaskScheduler.md[task scheduler] in local mode works with local/spark-LocalSchedulerBackend.md[LocalSchedulerBackend] task scheduler backend. == [[masterURL]] Master URL You can run Spark in local mode using local , local[n] or the most general local[*] for spark-deployment-environments.md#master-urls[the master URL]. The URL says how many threads can be used in total: local uses 1 thread only. local[n] uses n threads. local[*] uses as many threads as the number of processors available to the Java virtual machine (it uses https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--[Runtime.getRuntime.availableProcessors ()] to know the number). NOTE: What happens when there are less cores than n in local[n] master URL? \"Breaks\" scheduling as Spark assumes more CPU cores available to execute tasks. [[local-with-retries]] local[N, maxFailures] (called local-with-retries ) with N being * or the number of threads to use (as explained above) and maxFailures being the value of <<../configuration-properties.md#spark.task.maxFailures, spark.task.maxFailures>> configuration property. == [[task-submission]] Task Submission a.k.a. reviveOffers .TaskSchedulerImpl.submitTasks in local mode image::taskscheduler-submitTasks-local-mode.png[align=\"center\"] When ReviveOffers or StatusUpdate messages are received, local/spark-LocalEndpoint.md[LocalEndpoint] places an offer to TaskSchedulerImpl (using TaskSchedulerImpl.resourceOffers ). If there is one or more tasks that match the offer, they are launched (using executor.launchTask method). The number of tasks to be launched is controlled by the number of threads as specified in < >. The executor uses threads to spawn the tasks.","title":"Spark local"},{"location":"local/LauncherBackend/","text":"== [[LauncherBackend]] LauncherBackend LauncherBackend is the < > of < > that can < >. [[contract]] .LauncherBackend Contract (Abstract Methods Only) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | conf a| [[conf]] [source, scala] \u00b6 conf: SparkConf \u00b6 ROOT:SparkConf.md[] Used exclusively when LauncherBackend is requested to < > (to access ROOT:configuration-properties.md#spark.launcher.port[spark.launcher.port] and ROOT:configuration-properties.md#spark.launcher.secret[spark.launcher.secret] configuration properties) | onStopRequest a| [[onStopRequest]] [source, scala] \u00b6 onStopRequest(): Unit \u00b6 Handles stop requests (to stop the Spark application as gracefully as possible) Used exclusively when LauncherBackend is requested to < > |=== [[creating-instance]] LauncherBackend takes no arguments to be created. NOTE: LauncherBackend is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. [[internal-registries]] .LauncherBackend's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | _isConnected a| [[_isConnected]][[isConnected]] Flag that says whether...FIXME ( true ) or not ( false ) Default: false Used when...FIXME | clientThread a| [[clientThread]] Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.lang.Thread ] Used when...FIXME | connection a| [[connection]] BackendConnection Used when...FIXME | lastState a| [[lastState]] SparkAppHandle.State Used when...FIXME |=== [[implementations]] LauncherBackend is < > (as an anonymous class) for the following: Spark on YARN's < > Spark local's < > Spark on Mesos' < > Spark Standalone's < > === [[close]] Closing -- close Method [source, scala] \u00b6 close(): Unit \u00b6 close ...FIXME NOTE: close is used when...FIXME === [[connect]] Connecting -- connect Method [source, scala] \u00b6 connect(): Unit \u00b6 connect ...FIXME [NOTE] \u00b6 connect is used when: Spark Standalone's StandaloneSchedulerBackend is requested to < > (in client deploy mode) Spark local's LocalSchedulerBackend is < > Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to < > (in client deploy mode) * Spark on YARN's Client is requested to < > \u00b6 === [[fireStopRequest]] fireStopRequest Internal Method [source, scala] \u00b6 fireStopRequest(): Unit \u00b6 fireStopRequest ...FIXME NOTE: fireStopRequest is used exclusively when BackendConnection is requested to handle a Stop message. === [[onDisconnected]] Handling Disconnects From Scheduling Backend -- onDisconnected Method [source, scala] \u00b6 onDisconnected(): Unit \u00b6 onDisconnected does nothing by default and is expected to be overriden by < >. NOTE: onDisconnected is used when...FIXME === [[setAppId]] setAppId Method [source, scala] \u00b6 setAppId(appId: String): Unit \u00b6 setAppId ...FIXME NOTE: setAppId is used when...FIXME === [[setState]] setState Method [source, scala] \u00b6 setState(state: SparkAppHandle.State): Unit \u00b6 setState ...FIXME NOTE: setState is used when...FIXME","title":"LauncherBackend"},{"location":"local/LauncherBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#conf-sparkconf","text":"ROOT:SparkConf.md[] Used exclusively when LauncherBackend is requested to < > (to access ROOT:configuration-properties.md#spark.launcher.port[spark.launcher.port] and ROOT:configuration-properties.md#spark.launcher.secret[spark.launcher.secret] configuration properties) | onStopRequest a| [[onStopRequest]]","title":"conf: SparkConf"},{"location":"local/LauncherBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#onstoprequest-unit","text":"Handles stop requests (to stop the Spark application as gracefully as possible) Used exclusively when LauncherBackend is requested to < > |=== [[creating-instance]] LauncherBackend takes no arguments to be created. NOTE: LauncherBackend is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. [[internal-registries]] .LauncherBackend's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | _isConnected a| [[_isConnected]][[isConnected]] Flag that says whether...FIXME ( true ) or not ( false ) Default: false Used when...FIXME | clientThread a| [[clientThread]] Java's https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.lang.Thread ] Used when...FIXME | connection a| [[connection]] BackendConnection Used when...FIXME | lastState a| [[lastState]] SparkAppHandle.State Used when...FIXME |=== [[implementations]] LauncherBackend is < > (as an anonymous class) for the following: Spark on YARN's < > Spark local's < > Spark on Mesos' < > Spark Standalone's < > === [[close]] Closing -- close Method","title":"onStopRequest(): Unit"},{"location":"local/LauncherBackend/#source-scala_2","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#close-unit","text":"close ...FIXME NOTE: close is used when...FIXME === [[connect]] Connecting -- connect Method","title":"close(): Unit"},{"location":"local/LauncherBackend/#source-scala_3","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#connect-unit","text":"connect ...FIXME","title":"connect(): Unit"},{"location":"local/LauncherBackend/#note","text":"connect is used when: Spark Standalone's StandaloneSchedulerBackend is requested to < > (in client deploy mode) Spark local's LocalSchedulerBackend is < > Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to < > (in client deploy mode)","title":"[NOTE]"},{"location":"local/LauncherBackend/#spark-on-yarns-client-is-requested-to","text":"=== [[fireStopRequest]] fireStopRequest Internal Method","title":"* Spark on YARN's Client is requested to &lt;&gt;"},{"location":"local/LauncherBackend/#source-scala_4","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#firestoprequest-unit","text":"fireStopRequest ...FIXME NOTE: fireStopRequest is used exclusively when BackendConnection is requested to handle a Stop message. === [[onDisconnected]] Handling Disconnects From Scheduling Backend -- onDisconnected Method","title":"fireStopRequest(): Unit"},{"location":"local/LauncherBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#ondisconnected-unit","text":"onDisconnected does nothing by default and is expected to be overriden by < >. NOTE: onDisconnected is used when...FIXME === [[setAppId]] setAppId Method","title":"onDisconnected(): Unit"},{"location":"local/LauncherBackend/#source-scala_6","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#setappidappid-string-unit","text":"setAppId ...FIXME NOTE: setAppId is used when...FIXME === [[setState]] setState Method","title":"setAppId(appId: String): Unit"},{"location":"local/LauncherBackend/#source-scala_7","text":"","title":"[source, scala]"},{"location":"local/LauncherBackend/#setstatestate-sparkapphandlestate-unit","text":"setState ...FIXME NOTE: setState is used when...FIXME","title":"setState(state: SparkAppHandle.State): Unit"},{"location":"local/LocalEndpoint/","text":"== [[LocalEndpoint]] LocalEndpoint -- RPC Endpoint for LocalSchedulerBackend LocalEndpoint is the <<../index.md#ThreadSafeRpcEndpoint, ThreadSafeRpcEndpoint>> for < > and is registered under the LocalSchedulerBackendEndpoint name. LocalEndpoint is < > exclusively when LocalSchedulerBackend is requested to < >. Put simply, LocalEndpoint is the communication channel between < > and < >. LocalEndpoint is a (thread-safe) rpc:RpcEndpoint.md[RpcEndpoint] that hosts an < > (with driver ID and localhost hostname) for Spark local mode. [[messages]] .LocalEndpoint's RPC Messages [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Message | Description | < > | Requests the < > to executor:Executor.md#killTask[kill a given task] | < > | Calls < > < > | < > | Requests the < > to executor:Executor.md#stop[stop] |=== When a LocalEndpoint starts up (as part of Spark local's initialization) it prints out the following INFO messages to the logs: INFO Executor: Starting executor ID driver on host localhost INFO Executor: Using REPL class URI: http://192.168.1.4:56131 [[executor]] LocalEndpoint creates a single executor:Executor.md[] with the following properties: [[localExecutorId]] driver ID for the executor:Executor.md#executorId[executor ID] [[localExecutorHostname]] localhost for the executor:Executor.md#executorHostname[hostname] < > for the executor:Executor.md#userClassPath[user-defined CLASSPATH] executor:Executor.md#isLocal[isLocal] flag enabled The < > is then used when LocalEndpoint is requested to handle < > and < > RPC messages, and < >. [[internal-registries]] .LocalEndpoint's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | freeCores a| [[freeCores]] The number of CPU cores that are free to use (to schedule tasks) Default: Initial < > (aka totalCores ) Increments when LocalEndpoint is requested to handle < > RPC message with a finished state Decrements when LocalEndpoint is requested to < > and there were tasks to execute NOTE: A single task to execute costs scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ). Used when LocalEndpoint is requested to < > |=== [[logging]] [TIP] ==== Enable INFO logging level for org.apache.spark.scheduler.local.LocalEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.local.LocalEndpoint=INFO Refer to <<../spark-logging.md#, Logging>>. \u00b6 === [[creating-instance]] Creating LocalEndpoint Instance LocalEndpoint takes the following to be created: [[rpcEnv]] <<../index.md#, RpcEnv>> [[userClassPath]] User-defined class path ( Seq[URL] ) that is the < > configuration property and is used exclusively to create the < > [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[executorBackend]] < > [[totalCores]] Number of CPU cores (aka totalCores ) LocalEndpoint initializes the < >. === [[receive]] Processing Receive-Only RPC Messages -- receive Method [source, scala] \u00b6 receive: PartialFunction[Any, Unit] \u00b6 NOTE: receive is part of the rpc:RpcEndpoint.md#receive[RpcEndpoint] abstraction. receive handles ( processes ) < >, < >, and < > RPC messages. ==== [[ReviveOffers]] ReviveOffers RPC Message [source, scala] \u00b6 ReviveOffers() \u00b6 When < >, LocalEndpoint < >. NOTE: ReviveOffers RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[StatusUpdate]] StatusUpdate RPC Message [source, scala] \u00b6 StatusUpdate( taskId: Long, state: TaskState, serializedData: ByteBuffer) When < >, LocalEndpoint requests the < > to scheduler:TaskSchedulerImpl.md#statusUpdate[handle a task status update] (given the taskId , the task state and the data). If the given scheduler:Task.md#TaskState[TaskState] is a finished state (one of FINISHED , FAILED , KILLED , LOST states), LocalEndpoint adds scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ) to the < > registry followed by < >. NOTE: StatusUpdate RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[KillTask]] KillTask RPC Message [source, scala] \u00b6 KillTask( taskId: Long, interruptThread: Boolean, reason: String) When < >, LocalEndpoint requests the single < > to executor:Executor.md#killTask[kill a task] (given the taskId , the interruptThread flag and the reason). NOTE: KillTask RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. === [[reviveOffers]] Reviving Offers -- reviveOffers Method [source, scala] \u00b6 reviveOffers(): Unit \u00b6 reviveOffers ...FIXME NOTE: reviveOffers is used when LocalEndpoint is requested to < > (namely < > and < >). === [[receiveAndReply]] Processing Receive-Reply RPC Messages -- receiveAndReply Method [source, scala] \u00b6 receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] \u00b6 NOTE: receiveAndReply is part of the rpc:RpcEndpoint.md#receiveAndReply[RpcEndpoint] abstraction. receiveAndReply handles ( processes ) < > RPC message exclusively. ==== [[StopExecutor]] StopExecutor RPC Message [source, scala] \u00b6 StopExecutor() \u00b6 When < >, LocalEndpoint requests the single < > to executor:Executor.md#stop[stop] and requests the given RpcCallContext to reply with true (as the response). NOTE: StopExecutor RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >.","title":"LocalEndpoint"},{"location":"local/LocalEndpoint/#refer-to-spark-loggingmd-logging","text":"=== [[creating-instance]] Creating LocalEndpoint Instance LocalEndpoint takes the following to be created: [[rpcEnv]] <<../index.md#, RpcEnv>> [[userClassPath]] User-defined class path ( Seq[URL] ) that is the < > configuration property and is used exclusively to create the < > [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[executorBackend]] < > [[totalCores]] Number of CPU cores (aka totalCores ) LocalEndpoint initializes the < >. === [[receive]] Processing Receive-Only RPC Messages -- receive Method","title":"Refer to &lt;&lt;../spark-logging.md#, Logging&gt;&gt;."},{"location":"local/LocalEndpoint/#source-scala","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#receive-partialfunctionany-unit","text":"NOTE: receive is part of the rpc:RpcEndpoint.md#receive[RpcEndpoint] abstraction. receive handles ( processes ) < >, < >, and < > RPC messages. ==== [[ReviveOffers]] ReviveOffers RPC Message","title":"receive: PartialFunction[Any, Unit]"},{"location":"local/LocalEndpoint/#source-scala_1","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#reviveoffers","text":"When < >, LocalEndpoint < >. NOTE: ReviveOffers RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[StatusUpdate]] StatusUpdate RPC Message","title":"ReviveOffers()"},{"location":"local/LocalEndpoint/#source-scala_2","text":"StatusUpdate( taskId: Long, state: TaskState, serializedData: ByteBuffer) When < >, LocalEndpoint requests the < > to scheduler:TaskSchedulerImpl.md#statusUpdate[handle a task status update] (given the taskId , the task state and the data). If the given scheduler:Task.md#TaskState[TaskState] is a finished state (one of FINISHED , FAILED , KILLED , LOST states), LocalEndpoint adds scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ) to the < > registry followed by < >. NOTE: StatusUpdate RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. ==== [[KillTask]] KillTask RPC Message","title":"[source, scala]"},{"location":"local/LocalEndpoint/#source-scala_3","text":"KillTask( taskId: Long, interruptThread: Boolean, reason: String) When < >, LocalEndpoint requests the single < > to executor:Executor.md#killTask[kill a task] (given the taskId , the interruptThread flag and the reason). NOTE: KillTask RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >. === [[reviveOffers]] Reviving Offers -- reviveOffers Method","title":"[source, scala]"},{"location":"local/LocalEndpoint/#source-scala_4","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#reviveoffers-unit","text":"reviveOffers ...FIXME NOTE: reviveOffers is used when LocalEndpoint is requested to < > (namely < > and < >). === [[receiveAndReply]] Processing Receive-Reply RPC Messages -- receiveAndReply Method","title":"reviveOffers(): Unit"},{"location":"local/LocalEndpoint/#source-scala_5","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#receiveandreplycontext-rpccallcontext-partialfunctionany-unit","text":"NOTE: receiveAndReply is part of the rpc:RpcEndpoint.md#receiveAndReply[RpcEndpoint] abstraction. receiveAndReply handles ( processes ) < > RPC message exclusively. ==== [[StopExecutor]] StopExecutor RPC Message","title":"receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit]"},{"location":"local/LocalEndpoint/#source-scala_6","text":"","title":"[source, scala]"},{"location":"local/LocalEndpoint/#stopexecutor","text":"When < >, LocalEndpoint requests the single < > to executor:Executor.md#stop[stop] and requests the given RpcCallContext to reply with true (as the response). NOTE: StopExecutor RPC message is sent out exclusively when LocalSchedulerBackend is requested to < >.","title":"StopExecutor()"},{"location":"local/LocalSchedulerBackend/","text":"= LocalSchedulerBackend LocalSchedulerBackend is a <<../SchedulerBackend.md#, SchedulerBackend>> and an executor:ExecutorBackend.md[] for the < >. LocalSchedulerBackend is < > when SparkContext is requested to ROOT:SparkContext.md#createTaskScheduler[create the SchedulerBackend with the TaskScheduler] for the following master URLs: local (with exactly < >) local[n] (with exactly < >) ++local[ ]++* (with the < > that is the number of available CPU cores on the local machine) local[n, m] (with exactly < >) ++local[ , m]++* (with the < > that is the number of available CPU cores on the local machine) While being < >, LocalSchedulerBackend requests the < > to <<../spark-LauncherBackend.md#connect, connect>>. When an executor sends task status updates (using ExecutorBackend.statusUpdate ), they are passed along as < > to < >. .Task status updates flow in local mode image::LocalSchedulerBackend-LocalEndpoint-Executor-task-status-updates.png[align=\"center\"] [[appId]] [[applicationId]] When requested for the <<../SchedulerBackend.md#applicationId, applicationId>>, LocalSchedulerBackend uses local-[currentTimeMillis] . [[maxNumConcurrentTasks]] When requested for the <<../SchedulerBackend.md#maxNumConcurrentTasks, maxNumConcurrentTasks>>, LocalSchedulerBackend simply divides the < > by scheduler:TaskSchedulerImpl.md#CPUS_PER_TASK[spark.task.cpus] configuration (default: 1 ). [[defaultParallelism]] When requested for the <<../SchedulerBackend.md#defaultParallelism, defaultParallelism>>, LocalSchedulerBackend uses <<../configuration-properties.md#spark.default.parallelism, spark.default.parallelism>> configuration (if defined) or the < >. [[userClassPath]] When < >, LocalSchedulerBackend < > the <<../configuration-properties.md#spark.executor.extraClassPath, spark.executor.extraClassPath>> configuration property (in the given < >) for the user-defined class path for executors that is used exclusively when LocalSchedulerBackend is requested to < > (and creates a < > that in turn uses it to create the one < >). [[creating-instance]] LocalSchedulerBackend takes the following to be created: [[conf]] <<../SparkConf.md#, SparkConf>> [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[totalCores]] Total number of CPU cores (aka totalCores ) [[internal-registries]] .LocalSchedulerBackend's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | localEndpoint a| [[localEndpoint]] rpc:RpcEndpointRef.md[RpcEndpointRef] to LocalSchedulerBackendEndpoint RPC endpoint (that is < > which LocalSchedulerBackend registers when < >) Used when LocalSchedulerBackend is requested for the following: < > (and sends a < > one-way asynchronous message) < > (and sends a < > one-way asynchronous message) < > (and sends a < > one-way asynchronous message) < > (and sends a < > asynchronous message) | launcherBackend a| [[launcherBackend]] <<../spark-LauncherBackend.md#, LauncherBackend>> Used when LocalSchedulerBackend is < >, < > and < > | listenerBus a| [[listenerBus]] scheduler:LiveListenerBus.md[] that is used exclusively when LocalSchedulerBackend is requested to < > |=== [[logging]] [TIP] ==== Enable INFO logging level for org.apache.spark.scheduler.local.LocalSchedulerBackend logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.local.LocalSchedulerBackend=INFO Refer to <<../spark-logging.md#, Logging>>. \u00b6 == [[start]] Starting Scheduling Backend -- start Method [source, scala] \u00b6 start(): Unit \u00b6 NOTE: start is part of the <<../SchedulerBackend.md#start, SchedulerBackend Contract>> to start the scheduling backend. start requests the SparkEnv object for the current core:SparkEnv.md#rpcEnv[RpcEnv]. start then creates a < > and requests the RpcEnv to rpc:RpcEnv.md#setupEndpoint[register it] as LocalSchedulerBackendEndpoint RPC endpoint. start requests the < > to scheduler:LiveListenerBus.md#post[post] a ROOT:SparkListener.md#SparkListenerExecutorAdded[SparkListenerExecutorAdded] event. In the end, start requests the < > to <<../spark-LauncherBackend.md#setAppId, setAppId>> as the < > and <<../spark-LauncherBackend.md#setState, setState>> as RUNNING . == [[reviveOffers]] reviveOffers Method [source, scala] \u00b6 reviveOffers(): Unit \u00b6 NOTE: reviveOffers is part of the <<../SchedulerBackend.md#reviveOffers, SchedulerBackend Contract>> to...FIXME. reviveOffers ...FIXME == [[killTask]] killTask Method [source, scala] \u00b6 killTask( taskId: Long, executorId: String, interruptThread: Boolean, reason: String): Unit NOTE: killTask is part of the <<../SchedulerBackend.md#killTask, SchedulerBackend Contract>> to kill a task. killTask ...FIXME == [[statusUpdate]] statusUpdate Method [source, scala] \u00b6 statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit NOTE: statusUpdate is part of the executor:ExecutorBackend.md#statusUpdate[ExecutorBackend] abstraction. statusUpdate ...FIXME == [[stop]] Stopping Scheduling Backend -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the <<../SchedulerBackend.md#stop, SchedulerBackend Contract>> to stop a scheduling backend. stop ...FIXME == [[getUserClasspath]] User-Defined Class Path for Executors -- getUserClasspath Method [source, scala] \u00b6 getUserClasspath(conf: SparkConf): Seq[URL] \u00b6 getUserClasspath simply requests the given SparkConf for the <<../configuration-properties.md#spark.executor.extraClassPath, spark.executor.extraClassPath>> configuration property and converts the entries (separated by the system-dependent path separator) to URLs. NOTE: getUserClasspath is used exclusively when LocalSchedulerBackend is < >.","title":"LocalSchedulerBackend"},{"location":"local/LocalSchedulerBackend/#refer-to-spark-loggingmd-logging","text":"== [[start]] Starting Scheduling Backend -- start Method","title":"Refer to &lt;&lt;../spark-logging.md#, Logging&gt;&gt;."},{"location":"local/LocalSchedulerBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#start-unit","text":"NOTE: start is part of the <<../SchedulerBackend.md#start, SchedulerBackend Contract>> to start the scheduling backend. start requests the SparkEnv object for the current core:SparkEnv.md#rpcEnv[RpcEnv]. start then creates a < > and requests the RpcEnv to rpc:RpcEnv.md#setupEndpoint[register it] as LocalSchedulerBackendEndpoint RPC endpoint. start requests the < > to scheduler:LiveListenerBus.md#post[post] a ROOT:SparkListener.md#SparkListenerExecutorAdded[SparkListenerExecutorAdded] event. In the end, start requests the < > to <<../spark-LauncherBackend.md#setAppId, setAppId>> as the < > and <<../spark-LauncherBackend.md#setState, setState>> as RUNNING . == [[reviveOffers]] reviveOffers Method","title":"start(): Unit"},{"location":"local/LocalSchedulerBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#reviveoffers-unit","text":"NOTE: reviveOffers is part of the <<../SchedulerBackend.md#reviveOffers, SchedulerBackend Contract>> to...FIXME. reviveOffers ...FIXME == [[killTask]] killTask Method","title":"reviveOffers(): Unit"},{"location":"local/LocalSchedulerBackend/#source-scala_2","text":"killTask( taskId: Long, executorId: String, interruptThread: Boolean, reason: String): Unit NOTE: killTask is part of the <<../SchedulerBackend.md#killTask, SchedulerBackend Contract>> to kill a task. killTask ...FIXME == [[statusUpdate]] statusUpdate Method","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#source-scala_3","text":"statusUpdate( taskId: Long, state: TaskState, data: ByteBuffer): Unit NOTE: statusUpdate is part of the executor:ExecutorBackend.md#statusUpdate[ExecutorBackend] abstraction. statusUpdate ...FIXME == [[stop]] Stopping Scheduling Backend -- stop Method","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#source-scala_4","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#stop-unit","text":"NOTE: stop is part of the <<../SchedulerBackend.md#stop, SchedulerBackend Contract>> to stop a scheduling backend. stop ...FIXME == [[getUserClasspath]] User-Defined Class Path for Executors -- getUserClasspath Method","title":"stop(): Unit"},{"location":"local/LocalSchedulerBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"local/LocalSchedulerBackend/#getuserclasspathconf-sparkconf-sequrl","text":"getUserClasspath simply requests the given SparkConf for the <<../configuration-properties.md#spark.executor.extraClassPath, spark.executor.extraClassPath>> configuration property and converts the entries (separated by the system-dependent path separator) to URLs. NOTE: getUserClasspath is used exclusively when LocalSchedulerBackend is < >.","title":"getUserClasspath(conf: SparkConf): Seq[URL]"},{"location":"memory/","text":"Memory System \u00b6 Memory System is a core component of Apache Spark that is based on UnifiedMemoryManager . Resources \u00b6 SPARK-10000: Consolidate storage and execution memory management Videos \u00b6 Deep Dive: Apache Spark Memory Management Deep Dive into Project Tungsten Spark Performance: What's Next","title":"Memory System"},{"location":"memory/#memory-system","text":"Memory System is a core component of Apache Spark that is based on UnifiedMemoryManager .","title":"Memory System"},{"location":"memory/#resources","text":"SPARK-10000: Consolidate storage and execution memory management","title":"Resources"},{"location":"memory/#videos","text":"Deep Dive: Apache Spark Memory Management Deep Dive into Project Tungsten Spark Performance: What's Next","title":"Videos"},{"location":"memory/BytesToBytesMap/","text":"= [[BytesToBytesMap]] BytesToBytesMap BytesToBytesMap is a MemoryConsumer.md[MemoryConsumer]. BytesToBytesMap is used to create Spark SQL's UnsafeKVExternalSorter and UnsafeHashedRelation. == [[creating-instance]] Creating Instance BytesToBytesMap takes the following to be created: [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] [[blockManager]] storage:BlockManager.md[BlockManager] < > [[initialCapacity]] Initial capacity [[loadFactor]] Load factor (default: 0.5) [[pageSizeBytes]] Page size (in bytes) [[enablePerfMetrics]] enablePerfMetrics flag BytesToBytesMap is created for Spark SQL's UnsafeFixedWidthAggregationMap and UnsafeHashedRelation. == [[serializerManager]] SerializerManager BytesToBytesMap is given a serializer:SerializerManager.md[SerializerManager] when < >. BytesToBytesMap uses the SerializerManager when (MapIterator is) requested to advanceToNextPage (to request UnsafeSorterSpillWriter for a memory:UnsafeSorterSpillWriter.md#getReader[UnsafeSorterSpillReader]). == [[MAX_CAPACITY]] Maximum Supported Capacity BytesToBytesMap supports up to 1 << 29 keys. == [[spillWriters]] UnsafeSorterSpillWriters BytesToBytesMap manages UnsafeSorterSpillWriter.md[UnsafeSorterSpillWriters]. BytesToBytesMap registers a new UnsafeSorterSpillWriter when requested to < >. BytesToBytesMap uses the UnsafeSorterSpillWriters when: < > FIXME == [[destructiveIterator]] MapIterator BytesToBytesMap manages a \"destructive\" MapIterator. BytesToBytesMap creates it when requested for < >. BytesToBytesMap requests it to spill when requested to < >. == [[destructiveIterator]] Creating Destructive MapIterator [source, java] \u00b6 MapIterator destructiveIterator() \u00b6 destructiveIterator < > and creates a MapIterator (with the < > and < >). destructiveIterator is used when Spark SQL's UnsafeFixedWidthAggregationMap is requested for a key-value iterator. == [[allocate]] Allocating [source, java] \u00b6 void allocate( int capacity) allocate uses the input capacity to compute a number that is a power of 2 and greater or equal than capacity, but not greater than < >. The computed number is at least 64. [source,scala] \u00b6 def _c(capacity: Int) = { import org.apache.spark.unsafe.array.ByteArrayMethods val MAX_CAPACITY = (1 << 29) Math.max(Math.min(MAX_CAPACITY, ByteArrayMethods.nextPowerOf2(capacity)), 64) } allocate MemoryConsumer.md#allocateArray[allocates an array] twice as big as the power-of-two capacity and fills it all with 0s. allocate initializes the < > and < > internal properties. allocate requires that the input capacity is positive. allocate is used when...FIXME == [[spill]] Spilling [source, java] \u00b6 long spill( long size, MemoryConsumer trigger) NOTE: spill is part of the memory:MemoryConsumer.md#spill[MemoryConsumer] abstraction. spill requests the < > to spill when the given MemoryConsumer is not this BytesToBytesMap and the MapIterator is available. == [[free]] Freeing Up Allocated Memory [source, java] \u00b6 void free() \u00b6 free...FIXME free is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | growthThreshold a| [[growthThreshold]] Growth threshold | mask a| [[mask]] Mask for truncating hashcodes so that they do not exceed the long array's size |===","title":"BytesToBytesMap"},{"location":"memory/BytesToBytesMap/#source-java","text":"","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#mapiterator-destructiveiterator","text":"destructiveIterator < > and creates a MapIterator (with the < > and < >). destructiveIterator is used when Spark SQL's UnsafeFixedWidthAggregationMap is requested for a key-value iterator. == [[allocate]] Allocating","title":"MapIterator destructiveIterator()"},{"location":"memory/BytesToBytesMap/#source-java_1","text":"void allocate( int capacity) allocate uses the input capacity to compute a number that is a power of 2 and greater or equal than capacity, but not greater than < >. The computed number is at least 64.","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#sourcescala","text":"def _c(capacity: Int) = { import org.apache.spark.unsafe.array.ByteArrayMethods val MAX_CAPACITY = (1 << 29) Math.max(Math.min(MAX_CAPACITY, ByteArrayMethods.nextPowerOf2(capacity)), 64) } allocate MemoryConsumer.md#allocateArray[allocates an array] twice as big as the power-of-two capacity and fills it all with 0s. allocate initializes the < > and < > internal properties. allocate requires that the input capacity is positive. allocate is used when...FIXME == [[spill]] Spilling","title":"[source,scala]"},{"location":"memory/BytesToBytesMap/#source-java_2","text":"long spill( long size, MemoryConsumer trigger) NOTE: spill is part of the memory:MemoryConsumer.md#spill[MemoryConsumer] abstraction. spill requests the < > to spill when the given MemoryConsumer is not this BytesToBytesMap and the MapIterator is available. == [[free]] Freeing Up Allocated Memory","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#source-java_3","text":"","title":"[source, java]"},{"location":"memory/BytesToBytesMap/#void-free","text":"free...FIXME free is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | growthThreshold a| [[growthThreshold]] Growth threshold | mask a| [[mask]] Mask for truncating hashcodes so that they do not exceed the long array's size |===","title":"void free()"},{"location":"memory/ExecutionMemoryPool/","text":"= [[ExecutionMemoryPool]] ExecutionMemoryPool ExecutionMemoryPool is a memory:MemoryPool.md[MemoryPool].","title":"ExecutionMemoryPool"},{"location":"memory/MemoryConsumer/","text":"= [[MemoryConsumer]] MemoryConsumer MemoryConsumer is an abstraction of < > of memory:TaskMemoryManager.md#consumers[TaskMemoryManager]. MemoryConsumers correspond to individual operators and data structures within a task. The TaskMemoryManager receives memory allocation requests from MemoryConsumers and issues callbacks to consumers in order to trigger < > when running low on memory. A MemoryConsumer basically tracks < >. Creating a MemoryConsumer requires a TaskMemoryManager.md[TaskMemoryManager] with optional pageSize and a MemoryMode . == [[creating-instance]] Creating Instance MemoryConsumer takes the following to be created: [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] [[pageSize]] Page size (in bytes) [[mode]] MemoryMode MemoryConsumer initializes the < >. == [[extensions]] Extensions .MemoryConsumers (Direct Implementations and Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryConsumer | Description | BytesToBytesMap.md[BytesToBytesMap] | [[BytesToBytesMap]] Used in Spark SQL | HybridRowQueue | [[HybridRowQueue]] | LongToUnsafeRowMap | [[LongToUnsafeRowMap]] | RowBasedKeyValueBatch | [[RowBasedKeyValueBatch]] | ShuffleExternalSorter | [[ShuffleExternalSorter]] | shuffle:Spillable.md[Spillable] | [[Spillable]] | UnsafeExternalSorter | [[UnsafeExternalSorter]] |=== == [[contract]][[spill]] Spilling [source, java] \u00b6 long spill( long size, MemoryConsumer trigger) CAUTION: FIXME NOTE: spill is used when TaskMemoryManager.md#acquireExecutionMemory[ TaskMemoryManager forces MemoryConsumers to release memory when requested to acquire execution memory] == [[used]][[getUsed]] Memory Allocated (Used) used is the amount of memory in use (i.e. allocated) by the MemoryConsumer. == [[freePage]] Deallocate MemoryBlock [source, java] \u00b6 void freePage( MemoryBlock page) freePage is a protected method to deallocate the MemoryBlock . Internally, it decrements < > registry by the size of page and TaskMemoryManager.md#freePage[frees the page]. == [[allocateArray]] Allocating Array [source, java] \u00b6 LongArray allocateArray( long size) allocateArray allocates LongArray of size length. Internally, it TaskMemoryManager.md#allocatePage[allocates a page] for the requested size . The size is recorded in the internal < > counter. However, if it was not possible to allocate the size memory, it TaskMemoryManager.md#showMemoryUsage[shows the current memory usage] and a OutOfMemoryError is thrown. Unable to acquire [required] bytes of memory, got [got] allocateArray is used when: BytesToBytesMap is requested to memory:BytesToBytesMap.md#allocate[allocate] ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#growPointerArrayIfNecessary[growPointerArrayIfNecessary] ShuffleInMemorySorter is shuffle:ShuffleInMemorySorter.md[created] and shuffle:ShuffleInMemorySorter.md#reset[reset] UnsafeExternalSorter is requested to UnsafeExternalSorter.md#growPointerArrayIfNecessary[growPointerArrayIfNecessary] UnsafeInMemorySorter is UnsafeInMemorySorter.md[created] and UnsafeInMemorySorter.md#reset[reset] Spark SQL's UnsafeKVExternalSorter is created == [[acquireMemory]] Acquiring Execution Memory (Allocating Memory) [source, java] \u00b6 long acquireMemory( long size) acquireMemory requests the < > to memory:TaskMemoryManager.md#acquireExecutionMemory[acquire execution memory] (of the given size). The memory allocated is then added to the < > internal registry. acquireMemory is used when: Spillable is requested to shuffle:Spillable.md#maybeSpill[maybeSpill] Spark SQL's LongToUnsafeRowMap is requested to ensureAcquireMemory == [[allocatePage]] Allocating Memory Block (Page) [source, java] \u00b6 MemoryBlock allocatePage( long required) allocatePage...FIXME allocatePage is used when...FIXME == [[throwOom]] Throwing OutOfMemoryError [source, java] \u00b6 void throwOom( MemoryBlock page, long required) throwOom ...FIXME throwOom is used when MemoryConsumer is requested to < > or a < > and failed to acquire enough.","title":"MemoryConsumer"},{"location":"memory/MemoryConsumer/#source-java","text":"long spill( long size, MemoryConsumer trigger) CAUTION: FIXME NOTE: spill is used when TaskMemoryManager.md#acquireExecutionMemory[ TaskMemoryManager forces MemoryConsumers to release memory when requested to acquire execution memory] == [[used]][[getUsed]] Memory Allocated (Used) used is the amount of memory in use (i.e. allocated) by the MemoryConsumer. == [[freePage]] Deallocate MemoryBlock","title":"[source, java]"},{"location":"memory/MemoryConsumer/#source-java_1","text":"void freePage( MemoryBlock page) freePage is a protected method to deallocate the MemoryBlock . Internally, it decrements < > registry by the size of page and TaskMemoryManager.md#freePage[frees the page]. == [[allocateArray]] Allocating Array","title":"[source, java]"},{"location":"memory/MemoryConsumer/#source-java_2","text":"LongArray allocateArray( long size) allocateArray allocates LongArray of size length. Internally, it TaskMemoryManager.md#allocatePage[allocates a page] for the requested size . The size is recorded in the internal < > counter. However, if it was not possible to allocate the size memory, it TaskMemoryManager.md#showMemoryUsage[shows the current memory usage] and a OutOfMemoryError is thrown. Unable to acquire [required] bytes of memory, got [got] allocateArray is used when: BytesToBytesMap is requested to memory:BytesToBytesMap.md#allocate[allocate] ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#growPointerArrayIfNecessary[growPointerArrayIfNecessary] ShuffleInMemorySorter is shuffle:ShuffleInMemorySorter.md[created] and shuffle:ShuffleInMemorySorter.md#reset[reset] UnsafeExternalSorter is requested to UnsafeExternalSorter.md#growPointerArrayIfNecessary[growPointerArrayIfNecessary] UnsafeInMemorySorter is UnsafeInMemorySorter.md[created] and UnsafeInMemorySorter.md#reset[reset] Spark SQL's UnsafeKVExternalSorter is created == [[acquireMemory]] Acquiring Execution Memory (Allocating Memory)","title":"[source, java]"},{"location":"memory/MemoryConsumer/#source-java_3","text":"long acquireMemory( long size) acquireMemory requests the < > to memory:TaskMemoryManager.md#acquireExecutionMemory[acquire execution memory] (of the given size). The memory allocated is then added to the < > internal registry. acquireMemory is used when: Spillable is requested to shuffle:Spillable.md#maybeSpill[maybeSpill] Spark SQL's LongToUnsafeRowMap is requested to ensureAcquireMemory == [[allocatePage]] Allocating Memory Block (Page)","title":"[source, java]"},{"location":"memory/MemoryConsumer/#source-java_4","text":"MemoryBlock allocatePage( long required) allocatePage...FIXME allocatePage is used when...FIXME == [[throwOom]] Throwing OutOfMemoryError","title":"[source, java]"},{"location":"memory/MemoryConsumer/#source-java_5","text":"void throwOom( MemoryBlock page, long required) throwOom ...FIXME throwOom is used when MemoryConsumer is requested to < > or a < > and failed to acquire enough.","title":"[source, java]"},{"location":"memory/MemoryManager/","text":"MemoryManager \u00b6 MemoryManager is an < > of < > that manage shared memory for task execution (memory:TaskMemoryManager.md#memoryManager[TaskMemoryManager]) and block storage (storage:BlockManager.md#memoryManager[BlockManager]). MemoryManager splits available memory into two regions: Execution memory for computations in shuffles, joins, sorts and aggregations Storage memory for caching and propagating internal data across Spark nodes (in < > and < > mode) MemoryManager is used to create storage:BlockManager.md#memoryManager[BlockManager] (and storage:MemoryStore.md#memoryManager[MemoryStore]) and memory:TaskMemoryManager.md#memoryManager[TaskMemoryManager]. == [[contract]] Contract === [[acquireExecutionMemory]] Acquiring Execution Memory for Task [source,scala] \u00b6 acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long acquireExecutionMemory tries to acquire up to numBytes of execution memory for the current task (by taskAttemptId ) and return the number of bytes obtained, or 0 if none can be allocated. acquireExecutionMemory is used when TaskMemoryManager is requested to memory:TaskMemoryManager.md#acquireExecutionMemory[acquire execution memory]. === [[acquireStorageMemory]] Acquiring Storage Memory for Block [source, scala] \u00b6 acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean acquireStorageMemory tries to acquire numBytes bytes of memory to cache the given storage:BlockId.md[block], evicting existing ones if necessary. acquireStorageMemory is used when: UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#acquireUnrollMemory[acquireUnrollMemory] MemoryStore is requested to storage:MemoryStore.md#putBytes[putBytes] and storage:MemoryStore.md#putIterator[putIterator] === [[acquireUnrollMemory]] Acquiring Unroll Memory for Block [source, scala] \u00b6 acquireUnrollMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean acquireUnrollMemory tries to acquire numBytes bytes of memory to unroll the given storage:BlockId.md[block], evicting existing ones if necessary. acquireUnrollMemory is used when MemoryStore is requested to storage:MemoryStore.md#reserveUnrollMemoryForThisTask[reserveUnrollMemoryForThisTask]. === [[maxOffHeapStorageMemory]] Total Available Off-Heap Storage Memory [source, scala] \u00b6 maxOffHeapStorageMemory: Long \u00b6 maxOffHeapStorageMemory is the total available off-heap memory for storage (in bytes). maxOffHeapStorageMemory may vary over time. maxOffHeapStorageMemory is used when: UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#acquireStorageMemory[acquireStorageMemory] BlockManager is storage:BlockManager.md#maxOffHeapMemory[created] MemoryStore is requested for the storage:MemoryStore.md#maxMemory[total amount of memory available] === [[maxOnHeapStorageMemory]] Total Available On-Heap Storage Memory [source, scala] \u00b6 maxOnHeapStorageMemory: Long \u00b6 maxOnHeapStorageMemory is the total available on-heap memory for storage (in bytes). maxOnHeapStorageMemory may vary over time. maxOnHeapStorageMemory is used when: UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#acquireStorageMemory[acquireStorageMemory] BlockManager is storage:BlockManager.md#maxOnHeapMemory[created] MemoryStore is requested for the storage:MemoryStore.md#maxMemory[total amount of memory available] (legacy) StaticMemoryManager is memory:StaticMemoryManager.md#maxOnHeapStorageMemory[created] and requested to memory:StaticMemoryManager.md#acquireStorageMemory[acquireStorageMemory] == [[implementations]] Available MemoryManagers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | MemoryManager | Description | StaticMemoryManager.md[StaticMemoryManager] | [[StaticMemoryManager]] Legacy memory manager | UnifiedMemoryManager.md[UnifiedMemoryManager] | [[UnifiedMemoryManager]] Default memory manager |=== == [[creating-instance]] Creating Instance MemoryManager takes the following to be created: [[conf]] ROOT:SparkConf.md[] [[numCores]] Number of CPU cores [[onHeapStorageMemory]] Size of the on-heap storage memory [[onHeapExecutionMemory]] Size of the on-heap execution memory MemoryManager is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[onHeapStorageMemoryPool]][[offHeapStorageMemoryPool]] MemoryPools for Storage MemoryManager creates two memory:StorageMemoryPool.md[]s for on- and off-heap storage (ON_HEAP and OFF_HEAP memory modes, respectively) when < >. MemoryManager immediately requests them to memory:MemoryPool.md#incrementPoolSize[incrementPoolSize] as follows: On-heap storage memory pool is initialized to the assigned < > size Off-heap storage memory pool is initialized to the ROOT:configuration-properties.md#spark.memory.storageFraction[spark.memory.storageFraction] of ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] MemoryManager requests the MemoryPools to memory:StorageMemoryPool.md#setMemoryStore[use a given MemoryStore] when requested to < >. MemoryManager requests the MemoryPools to memory:StorageMemoryPool.md#releaseMemory[releaseMemory] when requested to < >. MemoryManager requests the MemoryPools to memory:StorageMemoryPool.md#releaseAllMemory[releaseAllMemory] when requested to < >. MemoryManager requests the MemoryPools for the memory:StorageMemoryPool.md#memoryUsed[memoryUsed] when requested for < >. == [[SparkEnv]] Accessing MemoryManager Using SparkEnv MemoryManager is available as core:SparkEnv.md#memoryManager[SparkEnv] on the driver and executors. [source,plaintext] \u00b6 import org.apache.spark.SparkEnv val mm = SparkEnv.get.memoryManager scala> :type mm org.apache.spark.memory.MemoryManager == [[spark.memory.useLegacyMode]] spark.memory.useLegacyMode Configuration Property A < > is chosen based on ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] configuration property (when core:SparkEnv.md#memoryManager[SparkEnv] is created for the driver and executors). == [[executionMemoryUsed]] executionMemoryUsed Method [source,scala] \u00b6 executionMemoryUsed: Long \u00b6 executionMemoryUsed...FIXME executionMemoryUsed is used when...FIXME == [[releaseAllStorageMemory]] releaseAllStorageMemory Method [source,scala] \u00b6 releaseAllStorageMemory(): Unit \u00b6 releaseAllStorageMemory...FIXME releaseAllStorageMemory is used when...FIXME == [[releaseUnrollMemory]] releaseUnrollMemory Method [source,scala] \u00b6 releaseUnrollMemory( numBytes: Long, memoryMode: MemoryMode): Unit releaseUnrollMemory...FIXME releaseUnrollMemory is used when...FIXME == [[setMemoryStore]] Associating MemoryStore with Storage MemoryPools [source,scala] \u00b6 setMemoryStore( store: MemoryStore): Unit setMemoryStore requests the < > and < > to memory:StorageMemoryPool.md#setMemoryStore[use] the given storage:MemoryStore.md[]. setMemoryStore is used when storage:BlockManager.md[] is created. == [[releaseExecutionMemory]] releaseExecutionMemory Method [source, scala] \u00b6 releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit releaseExecutionMemory ...FIXME NOTE: releaseExecutionMemory is used when TaskMemoryManager is requested to TaskMemoryManager.md#releaseExecutionMemory[releaseExecutionMemory] and TaskMemoryManager.md#cleanUpAllAllocatedMemory[cleanUpAllAllocatedMemory] == [[releaseAllExecutionMemoryForTask]] releaseAllExecutionMemoryForTask Method [source, scala] \u00b6 releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long \u00b6 releaseAllExecutionMemoryForTask ...FIXME NOTE: releaseAllExecutionMemoryForTask is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and cleans up after itself). == [[tungstenMemoryMode]] tungstenMemoryMode Flag [source, scala] \u00b6 tungstenMemoryMode: MemoryMode \u00b6 tungstenMemoryMode returns OFF_HEAP only when the following are all met: ROOT:configuration-properties.md#spark.memory.offHeap.enabled[spark.memory.offHeap.enabled] configuration property is enabled (it is not by default) ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] configuration property is greater than 0 (it is 0 by default) JVM supports unaligned memory access (aka unaligned Unsafe , i.e. sun.misc.Unsafe package is available and the underlying system has unaligned-access capability) Otherwise, tungstenMemoryMode returns ON_HEAP . NOTE: Given that ROOT:configuration-properties.md#spark.memory.offHeap.enabled[spark.memory.offHeap.enabled] configuration property is disabled ( false ) by default and ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] configuration property is 0 by default, Spark seems to encourage using Tungsten memory allocated on the JVM heap ( ON_HEAP ). NOTE: tungstenMemoryMode is a Scala final val and cannot be changed by custom < >. [NOTE] \u00b6 tungstenMemoryMode is used when: TaskMemoryManager is TaskMemoryManager.md#tungstenMemoryMode[created] * MemoryManager is created (and initializes the < > and < > internal properties) \u00b6 == [[freePage]] freePage Method [source, java] \u00b6 void freePage(MemoryBlock page) \u00b6 freePage ...FIXME NOTE: freePage is used when...FIXME == [[storageMemoryUsed]] storageMemoryUsed Method [source, scala] \u00b6 storageMemoryUsed: Long \u00b6 storageMemoryUsed gives the total of the memory used by the < > and < > StorageMemoryPools. storageMemoryUsed is used when: MemoryStore is requested for storage:MemoryStore.md#memoryUsed[memoryUsed] TaskMemoryManager is requested to memory:TaskMemoryManager.md#showMemoryUsage[showMemoryUsage] == [[releaseStorageMemory]] releaseStorageMemory Method [source, scala] \u00b6 releaseStorageMemory( numBytes: Long, memoryMode: MemoryMode): Unit releaseStorageMemory...FIXME releaseStorageMemory is used when: MemoryManager is requested to < > MemoryStore is requested to storage:MemoryStore.md#remove[remove a block] == [[getExecutionMemoryUsageForTask]] getExecutionMemoryUsageForTask Method [source, scala] \u00b6 getExecutionMemoryUsageForTask( taskAttemptId: Long): Long getExecutionMemoryUsageForTask...FIXME getExecutionMemoryUsageForTask is used when...FIXME == [[maxOffHeapMemory]] maxOffHeapMemory [source, scala] \u00b6 maxOffHeapMemory: Long \u00b6 maxOffHeapMemory...FIXME maxOffHeapMemory is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | pageSizeBytes | [[pageSizeBytes]] FIXME | tungstenMemoryAllocator a| [[tungstenMemoryAllocator]] FIXME |===","title":"MemoryManager"},{"location":"memory/MemoryManager/#memorymanager","text":"MemoryManager is an < > of < > that manage shared memory for task execution (memory:TaskMemoryManager.md#memoryManager[TaskMemoryManager]) and block storage (storage:BlockManager.md#memoryManager[BlockManager]). MemoryManager splits available memory into two regions: Execution memory for computations in shuffles, joins, sorts and aggregations Storage memory for caching and propagating internal data across Spark nodes (in < > and < > mode) MemoryManager is used to create storage:BlockManager.md#memoryManager[BlockManager] (and storage:MemoryStore.md#memoryManager[MemoryStore]) and memory:TaskMemoryManager.md#memoryManager[TaskMemoryManager]. == [[contract]] Contract === [[acquireExecutionMemory]] Acquiring Execution Memory for Task","title":"MemoryManager"},{"location":"memory/MemoryManager/#sourcescala","text":"acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long acquireExecutionMemory tries to acquire up to numBytes of execution memory for the current task (by taskAttemptId ) and return the number of bytes obtained, or 0 if none can be allocated. acquireExecutionMemory is used when TaskMemoryManager is requested to memory:TaskMemoryManager.md#acquireExecutionMemory[acquire execution memory]. === [[acquireStorageMemory]] Acquiring Storage Memory for Block","title":"[source,scala]"},{"location":"memory/MemoryManager/#source-scala","text":"acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean acquireStorageMemory tries to acquire numBytes bytes of memory to cache the given storage:BlockId.md[block], evicting existing ones if necessary. acquireStorageMemory is used when: UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#acquireUnrollMemory[acquireUnrollMemory] MemoryStore is requested to storage:MemoryStore.md#putBytes[putBytes] and storage:MemoryStore.md#putIterator[putIterator] === [[acquireUnrollMemory]] Acquiring Unroll Memory for Block","title":"[source, scala]"},{"location":"memory/MemoryManager/#source-scala_1","text":"acquireUnrollMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean acquireUnrollMemory tries to acquire numBytes bytes of memory to unroll the given storage:BlockId.md[block], evicting existing ones if necessary. acquireUnrollMemory is used when MemoryStore is requested to storage:MemoryStore.md#reserveUnrollMemoryForThisTask[reserveUnrollMemoryForThisTask]. === [[maxOffHeapStorageMemory]] Total Available Off-Heap Storage Memory","title":"[source, scala]"},{"location":"memory/MemoryManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"memory/MemoryManager/#maxoffheapstoragememory-long","text":"maxOffHeapStorageMemory is the total available off-heap memory for storage (in bytes). maxOffHeapStorageMemory may vary over time. maxOffHeapStorageMemory is used when: UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#acquireStorageMemory[acquireStorageMemory] BlockManager is storage:BlockManager.md#maxOffHeapMemory[created] MemoryStore is requested for the storage:MemoryStore.md#maxMemory[total amount of memory available] === [[maxOnHeapStorageMemory]] Total Available On-Heap Storage Memory","title":"maxOffHeapStorageMemory: Long"},{"location":"memory/MemoryManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"memory/MemoryManager/#maxonheapstoragememory-long","text":"maxOnHeapStorageMemory is the total available on-heap memory for storage (in bytes). maxOnHeapStorageMemory may vary over time. maxOnHeapStorageMemory is used when: UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#acquireStorageMemory[acquireStorageMemory] BlockManager is storage:BlockManager.md#maxOnHeapMemory[created] MemoryStore is requested for the storage:MemoryStore.md#maxMemory[total amount of memory available] (legacy) StaticMemoryManager is memory:StaticMemoryManager.md#maxOnHeapStorageMemory[created] and requested to memory:StaticMemoryManager.md#acquireStorageMemory[acquireStorageMemory] == [[implementations]] Available MemoryManagers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | MemoryManager | Description | StaticMemoryManager.md[StaticMemoryManager] | [[StaticMemoryManager]] Legacy memory manager | UnifiedMemoryManager.md[UnifiedMemoryManager] | [[UnifiedMemoryManager]] Default memory manager |=== == [[creating-instance]] Creating Instance MemoryManager takes the following to be created: [[conf]] ROOT:SparkConf.md[] [[numCores]] Number of CPU cores [[onHeapStorageMemory]] Size of the on-heap storage memory [[onHeapExecutionMemory]] Size of the on-heap execution memory MemoryManager is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[onHeapStorageMemoryPool]][[offHeapStorageMemoryPool]] MemoryPools for Storage MemoryManager creates two memory:StorageMemoryPool.md[]s for on- and off-heap storage (ON_HEAP and OFF_HEAP memory modes, respectively) when < >. MemoryManager immediately requests them to memory:MemoryPool.md#incrementPoolSize[incrementPoolSize] as follows: On-heap storage memory pool is initialized to the assigned < > size Off-heap storage memory pool is initialized to the ROOT:configuration-properties.md#spark.memory.storageFraction[spark.memory.storageFraction] of ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] MemoryManager requests the MemoryPools to memory:StorageMemoryPool.md#setMemoryStore[use a given MemoryStore] when requested to < >. MemoryManager requests the MemoryPools to memory:StorageMemoryPool.md#releaseMemory[releaseMemory] when requested to < >. MemoryManager requests the MemoryPools to memory:StorageMemoryPool.md#releaseAllMemory[releaseAllMemory] when requested to < >. MemoryManager requests the MemoryPools for the memory:StorageMemoryPool.md#memoryUsed[memoryUsed] when requested for < >. == [[SparkEnv]] Accessing MemoryManager Using SparkEnv MemoryManager is available as core:SparkEnv.md#memoryManager[SparkEnv] on the driver and executors.","title":"maxOnHeapStorageMemory: Long"},{"location":"memory/MemoryManager/#sourceplaintext","text":"import org.apache.spark.SparkEnv val mm = SparkEnv.get.memoryManager scala> :type mm org.apache.spark.memory.MemoryManager == [[spark.memory.useLegacyMode]] spark.memory.useLegacyMode Configuration Property A < > is chosen based on ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] configuration property (when core:SparkEnv.md#memoryManager[SparkEnv] is created for the driver and executors). == [[executionMemoryUsed]] executionMemoryUsed Method","title":"[source,plaintext]"},{"location":"memory/MemoryManager/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"memory/MemoryManager/#executionmemoryused-long","text":"executionMemoryUsed...FIXME executionMemoryUsed is used when...FIXME == [[releaseAllStorageMemory]] releaseAllStorageMemory Method","title":"executionMemoryUsed: Long"},{"location":"memory/MemoryManager/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"memory/MemoryManager/#releaseallstoragememory-unit","text":"releaseAllStorageMemory...FIXME releaseAllStorageMemory is used when...FIXME == [[releaseUnrollMemory]] releaseUnrollMemory Method","title":"releaseAllStorageMemory(): Unit"},{"location":"memory/MemoryManager/#sourcescala_3","text":"releaseUnrollMemory( numBytes: Long, memoryMode: MemoryMode): Unit releaseUnrollMemory...FIXME releaseUnrollMemory is used when...FIXME == [[setMemoryStore]] Associating MemoryStore with Storage MemoryPools","title":"[source,scala]"},{"location":"memory/MemoryManager/#sourcescala_4","text":"setMemoryStore( store: MemoryStore): Unit setMemoryStore requests the < > and < > to memory:StorageMemoryPool.md#setMemoryStore[use] the given storage:MemoryStore.md[]. setMemoryStore is used when storage:BlockManager.md[] is created. == [[releaseExecutionMemory]] releaseExecutionMemory Method","title":"[source,scala]"},{"location":"memory/MemoryManager/#source-scala_4","text":"releaseExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Unit releaseExecutionMemory ...FIXME NOTE: releaseExecutionMemory is used when TaskMemoryManager is requested to TaskMemoryManager.md#releaseExecutionMemory[releaseExecutionMemory] and TaskMemoryManager.md#cleanUpAllAllocatedMemory[cleanUpAllAllocatedMemory] == [[releaseAllExecutionMemoryForTask]] releaseAllExecutionMemoryForTask Method","title":"[source, scala]"},{"location":"memory/MemoryManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"memory/MemoryManager/#releaseallexecutionmemoryfortasktaskattemptid-long-long","text":"releaseAllExecutionMemoryForTask ...FIXME NOTE: releaseAllExecutionMemoryForTask is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and cleans up after itself). == [[tungstenMemoryMode]] tungstenMemoryMode Flag","title":"releaseAllExecutionMemoryForTask(taskAttemptId: Long): Long"},{"location":"memory/MemoryManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"memory/MemoryManager/#tungstenmemorymode-memorymode","text":"tungstenMemoryMode returns OFF_HEAP only when the following are all met: ROOT:configuration-properties.md#spark.memory.offHeap.enabled[spark.memory.offHeap.enabled] configuration property is enabled (it is not by default) ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] configuration property is greater than 0 (it is 0 by default) JVM supports unaligned memory access (aka unaligned Unsafe , i.e. sun.misc.Unsafe package is available and the underlying system has unaligned-access capability) Otherwise, tungstenMemoryMode returns ON_HEAP . NOTE: Given that ROOT:configuration-properties.md#spark.memory.offHeap.enabled[spark.memory.offHeap.enabled] configuration property is disabled ( false ) by default and ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] configuration property is 0 by default, Spark seems to encourage using Tungsten memory allocated on the JVM heap ( ON_HEAP ). NOTE: tungstenMemoryMode is a Scala final val and cannot be changed by custom < >.","title":"tungstenMemoryMode: MemoryMode"},{"location":"memory/MemoryManager/#note","text":"tungstenMemoryMode is used when: TaskMemoryManager is TaskMemoryManager.md#tungstenMemoryMode[created]","title":"[NOTE]"},{"location":"memory/MemoryManager/#memorymanager-is-created-and-initializes-the-and-internal-properties","text":"== [[freePage]] freePage Method","title":"* MemoryManager is created (and initializes the &lt;&gt; and &lt;&gt; internal properties)"},{"location":"memory/MemoryManager/#source-java","text":"","title":"[source, java]"},{"location":"memory/MemoryManager/#void-freepagememoryblock-page","text":"freePage ...FIXME NOTE: freePage is used when...FIXME == [[storageMemoryUsed]] storageMemoryUsed Method","title":"void freePage(MemoryBlock page)"},{"location":"memory/MemoryManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"memory/MemoryManager/#storagememoryused-long","text":"storageMemoryUsed gives the total of the memory used by the < > and < > StorageMemoryPools. storageMemoryUsed is used when: MemoryStore is requested for storage:MemoryStore.md#memoryUsed[memoryUsed] TaskMemoryManager is requested to memory:TaskMemoryManager.md#showMemoryUsage[showMemoryUsage] == [[releaseStorageMemory]] releaseStorageMemory Method","title":"storageMemoryUsed: Long"},{"location":"memory/MemoryManager/#source-scala_8","text":"releaseStorageMemory( numBytes: Long, memoryMode: MemoryMode): Unit releaseStorageMemory...FIXME releaseStorageMemory is used when: MemoryManager is requested to < > MemoryStore is requested to storage:MemoryStore.md#remove[remove a block] == [[getExecutionMemoryUsageForTask]] getExecutionMemoryUsageForTask Method","title":"[source, scala]"},{"location":"memory/MemoryManager/#source-scala_9","text":"getExecutionMemoryUsageForTask( taskAttemptId: Long): Long getExecutionMemoryUsageForTask...FIXME getExecutionMemoryUsageForTask is used when...FIXME == [[maxOffHeapMemory]] maxOffHeapMemory","title":"[source, scala]"},{"location":"memory/MemoryManager/#source-scala_10","text":"","title":"[source, scala]"},{"location":"memory/MemoryManager/#maxoffheapmemory-long","text":"maxOffHeapMemory...FIXME maxOffHeapMemory is used when...FIXME == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | pageSizeBytes | [[pageSizeBytes]] FIXME | tungstenMemoryAllocator a| [[tungstenMemoryAllocator]] FIXME |===","title":"maxOffHeapMemory: Long"},{"location":"memory/MemoryPool/","text":"= MemoryPool MemoryPool is a bookkeeping abstraction of < >. MemoryPool is used by the memory:MemoryManager.md[MemoryManager] to track the division of memory between storage and execution. == [[extensions]] Extensions .MemoryPools [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryPool | Description | ExecutionMemoryPool.md[ExecutionMemoryPool] | [[ExecutionMemoryPool]] | StorageMemoryPool.md[StorageMemoryPool] | [[StorageMemoryPool]] |=== == [[creating-instance]][[lock]] Creating Instance MemoryPool takes a single lock object to be created (used for synchronization). == [[_poolSize]][[poolSize]] Pool Size [source,scala] \u00b6 _poolSize: Long = 0 \u00b6 ++_poolSize++ is the maximum size of the memory pool. ++_poolSize++ can be < > or < >. ++_poolSize++ is used to report < >. == [[memoryUsed]] Amount of Memory Used [source, scala] \u00b6 memoryUsed: Long \u00b6 memoryUsed gives the amount of memory used in this pool (in bytes). memoryUsed is used when: MemoryManager is requested for the memory:MemoryManager.md#storageMemoryUsed[total storage memory in use] MemoryPool is requested for the current < > and to < > StorageMemoryPool is requested to acquireMemory UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#maxOnHeapStorageMemory[maxOnHeapStorageMemory], memory:UnifiedMemoryManager.md#maxOffHeapStorageMemory[maxOffHeapStorageMemory] and memory:UnifiedMemoryManager.md#acquireExecutionMemory[acquireExecutionMemory] == [[memoryFree]] Amount of Free Memory [source, scala] \u00b6 memoryFree: Long \u00b6 memoryFree gives the amount of free memory in the pool (in bytes) by simply subtracting the < > from the <<_poolSize, _poolSize>>. memoryFree is used when...FIXME == [[decrementPoolSize]] Shrinking Pool Size [source, scala] \u00b6 decrementPoolSize( delta: Long): Unit decrementPoolSize makes the <<_poolSize, _poolSize>> smaller by the given delta bytes. decrementPoolSize requires that the given delta bytes has to meet the requirements: be positive up to the current <<_poolSize, _poolSize>> Does not shrink the current <<_poolSize, _poolSize>> below the < > threshold decrementPoolSize is used when...FIXME == [[incrementPoolSize]] Expanding Pool Size [source, scala] \u00b6 incrementPoolSize( delta: Long): Unit incrementPoolSize makes the <<_poolSize, _poolSize>> bigger by the given delta bytes. incrementPoolSize requires that the given delta bytes has to be positive. incrementPoolSize is used when...FIXME","title":"MemoryPool"},{"location":"memory/MemoryPool/#sourcescala","text":"","title":"[source,scala]"},{"location":"memory/MemoryPool/#_poolsize-long-0","text":"++_poolSize++ is the maximum size of the memory pool. ++_poolSize++ can be < > or < >. ++_poolSize++ is used to report < >. == [[memoryUsed]] Amount of Memory Used","title":"_poolSize: Long = 0"},{"location":"memory/MemoryPool/#source-scala","text":"","title":"[source, scala]"},{"location":"memory/MemoryPool/#memoryused-long","text":"memoryUsed gives the amount of memory used in this pool (in bytes). memoryUsed is used when: MemoryManager is requested for the memory:MemoryManager.md#storageMemoryUsed[total storage memory in use] MemoryPool is requested for the current < > and to < > StorageMemoryPool is requested to acquireMemory UnifiedMemoryManager is requested to memory:UnifiedMemoryManager.md#maxOnHeapStorageMemory[maxOnHeapStorageMemory], memory:UnifiedMemoryManager.md#maxOffHeapStorageMemory[maxOffHeapStorageMemory] and memory:UnifiedMemoryManager.md#acquireExecutionMemory[acquireExecutionMemory] == [[memoryFree]] Amount of Free Memory","title":"memoryUsed: Long"},{"location":"memory/MemoryPool/#source-scala_1","text":"","title":"[source, scala]"},{"location":"memory/MemoryPool/#memoryfree-long","text":"memoryFree gives the amount of free memory in the pool (in bytes) by simply subtracting the < > from the <<_poolSize, _poolSize>>. memoryFree is used when...FIXME == [[decrementPoolSize]] Shrinking Pool Size","title":"memoryFree: Long"},{"location":"memory/MemoryPool/#source-scala_2","text":"decrementPoolSize( delta: Long): Unit decrementPoolSize makes the <<_poolSize, _poolSize>> smaller by the given delta bytes. decrementPoolSize requires that the given delta bytes has to meet the requirements: be positive up to the current <<_poolSize, _poolSize>> Does not shrink the current <<_poolSize, _poolSize>> below the < > threshold decrementPoolSize is used when...FIXME == [[incrementPoolSize]] Expanding Pool Size","title":"[source, scala]"},{"location":"memory/MemoryPool/#source-scala_3","text":"incrementPoolSize( delta: Long): Unit incrementPoolSize makes the <<_poolSize, _poolSize>> bigger by the given delta bytes. incrementPoolSize requires that the given delta bytes has to be positive. incrementPoolSize is used when...FIXME","title":"[source, scala]"},{"location":"memory/StaticMemoryManager/","text":"== [[StaticMemoryManager]] StaticMemoryManager -- Legacy Memory Manager StaticMemoryManager is...FIXME === [[acquireUnrollMemory]] acquireUnrollMemory Method [source, scala] \u00b6 acquireUnrollMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean NOTE: acquireUnrollMemory is part of MemoryManager.md#acquireUnrollMemory[MemoryManager Contract] to...FIXME. acquireUnrollMemory ...FIXME === [[acquireStorageMemory]] acquireStorageMemory Method [source, scala] \u00b6 acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean NOTE: acquireStorageMemory is part of MemoryManager.md#acquireStorageMemory[MemoryManager Contract] to...FIXME. acquireStorageMemory ...FIXME","title":"StaticMemoryManager"},{"location":"memory/StaticMemoryManager/#source-scala","text":"acquireUnrollMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean NOTE: acquireUnrollMemory is part of MemoryManager.md#acquireUnrollMemory[MemoryManager Contract] to...FIXME. acquireUnrollMemory ...FIXME === [[acquireStorageMemory]] acquireStorageMemory Method","title":"[source, scala]"},{"location":"memory/StaticMemoryManager/#source-scala_1","text":"acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean NOTE: acquireStorageMemory is part of MemoryManager.md#acquireStorageMemory[MemoryManager Contract] to...FIXME. acquireStorageMemory ...FIXME","title":"[source, scala]"},{"location":"memory/StorageMemoryPool/","text":"= StorageMemoryPool StorageMemoryPool is a memory:MemoryPool.md[]. StorageMemoryPool is < > along with MemoryManager.md#creating-instance[MemoryManager] (as MemoryManager.md#onHeapStorageMemoryPool[onHeapStorageMemoryPool] and MemoryManager.md#offHeapStorageMemoryPool[offHeapStorageMemoryPool] pools). [[internal-registries]] .StorageMemoryPool's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | poolName | [[poolName]] FIXME Used when...FIXME | _memoryUsed | [[_memoryUsed]][[memoryUsed]] The amount of memory in use for storage (caching) Used when...FIXME | _memoryStore | [[_memoryStore]][[memoryStore]] storage:MemoryStore.md[MemoryStore] Used when...FIXME |=== == [[memoryFree]] memoryFree Method [source, scala] \u00b6 memoryFree: Long \u00b6 memoryFree ...FIXME NOTE: memoryFree is used when...FIXME == [[acquireMemory]] acquireMemory Method [source, scala] \u00b6 acquireMemory(blockId: BlockId, numBytes: Long): Boolean // <1> acquireMemory( blockId: BlockId, numBytesToAcquire: Long, numBytesToFree: Long): Boolean <1> Calls acquireMemory with numBytesToFree as a difference between numBytes and < > acquireMemory ...FIXME [NOTE] \u00b6 acquireMemory is used when: StaticMemoryManager is requested to StaticMemoryManager.md#acquireUnrollMemory[acquireUnrollMemory] and StaticMemoryManager.md#acquireStorageMemory[acquireStorageMemory] * UnifiedMemoryManager is requested to UnifiedMemoryManager.md#acquireStorageMemory[acquireStorageMemory] \u00b6 == [[freeSpaceToShrinkPool]] freeSpaceToShrinkPool Method [source, scala] \u00b6 freeSpaceToShrinkPool(spaceToFree: Long): Long \u00b6 freeSpaceToShrinkPool ...FIXME NOTE: freeSpaceToShrinkPool is used exclusively when UnifiedMemoryManager is requested to UnifiedMemoryManager.md#acquireExecutionMemory[acquireExecutionMemory]. == [[creating-instance]] Creating StorageMemoryPool Instance StorageMemoryPool takes the following when created: [[lock]] Lock [[memoryMode]] MemoryMode (either ON_HEAP or OFF_HEAP ) StorageMemoryPool initializes the < >.","title":"StorageMemoryPool"},{"location":"memory/StorageMemoryPool/#source-scala","text":"","title":"[source, scala]"},{"location":"memory/StorageMemoryPool/#memoryfree-long","text":"memoryFree ...FIXME NOTE: memoryFree is used when...FIXME == [[acquireMemory]] acquireMemory Method","title":"memoryFree: Long"},{"location":"memory/StorageMemoryPool/#source-scala_1","text":"acquireMemory(blockId: BlockId, numBytes: Long): Boolean // <1> acquireMemory( blockId: BlockId, numBytesToAcquire: Long, numBytesToFree: Long): Boolean <1> Calls acquireMemory with numBytesToFree as a difference between numBytes and < > acquireMemory ...FIXME","title":"[source, scala]"},{"location":"memory/StorageMemoryPool/#note","text":"acquireMemory is used when: StaticMemoryManager is requested to StaticMemoryManager.md#acquireUnrollMemory[acquireUnrollMemory] and StaticMemoryManager.md#acquireStorageMemory[acquireStorageMemory]","title":"[NOTE]"},{"location":"memory/StorageMemoryPool/#unifiedmemorymanager-is-requested-to-unifiedmemorymanagermdacquirestoragememoryacquirestoragememory","text":"== [[freeSpaceToShrinkPool]] freeSpaceToShrinkPool Method","title":"* UnifiedMemoryManager is requested to UnifiedMemoryManager.md#acquireStorageMemory[acquireStorageMemory]"},{"location":"memory/StorageMemoryPool/#source-scala_2","text":"","title":"[source, scala]"},{"location":"memory/StorageMemoryPool/#freespacetoshrinkpoolspacetofree-long-long","text":"freeSpaceToShrinkPool ...FIXME NOTE: freeSpaceToShrinkPool is used exclusively when UnifiedMemoryManager is requested to UnifiedMemoryManager.md#acquireExecutionMemory[acquireExecutionMemory]. == [[creating-instance]] Creating StorageMemoryPool Instance StorageMemoryPool takes the following when created: [[lock]] Lock [[memoryMode]] MemoryMode (either ON_HEAP or OFF_HEAP ) StorageMemoryPool initializes the < >.","title":"freeSpaceToShrinkPool(spaceToFree: Long): Long"},{"location":"memory/TaskMemoryManager/","text":"TaskMemoryManager \u00b6 TaskMemoryManager manages the memory allocated to execute a single < > (using < >). TaskMemoryManager is < > when TaskRunner is requested to executor:TaskRunner.md#run[run]. TaskMemoryManager assumes that: The number of bits to address pages (aka PAGE_NUMBER_BITS ) is 13 The number of bits to encode offsets in data pages (aka OFFSET_BITS ) is 51 (i.e. 64 bits - PAGE_NUMBER_BITS ) The number of entries in the < > and < > (aka PAGE_TABLE_SIZE ) is 8192 (i.e. 1 << PAGE_NUMBER_BITS ) The maximum page size (aka MAXIMUM_PAGE_SIZE_BYTES ) is 15GB (i.e. ((1L << 31) - 1) * 8L ) == [[creating-instance]] Creating Instance TaskMemoryManager takes the following to be created: < > [[taskAttemptId]] executor:TaskRunner.md#taskId[Task attempt ID] TaskMemoryManager initializes the < >. == [[consumers]] Spillable Memory Consumers TaskMemoryManager tracks memory:MemoryConsumer.md[spillable memory consumers]. TaskMemoryManager registers a new memory consumer when requested to < >. TaskMemoryManager removes ( clears ) all registered memory consumer when requested to < >. Memory consumers are used to report memory usage when TaskMemoryManager is requested to < >. == [[memoryManager]] MemoryManager TaskMemoryManager is given a memory:MemoryManager.md[MemoryManager] when < >. TaskMemoryManager uses the MemoryManager for the following: < >, < > or < > execution memory < > < > < > < > < > == [[cleanUpAllAllocatedMemory]] Cleaning Up All Allocated Memory [source, java] \u00b6 long cleanUpAllAllocatedMemory() \u00b6 cleanUpAllAllocatedMemory clears < >. CAUTION: FIXME All recorded < > are queried for the size of used memory. If the memory used is greater than 0, the following WARN message is printed out to the logs: WARN TaskMemoryManager: leak [bytes] memory from [consumer] The consumers collection is then cleared. MemoryManager.md#releaseExecutionMemory[MemoryManager.releaseExecutionMemory] is executed to release the memory that is not used by any consumer. Before cleanUpAllAllocatedMemory returns, it calls MemoryManager.md#releaseAllExecutionMemoryForTask[MemoryManager.releaseAllExecutionMemoryForTask] that in turn becomes the return value. CAUTION: FIXME Image with the interactions to MemoryManager . NOTE: cleanUpAllAllocatedMemory is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and cleans up after itself). == [[acquireExecutionMemory]] Acquiring Execution Memory [source, java] \u00b6 long acquireExecutionMemory( long required, MemoryConsumer consumer) acquireExecutionMemory allocates up to required size of memory for the memory:MemoryConsumer.md[MemoryConsumer]. When no memory could be allocated, it calls spill on every consumer, itself including. Finally, acquireExecutionMemory returns the allocated memory. NOTE: acquireExecutionMemory synchronizes on itself, and so no other calls on the object could be completed. NOTE: memory:MemoryConsumer.md[MemoryConsumer] knows its mode -- on- or off-heap. acquireExecutionMemory first calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) . TIP: TaskMemoryManager is a mere wrapper of MemoryManager to track < >? CAUTION: FIXME When the memory obtained is less than requested (by required ), acquireExecutionMemory requests all < > to MemoryConsumer.md#spill[release memory (by spilling it to disk)]. NOTE: acquireExecutionMemory requests memory from consumers that work in the same mode except the requesting one. You may see the following DEBUG message when spill released some memory: DEBUG Task [taskAttemptId] released [bytes] from [consumer] for [consumer] acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) again (it called it at the beginning). It does the memory acquisition until it gets enough memory or there are no more consumers to request spill from. You may also see the following ERROR message in the logs when there is an error while requesting spill with OutOfMemoryError followed. ERROR error while calling spill() on [consumer] If the earlier spill on the consumers did not work out and there is still memory to be acquired, acquireExecutionMemory MemoryConsumer.md#spill[requests the input consumer to spill memory to disk] (that in fact requested more memory!) If the consumer releases some memory, you should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] released [bytes] from itself ([consumer]) acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) once more. NOTE: memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) could have been called \"three\" times, i.e. at the very beginning, for each consumer, and on itself. It records the consumer in < > registry. You should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] acquired [bytes] for [consumer] acquireExecutionMemory is used when: MemoryConsumer is requested to memory:MemoryConsumer.md#acquireMemory[acquire execution memory] TaskMemoryManager is requested to < > == [[allocatePage]] Allocating Memory Block for Tungsten Consumers [source, java] \u00b6 MemoryBlock allocatePage( long size, MemoryConsumer consumer) NOTE: It only handles Tungsten Consumers , i.e. MemoryConsumer.md[MemoryConsumers] in tungstenMemoryMode mode. allocatePage allocates a block of memory (aka page ) smaller than MAXIMUM_PAGE_SIZE_BYTES maximum size. It checks size against the internal MAXIMUM_PAGE_SIZE_BYTES maximum size. If it is greater than the maximum size, the following IllegalArgumentException is thrown: Cannot allocate a page with more than [MAXIMUM_PAGE_SIZE_BYTES] bytes It then < > (for the input size and consumer ). It finishes by returning null when no execution memory could be acquired. With the execution memory acquired, it finds the smallest unallocated page index and records the page number (using < > registry). If the index is PAGE_TABLE_SIZE or higher, < > is called and then the following IllegalStateException is thrown: Have already allocated a maximum of [PAGE_TABLE_SIZE] pages It then attempts to allocate a MemoryBlock from Tungsten MemoryAllocator (calling memoryManager.tungstenMemoryAllocator().allocate(acquired) ). CAUTION: FIXME What is MemoryAllocator ? When successful, MemoryBlock gets assigned pageNumber and it gets added to the internal < > registry. You should see the following TRACE message in the logs: TRACE Allocate page number [pageNumber] ([acquired] bytes) The page is returned. If a OutOfMemoryError is thrown when allocating a MemoryBlock page, the following WARN message is printed out to the logs: WARN Failed to allocate a page ([acquired] bytes), try again. And acquiredButNotUsed gets acquired memory space with the pageNumber cleared in < > (i.e. the index for pageNumber gets false ). CAUTION: FIXME Why is the code tracking acquiredButNotUsed ? Another < > attempt is recursively tried. CAUTION: FIXME Why is there a hope for being able to allocate a page? == [[releaseExecutionMemory]] releaseExecutionMemory Method [source, java] \u00b6 void releaseExecutionMemory(long size, MemoryConsumer consumer) \u00b6 releaseExecutionMemory ...FIXME [NOTE] \u00b6 releaseExecutionMemory is used when: MemoryConsumer is requested to MemoryConsumer.md#freeMemory[freeMemory] * TaskMemoryManager is requested to < > and < > \u00b6 == [[getMemoryConsumptionForThisTask]] getMemoryConsumptionForThisTask Method [source, java] \u00b6 long getMemoryConsumptionForThisTask() \u00b6 getMemoryConsumptionForThisTask ...FIXME NOTE: getMemoryConsumptionForThisTask is used exclusively in Spark tests. == [[showMemoryUsage]] Displaying Memory Usage [source, java] \u00b6 void showMemoryUsage() \u00b6 showMemoryUsage prints out the following INFO message to the logs (with the < >): [source,plaintext] \u00b6 Memory used in task [taskAttemptId] \u00b6 showMemoryUsage requests every < > to memory:MemoryConsumer.md#getUsed[report memory used]. showMemoryUsage prints out the following INFO message to the logs for a MemoryConsumer with some memory usage (and excludes zero-memory consumers): [source,plaintext] \u00b6 Acquired by [consumer]: [memUsage] \u00b6 showMemoryUsage prints out the following INFO messages to the logs: [source,plaintext] \u00b6 [amount] bytes of memory were used by task [taskAttemptId] but are not associated with specific consumers \u00b6 [source,plaintext] \u00b6 [executionMemoryUsed] bytes of memory are used for execution and [storageMemoryUsed] bytes of memory are used for storage \u00b6 showMemoryUsage is used when MemoryConsumer is requested to memory:MemoryConsumer.md#throwOom[throw an OutOfMemoryError]. == [[pageSizeBytes]] pageSizeBytes Method [source, java] \u00b6 long pageSizeBytes() \u00b6 pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when...FIXME == [[freePage]] Freeing Memory Page -- freePage Method [source, java] \u00b6 void freePage(MemoryBlock page, MemoryConsumer consumer) \u00b6 pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when MemoryConsumer is requested to MemoryConsumer.md#freePage[freePage] and MemoryConsumer.md#throwOom[throwOom]. == [[getPage]] Getting Page -- getPage Method [source, java] \u00b6 Object getPage(long pagePlusOffsetAddress) \u00b6 getPage ...FIXME NOTE: getPage is used when...FIXME == [[getOffsetInPage]] Getting Page Offset -- getOffsetInPage Method [source, java] \u00b6 long getOffsetInPage(long pagePlusOffsetAddress) \u00b6 getPage ...FIXME NOTE: getPage is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.memory.TaskMemoryManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.memory.TaskMemoryManager=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | acquiredButNotUsed | [[acquiredButNotUsed]] The size of memory allocated but not used. | allocatedPages | [[allocatedPages]] Collection of flags ( true or false values) of size PAGE_TABLE_SIZE with all bits initially disabled (i.e. false ). TIP: allocatedPages is https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html[java.util.BitSet ]. When < > is called, it will record the page in the registry by setting the bit at the specified index (that corresponds to the allocated page) to true . | pageTable | [[pageTable]] The array of size PAGE_TABLE_SIZE with indices being MemoryBlock objects. When < MemoryBlock page for Tungsten consumers>>, the index corresponds to pageNumber that points to the MemoryBlock page allocated. | tungstenMemoryMode | [[tungstenMemoryMode]] MemoryMode (i.e. OFF_HEAP or ON_HEAP ) Set to the MemoryManager.md#tungstenMemoryMode[tungstenMemoryMode] of the < > while TaskMemoryManager is < > |===","title":"TaskMemoryManager"},{"location":"memory/TaskMemoryManager/#taskmemorymanager","text":"TaskMemoryManager manages the memory allocated to execute a single < > (using < >). TaskMemoryManager is < > when TaskRunner is requested to executor:TaskRunner.md#run[run]. TaskMemoryManager assumes that: The number of bits to address pages (aka PAGE_NUMBER_BITS ) is 13 The number of bits to encode offsets in data pages (aka OFFSET_BITS ) is 51 (i.e. 64 bits - PAGE_NUMBER_BITS ) The number of entries in the < > and < > (aka PAGE_TABLE_SIZE ) is 8192 (i.e. 1 << PAGE_NUMBER_BITS ) The maximum page size (aka MAXIMUM_PAGE_SIZE_BYTES ) is 15GB (i.e. ((1L << 31) - 1) * 8L ) == [[creating-instance]] Creating Instance TaskMemoryManager takes the following to be created: < > [[taskAttemptId]] executor:TaskRunner.md#taskId[Task attempt ID] TaskMemoryManager initializes the < >. == [[consumers]] Spillable Memory Consumers TaskMemoryManager tracks memory:MemoryConsumer.md[spillable memory consumers]. TaskMemoryManager registers a new memory consumer when requested to < >. TaskMemoryManager removes ( clears ) all registered memory consumer when requested to < >. Memory consumers are used to report memory usage when TaskMemoryManager is requested to < >. == [[memoryManager]] MemoryManager TaskMemoryManager is given a memory:MemoryManager.md[MemoryManager] when < >. TaskMemoryManager uses the MemoryManager for the following: < >, < > or < > execution memory < > < > < > < > < > == [[cleanUpAllAllocatedMemory]] Cleaning Up All Allocated Memory","title":"TaskMemoryManager"},{"location":"memory/TaskMemoryManager/#source-java","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-cleanupallallocatedmemory","text":"cleanUpAllAllocatedMemory clears < >. CAUTION: FIXME All recorded < > are queried for the size of used memory. If the memory used is greater than 0, the following WARN message is printed out to the logs: WARN TaskMemoryManager: leak [bytes] memory from [consumer] The consumers collection is then cleared. MemoryManager.md#releaseExecutionMemory[MemoryManager.releaseExecutionMemory] is executed to release the memory that is not used by any consumer. Before cleanUpAllAllocatedMemory returns, it calls MemoryManager.md#releaseAllExecutionMemoryForTask[MemoryManager.releaseAllExecutionMemoryForTask] that in turn becomes the return value. CAUTION: FIXME Image with the interactions to MemoryManager . NOTE: cleanUpAllAllocatedMemory is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (and cleans up after itself). == [[acquireExecutionMemory]] Acquiring Execution Memory","title":"long cleanUpAllAllocatedMemory()"},{"location":"memory/TaskMemoryManager/#source-java_1","text":"long acquireExecutionMemory( long required, MemoryConsumer consumer) acquireExecutionMemory allocates up to required size of memory for the memory:MemoryConsumer.md[MemoryConsumer]. When no memory could be allocated, it calls spill on every consumer, itself including. Finally, acquireExecutionMemory returns the allocated memory. NOTE: acquireExecutionMemory synchronizes on itself, and so no other calls on the object could be completed. NOTE: memory:MemoryConsumer.md[MemoryConsumer] knows its mode -- on- or off-heap. acquireExecutionMemory first calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) . TIP: TaskMemoryManager is a mere wrapper of MemoryManager to track < >? CAUTION: FIXME When the memory obtained is less than requested (by required ), acquireExecutionMemory requests all < > to MemoryConsumer.md#spill[release memory (by spilling it to disk)]. NOTE: acquireExecutionMemory requests memory from consumers that work in the same mode except the requesting one. You may see the following DEBUG message when spill released some memory: DEBUG Task [taskAttemptId] released [bytes] from [consumer] for [consumer] acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) again (it called it at the beginning). It does the memory acquisition until it gets enough memory or there are no more consumers to request spill from. You may also see the following ERROR message in the logs when there is an error while requesting spill with OutOfMemoryError followed. ERROR error while calling spill() on [consumer] If the earlier spill on the consumers did not work out and there is still memory to be acquired, acquireExecutionMemory MemoryConsumer.md#spill[requests the input consumer to spill memory to disk] (that in fact requested more memory!) If the consumer releases some memory, you should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] released [bytes] from itself ([consumer]) acquireExecutionMemory calls memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) once more. NOTE: memoryManager.acquireExecutionMemory(required, taskAttemptId, mode) could have been called \"three\" times, i.e. at the very beginning, for each consumer, and on itself. It records the consumer in < > registry. You should see the following DEBUG message in the logs: DEBUG Task [taskAttemptId] acquired [bytes] for [consumer] acquireExecutionMemory is used when: MemoryConsumer is requested to memory:MemoryConsumer.md#acquireMemory[acquire execution memory] TaskMemoryManager is requested to < > == [[allocatePage]] Allocating Memory Block for Tungsten Consumers","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#source-java_2","text":"MemoryBlock allocatePage( long size, MemoryConsumer consumer) NOTE: It only handles Tungsten Consumers , i.e. MemoryConsumer.md[MemoryConsumers] in tungstenMemoryMode mode. allocatePage allocates a block of memory (aka page ) smaller than MAXIMUM_PAGE_SIZE_BYTES maximum size. It checks size against the internal MAXIMUM_PAGE_SIZE_BYTES maximum size. If it is greater than the maximum size, the following IllegalArgumentException is thrown: Cannot allocate a page with more than [MAXIMUM_PAGE_SIZE_BYTES] bytes It then < > (for the input size and consumer ). It finishes by returning null when no execution memory could be acquired. With the execution memory acquired, it finds the smallest unallocated page index and records the page number (using < > registry). If the index is PAGE_TABLE_SIZE or higher, < > is called and then the following IllegalStateException is thrown: Have already allocated a maximum of [PAGE_TABLE_SIZE] pages It then attempts to allocate a MemoryBlock from Tungsten MemoryAllocator (calling memoryManager.tungstenMemoryAllocator().allocate(acquired) ). CAUTION: FIXME What is MemoryAllocator ? When successful, MemoryBlock gets assigned pageNumber and it gets added to the internal < > registry. You should see the following TRACE message in the logs: TRACE Allocate page number [pageNumber] ([acquired] bytes) The page is returned. If a OutOfMemoryError is thrown when allocating a MemoryBlock page, the following WARN message is printed out to the logs: WARN Failed to allocate a page ([acquired] bytes), try again. And acquiredButNotUsed gets acquired memory space with the pageNumber cleared in < > (i.e. the index for pageNumber gets false ). CAUTION: FIXME Why is the code tracking acquiredButNotUsed ? Another < > attempt is recursively tried. CAUTION: FIXME Why is there a hope for being able to allocate a page? == [[releaseExecutionMemory]] releaseExecutionMemory Method","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#source-java_3","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#void-releaseexecutionmemorylong-size-memoryconsumer-consumer","text":"releaseExecutionMemory ...FIXME","title":"void releaseExecutionMemory(long size, MemoryConsumer consumer)"},{"location":"memory/TaskMemoryManager/#note","text":"releaseExecutionMemory is used when: MemoryConsumer is requested to MemoryConsumer.md#freeMemory[freeMemory]","title":"[NOTE]"},{"location":"memory/TaskMemoryManager/#taskmemorymanager-is-requested-to-and","text":"== [[getMemoryConsumptionForThisTask]] getMemoryConsumptionForThisTask Method","title":"* TaskMemoryManager is requested to &lt;&gt; and &lt;&gt;"},{"location":"memory/TaskMemoryManager/#source-java_4","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-getmemoryconsumptionforthistask","text":"getMemoryConsumptionForThisTask ...FIXME NOTE: getMemoryConsumptionForThisTask is used exclusively in Spark tests. == [[showMemoryUsage]] Displaying Memory Usage","title":"long getMemoryConsumptionForThisTask()"},{"location":"memory/TaskMemoryManager/#source-java_5","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#void-showmemoryusage","text":"showMemoryUsage prints out the following INFO message to the logs (with the < >):","title":"void showMemoryUsage()"},{"location":"memory/TaskMemoryManager/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#memory-used-in-task-taskattemptid","text":"showMemoryUsage requests every < > to memory:MemoryConsumer.md#getUsed[report memory used]. showMemoryUsage prints out the following INFO message to the logs for a MemoryConsumer with some memory usage (and excludes zero-memory consumers):","title":"Memory used in task [taskAttemptId]"},{"location":"memory/TaskMemoryManager/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#acquired-by-consumer-memusage","text":"showMemoryUsage prints out the following INFO messages to the logs:","title":"Acquired by [consumer]: [memUsage]"},{"location":"memory/TaskMemoryManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#amount-bytes-of-memory-were-used-by-task-taskattemptid-but-are-not-associated-with-specific-consumers","text":"","title":"[amount] bytes of memory were used by task [taskAttemptId] but are not associated with specific consumers"},{"location":"memory/TaskMemoryManager/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"memory/TaskMemoryManager/#executionmemoryused-bytes-of-memory-are-used-for-execution-and-storagememoryused-bytes-of-memory-are-used-for-storage","text":"showMemoryUsage is used when MemoryConsumer is requested to memory:MemoryConsumer.md#throwOom[throw an OutOfMemoryError]. == [[pageSizeBytes]] pageSizeBytes Method","title":"[executionMemoryUsed] bytes of memory are used for execution and [storageMemoryUsed] bytes of memory are used for storage"},{"location":"memory/TaskMemoryManager/#source-java_6","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-pagesizebytes","text":"pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when...FIXME == [[freePage]] Freeing Memory Page -- freePage Method","title":"long pageSizeBytes()"},{"location":"memory/TaskMemoryManager/#source-java_7","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#void-freepagememoryblock-page-memoryconsumer-consumer","text":"pageSizeBytes simply requests the < > for MemoryManager.md#pageSizeBytes[pageSizeBytes]. NOTE: pageSizeBytes is used when MemoryConsumer is requested to MemoryConsumer.md#freePage[freePage] and MemoryConsumer.md#throwOom[throwOom]. == [[getPage]] Getting Page -- getPage Method","title":"void freePage(MemoryBlock page, MemoryConsumer consumer)"},{"location":"memory/TaskMemoryManager/#source-java_8","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#object-getpagelong-pageplusoffsetaddress","text":"getPage ...FIXME NOTE: getPage is used when...FIXME == [[getOffsetInPage]] Getting Page Offset -- getOffsetInPage Method","title":"Object getPage(long pagePlusOffsetAddress)"},{"location":"memory/TaskMemoryManager/#source-java_9","text":"","title":"[source, java]"},{"location":"memory/TaskMemoryManager/#long-getoffsetinpagelong-pageplusoffsetaddress","text":"getPage ...FIXME NOTE: getPage is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.memory.TaskMemoryManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"long getOffsetInPage(long pagePlusOffsetAddress)"},{"location":"memory/TaskMemoryManager/#source","text":"","title":"[source]"},{"location":"memory/TaskMemoryManager/#log4jloggerorgapachesparkmemorytaskmemorymanagerall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | acquiredButNotUsed | [[acquiredButNotUsed]] The size of memory allocated but not used. | allocatedPages | [[allocatedPages]] Collection of flags ( true or false values) of size PAGE_TABLE_SIZE with all bits initially disabled (i.e. false ). TIP: allocatedPages is https://docs.oracle.com/javase/8/docs/api/java/util/BitSet.html[java.util.BitSet ]. When < > is called, it will record the page in the registry by setting the bit at the specified index (that corresponds to the allocated page) to true . | pageTable | [[pageTable]] The array of size PAGE_TABLE_SIZE with indices being MemoryBlock objects. When < MemoryBlock page for Tungsten consumers>>, the index corresponds to pageNumber that points to the MemoryBlock page allocated. | tungstenMemoryMode | [[tungstenMemoryMode]] MemoryMode (i.e. OFF_HEAP or ON_HEAP ) Set to the MemoryManager.md#tungstenMemoryMode[tungstenMemoryMode] of the < > while TaskMemoryManager is < > |===","title":"log4j.logger.org.apache.spark.memory.TaskMemoryManager=ALL"},{"location":"memory/UnifiedMemoryManager/","text":"= [[UnifiedMemoryManager]] UnifiedMemoryManager UnifiedMemoryManager is the default MemoryManager.md[MemoryManager] (based on ROOT:configuration-properties.md#spark.memory.useLegacyMode[spark.memory.useLegacyMode] configuration property). == [[creating-instance]] Creating Instance UnifiedMemoryManager takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] [[maxHeapMemory]] Maximum heap memory [[onHeapStorageRegionSize]] Size of the on-heap storage region [[numCores]] Number of CPU cores UnifiedMemoryManager requires that: Sum of the pool size of the MemoryManager.md#onHeapExecutionMemoryPool[on-heap ExecutionMemoryPool] and MemoryManager.md#onHeapStorageMemoryPool[on-heap StorageMemoryPool] is exactly the < > Sum of the pool size of the MemoryManager.md#offHeapExecutionMemoryPool[off-heap ExecutionMemoryPool] and MemoryManager.md#offHeapStorageMemoryPool[off-heap StorageMemoryPool] is exactly the maximum off-heap memory (based on ROOT:configuration-properties.md#spark.memory.offHeap.size[spark.memory.offHeap.size] configuration property) == [[apply]] Creating UnifiedMemoryManager [source, scala] \u00b6 apply( conf: SparkConf, numCores: Int): UnifiedMemoryManager apply computes the < > (using the input ROOT:SparkConf.md[SparkConf]). apply computes the size of the on-heap storage region which is a fraction of the maximum heap memory based on ROOT:configuration-properties.md#spark.memory.storageFraction[spark.memory.storageFraction] configuration property (default: 0.5 ). In the end, apply creates a < > (with the given and computed values). apply is used when SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv] (for the driver and executors). == [[getMaxMemory]] Calculating Maximum Heap Memory [source, scala] \u00b6 getMaxMemory( conf: SparkConf): Long getMaxMemory calculates the maximum memory to use for execution and storage. [source, scala] \u00b6 // local mode with --conf spark.driver.memory=2g scala> sc.getConf.getSizeAsBytes(\"spark.driver.memory\") res0: Long = 2147483648 scala> val systemMemory = Runtime.getRuntime.maxMemory // fixed amount of memory for non-storage, non-execution purposes val reservedMemory = 300 * 1024 * 1024 // minimum system memory required val minSystemMemory = (reservedMemory * 1.5).ceil.toLong val usableMemory = systemMemory - reservedMemory val memoryFraction = sc.getConf.getDouble(\"spark.memory.fraction\", 0.6) scala> val maxMemory = (usableMemory * memoryFraction).toLong maxMemory: Long = 956615884 import org.apache.spark.network.util.JavaUtils scala> JavaUtils.byteStringAsMb(maxMemory + \"b\") res1: Long = 912 getMaxMemory reads < > and decrements it by < > (for non-storage and non-execution purposes). getMaxMemory makes sure that the following requirements are met: System memory is not smaller than about 1,5 of the reserved system memory. executor:Executor.md#spark.executor.memory[spark.executor.memory] is not smaller than about 1,5 of the reserved system memory. Ultimately, getMaxMemory returns < > of the maximum amount of memory for the JVM (minus the reserved system memory). CAUTION: FIXME omnigraffle it. == [[acquireExecutionMemory]] acquireExecutionMemory Method [source, scala] \u00b6 acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long NOTE: acquireExecutionMemory is part of the memory:MemoryManager.md#acquireExecutionMemory[MemoryManager] contract acquireExecutionMemory does...FIXME Internally, acquireExecutionMemory varies per MemoryMode , i.e. ON_HEAP and OFF_HEAP . . acquireExecutionMemory and MemoryMode [cols=\"1m,1m,1m\",options=\"header\",width=\"100%\"] |=== | | ON_HEAP | OFF_HEAP | executionPool | onHeapExecutionMemoryPool | offHeapExecutionMemoryPool | storagePool | onHeapStorageMemoryPool | offHeapStorageMemoryPool | storageRegionSize | onHeapStorageRegionSize | offHeapStorageMemory | maxMemory | maxHeapMemory | maxOffHeapMemory |=== CAUTION: FIXME == [[acquireStorageMemory]] acquireStorageMemory Method [source, scala] \u00b6 acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean NOTE: acquireStorageMemory is part of the memory:MemoryManager.md#acquireStorageMemory[MemoryManager] contract. acquireStorageMemory has two modes of operation per memoryMode , i.e. MemoryMode.ON_HEAP or MemoryMode.OFF_HEAP , for execution and storage pools, and the maximum amount of memory to use. CAUTION: FIXME Where are they used? In MemoryMode.ON_HEAP , onHeapExecutionMemoryPool , onHeapStorageMemoryPool , and < > are used. In MemoryMode.OFF_HEAP , offHeapExecutionMemoryPool , offHeapStorageMemoryPool , and maxOffHeapMemory are used. CAUTION: FIXME What is the difference between them? It makes sure that the requested number of bytes numBytes (for a block to store) fits the available memory. If it is not the case, you should see the following INFO message in the logs and the method returns false . INFO Will not store [blockId] as the required space ([numBytes] bytes) exceeds our memory limit ([maxMemory] bytes) If the requested number of bytes numBytes is greater than memoryFree in the storage pool, acquireStorageMemory will attempt to use the free memory from the execution pool. NOTE: The storage pool can use the free memory from the execution pool. It will take as much memory as required to fit numBytes from memoryFree in the execution pool (up to the whole free memory in the pool). Ultimately, acquireStorageMemory requests the storage pool for numBytes for blockId . [NOTE] \u00b6 acquireStorageMemory is used when MemoryStore storage:MemoryStore.md#putBytes[acquires storage memory to putBytes] or storage:MemoryStore.md#putIteratorAsValues[putIteratorAsValues] and storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes]. It is also used internally when UnifiedMemoryManager < >. \u00b6 == [[acquireUnrollMemory]] acquireUnrollMemory Method NOTE: acquireUnrollMemory is part of the memory:MemoryManager.md#acquireUnrollMemory[MemoryManager] contract. acquireUnrollMemory simply forwards all the calls to < >. == [[maxOnHeapStorageMemory]] maxOnHeapStorageMemory Method [source, scala] \u00b6 maxOnHeapStorageMemory: Long \u00b6 NOTE: maxOnHeapStorageMemory is part of the memory:MemoryManager.md#acquireExecutionMemory[MemoryManager] contract maxOnHeapStorageMemory is the difference between maxHeapMemory of the UnifiedMemoryManager and the memory currently in use in onHeapExecutionMemoryPool execution memory pool.","title":"UnifiedMemoryManager"},{"location":"memory/UnifiedMemoryManager/#source-scala","text":"apply( conf: SparkConf, numCores: Int): UnifiedMemoryManager apply computes the < > (using the input ROOT:SparkConf.md[SparkConf]). apply computes the size of the on-heap storage region which is a fraction of the maximum heap memory based on ROOT:configuration-properties.md#spark.memory.storageFraction[spark.memory.storageFraction] configuration property (default: 0.5 ). In the end, apply creates a < > (with the given and computed values). apply is used when SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv] (for the driver and executors). == [[getMaxMemory]] Calculating Maximum Heap Memory","title":"[source, scala]"},{"location":"memory/UnifiedMemoryManager/#source-scala_1","text":"getMaxMemory( conf: SparkConf): Long getMaxMemory calculates the maximum memory to use for execution and storage.","title":"[source, scala]"},{"location":"memory/UnifiedMemoryManager/#source-scala_2","text":"// local mode with --conf spark.driver.memory=2g scala> sc.getConf.getSizeAsBytes(\"spark.driver.memory\") res0: Long = 2147483648 scala> val systemMemory = Runtime.getRuntime.maxMemory // fixed amount of memory for non-storage, non-execution purposes val reservedMemory = 300 * 1024 * 1024 // minimum system memory required val minSystemMemory = (reservedMemory * 1.5).ceil.toLong val usableMemory = systemMemory - reservedMemory val memoryFraction = sc.getConf.getDouble(\"spark.memory.fraction\", 0.6) scala> val maxMemory = (usableMemory * memoryFraction).toLong maxMemory: Long = 956615884 import org.apache.spark.network.util.JavaUtils scala> JavaUtils.byteStringAsMb(maxMemory + \"b\") res1: Long = 912 getMaxMemory reads < > and decrements it by < > (for non-storage and non-execution purposes). getMaxMemory makes sure that the following requirements are met: System memory is not smaller than about 1,5 of the reserved system memory. executor:Executor.md#spark.executor.memory[spark.executor.memory] is not smaller than about 1,5 of the reserved system memory. Ultimately, getMaxMemory returns < > of the maximum amount of memory for the JVM (minus the reserved system memory). CAUTION: FIXME omnigraffle it. == [[acquireExecutionMemory]] acquireExecutionMemory Method","title":"[source, scala]"},{"location":"memory/UnifiedMemoryManager/#source-scala_3","text":"acquireExecutionMemory( numBytes: Long, taskAttemptId: Long, memoryMode: MemoryMode): Long NOTE: acquireExecutionMemory is part of the memory:MemoryManager.md#acquireExecutionMemory[MemoryManager] contract acquireExecutionMemory does...FIXME Internally, acquireExecutionMemory varies per MemoryMode , i.e. ON_HEAP and OFF_HEAP . . acquireExecutionMemory and MemoryMode [cols=\"1m,1m,1m\",options=\"header\",width=\"100%\"] |=== | | ON_HEAP | OFF_HEAP | executionPool | onHeapExecutionMemoryPool | offHeapExecutionMemoryPool | storagePool | onHeapStorageMemoryPool | offHeapStorageMemoryPool | storageRegionSize | onHeapStorageRegionSize | offHeapStorageMemory | maxMemory | maxHeapMemory | maxOffHeapMemory |=== CAUTION: FIXME == [[acquireStorageMemory]] acquireStorageMemory Method","title":"[source, scala]"},{"location":"memory/UnifiedMemoryManager/#source-scala_4","text":"acquireStorageMemory( blockId: BlockId, numBytes: Long, memoryMode: MemoryMode): Boolean NOTE: acquireStorageMemory is part of the memory:MemoryManager.md#acquireStorageMemory[MemoryManager] contract. acquireStorageMemory has two modes of operation per memoryMode , i.e. MemoryMode.ON_HEAP or MemoryMode.OFF_HEAP , for execution and storage pools, and the maximum amount of memory to use. CAUTION: FIXME Where are they used? In MemoryMode.ON_HEAP , onHeapExecutionMemoryPool , onHeapStorageMemoryPool , and < > are used. In MemoryMode.OFF_HEAP , offHeapExecutionMemoryPool , offHeapStorageMemoryPool , and maxOffHeapMemory are used. CAUTION: FIXME What is the difference between them? It makes sure that the requested number of bytes numBytes (for a block to store) fits the available memory. If it is not the case, you should see the following INFO message in the logs and the method returns false . INFO Will not store [blockId] as the required space ([numBytes] bytes) exceeds our memory limit ([maxMemory] bytes) If the requested number of bytes numBytes is greater than memoryFree in the storage pool, acquireStorageMemory will attempt to use the free memory from the execution pool. NOTE: The storage pool can use the free memory from the execution pool. It will take as much memory as required to fit numBytes from memoryFree in the execution pool (up to the whole free memory in the pool). Ultimately, acquireStorageMemory requests the storage pool for numBytes for blockId .","title":"[source, scala]"},{"location":"memory/UnifiedMemoryManager/#note","text":"acquireStorageMemory is used when MemoryStore storage:MemoryStore.md#putBytes[acquires storage memory to putBytes] or storage:MemoryStore.md#putIteratorAsValues[putIteratorAsValues] and storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes].","title":"[NOTE]"},{"location":"memory/UnifiedMemoryManager/#it-is-also-used-internally-when-unifiedmemorymanager","text":"== [[acquireUnrollMemory]] acquireUnrollMemory Method NOTE: acquireUnrollMemory is part of the memory:MemoryManager.md#acquireUnrollMemory[MemoryManager] contract. acquireUnrollMemory simply forwards all the calls to < >. == [[maxOnHeapStorageMemory]] maxOnHeapStorageMemory Method","title":"It is also used internally when UnifiedMemoryManager &lt;&gt;."},{"location":"memory/UnifiedMemoryManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"memory/UnifiedMemoryManager/#maxonheapstoragememory-long","text":"NOTE: maxOnHeapStorageMemory is part of the memory:MemoryManager.md#acquireExecutionMemory[MemoryManager] contract maxOnHeapStorageMemory is the difference between maxHeapMemory of the UnifiedMemoryManager and the memory currently in use in onHeapExecutionMemoryPool execution memory pool.","title":"maxOnHeapStorageMemory: Long"},{"location":"memory/UnsafeExternalSorter/","text":"= [[UnsafeExternalSorter]] UnsafeExternalSorter UnsafeExternalSorter is...FIXME == [[creating-instance]] Creating Instance UnsafeExternalSorter takes the following to be created: [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] [[blockManager]] storage:BlockManager.md[BlockManager] < > [[taskContext]] scheduler:spark-TaskContext.md[TaskContext] [[recordComparatorSupplier]] Supplier [[prefixComparator]] PrefixComparator [[initialSize]] Initial size [[pageSizeBytes]] Page size (in bytes) [[numElementsForSpillThreshold]] numElementsForSpillThreshold [[existingInMemorySorter]] memory:UnsafeInMemorySorter.md[UnsafeInMemorySorter] [[canUseRadixSort]] canUseRadixSort flag == [[serializerManager]] SerializerManager UnsafeExternalSorter is given a serializer:SerializerManager.md[SerializerManager] when < >. UnsafeExternalSorter uses the SerializerManager for < >, < >, and (SpillableIterator) < > (to request UnsafeSorterSpillWriter for a memory:UnsafeSorterSpillWriter.md#getReader[UnsafeSorterSpillReader]).","title":"UnsafeExternalSorter"},{"location":"memory/UnsafeInMemorySorter/","text":"= [[UnsafeInMemorySorter]] UnsafeInMemorySorter UnsafeInMemorySorter is...FIXME","title":"UnsafeInMemorySorter"},{"location":"memory/UnsafeSorterSpillReader/","text":"= UnsafeSorterSpillReader UnsafeSorterSpillReader is...FIXME","title":"UnsafeSorterSpillReader"},{"location":"memory/UnsafeSorterSpillWriter/","text":"= [[UnsafeSorterSpillWriter]] UnsafeSorterSpillWriter UnsafeSorterSpillWriter is...FIXME","title":"UnsafeSorterSpillWriter"},{"location":"metrics/","text":"Spark Metrics \u00b6 Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment . Metrics Systems \u00b6 applicationMaster \u00b6 Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator applications \u00b6 Registered when Master (Spark Standalone) is created driver \u00b6 Registered when SparkEnv is created for the driver executor \u00b6 Registered when SparkEnv is created for an executor master \u00b6 Registered when Master (Spark Standalone) is created mesos_cluster \u00b6 Registered when MesosClusterScheduler (Apache Mesos) is created shuffleService \u00b6 Registered when ExternalShuffleService is created worker \u00b6 Registered when Worker (Spark Standalone) is created MetricsSystem \u00b6 Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' spark-metrics-MetricsSystem.md#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem MetricsConfig \u00b6 MetricsConfig is the configuration of the spark-metrics-MetricsSystem.md[MetricsSystem] (i.e. metrics spark-metrics-Source.md[sources] and spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsServlet Metrics Sink \u00b6 Among the metrics sinks is spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties JmxSink Metrics Sink \u00b6 Enable org.apache.spark.metrics.sink.JmxSink in spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink JSON URI Path \u00b6 Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output. Spark Standalone Master \u00b6 $ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Metrics"},{"location":"metrics/#spark-metrics","text":"Spark Metrics gives you execution metrics of Spark subsystems ( metrics instances , e.g. the driver of a Spark application or the master of a Spark Standalone cluster). Spark Metrics uses Dropwizard Metrics 3.1.0 Java library for the metrics infrastructure. Metrics is a Java library which gives you unparalleled insight into what your code does in production. Metrics provides a powerful toolkit of ways to measure the behavior of critical components in your production environment .","title":"Spark Metrics"},{"location":"metrics/#metrics-systems","text":"","title":"Metrics Systems"},{"location":"metrics/#applicationmaster","text":"Registered when ApplicationMaster (Hadoop YARN) is requested to createAllocator","title":"applicationMaster"},{"location":"metrics/#applications","text":"Registered when Master (Spark Standalone) is created","title":"applications"},{"location":"metrics/#driver","text":"Registered when SparkEnv is created for the driver","title":"driver"},{"location":"metrics/#executor","text":"Registered when SparkEnv is created for an executor","title":"executor"},{"location":"metrics/#master","text":"Registered when Master (Spark Standalone) is created","title":"master"},{"location":"metrics/#mesos_cluster","text":"Registered when MesosClusterScheduler (Apache Mesos) is created","title":"mesos_cluster"},{"location":"metrics/#shuffleservice","text":"Registered when ExternalShuffleService is created","title":"shuffleService"},{"location":"metrics/#worker","text":"Registered when Worker (Spark Standalone) is created","title":"worker"},{"location":"metrics/#metricssystem","text":"Spark Metrics uses MetricsSystem . MetricsSystem uses Dropwizard Metrics' spark-metrics-MetricsSystem.md#registry[MetricRegistry] that acts as the integration point between Spark and the metrics library. A Spark subsystem can access the MetricsSystem through the SparkEnv.metricsSystem property. val metricsSystem = SparkEnv.get.metricsSystem","title":" MetricsSystem"},{"location":"metrics/#metricsconfig","text":"MetricsConfig is the configuration of the spark-metrics-MetricsSystem.md[MetricsSystem] (i.e. metrics spark-metrics-Source.md[sources] and spark-metrics-Sink.md[sinks]). metrics.properties is the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig also accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration.","title":" MetricsConfig"},{"location":"metrics/#metricsservlet-metrics-sink","text":"Among the metrics sinks is spark-metrics-MetricsServlet.md[MetricsServlet] that is used when sink.servlet metrics sink is configured in spark-metrics-MetricsConfig.md[metrics configuration]. CAUTION: FIXME Describe configuration files and properties","title":" MetricsServlet Metrics Sink"},{"location":"metrics/#jmxsink-metrics-sink","text":"Enable org.apache.spark.metrics.sink.JmxSink in spark-metrics-MetricsConfig.md[metrics configuration]. You can then use jconsole to access Spark metrics through JMX. *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink","title":" JmxSink Metrics Sink"},{"location":"metrics/#json-uri-path","text":"Metrics System is available at http://localhost:4040/metrics/json (for the default setup of a Spark application). $ http --follow http://localhost:4040/metrics/json HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 2200 Content-Type: text/json;charset=utf-8 Date: Sat, 25 Feb 2017 14:14:16 GMT Server: Jetty(9.2.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": { \"app-20170225151406-0000.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 2 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"app-20170225151406-0000.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 } }, \"gauges\": { ... \"timers\": { \"app-20170225151406-0000.driver.DAGScheduler.messageProcessingTime\": { \"count\": 0, \"duration_units\": \"milliseconds\", \"m15_rate\": 0.0, \"m1_rate\": 0.0, \"m5_rate\": 0.0, \"max\": 0.0, \"mean\": 0.0, \"mean_rate\": 0.0, \"min\": 0.0, \"p50\": 0.0, \"p75\": 0.0, \"p95\": 0.0, \"p98\": 0.0, \"p99\": 0.0, \"p999\": 0.0, \"rate_units\": \"calls/second\", \"stddev\": 0.0 } }, \"version\": \"3.0.0\" } NOTE: You can access a Spark subsystem's MetricsSystem using its corresponding \"leading\" port, e.g. 4040 for the driver , 8080 for Spark Standalone's master and applications . NOTE: You have to use the trailing slash ( / ) to have the output.","title":"JSON URI Path"},{"location":"metrics/#spark-standalone-master","text":"$ http http://192.168.1.4:8080/metrics/master/json/path HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 207 Content-Type: text/json;charset=UTF-8 Server: Jetty(8.y.z-SNAPSHOT) X-Frame-Options: SAMEORIGIN { \"counters\": {}, \"gauges\": { \"master.aliveWorkers\": { \"value\": 0 }, \"master.apps\": { \"value\": 0 }, \"master.waitingApps\": { \"value\": 0 }, \"master.workers\": { \"value\": 0 } }, \"histograms\": {}, \"meters\": {}, \"timers\": {}, \"version\": \"3.0.0\" }","title":"Spark Standalone Master"},{"location":"metrics/DAGSchedulerSource/","text":"DAGSchedulerSource \u00b6 DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/DAGSchedulerSource/#dagschedulersource","text":"DAGSchedulerSource is the metrics source of DAGScheduler . DAGScheduler uses Spark Metrics System to report metrics about internal status. The name of the source is DAGScheduler . DAGSchedulerSource emits the following metrics: stage.failedStages - the number of failed stages stage.runningStages - the number of running stages stage.waitingStages - the number of waiting stages job.allJobs - the number of all jobs job.activeJobs - the number of active jobs","title":"DAGSchedulerSource"},{"location":"metrics/JvmSource/","text":"JvmSource \u00b6 JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/JvmSource/#jvmsource","text":"JvmSource is a metrics source . The name of the source is jvm . JvmSource registers the build-in Codehale metrics: GarbageCollectorMetricSet MemoryUsageGaugeSet BufferPoolMetricSet Among the metrics is total.committed (from MemoryUsageGaugeSet ) that describes the current usage of the heap and non-heap memories.","title":"JvmSource"},{"location":"metrics/MetricsConfig/","text":"MetricsConfig \u00b6 MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when spark-metrics-MetricsSystem.md#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |=== [NOTE] \u00b6 The order of precedence of metrics configuration settings is as follows: . < > . spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a ROOT:SparkConf.md[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize < > and < > (that is defined using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is spark-metrics-MetricsSystem.md#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method [source, scala] \u00b6 setDefaultProperties(prop: Properties): Unit \u00b6 setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method [source, scala] \u00b6 loadPropertiesFromFile(path: Option[String]): Unit \u00b6 loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method [source, scala] \u00b6 subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] \u00b6 subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es). [source, scala] \u00b6 driver.hello.world => (driver, (hello.world)) \u00b6 NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem spark-metrics-MetricsSystem.md#registerSources[registers metrics sources] and spark-metrics-MetricsSystem.md#registerSinks[sinks]. === [[getInstance]] getInstance Method [source, scala] \u00b6 getInstance(inst: String): Properties \u00b6 getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#metricsconfig","text":"MetricsConfig is the configuration of the MetricsSystem (i.e. metrics sources and sinks ). MetricsConfig is < > when spark-metrics-MetricsSystem.md#creating-instance[MetricsSystem] is. MetricsConfig uses metrics.properties as the default metrics configuration file. It is configured using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property. The file is first loaded from the path directly before using Spark's CLASSPATH. MetricsConfig accepts a metrics configuration using spark.metrics.conf. -prefixed configuration properties. Spark comes with conf/metrics.properties.template file that is a template of metrics configuration. MetricsConfig < > that the < > are always defined. [[default-properties]] .MetricsConfig's Default Metrics Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | *.sink.servlet.class | org.apache.spark.metrics.sink.MetricsServlet | *.sink.servlet.path | /metrics/json | master.sink.servlet.path | /metrics/master/json | applications.sink.servlet.path | /metrics/applications/json |===","title":"MetricsConfig"},{"location":"metrics/MetricsConfig/#note","text":"The order of precedence of metrics configuration settings is as follows: . < > . spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property or metrics.properties configuration file . spark.metrics.conf. -prefixed Spark properties ==== [[creating-instance]] [[conf]] MetricsConfig takes a ROOT:SparkConf.md[SparkConf] when created. [[internal-registries]] .MetricsConfig's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[properties]] properties | https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html[java.util.Properties ] with metrics properties Used to < > per-subsystem's < >. | [[perInstanceSubProperties]] perInstanceSubProperties | Lookup table of metrics properties per subsystem |=== === [[initialize]] Initializing MetricsConfig -- initialize Method","title":"[NOTE]"},{"location":"metrics/MetricsConfig/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#initialize-unit","text":"initialize < > and < > (that is defined using spark-metrics-properties.md#spark.metrics.conf[spark.metrics.conf] configuration property). initialize takes all Spark properties that start with spark.metrics.conf. prefix from < > and adds them to < > (without the prefix). In the end, initialize splits < > with the default configuration (denoted as * ) assigned to all subsystems afterwards. NOTE: initialize accepts * (star) for the default configuration or any combination of lower- and upper-case letters for Spark subsystem names. NOTE: initialize is used exclusively when MetricsSystem is spark-metrics-MetricsSystem.md#creating-instance[created]. === [[setDefaultProperties]] setDefaultProperties Internal Method","title":"initialize(): Unit"},{"location":"metrics/MetricsConfig/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#setdefaultpropertiesprop-properties-unit","text":"setDefaultProperties sets the < > (in the input prop ). NOTE: setDefaultProperties is used exclusively when MetricsConfig < >. === [[loadPropertiesFromFile]] Loading Custom Metrics Configuration File or metrics.properties -- loadPropertiesFromFile Method","title":"setDefaultProperties(prop: Properties): Unit"},{"location":"metrics/MetricsConfig/#source-scala_2","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#loadpropertiesfromfilepath-optionstring-unit","text":"loadPropertiesFromFile tries to open the input path file (if defined) or the default metrics configuration file metrics.properties (on CLASSPATH). If either file is available, loadPropertiesFromFile loads the properties (to < > registry). In case of exceptions, you should see the following ERROR message in the logs followed by the exception. ERROR Error loading configuration file [file] NOTE: loadPropertiesFromFile is used exclusively when MetricsConfig < >. === [[subProperties]] Grouping Properties Per Subsystem -- subProperties Method","title":"loadPropertiesFromFile(path: Option[String]): Unit"},{"location":"metrics/MetricsConfig/#source-scala_3","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#subpropertiesprop-properties-regex-regex-mutablehashmapstring-properties","text":"subProperties takes prop properties and destructures keys given regex . subProperties takes the matching prefix (of a key per regex ) and uses it as a new key with the value(s) being the matching suffix(es).","title":"subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties]"},{"location":"metrics/MetricsConfig/#source-scala_4","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#driverhelloworld-driver-helloworld","text":"NOTE: subProperties is used when MetricsConfig < > (to apply the default metrics configuration) and when MetricsSystem spark-metrics-MetricsSystem.md#registerSources[registers metrics sources] and spark-metrics-MetricsSystem.md#registerSinks[sinks]. === [[getInstance]] getInstance Method","title":"driver.hello.world =&gt; (driver, (hello.world))"},{"location":"metrics/MetricsConfig/#source-scala_5","text":"","title":"[source, scala]"},{"location":"metrics/MetricsConfig/#getinstanceinst-string-properties","text":"getInstance ...FIXME NOTE: getInstance is used when...FIXME","title":"getInstance(inst: String): Properties"},{"location":"metrics/MetricsServlet/","text":"MetricsServlet JSON Metrics Sink \u00b6 MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in spark-metrics-MetricsConfig.md[metrics configuration]). That is not required since MetricsConfig spark-metrics-MetricsConfig.md#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method [source, scala] \u00b6 getMetricsSnapshot(request: HttpServletRequest): String \u00b6 getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using ++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method [source, scala] \u00b6 getHandlers(conf: SparkConf): Array[ServletContextHandler] \u00b6 getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"MetricsServlet"},{"location":"metrics/MetricsServlet/#metricsservlet-json-metrics-sink","text":"MetricsServlet is a metrics sink that gives metrics snapshots in JSON format. MetricsServlet is a \"special\" sink as it is only available to the metrics instances with a web UI: Driver of a Spark application Spark Standalone's Master and Worker You can access the metrics from MetricsServlet at /metrics/json URI by default. The entire URL depends on a metrics instance, e.g. http://localhost:4040/metrics/json/ for a running Spark application. $ http http://localhost:4040/metrics/json/ HTTP/1.1 200 OK Cache-Control: no-cache, no-store, must-revalidate Content-Length: 5005 Content-Type: text/json;charset=utf-8 Date: Mon, 11 Jun 2018 06:29:03 GMT Server: Jetty(9.3.z-SNAPSHOT) X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block { \"counters\": { \"local-1528698499919.driver.HiveExternalCatalog.fileCacheHits\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.filesDiscovered\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.hiveClientCalls\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.parallelListingJobCount\": { \"count\": 0 }, \"local-1528698499919.driver.HiveExternalCatalog.partitionsFetched\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.numEventsPosted\": { \"count\": 7 }, \"local-1528698499919.driver.LiveListenerBus.queue.appStatus.numDroppedEvents\": { \"count\": 0 }, \"local-1528698499919.driver.LiveListenerBus.queue.executorManagement.numDroppedEvents\": { \"count\": 0 } }, ... MetricsServlet is < > exclusively when MetricsSystem is started (and requested to register metrics sinks ). MetricsServlet can be configured using configuration properties with sink.servlet prefix (in spark-metrics-MetricsConfig.md[metrics configuration]). That is not required since MetricsConfig spark-metrics-MetricsConfig.md#setDefaultProperties[makes sure] that MetricsServlet is always configured. MetricsServlet uses https://fasterxml.github.io/jackson-databind/[jackson-databind ], the general data-binding package for Jackson (as < >) with https://metrics.dropwizard.io/3.1.0/[Dropwizard Metrics] library (i.e. registering a Coda Hale MetricsModule ). [[properties]] .MetricsServlet's Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default | Description | path | /metrics/json/ | [[path]] Path URI prefix to bind to | sample | false | [[sample]] Whether to show entire set of samples for histograms |=== [[internal-registries]] .MetricsServlet's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | mapper | [[mapper]] Jaxson's https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html[com.fasterxml.jackson.databind.ObjectMapper ] that \"provides functionality for reading and writing JSON, either to and from basic POJOs (Plain Old Java Objects), or to and from a general-purpose JSON Tree Model (JsonNode), as well as related functionality for performing conversions.\" When created, mapper is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. Used exclusively when MetricsServlet is requested to < >. | servletPath | [[servletPath]] Value of < > configuration property | servletShowSample | [[servletShowSample]] Flag to control whether to show samples ( true ) or not ( false ). servletShowSample is the value of < > configuration property (if defined) or false . Used when < > is requested to register a Coda Hale https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/json/MetricsModule.html[com.codahale.metrics.json.MetricsModule ]. |=== === [[creating-instance]] Creating MetricsServlet Instance MetricsServlet takes the following when created: [[property]] Configuration Properties (as Java Properties ) [[registry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] [[securityMgr]] SecurityManager MetricsServlet initializes the < >. === [[getMetricsSnapshot]] Requesting Metrics Snapshot -- getMetricsSnapshot Method","title":"MetricsServlet JSON Metrics Sink"},{"location":"metrics/MetricsServlet/#source-scala","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#getmetricssnapshotrequest-httpservletrequest-string","text":"getMetricsSnapshot simply requests the < > to serialize the < > to a JSON string (using ++ https://fasterxml.github.io/jackson-databind/javadoc/2.6/com/fasterxml/jackson/databind/ObjectMapper.html#writeValueAsString-java.lang.Object-++[ObjectMapper.writeValueAsString ]). NOTE: getMetricsSnapshot is used exclusively when MetricsServlet is requested to < >. === [[getHandlers]] Requesting JSON Servlet Handler -- getHandlers Method","title":"getMetricsSnapshot(request: HttpServletRequest): String"},{"location":"metrics/MetricsServlet/#source-scala_1","text":"","title":"[source, scala]"},{"location":"metrics/MetricsServlet/#gethandlersconf-sparkconf-arrayservletcontexthandler","text":"getHandlers returns just a single ServletContextHandler (in a collection) that gives < > in JSON format at every request at < > URI path. NOTE: getHandlers is used exclusively when MetricsSystem is requested for MetricsSystem.md#getServletHandlers[metrics ServletContextHandlers].","title":"getHandlers(conf: SparkConf): Array[ServletContextHandler]"},{"location":"metrics/MetricsSystem/","text":"MetricsSystem \u00b6 MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem . Creating Instance \u00b6 MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems . Creating MetricsSystem \u00b6 createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ) : MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems . Metrics Sources for Spark SQL \u00b6 CodegenMetrics HiveCatalogMetrics Registering Metrics Source \u00b6 registerSource ( source : Source ) : Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource uses Metrics' MetricRegistry.register to register a metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered Building Metrics Source Identifier \u00b6 buildRegistryName ( source : Source ) : String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . Note buildRegistryName uses Dropwizard Metrics' MetricRegistry to build metrics source identifiers. FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source. Registering Metrics Sources for Spark Instance \u00b6 registerSources () : Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start . Requesting JSON Servlet Handler \u00b6 getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start Registering Metrics Sinks \u00b6 registerSinks () : Unit registerSinks requests the < > for the spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start . Stopping \u00b6 stop () : Unit stop ...FIXME Reporting Metrics \u00b6 report () : Unit report simply requests the registered metrics sinks to report metrics . Starting \u00b6 start () : Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running Logging \u00b6 Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging . Internal Registries \u00b6 MetricRegistry \u00b6 Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks ) MetricsConfig \u00b6 MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >. MetricsServlet \u00b6 MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >. running Flag \u00b6 Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false sinks \u00b6 Metrics sinks Used when MetricsSystem < > and < >. sources \u00b6 Metrics sources Used when MetricsSystem < >.","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#metricssystem","text":"MetricsSystem is a registry of metrics sources and sinks of a Spark subsystem .","title":"MetricsSystem"},{"location":"metrics/MetricsSystem/#creating-instance","text":"MetricsSystem takes the following to be created: Instance Name SparkConf SecurityManager While being created, MetricsSystem requests the MetricsConfig to initialize . MetricsSystem is created (using createMetricsSystem utility) for the Metrics Systems .","title":"Creating Instance"},{"location":"metrics/MetricsSystem/#creating-metricssystem","text":"createMetricsSystem ( instance : String conf : SparkConf securityMgr : SecurityManager ) : MetricsSystem createMetricsSystem creates a new MetricsSystem (for the given parameters). createMetricsSystem is used to create metrics systems .","title":" Creating MetricsSystem"},{"location":"metrics/MetricsSystem/#metrics-sources-for-spark-sql","text":"CodegenMetrics HiveCatalogMetrics","title":" Metrics Sources for Spark SQL"},{"location":"metrics/MetricsSystem/#registering-metrics-source","text":"registerSource ( source : Source ) : Unit registerSource adds source to the sources internal registry. registerSource creates an identifier for the metrics source and registers it with the MetricRegistry . registerSource uses Metrics' MetricRegistry.register to register a metrics source under a given name. registerSource prints out the following INFO message to the logs when registering a name more than once: Metrics already registered","title":" Registering Metrics Source"},{"location":"metrics/MetricsSystem/#building-metrics-source-identifier","text":"buildRegistryName ( source : Source ) : String buildRegistryName uses spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace] and executor:Executor.md#spark.executor.id[spark.executor.id] Spark properties to differentiate between a Spark application's driver and executors, and the other Spark framework's components. (only when < > is driver or executor ) buildRegistryName builds metrics source name that is made up of spark-metrics-properties.md#spark.metrics.namespace[spark.metrics.namespace], executor:Executor.md#spark.executor.id[spark.executor.id] and the name of the source . Note buildRegistryName uses Dropwizard Metrics' MetricRegistry to build metrics source identifiers. FIXME Finish for the other components. buildRegistryName is used when MetricsSystem is requested to register or remove a metrics source.","title":" Building Metrics Source Identifier"},{"location":"metrics/MetricsSystem/#registering-metrics-sources-for-spark-instance","text":"registerSources () : Unit registerSources finds < > configuration for the < >. NOTE: instance is defined when MetricsSystem < >. registerSources finds the configuration of all the spark-metrics-Source.md[metrics sources] for the subsystem (as described with source. prefix). For every metrics source, registerSources finds class property, creates an instance, and in the end < >. When registerSources fails, you should see the following ERROR message in the logs followed by the exception. Source class [classPath] cannot be instantiated registerSources is used when MetricsSystem is requested to start .","title":" Registering Metrics Sources for Spark Instance"},{"location":"metrics/MetricsSystem/#requesting-json-servlet-handler","text":"getServletHandlers : Array [ ServletContextHandler ] If the MetricsSystem is < > and the < > is defined for the metrics system, getServletHandlers simply requests the < > for the spark-metrics-MetricsServlet.md#getHandlers[JSON servlet handler]. When MetricsSystem is not < > getServletHandlers throws an IllegalArgumentException . Can only call getServletHandlers on a running MetricsSystem getServletHandlers is used when: SparkContext is created (Spark Standalone) Master and Worker are requested to start","title":" Requesting JSON Servlet Handler"},{"location":"metrics/MetricsSystem/#registering-metrics-sinks","text":"registerSinks () : Unit registerSinks requests the < > for the spark-metrics-MetricsConfig.md#getInstance[configuration] of the < >. registerSinks requests the < > for the spark-metrics-MetricsConfig.md#subProperties[configuration] of all metrics sinks (i.e. configuration entries that match ^sink\\\\.(.+)\\\\.(.+) regular expression). For every metrics sink configuration, registerSinks takes class property and (if defined) creates an instance of the metric sink using an constructor that takes the configuration, < > and < >. For a single servlet metrics sink, registerSinks converts the sink to a spark-metrics-MetricsServlet.md[MetricsServlet] and sets the < > internal registry. For all other metrics sinks, registerSinks adds the sink to the < > internal registry. In case of an Exception , registerSinks prints out the following ERROR message to the logs: Sink class [classPath] cannot be instantiated registerSinks is used when MetricsSystem is requested to start .","title":" Registering Metrics Sinks"},{"location":"metrics/MetricsSystem/#stopping","text":"stop () : Unit stop ...FIXME","title":" Stopping"},{"location":"metrics/MetricsSystem/#reporting-metrics","text":"report () : Unit report simply requests the registered metrics sinks to report metrics .","title":" Reporting Metrics"},{"location":"metrics/MetricsSystem/#starting","text":"start () : Unit start turns < > flag on. NOTE: start can only be called once and < > an IllegalArgumentException when called multiple times. start < > the < > for Spark SQL, i.e. CodegenMetrics and HiveCatalogMetrics . start then registers the configured metrics < > and < > for the < >. In the end, start requests the registered < > to spark-metrics-Sink.md#start[start]. [[start-IllegalArgumentException]] start throws an IllegalArgumentException when < > flag is on. requirement failed: Attempting to start a MetricsSystem that is already running","title":" Starting"},{"location":"metrics/MetricsSystem/#logging","text":"Enable ALL logging level for org.apache.spark.metrics.MetricsSystem logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.metrics.MetricsSystem=ALL Refer to Logging .","title":"Logging"},{"location":"metrics/MetricsSystem/#internal-registries","text":"","title":"Internal Registries"},{"location":"metrics/MetricsSystem/#metricregistry","text":"Integration point to Dropwizard Metrics' MetricRegistry Used when MetricsSystem is requested to: Register or remove a metrics source Start (that in turn registers metrics sinks )","title":" MetricRegistry"},{"location":"metrics/MetricsSystem/#metricsconfig","text":"MetricsConfig Initialized when MetricsSystem is < >. Used when MetricsSystem registers < > and < >.","title":" MetricsConfig"},{"location":"metrics/MetricsSystem/#metricsservlet","text":"MetricsServlet JSON metrics sink that is only available for the < > with a web UI (i.e. the driver of a Spark application and Spark Standalone's Master ). MetricsSystem may have at most one MetricsServlet JSON metrics sink (which is registered by default ). Initialized when MetricsSystem registers < > (and finds a configuration entry with servlet sink name). Used when MetricsSystem is requested for a < >.","title":" MetricsServlet"},{"location":"metrics/MetricsSystem/#running-flag","text":"Indicates whether MetricsSystem has been started ( true ) or not ( false ) Default: false","title":" running Flag"},{"location":"metrics/MetricsSystem/#sinks","text":"Metrics sinks Used when MetricsSystem < > and < >.","title":" sinks"},{"location":"metrics/MetricsSystem/#sources","text":"Metrics sources Used when MetricsSystem < >.","title":" sources"},{"location":"metrics/Sink/","text":"Sink \u00b6 Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | spark-metrics-MetricsServlet.md[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Sink/#sink","text":"Sink is a < > of metrics sinks . [[contract]] [source, scala] package org.apache.spark.metrics.sink trait Sink { def start(): Unit def stop(): Unit def report(): Unit } NOTE: Sink is a private[spark] contract. .Sink Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | start | [[start]] Used when...FIXME | stop | [[stop]] Used when...FIXME | report | [[report]] Used when...FIXME |=== [[implementations]] .Sinks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Sink | Description | ConsoleSink | [[ConsoleSink]] | CsvSink | [[CsvSink]] | GraphiteSink | [[GraphiteSink]] | JmxSink | [[JmxSink]] | spark-metrics-MetricsServlet.md[MetricsServlet] | [[MetricsServlet]] | Slf4jSink | [[Slf4jSink]] | StatsdSink | [[StatsdSink]] |=== NOTE: All known < > in Spark 2.3 are in org.apache.spark.metrics.sink Scala package.","title":"Sink"},{"location":"metrics/Source/","text":"== [[Source]] Source -- Contract of Metrics Sources Source is a < > of metrics sources . [[contract]] [source, scala] package org.apache.spark.metrics.source trait Source { def sourceName: String def metricRegistry: MetricRegistry } NOTE: Source is a private[spark] contract. .Source Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | sourceName | [[sourceName]] Used when...FIXME | metricRegistry | [[metricRegistry]] Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html[MetricRegistry ] Used when...FIXME |=== [[implementations]] .Sources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Source | Description | ApplicationSource | [[ApplicationSource]] | storage:spark-BlockManager-BlockManagerSource.md[BlockManagerSource] | [[BlockManagerSource]] | CacheMetrics | [[CacheMetrics]] | CodegenMetrics | [[CodegenMetrics]] | metrics:spark-scheduler-DAGSchedulerSource.md[DAGSchedulerSource] | [[DAGSchedulerSource]] | ROOT:spark-service-ExecutorAllocationManagerSource.md[ExecutorAllocationManagerSource] | [[ExecutorAllocationManagerSource]] | executor:ExecutorSource.md[] | [[ExecutorSource]] | ExternalShuffleServiceSource | [[ExternalShuffleServiceSource]] | HiveCatalogMetrics | [[HiveCatalogMetrics]] | metrics:JvmSource.md[JvmSource] | [[JvmSource]] | LiveListenerBusMetrics | [[LiveListenerBusMetrics]] | MasterSource | [[MasterSource]] | MesosClusterSchedulerSource | [[MesosClusterSchedulerSource]] | storage:ShuffleMetricsSource.md[] | [[ShuffleMetricsSource]] | StreamingSource | [[StreamingSource]] | WorkerSource | [[WorkerSource]] |===","title":"Source"},{"location":"metrics/configuration-properties/","text":"Configuration Properties \u00b6 spark.metrics.conf \u00b6 The metrics configuration file Default: metrics.properties spark.metrics.namespace \u00b6 Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"metrics/configuration-properties/#sparkmetricsconf","text":"The metrics configuration file Default: metrics.properties","title":" spark.metrics.conf"},{"location":"metrics/configuration-properties/#sparkmetricsnamespace","text":"Root namespace for metrics reporting Default: Spark Application ID (i.e. spark.app.id configuration property) Since a Spark application's ID changes with every execution of a Spark application, a custom namespace can be specified for an easier metrics reporting. Used when MetricsSystem is requested for a metrics source identifier ( metrics namespace )","title":" spark.metrics.namespace"},{"location":"network/ManagedBuffer/","text":"= ManagedBuffer ManagedBuffer is the < > of < > that < >. == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | convertToNetty a| [[convertToNetty]] [source, java] \u00b6 Object convertToNetty() \u00b6 Used exclusively when MessageEncoder is requested to encode a message | createInputStream a| [[createInputStream]] [source, java] \u00b6 InputStream createInputStream() \u00b6 Used exclusively when ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#next[retrieve the next element] | nioByteBuffer a| [[nioByteBuffer]] [source, java] \u00b6 ByteBuffer nioByteBuffer() \u00b6 Used when...FIXME | release a| [[release]] [source, java] \u00b6 ManagedBuffer release() \u00b6 Used when...FIXME | retain a| [[retain]] [source, java] \u00b6 ManagedBuffer retain() \u00b6 Used when: MessageWithHeader is requested to retain ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[send a remote shuffle block fetch request] and storage:ShuffleBlockFetcherIterator.md#fetchLocalBlocks[fetchLocalBlocks] | size a| [[size]] [source, java] \u00b6 long size() \u00b6 Number of bytes of the data Used when...FIXME |=== == [[implementations]] Available ManagedBuffers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | ManagedBuffer | Description | BlockManagerManagedBuffer | [[BlockManagerManagedBuffer]] | EncryptedManagedBuffer | [[EncryptedManagedBuffer]] | FileSegmentManagedBuffer | [[FileSegmentManagedBuffer]] | NettyManagedBuffer | [[NettyManagedBuffer]] | NioManagedBuffer | [[NioManagedBuffer]] |===","title":"ManagedBuffer"},{"location":"network/ManagedBuffer/#source-java","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#object-converttonetty","text":"Used exclusively when MessageEncoder is requested to encode a message | createInputStream a| [[createInputStream]]","title":"Object convertToNetty()"},{"location":"network/ManagedBuffer/#source-java_1","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#inputstream-createinputstream","text":"Used exclusively when ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#next[retrieve the next element] | nioByteBuffer a| [[nioByteBuffer]]","title":"InputStream createInputStream()"},{"location":"network/ManagedBuffer/#source-java_2","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#bytebuffer-niobytebuffer","text":"Used when...FIXME | release a| [[release]]","title":"ByteBuffer nioByteBuffer()"},{"location":"network/ManagedBuffer/#source-java_3","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#managedbuffer-release","text":"Used when...FIXME | retain a| [[retain]]","title":"ManagedBuffer release()"},{"location":"network/ManagedBuffer/#source-java_4","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#managedbuffer-retain","text":"Used when: MessageWithHeader is requested to retain ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[send a remote shuffle block fetch request] and storage:ShuffleBlockFetcherIterator.md#fetchLocalBlocks[fetchLocalBlocks] | size a| [[size]]","title":"ManagedBuffer retain()"},{"location":"network/ManagedBuffer/#source-java_5","text":"","title":"[source, java]"},{"location":"network/ManagedBuffer/#long-size","text":"Number of bytes of the data Used when...FIXME |=== == [[implementations]] Available ManagedBuffers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | ManagedBuffer | Description | BlockManagerManagedBuffer | [[BlockManagerManagedBuffer]] | EncryptedManagedBuffer | [[EncryptedManagedBuffer]] | FileSegmentManagedBuffer | [[FileSegmentManagedBuffer]] | NettyManagedBuffer | [[NettyManagedBuffer]] | NioManagedBuffer | [[NioManagedBuffer]] |===","title":"long size()"},{"location":"network/MessageHandler/","text":"= MessageHandler MessageHandler is a < > of < > that can < > messages. [[contract]] [source, java] package org.apache.spark.network.server; abstract class MessageHandler { abstract void handle(T message) throws Exception; abstract void channelActive(); abstract void exceptionCaught(Throwable cause); abstract void channelInactive(); } .MessageHandler Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | handle | [[handle]] Used when...FIXME | channelActive | [[channelActive]] Used when...FIXME | exceptionCaught | [[exceptionCaught]] Used when...FIXME | channelInactive | [[channelInactive]] Used when...FIXME |=== == [[implementations]] MessageHandlers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | MessageHandler | Description | network:TransportRequestHandler.md[] | [[TransportRequestHandler]] | TransportResponseHandler | [[TransportResponseHandler]] |===","title":"MessageHandler"},{"location":"network/OneForOneStreamManager/","text":"= OneForOneStreamManager OneForOneStreamManager is a network:StreamManager.md[]. == [[creating-instance]] Creating Instance OneForOneStreamManager takes no arguments to be created. OneForOneStreamManager is created for deploy:ExternalShuffleBlockHandler.md[] and storage:ExternalShuffleClient.md[ExternalShuffleClient] (indirectly via NoOpRpcHandler). == [[registerStream]] registerStream Method [source,java] \u00b6 long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"OneForOneStreamManager"},{"location":"network/OneForOneStreamManager/#sourcejava","text":"long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"[source,java]"},{"location":"network/RpcHandler/","text":"= RpcHandler RpcHandler is the < > of...FIXME [[ONE_WAY_CALLBACK]] RpcHandler uses a < > that...FIXME [[contract]] [source, java] package org.apache.spark.network.server; abstract class RpcHandler { // only required methods that have no implementation // the others follow abstract void receive( TransportClient client, ByteBuffer message, RpcResponseCallback callback); abstract StreamManager getStreamManager(); } .(Subset of) RpcHandler Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | receive a| [[receive]] Used when: AuthRpcHandler is requested to receive SaslRpcHandler is requested to receive (after authentication is complete) TransportRequestHandler is requested to network:TransportRequestHandler.md#processRpcRequest[processRpcRequest] | getStreamManager | [[getStreamManager]] Used when...FIXME |=== [[implementations]] .RpcHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | RpcHandler | Description | AuthRpcHandler | [[AuthRpcHandler]] | deploy:ExternalShuffleBlockHandler.md[] | [[ExternalShuffleBlockHandler]] | storage:NettyBlockRpcServer.md[] | [[NettyBlockRpcServer]] | NettyRpcHandler | [[NettyRpcHandler]] | NoOpRpcHandler | [[NoOpRpcHandler]] | SaslRpcHandler | [[SaslRpcHandler]] |=== == [[OneWayRpcCallback]] OneWayRpcCallback RpcResponseCallback OneWayRpcCallback is a RpcResponseCallback that simply prints out the WARN and ERROR for the following methods onSuccess and onFailure respectively. .void onSuccess(ByteBuffer response) Response provided for one-way RPC. .void onFailure(Throwable e) Error response provided for one-way RPC.","title":"RpcHandler"},{"location":"network/RpcResponseCallback/","text":"= RpcResponseCallback RpcResponseCallback is the < > of...FIXME [[contract]] [source, java] package org.apache.spark.network.client; interface RpcResponseCallback { void onSuccess(ByteBuffer response); void onFailure(Throwable e); } .RpcResponseCallback Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | onSuccess a| [[onSuccess]] Used when: NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#receive[receive RPC messages] (i.e. OpenBlocks and UploadBlock messages) RemoteNettyRpcCallContext is requested to send TransportResponseHandler is requested to handle a RpcResponse message ExternalShuffleBlockHandler is requested to handleMessage (i.e. OpenBlocks and RegisterExecutor messages) AuthRpcHandler and SaslRpcHandler are requested to receive Spark on Mesos' MesosExternalShuffleBlockHandler is requested to handleMessage (i.e. RegisterDriverParam message) | onFailure | [[onFailure]] Used when...FIXME |=== [[implementations]] .RpcResponseCallbacks [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | RpcResponseCallback | Description \"Unnamed\" in NettyBlockTransferService \"Unnamed\" in TransportRequestHandler \"Unnamed\" in TransportClient \"Unnamed\" in storage:OneForOneBlockFetcher.md[] | OneWayRpcCallback | [[OneWayRpcCallback]] | RegisterDriverCallback | [[RegisterDriverCallback]] | RpcOutboxMessage | [[RpcOutboxMessage]] |===","title":"RpcResponseCallback"},{"location":"network/SparkTransportConf/","text":"= SparkTransportConf SparkTransportConf is...FIXME == [[fromSparkConf]] Creating TransportConf [source,scala] \u00b6 fromSparkConf( _conf: SparkConf, module: String, numUsableCores: Int = 0): TransportConf fromSparkConf...FIXME fromSparkConf is used when...FIXME == [[defaultNumThreads]] Calculating Default Number of Threads [source, scala] \u00b6 defaultNumThreads( numUsableCores: Int): Int defaultNumThreads calculates the default number of threads for both the Netty client and server thread pools that is 8 maximum or numUsableCores is smaller. If numUsableCores is not specified, defaultNumThreads uses the number of processors available to the Java virtual machine. NOTE: 8 is the maximum number of threads for Netty and is not configurable. NOTE: defaultNumThreads uses ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--++[Java's Runtime for the number of processors in JVM].","title":"SparkTransportConf"},{"location":"network/SparkTransportConf/#sourcescala","text":"fromSparkConf( _conf: SparkConf, module: String, numUsableCores: Int = 0): TransportConf fromSparkConf...FIXME fromSparkConf is used when...FIXME == [[defaultNumThreads]] Calculating Default Number of Threads","title":"[source,scala]"},{"location":"network/SparkTransportConf/#source-scala","text":"defaultNumThreads( numUsableCores: Int): Int defaultNumThreads calculates the default number of threads for both the Netty client and server thread pools that is 8 maximum or numUsableCores is smaller. If numUsableCores is not specified, defaultNumThreads uses the number of processors available to the Java virtual machine. NOTE: 8 is the maximum number of threads for Netty and is not configurable. NOTE: defaultNumThreads uses ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--++[Java's Runtime for the number of processors in JVM].","title":"[source, scala]"},{"location":"network/StreamManager/","text":"= StreamManager StreamManager is an abstraction of...FIXME == [[implementations]] Available StreamManagers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StreamManager | Description | rpc:NettyStreamManager.md[] | [[NettyStreamManager]] | network:OneForOneStreamManager.md[OneForOneStreamManager] | [[OneForOneStreamManager]] |===","title":"StreamManager"},{"location":"network/TransportClientFactory/","text":"= TransportClientFactory TransportClientFactory is...FIXME == [[createUnmanagedClient]] createUnmanagedClient Method [source, java] \u00b6 TransportClient createUnmanagedClient(String remoteHost, int remotePort) throws IOException, InterruptedException createUnmanagedClient ...FIXME NOTE: createUnmanagedClient is used when...FIXME == [[createClient]] createClient Internal Method [source, java] \u00b6 TransportClient createClient(String remoteHost, int remotePort) throws IOException, InterruptedException TransportClient createClient(InetSocketAddress address) throws IOException, InterruptedException createClient ...FIXME [NOTE] \u00b6 createClient is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] and storage:NettyBlockTransferService.md#uploadBlock[uploadBlock] NettyRpcEnv is requested to createClient and downloadClient TransportClientFactory is requested to < >. ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#fetchBlocks[fetchBlocks] * Spark on Mesos' MesosExternalShuffleClient is requested to registerDriverWithShuffleService \u00b6","title":"TransportClientFactory"},{"location":"network/TransportClientFactory/#source-java","text":"TransportClient createUnmanagedClient(String remoteHost, int remotePort) throws IOException, InterruptedException createUnmanagedClient ...FIXME NOTE: createUnmanagedClient is used when...FIXME == [[createClient]] createClient Internal Method","title":"[source, java]"},{"location":"network/TransportClientFactory/#source-java_1","text":"TransportClient createClient(String remoteHost, int remotePort) throws IOException, InterruptedException TransportClient createClient(InetSocketAddress address) throws IOException, InterruptedException createClient ...FIXME","title":"[source, java]"},{"location":"network/TransportClientFactory/#note","text":"createClient is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#fetchBlocks[fetchBlocks] and storage:NettyBlockTransferService.md#uploadBlock[uploadBlock] NettyRpcEnv is requested to createClient and downloadClient TransportClientFactory is requested to < >. ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#fetchBlocks[fetchBlocks]","title":"[NOTE]"},{"location":"network/TransportClientFactory/#spark-on-mesos-mesosexternalshuffleclient-is-requested-to-registerdriverwithshuffleservice","text":"","title":"* Spark on Mesos' MesosExternalShuffleClient is requested to registerDriverWithShuffleService"},{"location":"network/TransportConf/","text":"= TransportConf TransportConf is a class for the transport-related network configuration for modules, e.g. deploy:ExternalShuffleService.md[ExternalShuffleService] or spark-on-yarn:spark-yarn-YarnShuffleService.md[YarnShuffleService]. TransportConf exposes methods to access settings for a single module as < > or < >. == [[spark.module.prefix]] spark.module.prefix Settings The settings can be in the form of spark.[module].[prefix] with the following prefixes: io.mode (default: NIO ) -- the IO mode: nio or epoll . io.preferDirectBufs (default: true ) -- a flag to control whether Spark prefers allocating off-heap byte buffers within Netty ( true ) or not ( false ). io.connectionTimeout (default: rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s ) -- the connection timeout in milliseconds. io.backLog (default: -1 for no backlog) -- the requested maximum length of the queue of incoming connections. io.numConnectionsPerPeer (default: 1 ) -- the number of concurrent connections between two nodes for fetching data. io.serverThreads (default: 0 i.e. 2x#cores) -- the number of threads used in the server thread pool. io.clientThreads (default: 0 i.e. 2x#cores) -- the number of threads used in the client thread pool. io.receiveBuffer (default: -1 ) -- the receive buffer size (SO_RCVBUF). io.sendBuffer (default: -1 ) -- the send buffer size (SO_SNDBUF). sasl.timeout (default: 30s ) -- the timeout (in milliseconds) for a single round trip of SASL token exchange. [[io.maxRetries]] io.maxRetries (default: 3 ) -- the maximum number of times Spark will try IO exceptions (such as connection timeouts) per request. If set to 0 , Spark will not do any retries. io.retryWait (default: 5s ) -- the time (in milliseconds) that Spark will wait in order to perform a retry after an IOException . Only relevant if io.maxRetries > 0. io.lazyFD (default: true ) -- controls whether to initialize FileDescriptor lazily ( true ) or not ( false ). If true , file descriptors are created only when data is going to be transferred. This can reduce the number of open files. == [[general-settings]] General Network-Related Settings === [[spark.storage.memoryMapThreshold]] spark.storage.memoryMapThreshold spark.storage.memoryMapThreshold (default: 2m ) is the minimum size of a block that we should start using memory map rather than reading in through normal IO operations. This prevents Spark from memory mapping very small blocks. In general, memory mapping has high overhead for blocks close to or below the page size of the OS. === [[spark.network.sasl.maxEncryptedBlockSize]] spark.network.sasl.maxEncryptedBlockSize spark.network.sasl.maxEncryptedBlockSize (default: 64k ) is the maximum number of bytes to be encrypted at a time when SASL encryption is enabled. === [[spark.network.sasl.serverAlwaysEncrypt]] spark.network.sasl.serverAlwaysEncrypt spark.network.sasl.serverAlwaysEncrypt (default: false ) controls whether the server should enforce encryption on SASL-authenticated connections ( true ) or not ( false ).","title":"TransportConf"},{"location":"network/TransportContext/","text":"= TransportContext TransportContext is used to create a < > or < >. == [[creating-instance]] Creating Instance TransportContext takes the following to be created: [[conf]] network:TransportConf.md[] [[rpcHandler]] network:RpcHandler.md[] [[closeIdleConnections]] closeIdleConnections flag (default: false ) TransportContext is created when: ExternalShuffleClient is requested to storage:ExternalShuffleClient.md#init[initialize] YarnShuffleService is requested to spark-on-yarn:spark-yarn-YarnShuffleService.md#serviceInit[serviceInit] == [[createClientFactory]] createClientFactory Method [source,java] \u00b6 TransportClientFactory createClientFactory( List bootstraps) createClientFactory...FIXME createClientFactory is used when TransportContext is requested to < >. == [[createChannelHandler]] createChannelHandler Method [source, java] \u00b6 TransportChannelHandler createChannelHandler( Channel channel, RpcHandler rpcHandler) createChannelHandler...FIXME createChannelHandler is used when TransportContext is requested to < >. == [[initializePipeline]] initializePipeline Method [source, java] \u00b6 TransportChannelHandler initializePipeline( SocketChannel channel) // <1> TransportChannelHandler initializePipeline( SocketChannel channel, RpcHandler channelRpcHandler) <1> Uses the < > initializePipeline...FIXME initializePipeline is used when: TransportServer is requested to network:TransportServer.md#init[init] TransportClientFactory is requested to network:TransportClientFactory.md#createClient[createClient] == [[createServer]] Creating Server [source, java] \u00b6 TransportServer createServer() TransportServer createServer( int port, List bootstraps) TransportServer createServer( List bootstraps) TransportServer createServer( String host, int port, List bootstraps) createServer simply creates a TransportServer (with the current TransportContext, the host, the port, the < > and the bootstraps). createServer is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#createServer[createServer] NettyRpcEnv is requested to startServer ExternalShuffleService is requested to deploy:ExternalShuffleService.md#start[start] Spark on YARN's YarnShuffleService is requested to serviceInit","title":"TransportContext"},{"location":"network/TransportContext/#sourcejava","text":"TransportClientFactory createClientFactory( List bootstraps) createClientFactory...FIXME createClientFactory is used when TransportContext is requested to < >. == [[createChannelHandler]] createChannelHandler Method","title":"[source,java]"},{"location":"network/TransportContext/#source-java","text":"TransportChannelHandler createChannelHandler( Channel channel, RpcHandler rpcHandler) createChannelHandler...FIXME createChannelHandler is used when TransportContext is requested to < >. == [[initializePipeline]] initializePipeline Method","title":"[source, java]"},{"location":"network/TransportContext/#source-java_1","text":"TransportChannelHandler initializePipeline( SocketChannel channel) // <1> TransportChannelHandler initializePipeline( SocketChannel channel, RpcHandler channelRpcHandler) <1> Uses the < > initializePipeline...FIXME initializePipeline is used when: TransportServer is requested to network:TransportServer.md#init[init] TransportClientFactory is requested to network:TransportClientFactory.md#createClient[createClient] == [[createServer]] Creating Server","title":"[source, java]"},{"location":"network/TransportContext/#source-java_2","text":"TransportServer createServer() TransportServer createServer( int port, List bootstraps) TransportServer createServer( List bootstraps) TransportServer createServer( String host, int port, List bootstraps) createServer simply creates a TransportServer (with the current TransportContext, the host, the port, the < > and the bootstraps). createServer is used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#createServer[createServer] NettyRpcEnv is requested to startServer ExternalShuffleService is requested to deploy:ExternalShuffleService.md#start[start] Spark on YARN's YarnShuffleService is requested to serviceInit","title":"[source, java]"},{"location":"network/TransportRequestHandler/","text":"= TransportRequestHandler TransportRequestHandler is a network:MessageHandler.md[] of < > from Netty's < >. == [[creating-instance]] Creating Instance TransportRequestHandler takes the following to be created: [[channel]] Netty's https://netty.io/4.1/api/io/netty/channel/Channel.html[Channel ] [[reverseClient]] TransportClient [[rpcHandler]] network:RpcHandler.md[] [[maxChunksBeingTransferred]] Maximum number of chunks allowed to be transferred at the same time TransportRequestHandler is created when TransportContext is requested to network:TransportContext.md#createChannelHandler[create a ChannelHandler]. == [[processRpcRequest]] processRpcRequest Internal Method [source, java] \u00b6 void processRpcRequest( RpcRequest req) processRpcRequest...FIXME processRpcRequest is used when TransportRequestHandler is requested to < >. == [[processFetchRequest]] processFetchRequest Internal Method [source, java] \u00b6 void processFetchRequest( ChunkFetchRequest req) processFetchRequest...FIXME processFetchRequest is used when TransportRequestHandler is requested to < >. == [[processOneWayMessage]] processOneWayMessage Internal Method [source, java] \u00b6 void processOneWayMessage(OneWayMessage req) \u00b6 processOneWayMessage ...FIXME NOTE: processOneWayMessage is used exclusively when TransportRequestHandler is requested to < > a OneWayMessage request. == [[processStreamRequest]] processStreamRequest Internal Method [source, java] \u00b6 void processStreamRequest(final StreamRequest req) \u00b6 processStreamRequest ...FIXME NOTE: processStreamRequest is used exclusively when TransportRequestHandler is requested to < > a StreamRequest request. == [[handle]] Handling RequestMessages -- handle Method [source, java] \u00b6 void handle(RequestMessage request) \u00b6 handle branches off per the type of the input RequestMessage : For ChunkFetchRequest requests, handle < > For RpcRequest requests, handle < > For OneWayMessage requests, handle < > For StreamRequest requests, handle < > For unknown requests, handle simply throws a IllegalArgumentException . Unknown request type: [request] handle is part of network:MessageHandler.md#handle[MessageHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.server.TransportRequestHandler logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.server.TransportRequestHandler=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"TransportRequestHandler"},{"location":"network/TransportRequestHandler/#source-java","text":"void processRpcRequest( RpcRequest req) processRpcRequest...FIXME processRpcRequest is used when TransportRequestHandler is requested to < >. == [[processFetchRequest]] processFetchRequest Internal Method","title":"[source, java]"},{"location":"network/TransportRequestHandler/#source-java_1","text":"void processFetchRequest( ChunkFetchRequest req) processFetchRequest...FIXME processFetchRequest is used when TransportRequestHandler is requested to < >. == [[processOneWayMessage]] processOneWayMessage Internal Method","title":"[source, java]"},{"location":"network/TransportRequestHandler/#source-java_2","text":"","title":"[source, java]"},{"location":"network/TransportRequestHandler/#void-processonewaymessageonewaymessage-req","text":"processOneWayMessage ...FIXME NOTE: processOneWayMessage is used exclusively when TransportRequestHandler is requested to < > a OneWayMessage request. == [[processStreamRequest]] processStreamRequest Internal Method","title":"void processOneWayMessage(OneWayMessage req)"},{"location":"network/TransportRequestHandler/#source-java_3","text":"","title":"[source, java]"},{"location":"network/TransportRequestHandler/#void-processstreamrequestfinal-streamrequest-req","text":"processStreamRequest ...FIXME NOTE: processStreamRequest is used exclusively when TransportRequestHandler is requested to < > a StreamRequest request. == [[handle]] Handling RequestMessages -- handle Method","title":"void processStreamRequest(final StreamRequest req)"},{"location":"network/TransportRequestHandler/#source-java_4","text":"","title":"[source, java]"},{"location":"network/TransportRequestHandler/#void-handlerequestmessage-request","text":"handle branches off per the type of the input RequestMessage : For ChunkFetchRequest requests, handle < > For RpcRequest requests, handle < > For OneWayMessage requests, handle < > For StreamRequest requests, handle < > For unknown requests, handle simply throws a IllegalArgumentException . Unknown request type: [request] handle is part of network:MessageHandler.md#handle[MessageHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.server.TransportRequestHandler logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"void handle(RequestMessage request)"},{"location":"network/TransportRequestHandler/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"network/TransportRequestHandler/#log4jloggerorgapachesparknetworkservertransportrequesthandlerall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.network.server.TransportRequestHandler=ALL"},{"location":"network/TransportServer/","text":"= TransportServer TransportServer is...FIXME == [[creating-instance]] Creating TransportServer Instance TransportServer takes the following when created: [[context]] network:TransportContext.md[] [[hostToBind]] Host name to bind to [[portToBind]] Port number to bind to [[appRpcHandler]] network:RpcHandler.md[] [[bootstraps]] TransportServerBootstraps When created, TransportServer < > with the < > and < > to bind to. TransportServer is created when TransportContext is requested to network:TransportContext.md#createServer[create a server]. == [[init]] init Internal Method [source, java] \u00b6 void init(String hostToBind, int portToBind) \u00b6 init ...FIXME NOTE: init is used exclusively when TransportServer is < >. == [[getPort]] getPort Method [source, java] \u00b6 int getPort() \u00b6 getPort ...FIXME [NOTE] \u00b6 getPort is used when: NettyRpcEnv is requested for the address * Spark on YARN's YarnShuffleService is requested to serviceInit \u00b6","title":"TransportServer"},{"location":"network/TransportServer/#source-java","text":"","title":"[source, java]"},{"location":"network/TransportServer/#void-initstring-hosttobind-int-porttobind","text":"init ...FIXME NOTE: init is used exclusively when TransportServer is < >. == [[getPort]] getPort Method","title":"void init(String hostToBind, int portToBind)"},{"location":"network/TransportServer/#source-java_1","text":"","title":"[source, java]"},{"location":"network/TransportServer/#int-getport","text":"getPort ...FIXME","title":"int getPort()"},{"location":"network/TransportServer/#note","text":"getPort is used when: NettyRpcEnv is requested for the address","title":"[NOTE]"},{"location":"network/TransportServer/#spark-on-yarns-yarnshuffleservice-is-requested-to-serviceinit","text":"","title":"* Spark on YARN's YarnShuffleService is requested to serviceInit"},{"location":"plugins/","text":"Plugin Framework \u00b6 Plugin Framework is an API for registering custom extensions ( plugins ) to be executed on the driver and executors. Plugin Framework uses the following main abstractions: PluginContainer SparkPlugin Plugin Framework was introduced in Spark 2.4.4 (that only offered an API for executors) with further changes in Spark 3.0.0 (to cover the driver). Resources \u00b6 Advanced Instrumentation in the official documentation of Apache Spark Commit for SPARK-29397 Spark Plugin Framework in 3.0 - Part 1: Introduction by Madhukara Phatak Spark Memory Monitor by squito SparkPlugins by Luca Canali (CERN)","title":"Plugin Framework"},{"location":"plugins/#plugin-framework","text":"Plugin Framework is an API for registering custom extensions ( plugins ) to be executed on the driver and executors. Plugin Framework uses the following main abstractions: PluginContainer SparkPlugin Plugin Framework was introduced in Spark 2.4.4 (that only offered an API for executors) with further changes in Spark 3.0.0 (to cover the driver).","title":"Plugin Framework"},{"location":"plugins/#resources","text":"Advanced Instrumentation in the official documentation of Apache Spark Commit for SPARK-29397 Spark Plugin Framework in 3.0 - Part 1: Introduction by Madhukara Phatak Spark Memory Monitor by squito SparkPlugins by Luca Canali (CERN)","title":"Resources"},{"location":"plugins/DriverPlugin/","text":"DriverPlugin \u00b6 DriverPlugin is...FIXME","title":"DriverPlugin"},{"location":"plugins/DriverPlugin/#driverplugin","text":"DriverPlugin is...FIXME","title":"DriverPlugin"},{"location":"plugins/DriverPluginContainer/","text":"DriverPluginContainer \u00b6 DriverPluginContainer is...FIXME","title":"DriverPluginContainer"},{"location":"plugins/DriverPluginContainer/#driverplugincontainer","text":"DriverPluginContainer is...FIXME","title":"DriverPluginContainer"},{"location":"plugins/ExecutorPlugin/","text":"ExecutorPlugin \u00b6 ExecutorPlugin is...FIXME","title":"ExecutorPlugin"},{"location":"plugins/ExecutorPlugin/#executorplugin","text":"ExecutorPlugin is...FIXME","title":"ExecutorPlugin"},{"location":"plugins/ExecutorPluginContainer/","text":"ExecutorPluginContainer \u00b6 ExecutorPluginContainer is...FIXME","title":"ExecutorPluginContainer"},{"location":"plugins/ExecutorPluginContainer/#executorplugincontainer","text":"ExecutorPluginContainer is...FIXME","title":"ExecutorPluginContainer"},{"location":"plugins/PluginContainer/","text":"PluginContainer \u00b6 PluginContainer is an abstraction of plugin containers that can registerMetrics (for the driver and executors). PluginContainer is created for the driver and executors using apply utility. Contract \u00b6 registerMetrics \u00b6 registerMetrics ( appId : String ) : Unit Used when SparkContext is created shutdown \u00b6 shutdown () : Unit Used when: SparkContext is requested to stop Executor is requested to stop Implementations \u00b6 Sealed Abstract Class PluginContainer is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). DriverPluginContainer ExecutorPluginContainer Creating PluginContainer \u00b6 // the driver apply ( sc : SparkContext , resources : java.util.Map [ String , ResourceInformation ]) : Option [ PluginContainer ] // executors apply ( env : SparkEnv , resources : java.util.Map [ String , ResourceInformation ]) : Option [ PluginContainer ] // private helper apply ( ctx : Either [ SparkContext , SparkEnv ], resources : java.util.Map [ String , ResourceInformation ]) : Option [ PluginContainer ] apply creates a PluginContainer for the driver or executors (based on the type of the first input argument, i.e. SparkContext or SparkEnv , respectively). apply first loads the SparkPlugin s defined by spark.plugins configuration property. Only when there was at least one plugin loaded, apply creates a DriverPluginContainer or ExecutorPluginContainer . apply is used when: SparkContext is created Executor is created","title":"PluginContainer"},{"location":"plugins/PluginContainer/#plugincontainer","text":"PluginContainer is an abstraction of plugin containers that can registerMetrics (for the driver and executors). PluginContainer is created for the driver and executors using apply utility.","title":"PluginContainer"},{"location":"plugins/PluginContainer/#contract","text":"","title":"Contract"},{"location":"plugins/PluginContainer/#registermetrics","text":"registerMetrics ( appId : String ) : Unit Used when SparkContext is created","title":" registerMetrics"},{"location":"plugins/PluginContainer/#shutdown","text":"shutdown () : Unit Used when: SparkContext is requested to stop Executor is requested to stop","title":" shutdown"},{"location":"plugins/PluginContainer/#implementations","text":"Sealed Abstract Class PluginContainer is a Scala sealed abstract class which means that all of the implementations are in the same compilation unit (a single file). DriverPluginContainer ExecutorPluginContainer","title":"Implementations"},{"location":"plugins/PluginContainer/#creating-plugincontainer","text":"// the driver apply ( sc : SparkContext , resources : java.util.Map [ String , ResourceInformation ]) : Option [ PluginContainer ] // executors apply ( env : SparkEnv , resources : java.util.Map [ String , ResourceInformation ]) : Option [ PluginContainer ] // private helper apply ( ctx : Either [ SparkContext , SparkEnv ], resources : java.util.Map [ String , ResourceInformation ]) : Option [ PluginContainer ] apply creates a PluginContainer for the driver or executors (based on the type of the first input argument, i.e. SparkContext or SparkEnv , respectively). apply first loads the SparkPlugin s defined by spark.plugins configuration property. Only when there was at least one plugin loaded, apply creates a DriverPluginContainer or ExecutorPluginContainer . apply is used when: SparkContext is created Executor is created","title":" Creating PluginContainer"},{"location":"plugins/SparkPlugin/","text":"SparkPlugin \u00b6 SparkPlugin is...FIXME","title":"SparkPlugin"},{"location":"plugins/SparkPlugin/#sparkplugin","text":"SparkPlugin is...FIXME","title":"SparkPlugin"},{"location":"rdd/","text":"Resilient Distributed Dataset (RDD) \u00b6 Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a spark-cluster.md[cluster]. Dataset is a collection of spark-rdd-partitions.md[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's scheduler:DAGScheduler.md#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage spark-data-locality.md[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of spark-rdd-partitions.md[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] spark-rdd-partitions.md[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of spark-rdd-shuffle.md[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka rdd:RDD.md#dependencies[RDD dependencies]) An array of spark-rdd-partitions.md[partitions] that a dataset is divided to. A rdd:RDD.md#compute[compute] function to do a computation on partitions. An optional rdd:Partitioner.md[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a ROOT:SparkContext.md[] (available as context property). RDDs live in one and only one ROOT:SparkContext.md[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see ROOT:SparkContext.md#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using executor:Executor.md[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in rdd:spark-rdd-transformations.md[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section spark-rdd-actions.md[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You spark-rdd-caching.md[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to spark-io.md[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for spark-shell.md[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ): [source,scala] \u00b6 val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"Resilient Distributed Dataset"},{"location":"rdd/#resilient-distributed-dataset-rdd","text":"Resilient Distributed Dataset (aka RDD ) is the primary data abstraction in Apache Spark and the core of Spark (that I often refer to as \"Spark Core\"). .The origins of RDD The original paper that gave birth to the concept of RDD is https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing] by Matei Zaharia, et al. An RDD is a description of a fault-tolerant and resilient computation over a distributed collection of records (spread over < >). NOTE: One could compare RDDs to collections in Scala, i.e. a RDD is computed on many JVMs while a Scala collection lives on a single JVM. Using RDD Spark hides data partitioning and so distribution that in turn allowed them to design parallel computational framework with a higher-level programming interface (API) for four mainstream programming languages. The features of RDDs (decomposing the name): Resilient , i.e. fault-tolerant with the help of < > and so able to recompute missing or damaged partitions due to node failures. Distributed with data residing on multiple nodes in a spark-cluster.md[cluster]. Dataset is a collection of spark-rdd-partitions.md[partitioned data] with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with). .RDDs image::spark-rdds.png[align=\"center\"] From the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. From the original paper about RDD - https://cs.stanford.edu/~matei/papers/2012/nsdi_spark.pdf[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]: Resilient Distributed Datasets (RDDs) are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Beside the above traits (that are directly embedded in the name of the data abstraction - RDD) it has the following additional traits: In-Memory , i.e. data inside RDD is stored in memory as much (size) and long (time) as possible. Immutable or Read-Only , i.e. it does not change once created and can only be transformed using transformations to new RDDs. Lazy evaluated , i.e. the data inside RDD is not available or transformed until an action is executed that triggers the execution. Cacheable , i.e. you can hold all the data in a persistent \"storage\" like memory (default and the most preferred) or disk (the least preferred due to access speed). Parallel , i.e. process data in parallel. Typed -- RDD records have types, e.g. Long in RDD[Long] or (Int, String) in RDD[(Int, String)] . Partitioned -- records are partitioned (split into logical partitions) and distributed across nodes in a cluster. Location-Stickiness -- RDD can define < > to compute partitions (as close to the records as possible). NOTE: Preferred location (aka locality preferences or placement preferences or locality info ) is information about the locations of RDD records (that Spark's scheduler:DAGScheduler.md#preferred-locations[DAGScheduler] uses to place computing partitions on to have the tasks as close to the data as possible). Computing partitions in a RDD is a distributed process by design and to achieve even data distribution as well as leverage spark-data-locality.md[data locality] (in distributed systems like HDFS or Cassandra in which data is partitioned by default), they are partitioned to a fixed number of spark-rdd-partitions.md[partitions] - logical chunks (parts) of data. The logical division is for processing only and internally it is not divided whatsoever. Each partition comprises of records . .RDDs image::spark-rdd-partitioned-distributed.png[align=\"center\"] spark-rdd-partitions.md[Partitions are the units of parallelism]. You can control the number of partitions of a RDD using spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations. Spark tries to be as close to data as possible without wasting time to send data across network by means of spark-rdd-shuffle.md[RDD shuffling], and creates as many partitions as required to follow the storage layout and thus optimize data access. It leads to a one-to-one mapping between (physical) data in distributed data storage, e.g. HDFS or Cassandra, and partitions. RDDs support two kinds of operations: < > - lazy operations that return another RDD. < > - operations that trigger computation and return values. The motivation to create RDD were ( https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf[after the authors]) two types of applications that current computing frameworks handle inefficiently: iterative algorithms in machine learning and graph computations. interactive data mining tools as ad-hoc queries on the same dataset. The goal is to reuse intermediate in-memory results across multiple data-intensive workloads with no need for copying large amounts of data over the network. Technically, RDDs follow the < > defined by the five main intrinsic properties: [[dependencies]] Parent RDDs (aka rdd:RDD.md#dependencies[RDD dependencies]) An array of spark-rdd-partitions.md[partitions] that a dataset is divided to. A rdd:RDD.md#compute[compute] function to do a computation on partitions. An optional rdd:Partitioner.md[Partitioner] that defines how keys are hashed, and the pairs partitioned (for key-value RDDs) Optional < > (aka locality info ), i.e. hosts for a partition where the records live or are the closest to read from. This RDD abstraction supports an expressive set of operations without having to modify scheduler for each one. [[context]] An RDD is a named (by name ) and uniquely identified (by id ) entity in a ROOT:SparkContext.md[] (available as context property). RDDs live in one and only one ROOT:SparkContext.md[] that creates a logical boundary. NOTE: RDDs cannot be shared between SparkContexts (see ROOT:SparkContext.md#sparkcontext-and-rdd[SparkContext and RDDs]). An RDD can optionally have a friendly name accessible using name that can be changed using = : scala> val ns = sc.parallelize(0 to 10) ns: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:24 scala> ns.id res0: Int = 2 scala> ns.name res1: String = null scala> ns.name = \"Friendly name\" ns.name: String = Friendly name scala> ns.name res2: String = Friendly name scala> ns.toDebugString res3: String = (8) Friendly name ParallelCollectionRDD[2] at parallelize at <console>:24 [] RDDs are a container of instructions on how to materialize big (arrays of) distributed data, and how to split it into partitions so Spark (using executor:Executor.md[executors]) can hold some of them. In general data distribution can help executing processing in parallel so a task processes a chunk of data that it could eventually keep in memory. Spark does jobs in parallel, and RDDs are split into partitions to be processed and written in parallel. Inside a partition, data is processed sequentially. Saving partitions results in part-files instead of one single file (unless there is a single partition). == [[transformations]] Transformations A transformation is a lazy operation on a RDD that returns another RDD, e.g. map , flatMap , filter , reduceByKey , join , cogroup , etc. Find out more in rdd:spark-rdd-transformations.md[Transformations]. == [[actions]] Actions An action is an operation that triggers execution of < > and returns a value (to a Spark driver - the user program). TIP: Go in-depth in the section spark-rdd-actions.md[Actions]. == [[creating-rdds]] Creating RDDs === SparkContext.parallelize One way to create a RDD is with SparkContext.parallelize method. It accepts a collection of elements as shown below ( sc is a SparkContext instance): scala> val rdd = sc.parallelize(1 to 1000) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25 You may also want to randomize the sample data: scala> val data = Seq.fill(10)(util.Random.nextInt) data: Seq[Int] = List(-964985204, 1662791, -1820544313, -383666422, -111039198, 310967683, 1114081267, 1244509086, 1797452433, 124035586) scala> val rdd = sc.parallelize(data) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:29 Given the reason to use Spark to process more data than your own laptop could handle, SparkContext.parallelize is mainly used to learn Spark in the Spark shell. SparkContext.parallelize requires all the data to be available on a single machine - the Spark driver - that eventually hits the limits of your laptop. === SparkContext.makeRDD CAUTION: FIXME What's the use case for makeRDD ? scala> sc.makeRDD(0 to 1000) res0: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at makeRDD at <console>:25 === SparkContext.textFile One of the easiest ways to create an RDD is to use SparkContext.textFile to read files. You can use the local README.md file (and then flatMap over the lines inside to have an RDD of words): scala> val words = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\W+\")).cache words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at flatMap at <console>:24 NOTE: You spark-rdd-caching.md[cache] it so the computation is not performed every time you work with words . == [[creating-rdds-from-input]] Creating RDDs from Input Refer to spark-io.md[Using Input and Output (I/O)] to learn about the IO API to create RDDs. === Transformations RDD transformations by definition transform an RDD into another RDD and hence are the way to create new ones. Refer to < > section to learn more. == RDDs in Web UI It is quite informative to look at RDDs in the Web UI that is at http://localhost:4040 for spark-shell.md[Spark shell]. Execute the following Spark application (type all the lines in spark-shell ):","title":"Resilient Distributed Dataset (RDD)"},{"location":"rdd/#sourcescala","text":"val ints = sc.parallelize(1 to 100) // <1> ints.setName(\"Hundred ints\") // <2> ints.cache // <3> ints.count // <4> <1> Creates an RDD with hundred of numbers (with as many partitions as possible) <2> Sets the name of the RDD <3> Caches the RDD for performance reasons that also makes it visible in Storage tab in the web UI <4> Executes action (and materializes the RDD) With the above executed, you should see the following in the Web UI: .RDD with custom name image::spark-ui-rdd-name.png[align=\"center\"] Click the name of the RDD (under RDD Name ) and you will get the details of how the RDD is cached. .RDD Storage Info image::spark-ui-storage-hundred-ints.png[align=\"center\"] Execute the following Spark job and you will see how the number of partitions decreases. ints.repartition(2).count .Number of tasks after repartition image::spark-ui-repartition-2.png[align=\"center\"]","title":"[source,scala]"},{"location":"rdd/Aggregator/","text":"Aggregator \u00b6 Aggregator is a set of < > used to aggregate data using rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a ShuffleDependency and ExternalSorter . == [[combineValuesByKey]] combineValuesByKey Method [source, scala] \u00b6 combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to shuffle:ExternalAppendOnlyMap.md#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an shuffle:ExternalAppendOnlyMap.md#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag off) == [[combineCombinersByKey]] combineCombinersByKey Method [source, scala] \u00b6 combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag on). == [[updateMetrics]] Updating Task Metrics [source, scala] \u00b6 updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input scheduler:spark-TaskContext.md[TaskContext] for the scheduler:spark-TaskContext.md#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap]: executor:TaskMetrics.md#incMemoryBytesSpilled[Increment memory bytes spilled] executor:TaskMetrics.md#incDiskBytesSpilled[Increment disk bytes spilled] executor:TaskMetrics.md#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"Aggregator"},{"location":"rdd/Aggregator/#aggregator","text":"Aggregator is a set of < > used to aggregate data using rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation. Aggregator[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. [[creating-instance]][[aggregation-functions]] Aggregator transforms an RDD[(K, V)] into an RDD[(K, C)] (for a \"combined type\" C) using the functions: [[createCombiner]] createCombiner: V => C [[mergeValue]] mergeValue: (C, V) => C [[mergeCombiners]] mergeCombiners: (C, C) => C Aggregator is used to create a ShuffleDependency and ExternalSorter . == [[combineValuesByKey]] combineValuesByKey Method","title":"Aggregator"},{"location":"rdd/Aggregator/#source-scala","text":"combineValuesByKey( iter: Iterator[_ <: Product2[K, V]], context: TaskContext): Iterator[(K, C)] combineValuesByKey creates a new shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap] (with the < >). combineValuesByKey requests the ExternalAppendOnlyMap to shuffle:ExternalAppendOnlyMap.md#insertAll[insert all key-value pairs] from the given iterator (that is the values of a partition). combineValuesByKey < >. In the end, combineValuesByKey requests the ExternalAppendOnlyMap for an shuffle:ExternalAppendOnlyMap.md#iterator[iterator of \"combined\" pairs]. combineValuesByKey is used when: rdd:PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation is used (with the same Partitioner as the RDD's) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag off) == [[combineCombinersByKey]] combineCombinersByKey Method","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_1","text":"combineCombinersByKey( iter: Iterator[_ <: Product2[K, C]], context: TaskContext): Iterator[(K, C)] combineCombinersByKey...FIXME combineCombinersByKey is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with the Map-Size Partial Aggregation Flag on). == [[updateMetrics]] Updating Task Metrics","title":"[source, scala]"},{"location":"rdd/Aggregator/#source-scala_2","text":"updateMetrics( context: TaskContext, map: ExternalAppendOnlyMap[_, _, _]): Unit updateMetrics requests the input scheduler:spark-TaskContext.md[TaskContext] for the scheduler:spark-TaskContext.md#taskMetrics[TaskMetrics] to update the metrics based on the metrics of the input shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap]: executor:TaskMetrics.md#incMemoryBytesSpilled[Increment memory bytes spilled] executor:TaskMetrics.md#incDiskBytesSpilled[Increment disk bytes spilled] executor:TaskMetrics.md#incPeakExecutionMemory[Increment peak execution memory] updateMetrics is used when Aggregator is requested to < > and < >.","title":"[source, scala]"},{"location":"rdd/AsyncRDDActions/","text":"AsyncRDDActions \u00b6 AsyncRDDActions is...FIXME","title":"AsyncRDDActions"},{"location":"rdd/AsyncRDDActions/#asyncrddactions","text":"AsyncRDDActions is...FIXME","title":"AsyncRDDActions"},{"location":"rdd/CheckpointRDD/","text":"= CheckpointRDD CheckpointRDD is...FIXME","title":"CheckpointRDD"},{"location":"rdd/CoGroupedRDD/","text":"CoGroupedRDD \u00b6 A RDD that cogroups its pair RDD parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Use RDD.cogroup(...) to create one. == [[getDependencies]] getDependencies Method CAUTION: FIXME == [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])] \u00b6 compute...FIXME compute is part of rdd:RDD.md#compute[RDD] abstraction.","title":"CoGroupedRDD"},{"location":"rdd/CoGroupedRDD/#cogroupedrdd","text":"A RDD that cogroups its pair RDD parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key. Use RDD.cogroup(...) to create one. == [[getDependencies]] getDependencies Method CAUTION: FIXME == [[compute]] Computing Partition (in TaskContext)","title":"CoGroupedRDD"},{"location":"rdd/CoGroupedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/CoGroupedRDD/#computes-partition-context-taskcontext-iteratork-arrayiterable_","text":"compute...FIXME compute is part of rdd:RDD.md#compute[RDD] abstraction.","title":"compute(s: Partition, context: TaskContext): Iterator[(K, Array[Iterable[_]])]"},{"location":"rdd/Dependency/","text":"RDD Dependencies \u00b6 Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency. [source, scala] \u00b6 def rdd: RDD[T] \u00b6 Whenever you apply a spark-rdd-transformations.md[transformation] (e.g. map , flatMap ) to a RDD you build the so-called spark-rdd-lineage.md[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: NarrowDependency and ShuffleDependency are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | NarrowDependency | | ShuffleDependency | | OneToOneDependency | | PruneDependency | | RangeDependency | |=== [NOTE] \u00b6 The dependencies of a RDD are available using rdd:index.md#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use spark-rdd-lineage.md#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way. scala> myRdd.toDebugString res6: String = (8) ShuffledRDD[8] at groupBy at <console>:24 [] +-(8) MapPartitionsRDD[7] at groupBy at <console>:24 [] | ParallelCollectionRDD[6] at parallelize at <console>:24 [] \u00b6","title":"Dependencies"},{"location":"rdd/Dependency/#rdd-dependencies","text":"Dependency class is the base (abstract) class to model a dependency relationship between two or more RDDs. [[rdd]] Dependency has a single method rdd to access the RDD that is behind a dependency.","title":"RDD Dependencies"},{"location":"rdd/Dependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Dependency/#def-rdd-rddt","text":"Whenever you apply a spark-rdd-transformations.md[transformation] (e.g. map , flatMap ) to a RDD you build the so-called spark-rdd-lineage.md[RDD lineage graph]. Dependency -ies represent the edges in a lineage graph. NOTE: NarrowDependency and ShuffleDependency are the two top-level subclasses of Dependency abstract class. .Kinds of Dependencies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | NarrowDependency | | ShuffleDependency | | OneToOneDependency | | PruneDependency | | RangeDependency | |===","title":"def rdd: RDD[T]"},{"location":"rdd/Dependency/#note","text":"The dependencies of a RDD are available using rdd:index.md#dependencies[dependencies] method. // A demo RDD scala> val myRdd = sc.parallelize(0 to 9).groupBy(_ % 2) myRdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[8] at groupBy at <console>:24 scala> myRdd.foreach(println) (0,CompactBuffer(0, 2, 4, 6, 8)) (1,CompactBuffer(1, 3, 5, 7, 9)) scala> myRdd.dependencies res5: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@27ace619) // Access all RDDs in the demo RDD lineage scala> myRdd.dependencies.map(_.rdd).foreach(println) MapPartitionsRDD[7] at groupBy at <console>:24 You use spark-rdd-lineage.md#toDebugString[toDebugString] method to print out the RDD lineage in a user-friendly way.","title":"[NOTE]"},{"location":"rdd/Dependency/#scala-myrddtodebugstring-res6-string-8-shuffledrdd8-at-groupby-at-console24-8-mappartitionsrdd7-at-groupby-at-console24-parallelcollectionrdd6-at-parallelize-at-console24","text":"","title":"scala&gt; myRdd.toDebugString\nres6: String =\n(8) ShuffledRDD[8] at groupBy at &lt;console&gt;:24 []\n +-(8) MapPartitionsRDD[7] at groupBy at &lt;console&gt;:24 []\n    |  ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24 []\n"},{"location":"rdd/HashPartitioner/","text":"= HashPartitioner HashPartitioner is a rdd:Partitioner.md[Partitioner] for hash-based partitioning. HashPartitioner is used as the default Partitioner. == [[partitions]][[numPartitions]] Number of Partitions HashPartitioner takes a number of partitions to be created. HashPartitioner uses the number of partitions to find the < > (of a key-value record). == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the rdd:Partitioner.md#getPartition[Partitioner] abstraction. == [[equals]] equals Method [source, scala] \u00b6 equals(other: Any): Boolean \u00b6 Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method [source, scala] \u00b6 hashCode: Int \u00b6 hashCode is the < >.","title":"HashPartitioner"},{"location":"rdd/HashPartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#getpartitionkey-any-int","text":"getPartition returns 0 as the partition ID for null keys. For non- null keys, getPartition uses the key's {java-javadoc-url}/java/lang/Object.html#++hashCode--++[Object.hashCode] modulo the configured < >. For a negative result, getPartition adds the < > (used for the modulo operator) to make it positive. getPartition is part of the rdd:Partitioner.md#getPartition[Partitioner] abstraction. == [[equals]] equals Method","title":"getPartition(key: Any): Int"},{"location":"rdd/HashPartitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#equalsother-any-boolean","text":"Two HashPartitioners are considered equal when the < > are the same. == [[hashCode]] hashCode Method","title":"equals(other: Any): Boolean"},{"location":"rdd/HashPartitioner/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/HashPartitioner/#hashcode-int","text":"hashCode is the < >.","title":"hashCode: Int"},{"location":"rdd/LocalRDDCheckpointData/","text":"= LocalRDDCheckpointData LocalRDDCheckpointData is...FIXME","title":"LocalRDDCheckpointData"},{"location":"rdd/NarrowDependency/","text":"== [[NarrowDependency]] NarrowDependency -- Narrow Dependencies NarrowDependency is a base (abstract) Dependency.md[Dependency] with narrow (limited) number of spark-rdd-Partition.md[partitions] of the parent RDD that are required to compute a partition of the child RDD. NOTE: Narrow dependencies allow for pipelined execution. .Concrete NarrowDependency -ies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | | < > | | < > | |=== === [[contract]] NarrowDependency Contract NarrowDependency contract assumes that extensions implement getParents method. [source, scala] \u00b6 def getParents(partitionId: Int): Seq[Int] \u00b6 getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"NarrowDependency"},{"location":"rdd/NarrowDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/NarrowDependency/#def-getparentspartitionid-int-seqint","text":"getParents returns the partitions of the parent RDD that the input partitionId depends on. === [[OneToOneDependency]] OneToOneDependency OneToOneDependency is a narrow dependency that represents a one-to-one dependency between partitions of the parent and child RDDs. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r3 = r1.map((_, 1)) r3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[19] at map at <console>:20 scala> r3.dependencies res32: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.OneToOneDependency@7353a0fb) scala> r3.toDebugString res33: String = (8) MapPartitionsRDD[19] at map at <console>:20 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] === [[PruneDependency]] PruneDependency PruneDependency is a narrow dependency that represents a dependency between the PartitionPruningRDD and its parent RDD. === [[RangeDependency]] RangeDependency RangeDependency is a narrow dependency that represents a one-to-one dependency between ranges of partitions in the parent and child RDDs. It is used in UnionRDD for SparkContext.union , RDD.union transformation to list only a few. scala> val r1 = sc.parallelize(0 to 9) r1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:18 scala> val r2 = sc.parallelize(10 to 19) r2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:18 scala> val unioned = sc.union(r1, r2) unioned: org.apache.spark.rdd.RDD[Int] = UnionRDD[16] at union at <console>:22 scala> unioned.dependencies res19: Seq[org.apache.spark.Dependency[_]] = ArrayBuffer(org.apache.spark.RangeDependency@28408ad7, org.apache.spark.RangeDependency@6e1d2e9f) scala> unioned.toDebugString res18: String = (16) UnionRDD[16] at union at <console>:22 [] | ParallelCollectionRDD[13] at parallelize at <console>:18 [] | ParallelCollectionRDD[14] at parallelize at <console>:18 []","title":"def getParents(partitionId: Int): Seq[Int]"},{"location":"rdd/PairRDDFunctions/","text":"= [[PairRDDFunctions]] PairRDDFunctions :page-toctitle: Transformations PairRDDFunctions is an extension of RDD API to provide additional < > for RDDs of key-value pairs ( RDD[(K, V)] ). PairRDDFunctions is available in RDDs of key-value pairs via Scala implicit conversion. [[transformations]] .PairRDDFunctions' Transformations [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregateByKey a| [[aggregateByKey]] [source, scala] \u00b6 aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]] [source, scala] \u00b6 combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]] [source, scala] \u00b6 countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]] [source, scala] \u00b6 flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]] [source, scala] \u00b6 foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]] [source, scala] \u00b6 mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]] [source, scala] \u00b6 partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]] [source, scala] \u00b6 saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]] [source, scala] \u00b6 saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]] [source, scala] \u00b6 saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]] [source, scala] \u00b6 saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon spark-data-sources.md[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag [source, scala] \u00b6 combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the rdd:Partitioner.md#defaultPartitioner[default partitioner] <2> Uses a rdd:HashPartitioner.md[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an rdd:Aggregator.md[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given rdd:Partitioner.md[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply rdd:spark-rdd-transformations.md#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the rdd:Aggregator.md#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a rdd:ShuffledRDD.md[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException: [source,plaintext] \u00b6 mergeCombiners must be defined \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled: [source,plaintext] \u00b6 Cannot use map-side combining with array keys. \u00b6 combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a rdd:HashPartitioner.md[HashPartitioner]: [source,plaintext] \u00b6 HashPartitioner cannot partition array keys. \u00b6 === [[combineByKeyWithClassTag-example]] Example [source,scala] \u00b6 val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"PairRDDFunctions"},{"location":"rdd/PairRDDFunctions/#source-scala","text":"aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] aggregateByKey U: ClassTag ( seqOp: (U, V) => U, combOp: (U, U) => U): RDD[(K, U)] | combineByKey a| [[combineByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_1","text":"combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] combineByKey C : RDD[(K, C)] | countApproxDistinctByKey a| [[countApproxDistinctByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_2","text":"countApproxDistinctByKey( relativeSD: Double = 0.05): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, numPartitions: Int): RDD[(K, Long)] countApproxDistinctByKey( relativeSD: Double, partitioner: Partitioner): RDD[(K, Long)] countApproxDistinctByKey( p: Int, sp: Int, partitioner: Partitioner): RDD[(K, Long)] | flatMapValues a| [[flatMapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_3","text":"flatMapValues U : RDD[(K, U)] | foldByKey a| [[foldByKey]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_4","text":"foldByKey( zeroValue: V)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, numPartitions: Int)( func: (V, V) => V): RDD[(K, V)] foldByKey( zeroValue: V, partitioner: Partitioner)( func: (V, V) => V): RDD[(K, V)] | mapValues a| [[mapValues]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_5","text":"mapValues U : RDD[(K, U)] | partitionBy a| [[partitionBy]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_6","text":"partitionBy( partitioner: Partitioner): RDD[(K, V)] | saveAsHadoopDataset a| [[saveAsHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_7","text":"saveAsHadoopDataset( conf: JobConf): Unit saveAsHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapred/JobConf.html[JobConf ]) | saveAsHadoopFile a| [[saveAsHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_8","text":"saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], codec: Class[ <: CompressionCodec]): Unit saveAsHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: OutputFormat[ , _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[ <: CompressionCodec]] = None): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit saveAsHadoopFile F <: OutputFormat[K, V] (implicit fm: ClassTag[F]): Unit | saveAsNewAPIHadoopDataset a| [[saveAsNewAPIHadoopDataset]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_9","text":"saveAsNewAPIHadoopDataset( conf: Configuration): Unit Saves this RDD of key-value pairs ( RDD[K,V] ) to any Hadoop-supported storage system with new Hadoop API (using a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] object for that storage system). The configuration should set relevant output params (an https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/mapreduce/OutputFormat.html[output format], output paths, e.g. a table name to write to) in the same way as it would be configured for a Hadoop MapReduce job. saveAsNewAPIHadoopDataset uses the SparkHadoopWriter utility to < > with a < > (for the given Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) | saveAsNewAPIHadoopFile a| [[saveAsNewAPIHadoopFile]]","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_10","text":"saveAsNewAPIHadoopFile( path: String, keyClass: Class[ ], valueClass: Class[ ], outputFormatClass: Class[_ <: NewOutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile F <: NewOutputFormat[K, V] (implicit fm: ClassTag[F]): Unit |=== == [[reduceByKey]][[groupByKey]] groupByKey and reduceByKey reduceByKey is sort of a particular case of < >. You may want to look at the number of partitions from another angle. It may often not be important to have a given number of partitions upfront (at RDD creation time upon spark-data-sources.md[loading data from data sources]), so only \"regrouping\" the data by key after it is an RDD might be...the key ( pun not intended ). You can use groupByKey or another PairRDDFunctions method to have a key in one processing flow. You could use partitionBy that is available for RDDs to be RDDs of tuples, i.e. PairRDD : rdd.keyBy(_.kind) .partitionBy(new HashPartitioner(PARTITIONS)) .foreachPartition(...) Think of situations where kind has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution. You could do as follows: rdd.keyBy(_.kind).reduceByKey(....) or mapValues or plenty of other solutions. FIXME, man . == [[combineByKeyWithClassTag]] combineByKeyWithClassTag","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#source-scala_11","text":"combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <1> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] // <2> combineByKeyWithClassTag C (implicit ct: ClassTag[C]): RDD[(K, C)] <1> Uses the rdd:Partitioner.md#defaultPartitioner[default partitioner] <2> Uses a rdd:HashPartitioner.md[HashPartitioner] with the given number of partitions combineByKeyWithClassTag creates an rdd:Aggregator.md[Aggregator] for the given aggregation functions. combineByKeyWithClassTag branches off per the given rdd:Partitioner.md[Partitioner]. If the input partitioner and the RDD's are the same, combineByKeyWithClassTag simply rdd:spark-rdd-transformations.md#mapPartitions[mapPartitions] on the RDD with the following arguments: Iterator of the rdd:Aggregator.md#combineValuesByKey[Aggregator] preservesPartitioning flag turned on If the input partitioner is different than the RDD's, combineByKeyWithClassTag creates a rdd:ShuffledRDD.md[ShuffledRDD] (with the Serializer, the Aggregator, and the mapSideCombine flag). === [[combineByKeyWithClassTag-usage]] Usage combineByKeyWithClassTag lays the foundation for the following transformations: < > < > < > < > < > < > === [[combineByKeyWithClassTag-requirements]] Requirements combineByKeyWithClassTag requires that the mergeCombiners is defined (not- null ) or throws an IllegalArgumentException:","title":"[source, scala]"},{"location":"rdd/PairRDDFunctions/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#mergecombiners-must-be-defined","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the mapSideCombine flag enabled:","title":"mergeCombiners must be defined"},{"location":"rdd/PairRDDFunctions/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#cannot-use-map-side-combining-with-array-keys","text":"combineByKeyWithClassTag throws a SparkException for the keys being of type array with the partitioner being a rdd:HashPartitioner.md[HashPartitioner]:","title":"Cannot use map-side combining with array keys."},{"location":"rdd/PairRDDFunctions/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/PairRDDFunctions/#hashpartitioner-cannot-partition-array-keys","text":"=== [[combineByKeyWithClassTag-example]] Example","title":"HashPartitioner cannot partition array keys."},{"location":"rdd/PairRDDFunctions/#sourcescala","text":"val nums = sc.parallelize(0 to 9, numSlices = 4) val groups = nums.keyBy(_ % 2) def createCombiner(n: Int) = { println(s\"createCombiner( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n)\") n } def mergeValue(n1: Int, n2: Int) = { println(s\"mergeValue( n1, n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( n2)\") n1 + n2 } def mergeCombiners(c1: Int, c2: Int) = { println(s\"mergeCombiners( c1, $c2)\") c1 + c2 } val countByGroup = groups.combineByKeyWithClassTag( createCombiner, mergeValue, mergeCombiners) println(countByGroup.toDebugString) /* (4) ShuffledRDD[3] at combineByKeyWithClassTag at :31 [] +-(4) MapPartitionsRDD[1] at keyBy at :25 [] | ParallelCollectionRDD[0] at parallelize at :24 [] */","title":"[source,scala]"},{"location":"rdd/Partitioner/","text":"= Partitioner Partitioner is an abstraction to define how the elements in a key-value pair RDD are partitioned by key. Partitioner < > (from 0 to < > - 1). Partitioner is used to ensure that records for a given key have to reside on a single partition. == [[implementations]] Available Partitioners [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Partitioner | Description | rdd:HashPartitioner.md[HashPartitioner] | [[HashPartitioner]] Hash-based partitioning | rdd:RangePartitioner.md[RangePartitioner] | [[RangePartitioner]] |=== == [[numPartitions]] numPartitions Method [source, scala] \u00b6 numPartitions: Int \u00b6 numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method [source, scala] \u00b6 defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"Partitioner"},{"location":"rdd/Partitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#numpartitions-int","text":"numPartitions is the number of partition to use for < >. numPartitions is used when...FIXME == [[getPartition]] getPartition Method","title":"numPartitions: Int"},{"location":"rdd/Partitioner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/Partitioner/#getpartitionkey-any-int","text":"getPartition maps a given key to a partition ID (from 0 to < > - 1) getPartition is used when...FIXME == [[defaultPartitioner]] defaultPartitioner Method","title":"getPartition(key: Any): Int"},{"location":"rdd/Partitioner/#source-scala_2","text":"defaultPartitioner( rdd: RDD[ ], others: RDD[ ]*): Partitioner defaultPartitioner...FIXME defaultPartitioner is used when...FIXME","title":"[source, scala]"},{"location":"rdd/RDD/","text":"RDD \u2014 Description of Distributed Computation \u00b6 [[T]] RDD is a description of a fault-tolerant and resilient computation over a possibly distributed collection of records (of type T ). Recursive Dependencies \u00b6 toDebugString : String toDebugString ...FIXME doCheckpoint \u00b6 doCheckpoint () : Unit doCheckpoint ...FIXME doCheckpoint is used when SparkContext is requested to run a job synchronously . == [[contract]] RDD Contract === [[compute]] Computing Partition (in TaskContext) [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split spark-rdd-partitions.md[partition] in the scheduler:spark-TaskContext.md[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is spark-rdd-caching.md[cached] or ROOT:rdd-checkpointing.md[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is spark-rdd-caching.md[cached], for specified storage:StorageLevel.md[storage levels] (i.e. all but NONE )...FIXME compute runs on the ROOT:spark-driver.md[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences) [source, scala] \u00b6 getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given spark-rdd-Partition.md[partition]. === [[partitioner]] Partitioner [source, scala] \u00b6 partitioner: Option[Partitioner] = None \u00b6 RDD can have a Partitioner.md[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | CoGroupedRDD | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations | spark-rdd-HadoopRDD.md[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , spark-rdd-transformations.md#mapPartitions[mapPartitions]) | spark-rdd-ParallelCollectionRDD.md[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | ShuffledRDD.md[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] ROOT:SparkContext.md[] [[deps]] Parent RDDs , i.e. Dependencies (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a storage:StorageLevel.md[StorageLevel] specified. The default StorageLevel is storage:StorageLevel.md#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method. [source, scala] \u00b6 getStorageLevel: StorageLevel \u00b6 == [[id]] Unique Identifier [source, scala] \u00b6 id: Int \u00b6 id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for ROOT:SparkContext.md#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a ROOT:spark-barrier-execution-mode.md#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no ShuffleDependencies among the < > . There is at least one parent RDD that has the flag enabled ShuffledRDD.md[ShuffledRDD] has the flag always disabled. spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition [source, scala] \u00b6 getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a storage:BlockId.md#RDDBlockId[RDDBlockId] for the < > and the spark-rdd-Partition.md#index[partition index]. getOrCompute requests the BlockManager to storage:BlockManager.md#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses core:SparkEnv.md#get[SparkEnv] to access the current core:SparkEnv.md#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the storage:BlockManager.md#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an spark-InterruptibleIterator.md[InterruptibleIterator]. NOTE: spark-InterruptibleIterator.md[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for spark-TaskContext.md#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies [source, scala] \u00b6 dependencies: Seq[Dependency[_]] \u00b6 dependencies returns the dependencies of a RDD . NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is ROOT:rdd-checkpointing.md[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a OneToOneDependency . For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily [source, scala] \u00b6 iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when spark-rdd-caching.md[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD [source, scala] \u00b6 checkpoint Option[CheckpointRDD[T]] \u00b6 checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method [source, scala] \u00b6 isCheckpointedAndMaterialized: Boolean \u00b6 isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method [source, scala] \u00b6 getNarrowAncestors: Seq[RDD[_]] \u00b6 getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to scheduler:spark-scheduler-StageInfo.md#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method [source, scala] \u00b6 toLocalIterator: Iterator[T] \u00b6 toLocalIterator...FIXME == [[persist]] Persisting RDD [source, scala] \u00b6 persist(): this.type persist( newLevel: StorageLevel): this.type Refer to spark-rdd-caching.md#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method [source, scala] \u00b6 persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method [source, scala] \u00b6 localCheckpoint(): this.type \u00b6 localCheckpoint marks this RDD for ROOT:rdd-checkpointing.md[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint [source, scala] \u00b6 computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions [source, scala] \u00b6 getNumPartitions: Int \u00b6 getNumPartitions gives the number of partitions of a RDD. [source, scala] \u00b6 scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition [source, scala] \u00b6 preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to scheduler:DAGScheduler.md#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions [source, scala] \u00b6 partitions: Array[Partition] \u00b6 partitions returns the spark-rdd-partitions.md[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method [source, scala] \u00b6 markCheckpointed(): Unit \u00b6 markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[checkpoint]] Reliable Checkpointing -- checkpoint Method [source, scala] \u00b6 checkpoint(): Unit \u00b6 checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method [source, scala] \u00b6 isReliablyCheckpointed: Boolean \u00b6 isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method [source, scala] \u00b6 getCheckpointFile: Option[String] \u00b6 getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"RDD"},{"location":"rdd/RDD/#rdd-description-of-distributed-computation","text":"[[T]] RDD is a description of a fault-tolerant and resilient computation over a possibly distributed collection of records (of type T ).","title":"RDD &mdash; Description of Distributed Computation"},{"location":"rdd/RDD/#recursive-dependencies","text":"toDebugString : String toDebugString ...FIXME","title":" Recursive Dependencies"},{"location":"rdd/RDD/#docheckpoint","text":"doCheckpoint () : Unit doCheckpoint ...FIXME doCheckpoint is used when SparkContext is requested to run a job synchronously . == [[contract]] RDD Contract === [[compute]] Computing Partition (in TaskContext)","title":" doCheckpoint"},{"location":"rdd/RDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[T] compute computes the input split spark-rdd-partitions.md[partition] in the scheduler:spark-TaskContext.md[TaskContext] to produce a collection of values (of type T ). compute is implemented by any type of RDD in Spark and is called every time the records are requested unless RDD is spark-rdd-caching.md[cached] or ROOT:rdd-checkpointing.md[checkpointed] (and the records can be read from an external storage, but this time closer to the compute node). When an RDD is spark-rdd-caching.md[cached], for specified storage:StorageLevel.md[storage levels] (i.e. all but NONE )...FIXME compute runs on the ROOT:spark-driver.md[driver]. compute is used when RDD is requested to < >. === [[getPartitions]] Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getpartitions-arraypartition","text":"getPartitions is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getDependencies]] Dependencies","title":"getPartitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getdependencies-seqdependency_","text":"getDependencies is used when RDD is requested for the < > (called only once as the value is cached afterwards). === [[getPreferredLocations]] Preferred Locations (Placement Preferences)","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_3","text":"getPreferredLocations( split: Partition): Seq[String] = Nil getPreferredLocations is used when RDD is requested for the < > of a given spark-rdd-Partition.md[partition]. === [[partitioner]] Partitioner","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitioner-optionpartitioner-none","text":"RDD can have a Partitioner.md[Partitioner] defined. == [[extensions]][[implementations]] (Subset of) Available RDDs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDD | Description | CoGroupedRDD | [[CoGroupedRDD]] | CoalescedRDD | [[CoalescedRDD]] Result of spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce] transformations | spark-rdd-HadoopRDD.md[HadoopRDD] | [[HadoopRDD]] Allows for reading data stored in HDFS using the older MapReduce API. The most notable use case is the return RDD of SparkContext.textFile . | spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] | [[MapPartitionsRDD]] Result of calling map-like operations (e.g. map , flatMap , filter , spark-rdd-transformations.md#mapPartitions[mapPartitions]) | spark-rdd-ParallelCollectionRDD.md[ParallelCollectionRDD] | [[ParallelCollectionRDD]] | ShuffledRDD.md[ShuffledRDD] | [[ShuffledRDD]] Result of \"shuffle\" operators (e.g. spark-rdd-partitions.md#repartition[repartition] or spark-rdd-partitions.md#coalesce[coalesce]) |=== == [[creating-instance]] Creating Instance RDD takes the following to be created: [[_sc]] ROOT:SparkContext.md[] [[deps]] Parent RDDs , i.e. Dependencies (that have to be all computed successfully before this RDD) RDD is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[storageLevel]][[getStorageLevel]] StorageLevel RDD can have a storage:StorageLevel.md[StorageLevel] specified. The default StorageLevel is storage:StorageLevel.md#NONE[NONE]. storageLevel can be specified using < > method. storageLevel becomes NONE again after < >. The current StorageLevel is available using getStorageLevel method.","title":"partitioner: Option[Partitioner] = None"},{"location":"rdd/RDD/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getstoragelevel-storagelevel","text":"== [[id]] Unique Identifier","title":"getStorageLevel: StorageLevel"},{"location":"rdd/RDD/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#id-int","text":"id is an unique identifier (aka RDD ID ) in the given <<_sc, SparkContext>>. id requests the < > for ROOT:SparkContext.md#newRddId[newRddId] right when RDD is created. == [[isBarrier_]][[isBarrier]] Barrier Stage An RDD can be part of a ROOT:spark-barrier-execution-mode.md#barrier-stage[barrier stage]. By default, isBarrier flag is enabled ( true ) when: . There are no ShuffleDependencies among the < > . There is at least one parent RDD that has the flag enabled ShuffledRDD.md[ShuffledRDD] has the flag always disabled. spark-rdd-MapPartitionsRDD.md[MapPartitionsRDD] is the only one RDD that can have the flag enabled. == [[getOrCompute]] Getting Or Computing RDD Partition","title":"id: Int"},{"location":"rdd/RDD/#source-scala_7","text":"getOrCompute( partition: Partition, context: TaskContext): Iterator[T] getOrCompute creates a storage:BlockId.md#RDDBlockId[RDDBlockId] for the < > and the spark-rdd-Partition.md#index[partition index]. getOrCompute requests the BlockManager to storage:BlockManager.md#getOrElseUpdate[getOrElseUpdate] for the block ID (with the < > and the makeIterator function). NOTE: getOrCompute uses core:SparkEnv.md#get[SparkEnv] to access the current core:SparkEnv.md#blockManager[BlockManager]. [[getOrCompute-readCachedBlock]] getOrCompute records whether...FIXME (readCachedBlock) getOrCompute branches off per the response from the storage:BlockManager.md#getOrElseUpdate[BlockManager] and whether the internal readCachedBlock flag is now on or still off. In either case, getOrCompute creates an spark-InterruptibleIterator.md[InterruptibleIterator]. NOTE: spark-InterruptibleIterator.md[InterruptibleIterator] simply delegates to a wrapped internal Iterator , but allows for spark-TaskContext.md#isInterrupted[task killing functionality]. For a BlockResult available and readCachedBlock flag on, getOrCompute ...FIXME For a BlockResult available and readCachedBlock flag off, getOrCompute ...FIXME NOTE: The BlockResult could be found in a local block manager or fetched from a remote block manager. It may also have been stored (persisted) just now. In either case, the BlockResult is available (and storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Left value with the BlockResult ). For Right(iter) (regardless of the value of readCachedBlock flag since...FIXME), getOrCompute ...FIXME NOTE: storage:BlockManager.md#getOrElseUpdate[BlockManager.getOrElseUpdate] gives a Right(iter) value to indicate an error with a block. NOTE: getOrCompute is used on Spark executors. NOTE: getOrCompute is used exclusively when RDD is requested for the < >. == [[dependencies]] RDD Dependencies","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_8","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#dependencies-seqdependency_","text":"dependencies returns the dependencies of a RDD . NOTE: dependencies is a final method that no class in Spark can ever override. Internally, dependencies checks out whether the RDD is ROOT:rdd-checkpointing.md[checkpointed] and acts accordingly. For a RDD being checkpointed, dependencies returns a single-element collection with a OneToOneDependency . For a non-checkpointed RDD, dependencies collection is computed using < getDependencies method>>. NOTE: getDependencies method is an abstract method that custom RDDs are required to provide. == [[iterator]] Accessing Records For Partition Lazily","title":"dependencies: Seq[Dependency[_]]"},{"location":"rdd/RDD/#source-scala_9","text":"iterator( split: Partition, context: TaskContext): Iterator[T] iterator < split partition>> when spark-rdd-caching.md[cached] or < >. == [[checkpointRDD]] Getting CheckpointRDD","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_10","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-optioncheckpointrddt","text":"checkpointRDD gives the CheckpointRDD from the < > internal registry if available (if the RDD was checkpointed). checkpointRDD is used when RDD is requested for the < >, < > and < >. == [[isCheckpointedAndMaterialized]] isCheckpointedAndMaterialized Method","title":"checkpoint Option[CheckpointRDD[T]]"},{"location":"rdd/RDD/#source-scala_11","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#ischeckpointedandmaterialized-boolean","text":"isCheckpointedAndMaterialized...FIXME isCheckpointedAndMaterialized is used when RDD is requested to < >, < > and < >. == [[getNarrowAncestors]] getNarrowAncestors Method","title":"isCheckpointedAndMaterialized: Boolean"},{"location":"rdd/RDD/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnarrowancestors-seqrdd_","text":"getNarrowAncestors...FIXME getNarrowAncestors is used when StageInfo is requested to scheduler:spark-scheduler-StageInfo.md#fromStage[fromStage]. == [[toLocalIterator]] toLocalIterator Method","title":"getNarrowAncestors: Seq[RDD[_]]"},{"location":"rdd/RDD/#source-scala_13","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#tolocaliterator-iteratort","text":"toLocalIterator...FIXME == [[persist]] Persisting RDD","title":"toLocalIterator: Iterator[T]"},{"location":"rdd/RDD/#source-scala_14","text":"persist(): this.type persist( newLevel: StorageLevel): this.type Refer to spark-rdd-caching.md#persist[Persisting RDD]. == [[persist-internal]] persist Internal Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_15","text":"persist( newLevel: StorageLevel, allowOverride: Boolean): this.type persist...FIXME persist (private) is used when RDD is requested to < > and < >. == [[unpersist]] unpersist Method","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_16","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#unpersistblocking-boolean-true-thistype","text":"unpersist...FIXME == [[localCheckpoint]] localCheckpoint Method","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/RDD/#source-scala_17","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#localcheckpoint-thistype","text":"localCheckpoint marks this RDD for ROOT:rdd-checkpointing.md[local checkpointing] using Spark's caching layer. == [[computeOrReadCheckpoint]] Computing Partition or Reading From Checkpoint","title":"localCheckpoint(): this.type"},{"location":"rdd/RDD/#source-scala_18","text":"computeOrReadCheckpoint( split: Partition, context: TaskContext): Iterator[T] computeOrReadCheckpoint reads split partition from a checkpoint (< >) or < > yourself. computeOrReadCheckpoint is used when RDD is requested to < > or < >. == [[getNumPartitions]] Getting Number of Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_19","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getnumpartitions-int","text":"getNumPartitions gives the number of partitions of a RDD.","title":"getNumPartitions: Int"},{"location":"rdd/RDD/#source-scala_20","text":"scala> sc.textFile(\"README.md\").getNumPartitions res0: Int = 2 scala> sc.textFile(\"README.md\", 5).getNumPartitions res1: Int = 5 == [[preferredLocations]] Defining Placement Preferences of RDD Partition","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_21","text":"preferredLocations( split: Partition): Seq[String] preferredLocations requests the CheckpointRDD for < > (if the RDD is checkpointed) or < >. preferredLocations is a template method that uses < > that custom RDDs can override to specify placement preferences for a partition. getPreferredLocations defines no placement preferences by default. preferredLocations is mainly used when DAGScheduler is requested to scheduler:DAGScheduler.md#getPreferredLocs[compute the preferred locations for missing partitions]. == [[partitions]] Accessing RDD Partitions","title":"[source, scala]"},{"location":"rdd/RDD/#source-scala_22","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#partitions-arraypartition","text":"partitions returns the spark-rdd-partitions.md[Partitions] of a RDD . partitions requests CheckpointRDD for the < > (if the RDD is checkpointed) or < > and cache (in < > internal registry that is used next time). Partitions have the property that their internal index should be equal to their position in the owning RDD. == [[markCheckpointed]] markCheckpointed Method","title":"partitions: Array[Partition]"},{"location":"rdd/RDD/#source-scala_23","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#markcheckpointed-unit","text":"markCheckpointed...FIXME markCheckpointed is used when...FIXME == [[checkpoint]] Reliable Checkpointing -- checkpoint Method","title":"markCheckpointed(): Unit"},{"location":"rdd/RDD/#source-scala_24","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#checkpoint-unit","text":"checkpoint...FIXME checkpoint is used when...FIXME == [[isReliablyCheckpointed]] isReliablyCheckpointed Method","title":"checkpoint(): Unit"},{"location":"rdd/RDD/#source-scala_25","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#isreliablycheckpointed-boolean","text":"isReliablyCheckpointed...FIXME isReliablyCheckpointed is used when...FIXME == [[getCheckpointFile]] getCheckpointFile Method","title":"isReliablyCheckpointed: Boolean"},{"location":"rdd/RDD/#source-scala_26","text":"","title":"[source, scala]"},{"location":"rdd/RDD/#getcheckpointfile-optionstring","text":"getCheckpointFile...FIXME getCheckpointFile is used when...FIXME","title":"getCheckpointFile: Option[String]"},{"location":"rdd/RDDCheckpointData/","text":"= RDDCheckpointData RDDCheckpointData is an abstraction of information related to RDD checkpointing. == [[implementations]] Available RDDCheckpointDatas [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | RDDCheckpointData | Description | rdd:LocalRDDCheckpointData.md[LocalRDDCheckpointData] | [[LocalRDDCheckpointData]] | rdd:ReliableRDDCheckpointData.md[ReliableRDDCheckpointData] | [[ReliableRDDCheckpointData]] ROOT:rdd-checkpointing.md#reliable-checkpointing[Reliable Checkpointing] |=== == [[creating-instance]] Creating Instance RDDCheckpointData takes the following to be created: [[rdd]] rdd:RDD.md[RDD] == [[Serializable]] RDDCheckpointData as Serializable RDDCheckpointData is java.io.Serializable. == [[cpState]] States [[Initialized]] Initialized [[CheckpointingInProgress]] CheckpointingInProgress [[Checkpointed]] Checkpointed == [[checkpoint]] Checkpointing RDD [source, scala] \u00b6 checkpoint(): CheckpointRDD[T] \u00b6 checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to rdd:RDD.md#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to rdd:RDD.md#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"RDDCheckpointData"},{"location":"rdd/RDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#checkpoint-checkpointrddt","text":"checkpoint changes the < > to < > only when in < > state. Otherwise, checkpoint does nothing and returns. checkpoint < > that gives an CheckpointRDD (that is the < > internal registry). checkpoint changes the < > to < >. In the end, checkpoint requests the given < > to rdd:RDD.md#markCheckpointed[markCheckpointed]. checkpoint is used when RDD is requested to rdd:RDD.md#doCheckpoint[doCheckpoint]. == [[doCheckpoint]] doCheckpoint Method","title":"checkpoint(): CheckpointRDD[T]"},{"location":"rdd/RDDCheckpointData/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/RDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint is used when RDDCheckpointData is requested to < >.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/RangePartitioner/","text":"= RangePartitioner RangePartitioner is a rdd:Partitioner.md[Partitioner] for...FIXME [[ordering]] RangePartitioner[K : Ordering : ClassTag, V] is a parameterized type of K keys that can be sorted ( ordered ) and V values. RangePartitioner is used for rdd:spark-rdd-OrderedRDDFunctions.md#sortByKey[sortByKey] operator. == [[creating-instance]] Creating Instance RangePartitioner takes the following to be created: [[partitions]] Number of partitions [[rdd]] rdd:RDD.md[RDD] ( RDD[_ <: Product2[K, V]] ) [[ascending]] ascending flag (default: true ) [[samplePointsPerPartitionHint]] samplePointsPerPartitionHint (default: 20) == [[rangeBounds]] rangeBounds Array RangePartitioner uses rangeBounds registry (of type Array[K] ) when requested for < > and < >, < >. == [[numPartitions]] Number of Partitions [source,scala] \u00b6 numPartitions: Int \u00b6 numPartitions is simply one more than the length of the < > array. numPartitions is part of the rdd:Partitioner.md#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key [source, scala] \u00b6 getPartition(key: Any): Int \u00b6 getPartition...FIXME getPartition is part of the rdd:Partitioner.md#getPartition[Partitioner] abstraction.","title":"RangePartitioner"},{"location":"rdd/RangePartitioner/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/RangePartitioner/#numpartitions-int","text":"numPartitions is simply one more than the length of the < > array. numPartitions is part of the rdd:Partitioner.md#numPartitions[Partitioner] abstraction. == [[getPartition]] Finding Partition ID for Key","title":"numPartitions: Int"},{"location":"rdd/RangePartitioner/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/RangePartitioner/#getpartitionkey-any-int","text":"getPartition...FIXME getPartition is part of the rdd:Partitioner.md#getPartition[Partitioner] abstraction.","title":"getPartition(key: Any): Int"},{"location":"rdd/ReliableCheckpointRDD/","text":"= ReliableCheckpointRDD ReliableCheckpointRDD is an rdd:CheckpointRDD.md[CheckpointRDD]...FIXME == [[creating-instance]] Creating Instance ReliableCheckpointRDD takes the following to be created: [[sc]] ROOT:SparkContext.md[] [[checkpointPath]] Checkpoint Directory (on a Hadoop DFS-compatible file system) <<_partitioner, Partitioner>> ReliableCheckpointRDD is created when: ReliableCheckpointRDD utility is used to < >. SparkContext is requested to ROOT:SparkContext.md#checkpointFile[checkpointFile] == [[checkpointPartitionerFileName]] Checkpointed Partitioner File ReliableCheckpointRDD uses _partitioner as the name of the file in the < > with the < > serialized to. == [[partitioner]] Partitioner ReliableCheckpointRDD can be given a rdd:Partitioner.md[Partitioner] to be created. When rdd:RDD.md#partitioner[requested for the Partitioner] (as an RDD), ReliableCheckpointRDD returns the one it was created with or < >. == [[writeRDDToCheckpointDirectory]] Writing RDD to Checkpoint Directory [source, scala] \u00b6 writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to rdd:ReliableRDDCheckpointData.md#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory [source,scala] \u00b6 writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on ROOT:configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to serializer:SerializerInstance.md#serializeStream[serialize the output stream] and serializer:DeserializationStream.md#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Written partitioner to [partitionerFilePath] \u00b6 In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Error writing partitioner [partitioner] to [checkpointDirPath] \u00b6 writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory [source,scala] \u00b6 readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on ROOT:configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to serializer:SerializerInstance.md#deserializeStream[deserialize the input stream] and serializer:DeserializationStream.md#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner. [source,plaintext] \u00b6 Read partitioner from [partitionerFilePath] \u00b6 In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"ReliableCheckpointRDD"},{"location":"rdd/ReliableCheckpointRDD/#source-scala","text":"writeRDDToCheckpointDirectory T: ClassTag : ReliableCheckpointRDD[T] writeRDDToCheckpointDirectory...FIXME writeRDDToCheckpointDirectory is used when ReliableRDDCheckpointData is requested to rdd:ReliableRDDCheckpointData.md#doCheckpoint[doCheckpoint]. == [[writePartitionerToCheckpointDir]] Writing Partitioner to Checkpoint Directory","title":"[source, scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala","text":"writePartitionerToCheckpointDir( sc: SparkContext, partitioner: Partitioner, checkpointDirPath: Path): Unit writePartitionerToCheckpointDir creates the < > with the buffer size based on ROOT:configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. writePartitionerToCheckpointDir requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. writePartitionerToCheckpointDir requests the SerializerInstance to serializer:SerializerInstance.md#serializeStream[serialize the output stream] and serializer:DeserializationStream.md#writeObject[writes] the given Partitioner. In the end, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#written-partitioner-to-partitionerfilepath","text":"In case of any non-fatal exception, writePartitionerToCheckpointDir prints out the following DEBUG message to the logs:","title":"Written partitioner to [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#error-writing-partitioner-partitioner-to-checkpointdirpath","text":"writePartitionerToCheckpointDir is used when ReliableCheckpointRDD is requested to < >. == [[readCheckpointedPartitionerFile]] Reading Partitioner from Checkpointed Directory","title":"Error writing partitioner [partitioner] to [checkpointDirPath]"},{"location":"rdd/ReliableCheckpointRDD/#sourcescala_1","text":"readCheckpointedPartitionerFile( sc: SparkContext, checkpointDirPath: String): Option[Partitioner] readCheckpointedPartitionerFile opens the < > with the buffer size based on ROOT:configuration-properties.md#spark.buffer.size[spark.buffer.size] configuration property. readCheckpointedPartitionerFile requests the core:SparkEnv.md#serializer[default Serializer] for a new serializer:Serializer.md#newInstance[SerializerInstance]. readCheckpointedPartitionerFile requests the SerializerInstance to serializer:SerializerInstance.md#deserializeStream[deserialize the input stream] and serializer:DeserializationStream.md#readObject[read the Partitioner] from the partitioner file. readCheckpointedPartitionerFile prints out the following DEBUG message to the logs and returns the partitioner.","title":"[source,scala]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#read-partitioner-from-partitionerfilepath","text":"In case of FileNotFoundException or any non-fatal exceptions, readCheckpointedPartitionerFile prints out a corresponding message to the logs and returns None. readCheckpointedPartitionerFile is used when ReliableCheckpointRDD is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.rdd.ReliableCheckpointRDD$ logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Read partitioner from [partitionerFilePath]"},{"location":"rdd/ReliableCheckpointRDD/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableCheckpointRDD/#log4jloggerorgapachesparkrddreliablecheckpointrddall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.rdd.ReliableCheckpointRDD$=ALL"},{"location":"rdd/ReliableRDDCheckpointData/","text":"= ReliableRDDCheckpointData ReliableRDDCheckpointData is a rdd:RDDCheckpointData.md[RDDCheckpointData] for ROOT:rdd-checkpointing.md#reliable-checkpointing[Reliable Checkpointing]. == [[creating-instance]] Creating Instance ReliableRDDCheckpointData takes the following to be created: [[rdd]] rdd:RDD.md[++RDD[T]++] ReliableRDDCheckpointData is created for rdd:RDD.md#checkpoint[RDD.checkpoint] operator. == [[cpDir]][[checkpointPath]] Checkpoint Directory ReliableRDDCheckpointData creates a subdirectory of the ROOT:SparkContext.md#checkpointDir[application-wide checkpoint directory] for < > the given < >. The name of the subdirectory uses the rdd:RDD.md#id[unique identifier] of the < >: [source,plaintext] \u00b6 rdd-[id] \u00b6 == [[doCheckpoint]] Checkpointing RDD [source, scala] \u00b6 doCheckpoint(): CheckpointRDD[T] \u00b6 doCheckpoint rdd:ReliableCheckpointRDD.md#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With ROOT:configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the ROOT:SparkContext.md#cleaner[ContextCleaner] to core:ContextCleaner.md#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD. [source,plaintext] \u00b6 Done checkpointing RDD [id] to [cpDir], new parent is RDD [id] \u00b6 doCheckpoint is part of the rdd:RDDCheckpointData.md#doCheckpoint[RDDCheckpointData] abstraction.","title":"ReliableRDDCheckpointData"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#rdd-id","text":"== [[doCheckpoint]] Checkpointing RDD","title":"rdd-[id]"},{"location":"rdd/ReliableRDDCheckpointData/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ReliableRDDCheckpointData/#docheckpoint-checkpointrddt","text":"doCheckpoint rdd:ReliableCheckpointRDD.md#writeRDDToCheckpointDirectory[writes] the < > to the < > (that creates a new RDD). With ROOT:configuration-properties.md#spark.cleaner.referenceTracking.cleanCheckpoints[spark.cleaner.referenceTracking.cleanCheckpoints] configuration property enabled, doCheckpoint requests the ROOT:SparkContext.md#cleaner[ContextCleaner] to core:ContextCleaner.md#registerRDDCheckpointDataForCleanup[registerRDDCheckpointDataForCleanup] for the new RDD. In the end, doCheckpoint prints out the following INFO message to the logs and returns the new RDD.","title":"doCheckpoint(): CheckpointRDD[T]"},{"location":"rdd/ReliableRDDCheckpointData/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"rdd/ReliableRDDCheckpointData/#done-checkpointing-rdd-id-to-cpdir-new-parent-is-rdd-id","text":"doCheckpoint is part of the rdd:RDDCheckpointData.md#doCheckpoint[RDDCheckpointData] abstraction.","title":"Done checkpointing RDD [id] to [cpDir], new parent is RDD [id]"},{"location":"rdd/ShuffleDependency/","text":"ShuffleDependency \u00b6 ShuffleDependency is a Dependency on the output of a scheduler:ShuffleMapStage.md[ShuffleMapStage] for a < >. ShuffleDependency uses the < > to know the number of (map-side/pre-shuffle) partitions and the < > for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency is created as a dependency of ShuffledRDD.md[ShuffledRDD]. ShuffleDependency can also be created as a dependency of CoGroupedRDD and SubtractedRDD . == [[creating-instance]] Creating Instance ShuffleDependency takes the following to be created: < > of key-value pairs ( RDD[_ <: Product2[K, V]] ) < > [[serializer]] serializer:Serializer.md[Serializer] [[keyOrdering]] Ordering for K keys ( Option[Ordering[K]] ) < > ( Option[Aggregator[K, V, C]] ) < > flag (default: false ) When created, ShuffleDependency gets ROOT:SparkContext.md#nextShuffleId[shuffle id] (as shuffleId ). NOTE: ShuffleDependency uses the index.md#context[input RDD to access SparkContext ] and so the shuffleId . ShuffleDependency shuffle:ShuffleManager.md#registerShuffle[registers itself with ShuffleManager ] and gets a ShuffleHandle (available as < > property). NOTE: ShuffleDependency accesses core:SparkEnv.md#shuffleManager[ ShuffleManager using SparkEnv ]. In the end, ShuffleDependency core:ContextCleaner.md#registerShuffleForCleanup[registers itself for cleanup with ContextCleaner ]. NOTE: ShuffleDependency accesses the ROOT:SparkContext.md#cleaner[optional ContextCleaner through SparkContext ]. NOTE: ShuffleDependency is created when ShuffledRDD.md#getDependencies[ShuffledRDD], CoGroupedRDD , and SubtractedRDD return their RDD dependencies. == [[shuffleId]] Shuffle ID Every ShuffleDependency has a unique application-wide shuffle ID that is assigned when < > (and is used throughout Spark's code to reference a ShuffleDependency). Shuffle IDs are tracked by ROOT:SparkContext.md#nextShuffleId[SparkContext]. == [[rdd]] Parent RDD ShuffleDependency is given the parent RDD.md[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ). The parent RDD is available as rdd property that is part of the Dependency abstraction. [source,scala] \u00b6 RDD[Product2[K, V]] \u00b6 == [[partitioner]] Partitioner ShuffleDependency is given a Partitioner.md[Partitioner] that is used to partition the shuffle output (when shuffle:SortShuffleWriter.md[SortShuffleWriter], shuffle:BypassMergeSortShuffleWriter.md[BypassMergeSortShuffleWriter] and shuffle:UnsafeShuffleWriter.md[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle [source, scala] \u00b6 shuffleHandle: ShuffleHandle \u00b6 shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute CoGroupedRDDs , ShuffledRDD.md#compute[ShuffledRDD], SubtractedRDD , and spark-sql-ShuffledRowRDD.md[ShuffledRowRDD] (to get a spark-shuffle-ShuffleReader.md[ShuffleReader] for a ShuffleDependency) and when a scheduler:ShuffleMapTask.md#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of ShuffledRDD.md#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] SortShuffleManager is requested to shuffle:SortShuffleManager.md#registerShuffle[register a shuffle] SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records] == [[aggregator]] Optional Aggregator [source, scala] \u00b6 aggregator: Option[Aggregator[K, V, C]] = None \u00b6 aggregator is a Aggregator.md[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when shuffle:SortShuffleWriter.md#write[ SortShuffleWriter writes records] and shuffle:BlockStoreShuffleReader.md#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#shuffledependency","text":"ShuffleDependency is a Dependency on the output of a scheduler:ShuffleMapStage.md[ShuffleMapStage] for a < >. ShuffleDependency uses the < > to know the number of (map-side/pre-shuffle) partitions and the < > for the number of (reduce-size/post-shuffle) partitions. ShuffleDependency is created as a dependency of ShuffledRDD.md[ShuffledRDD]. ShuffleDependency can also be created as a dependency of CoGroupedRDD and SubtractedRDD . == [[creating-instance]] Creating Instance ShuffleDependency takes the following to be created: < > of key-value pairs ( RDD[_ <: Product2[K, V]] ) < > [[serializer]] serializer:Serializer.md[Serializer] [[keyOrdering]] Ordering for K keys ( Option[Ordering[K]] ) < > ( Option[Aggregator[K, V, C]] ) < > flag (default: false ) When created, ShuffleDependency gets ROOT:SparkContext.md#nextShuffleId[shuffle id] (as shuffleId ). NOTE: ShuffleDependency uses the index.md#context[input RDD to access SparkContext ] and so the shuffleId . ShuffleDependency shuffle:ShuffleManager.md#registerShuffle[registers itself with ShuffleManager ] and gets a ShuffleHandle (available as < > property). NOTE: ShuffleDependency accesses core:SparkEnv.md#shuffleManager[ ShuffleManager using SparkEnv ]. In the end, ShuffleDependency core:ContextCleaner.md#registerShuffleForCleanup[registers itself for cleanup with ContextCleaner ]. NOTE: ShuffleDependency accesses the ROOT:SparkContext.md#cleaner[optional ContextCleaner through SparkContext ]. NOTE: ShuffleDependency is created when ShuffledRDD.md#getDependencies[ShuffledRDD], CoGroupedRDD , and SubtractedRDD return their RDD dependencies. == [[shuffleId]] Shuffle ID Every ShuffleDependency has a unique application-wide shuffle ID that is assigned when < > (and is used throughout Spark's code to reference a ShuffleDependency). Shuffle IDs are tracked by ROOT:SparkContext.md#nextShuffleId[SparkContext]. == [[rdd]] Parent RDD ShuffleDependency is given the parent RDD.md[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ). The parent RDD is available as rdd property that is part of the Dependency abstraction.","title":"ShuffleDependency"},{"location":"rdd/ShuffleDependency/#sourcescala","text":"","title":"[source,scala]"},{"location":"rdd/ShuffleDependency/#rddproduct2k-v","text":"== [[partitioner]] Partitioner ShuffleDependency is given a Partitioner.md[Partitioner] that is used to partition the shuffle output (when shuffle:SortShuffleWriter.md[SortShuffleWriter], shuffle:BypassMergeSortShuffleWriter.md[BypassMergeSortShuffleWriter] and shuffle:UnsafeShuffleWriter.md[UnsafeShuffleWriter] are requested to write). == [[shuffleHandle]] ShuffleHandle","title":"RDD[Product2[K, V]]"},{"location":"rdd/ShuffleDependency/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#shufflehandle-shufflehandle","text":"shuffleHandle is the ShuffleHandle of a ShuffleDependency as assigned eagerly when < >. shuffleHandle is used to compute CoGroupedRDDs , ShuffledRDD.md#compute[ShuffledRDD], SubtractedRDD , and spark-sql-ShuffledRowRDD.md[ShuffledRowRDD] (to get a spark-shuffle-ShuffleReader.md[ShuffleReader] for a ShuffleDependency) and when a scheduler:ShuffleMapTask.md#runTask[ ShuffleMapTask runs] (to get a ShuffleWriter for a ShuffleDependency). == [[mapSideCombine]] Map-Size Partial Aggregation Flag ShuffleDependency uses a mapSideCombine flag that controls whether to perform map-side partial aggregation ( map-side combine ) using an < >. mapSideCombine is disabled ( false ) by default and can be enabled ( true ) for some use cases of ShuffledRDD.md#mapSideCombine[ShuffledRDD]. ShuffleDependency requires that the optional < > is defined when the flag is enabled. mapSideCombine is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] SortShuffleManager is requested to shuffle:SortShuffleManager.md#registerShuffle[register a shuffle] SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records] == [[aggregator]] Optional Aggregator","title":"shuffleHandle: ShuffleHandle"},{"location":"rdd/ShuffleDependency/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/ShuffleDependency/#aggregator-optionaggregatork-v-c-none","text":"aggregator is a Aggregator.md[map/reduce-side Aggregator] (for a RDD's shuffle). aggregator is by default undefined (i.e. None ) when < >. NOTE: aggregator is used when shuffle:SortShuffleWriter.md#write[ SortShuffleWriter writes records] and shuffle:BlockStoreShuffleReader.md#read[ BlockStoreShuffleReader reads combined key-values for a reduce task].","title":"aggregator: Option[Aggregator[K, V, C]] = None"},{"location":"rdd/ShuffledRDD/","text":"= [[ShuffledRDD]] ShuffledRDD ShuffledRDD is an RDD.md[RDD] of key-value pairs that represents a shuffle step in a spark-rdd-lineage.md[RDD lineage]. ShuffledRDD is given an < > of key-value pairs of K and V types, respectively, when < > and < > key-value pairs of K and C types, respectively. ShuffledRDD is < > for the following RDD transformations: spark-rdd-OrderedRDDFunctions.md#sortByKey[OrderedRDDFunctions.sortByKey] and spark-rdd-OrderedRDDFunctions.md#repartitionAndSortWithinPartitions[OrderedRDDFunctions.repartitionAndSortWithinPartitions] PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] and PairRDDFunctions.md#partitionBy[PairRDDFunctions.partitionBy] spark-rdd-transformations.md#coalesce[RDD.coalesce] (with shuffle flag enabled) ShuffledRDD uses custom < > partitions. [[isBarrier]] ShuffledRDD has RDD.md#isBarrier[isBarrier] flag always disabled ( false ). == [[creating-instance]] Creating Instance ShuffledRDD takes the following to be created: [[prev]] Previous RDD.md[RDD] of key-value pairs ( RDD[_ <: Product2[K, V]] ) [[part]] Partitioner.md[Partitioner] == [[mapSideCombine]][[setMapSideCombine]] Map-Side Combine Flag ShuffledRDD uses a map-side combine flag to create a ShuffleDependency when requested for the < > (there is always only one). The flag is disabled ( false ) by default and can be changed using setMapSideCombine method. [source,scala] \u00b6 setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only RDD.md#dependencies[dependency] (that is assumed a ShuffleDependency ) for the ShuffleHandle . compute uses the SparkEnv to access the ShuffleManager . compute requests the shuffle:ShuffleManager.md#shuffleManager[ShuffleManager] for the shuffle:ShuffleManager.md#getReader[ShuffleReader] (for the ShuffleHandle, the spark-rdd-Partition.md[partition]). In the end, compute requests the ShuffleReader to shuffle:spark-shuffle-ShuffleReader.md#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the RDD.md#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition [source, scala] \u00b6 getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the scheduler:MapOutputTrackerMaster.md#getPreferredLocationsForShuffle[preferred locations] of the given spark-rdd-Partition.md[partition] (storage:BlockManager.md[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current core:SparkEnv.md#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the RDD.md#compute[RDD] abstraction. == [[getDependencies]] Dependencies [source, scala] \u00b6 getDependencies: Seq[Dependency[_]] \u00b6 getDependencies uses the < > if defined or requests the current serializer:SerializerManager.md[SerializerManager] for serializer:SerializerManager.md#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single ShuffleDependency (with the < >, the < >, and the Serializer). getDependencies is part of the RDD.md#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the Partitioner.md[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey [source,plaintext] \u00b6 val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified Serializer for the single ShuffleDependency dependency [source, scala] \u00b6 userSpecifiedSerializer: Option[Serializer] = None \u00b6 userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"ShuffledRDD"},{"location":"rdd/ShuffledRDD/#sourcescala","text":"setMapSideCombine( mapSideCombine: Boolean): ShuffledRDD[K, V, C] setMapSideCombine is used for PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation (which defaults to the flag enabled). == [[compute]] Computing Partition","title":"[source,scala]"},{"location":"rdd/ShuffledRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[(K, C)] compute requests the only RDD.md#dependencies[dependency] (that is assumed a ShuffleDependency ) for the ShuffleHandle . compute uses the SparkEnv to access the ShuffleManager . compute requests the shuffle:ShuffleManager.md#shuffleManager[ShuffleManager] for the shuffle:ShuffleManager.md#getReader[ShuffleReader] (for the ShuffleHandle, the spark-rdd-Partition.md[partition]). In the end, compute requests the ShuffleReader to shuffle:spark-shuffle-ShuffleReader.md#read[read] the combined key-value pairs (of type (K, C) ). compute is part of the RDD.md#compute[RDD] abstraction. == [[getPreferredLocations]] Placement Preferences of Partition","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_1","text":"getPreferredLocations( partition: Partition): Seq[String] getPreferredLocations requests MapOutputTrackerMaster for the scheduler:MapOutputTrackerMaster.md#getPreferredLocationsForShuffle[preferred locations] of the given spark-rdd-Partition.md[partition] (storage:BlockManager.md[BlockManagers] with the most map outputs). getPreferredLocations uses SparkEnv to access the current core:SparkEnv.md#mapOutputTracker[MapOutputTrackerMaster]. getPreferredLocations is part of the RDD.md#compute[RDD] abstraction. == [[getDependencies]] Dependencies","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#getdependencies-seqdependency_","text":"getDependencies uses the < > if defined or requests the current serializer:SerializerManager.md[SerializerManager] for serializer:SerializerManager.md#getSerializer[one]. getDependencies uses the < > internal flag for the types of the keys and values (i.e. K and C or K and V when the flag is enabled or not, respectively). In the end, getDependencies returns a single ShuffleDependency (with the < >, the < >, and the Serializer). getDependencies is part of the RDD.md#getDependencies[RDD] abstraction. == [[ShuffledRDDPartition]] ShuffledRDDPartition ShuffledRDDPartition gets an index to be created (that in turn is the index of partitions as calculated by the Partitioner.md[Partitioner] of a < >). == Demos === Demo: ShuffledRDD and coalesce","title":"getDependencies: Seq[Dependency[_]]"},{"location":"rdd/ShuffledRDD/#sourceplaintext","text":"val data = sc.parallelize(0 to 9) val coalesced = data.coalesce(numPartitions = 4, shuffle = true) scala> println(coalesced.toDebugString) (4) MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] === Demo: ShuffledRDD and sortByKey","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#sourceplaintext_1","text":"val data = sc.parallelize(0 to 9) val grouped = rdd.groupBy(_ % 2) val sorted = grouped.sortByKey(numPartitions = 2) scala> println(sorted.toDebugString) (2) ShuffledRDD[15] at sortByKey at :74 [] +-(4) ShuffledRDD[12] at groupBy at :74 [] +-(4) MapPartitionsRDD[11] at groupBy at :74 [] | MapPartitionsRDD[9] at coalesce at :75 [] | CoalescedRDD[8] at coalesce at :75 [] | ShuffledRDD[7] at coalesce at :75 [] +-(16) MapPartitionsRDD[6] at coalesce at :75 [] | ParallelCollectionRDD[5] at parallelize at :74 [] == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | userSpecifiedSerializer a| [[userSpecifiedSerializer]] User-specified Serializer for the single ShuffleDependency dependency","title":"[source,plaintext]"},{"location":"rdd/ShuffledRDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rdd/ShuffledRDD/#userspecifiedserializer-optionserializer-none","text":"userSpecifiedSerializer is undefined ( None ) by default and can be changed using setSerializer method (that is used for PairRDDFunctions.md#combineByKeyWithClassTag[PairRDDFunctions.combineByKeyWithClassTag] transformation). |===","title":"userSpecifiedSerializer: Option[Serializer] = None"},{"location":"rdd/SubtractedRDD/","text":"== [[SubtractedRDD]] SubtractedRDD CAUTION: FIXME === [[compute]] Computing Partition (in TaskContext) -- compute Method [source, scala] \u00b6 compute(p: Partition, context: TaskContext): Iterator[(K, V)] \u00b6 NOTE: compute is part of rdd:RDD.md#compute[RDD Contract] to compute a spark-rdd-Partition.md[partition] (in a spark-TaskContext.md[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"SubtractedRDD"},{"location":"rdd/SubtractedRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/SubtractedRDD/#computep-partition-context-taskcontext-iteratork-v","text":"NOTE: compute is part of rdd:RDD.md#compute[RDD Contract] to compute a spark-rdd-Partition.md[partition] (in a spark-TaskContext.md[TaskContext]). compute ...FIXME === [[getDependencies]] getDependencies Method CAUTION: FIXME","title":"compute(p: Partition, context: TaskContext): Iterator[(K, V)]"},{"location":"rdd/spark-rdd-HadoopRDD/","text":"== HadoopRDD https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.HadoopRDD[HadoopRDD ] is an RDD that provides core functionality for reading data stored in HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI using the older MapReduce API ( https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/package-summary.html[org.apache.hadoop.mapred ]). HadoopRDD is created as a result of calling the following methods in ROOT:SparkContext.md[]: hadoopFile textFile (the most often used in examples!) sequenceFile Partitions are of type HadoopPartition . When an HadoopRDD is computed, i.e. an action is called, you should see the INFO message Input split: in the logs. scala> sc.textFile(\"README.md\").count ... 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/10 18:03:21 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:1784+1784 ... The following properties are set upon partition execution: mapred.tip.id - task id of this task's attempt mapred.task.id - task attempt's id mapred.task.is.map as true mapred.task.partition - split id mapred.job.id Spark settings for HadoopRDD : spark.hadoop.cloneConf (default: false ) - shouldCloneJobConf - should a Hadoop job configuration JobConf object be cloned before spawning a Hadoop job. Refer to https://issues.apache.org/jira/browse/SPARK-2546[[SPARK-2546 ] Configuration object thread safety issue]. When true , you should see a DEBUG message Cloning Hadoop Configuration . You can register callbacks on spark-TaskContext.md[TaskContext]. HadoopRDDs are not checkpointed. They do nothing when checkpoint() is called. [CAUTION] \u00b6 FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they? What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits \u00b6 === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"HadoopRDD"},{"location":"rdd/spark-rdd-HadoopRDD/#caution","text":"FIXME What are InputMetrics ? What is JobConf ? What are the InputSplits: FileSplit and CombineFileSplit ? * What are InputFormat and Configurable subtypes? What's InputFormat's RecordReader? It creates a key and a value. What are they?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-HadoopRDD/#whats-hadoop-split-input-splits-for-hadoop-reads-see-inputformatgetsplits","text":"=== [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[getPartitions]] getPartitions Method The number of partition for HadoopRDD, i.e. the return value of getPartitions , is calculated using InputFormat.getSplits(jobConf, minPartitions) where minPartitions is only a hint of how many partitions one may want at minimum. As a hint it does not mean the number of partitions will be exactly the number given. For SparkContext.textFile the input format class is https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[org.apache.hadoop.mapred.TextInputFormat ]. The https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/FileInputFormat.html[javadoc of org.apache.hadoop.mapred.FileInputFormat] says: FileInputFormat is the base class for all file-based InputFormats. This provides a generic implementation of getSplits(JobConf, int). Subclasses of FileInputFormat can also override the isSplitable(FileSystem, Path) method to ensure input-files are not split-up and are processed as a whole by Mappers. TIP: You may find https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java#L319[the sources of org.apache.hadoop.mapred.FileInputFormat.getSplits] enlightening.","title":"What's Hadoop Split? input splits for Hadoop reads? See InputFormat.getSplits"},{"location":"rdd/spark-rdd-MapPartitionsRDD/","text":"MapPartitionsRDD \u00b6 MapPartitionsRDD is an RDD that has exactly one-to-one narrow dependency on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to rdd:PairRDDFunctions.md#mapValues[mapValues] and rdd:PairRDDFunctions.md#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom spark-rdd-transformations.md#mapPartitions[mapPartitions] mapPartitionsWithIndex rdd:PairRDDFunctions.md#mapValues[PairRDDFunctions.mapValues] rdd:PairRDDFunctions.md#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the rdd:RDD.md#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the rdd:RDD.md#dependencies[RDD dependencies] are rdd:RDD.md#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent rdd:RDD.md[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/spark-rdd-MapPartitionsRDD/#mappartitionsrdd","text":"MapPartitionsRDD is an RDD that has exactly one-to-one narrow dependency on the < > and \"describes\" a distributed computation of the given < > to every RDD partition. MapPartitionsRDD is < > when: PairRDDFunctions ( RDD[(K, V)] ) is requested to rdd:PairRDDFunctions.md#mapValues[mapValues] and rdd:PairRDDFunctions.md#flatMapValues[flatMapValues] (with the < > flag enabled) RDD[T] is requested to < >, < >, < >, < >, < >, < >, < >, and < > RDDBarrier[T] is requested to < > (with the < > flag enabled) By default, it does not preserve partitioning -- the last input parameter preservesPartitioning is false . If it is true , it retains the original RDD's partitioning. MapPartitionsRDD is the result of the following transformations: filter glom spark-rdd-transformations.md#mapPartitions[mapPartitions] mapPartitionsWithIndex rdd:PairRDDFunctions.md#mapValues[PairRDDFunctions.mapValues] rdd:PairRDDFunctions.md#flatMapValues[PairRDDFunctions.flatMapValues] [[isBarrier_]] When requested for the rdd:RDD.md#isBarrier_[isBarrier_] flag, MapPartitionsRDD gives the < > flag or check whether any of the RDDs of the rdd:RDD.md#dependencies[RDD dependencies] are rdd:RDD.md#isBarrier[barrier-enabled]. === [[creating-instance]] Creating MapPartitionsRDD Instance MapPartitionsRDD takes the following to be created: [[prev]] Parent rdd:RDD.md[RDD] ( RDD[T] ) [[f]] Function to execute on partitions + (TaskContext, partitionID, Iterator[T]) => Iterator[U] [[preservesPartitioning]] preservesPartitioning flag (default: false ) [[isFromBarrier]] isFromBarrier flag for < > (default: false ) [[isOrderSensitive]] isOrderSensitive flag (default: false )","title":"MapPartitionsRDD"},{"location":"rdd/spark-rdd-NewHadoopRDD/","text":"== [[NewHadoopRDD]] NewHadoopRDD NewHadoopRDD is an rdd:index.md[RDD] of K keys and V values. < NewHadoopRDD is created>> when: SparkContext.newAPIHadoopFile SparkContext.newAPIHadoopRDD (indirectly) SparkContext.binaryFiles (indirectly) SparkContext.wholeTextFiles NOTE: NewHadoopRDD is the base RDD of BinaryFileRDD and WholeTextFileRDD . === [[getPreferredLocations]] getPreferredLocations Method CAUTION: FIXME === [[creating-instance]] Creating NewHadoopRDD Instance NewHadoopRDD takes the following when created: [[sc]] ROOT:SparkContext.md[] [[inputFormatClass]] HDFS' InputFormat[K, V] [[keyClass]] K class name [[valueClass]] V class name [[_conf]] transient HDFS' Configuration NewHadoopRDD initializes the < >.","title":"NewHadoopRDD"},{"location":"rdd/spark-rdd-OrderedRDDFunctions/","text":"== [[OrderedRDDFunctions]] OrderedRDDFunctions === [[repartitionAndSortWithinPartitions]] repartitionAndSortWithinPartitions Operator CAUTION: FIXME === [[sortByKey]] sortByKey Operator CAUTION: FIXME","title":"OrderedRDDFunctions"},{"location":"rdd/spark-rdd-ParallelCollectionRDD/","text":"== ParallelCollectionRDD ParallelCollectionRDD is an RDD of a collection of elements with numSlices partitions and optional locationPrefs . ParallelCollectionRDD is the result of SparkContext.parallelize and SparkContext.makeRDD methods. The data collection is split on to numSlices slices. It uses ParallelCollectionPartition .","title":"ParallelCollectionRDD"},{"location":"rdd/spark-rdd-Partition/","text":"== [[Partition]] Partition Partition is a < > of a < > of a RDD. NOTE: A partition is missing when it has not be computed yet. [[contract]] [[index]] Partition is identified by an partition index that is a unique identifier of a partition of a RDD. [source, scala] \u00b6 index: Int \u00b6","title":"Partition"},{"location":"rdd/spark-rdd-Partition/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-Partition/#index-int","text":"","title":"index: Int"},{"location":"rdd/spark-rdd-actions/","text":"== Actions Actions are spark-rdd-operations.md[RDD operations] that produce non-RDD values. They materialize a value in a Spark program. In other words, a RDD operation that returns a value of any type but RDD[T] is an action. action: RDD => a value NOTE: Actions are synchronous. You can use < > to release a calling thread while calling actions. They trigger execution of < > to return values. Simply put, an action evaluates the spark-rdd-lineage.md[RDD lineage graph]. You can think of actions as a valve and until action is fired, the data to be processed is not even in the pipes, i.e. transformations. Only actions can materialize the entire processing pipeline with real data. Actions are one of two ways to send data from executor:Executor.md[executors] to the spark-driver.md[driver] (the other being spark-accumulators.md[accumulators]). Actions in http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD[org.apache.spark.rdd.RDD ]: aggregate collect count countApprox* countByValue* first fold foreach foreachPartition max min reduce spark-io.md#saving-rdds-to-files[saveAs* actions], e.g. saveAsTextFile , saveAsHadoopFile take takeOrdered takeSample toLocalIterator top treeAggregate treeReduce Actions run spark-scheduler-ActiveJob.md[jobs] using ROOT:SparkContext.md#runJob[SparkContext.runJob] or directly scheduler:DAGScheduler.md#runJob[DAGScheduler.runJob]. [source,scala] \u00b6 scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to spark-rdd-caching.md[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"Actions"},{"location":"rdd/spark-rdd-actions/#sourcescala","text":"scala> words.count // <1> res0: Long = 502 <1> words is an RDD of String . TIP: You should cache RDDs you work with when you want to execute two or more actions on it for a better performance. Refer to spark-rdd-caching.md[RDD Caching and Persistence]. Before calling an action, Spark does closure/function cleaning (using SparkContext.clean ) to make it ready for serialization and sending over the wire to executors. Cleaning can throw a SparkException if the computation cannot be cleaned. NOTE: Spark uses ClosureCleaner to clean closures. === [[AsyncRDDActions]] AsyncRDDActions AsyncRDDActions class offers asynchronous actions that you can use on RDDs (thanks to the implicit conversion rddToAsyncRDDActions in RDD class). The methods return a < >. The following asynchronous methods are available: countAsync collectAsync takeAsync foreachAsync foreachPartitionAsync === [[FutureAction]] FutureActions CAUTION: FIXME","title":"[source,scala]"},{"location":"rdd/spark-rdd-caching/","text":"== RDD Caching and Persistence Caching or persistence are optimisation techniques for (iterative and interactive) Spark computations. They help saving interim partial results so they can be reused in subsequent stages. These interim results as RDDs are thus kept in memory (default) or more solid storages like disk and/or replicated. RDDs can be cached using < > operation. They can also be persisted using < > operation. The difference between cache and persist operations is purely syntactic. cache is a synonym of persist or persist(MEMORY_ONLY) , i.e. cache is merely persist with the default storage level MEMORY_ONLY . NOTE: Due to the very small and purely syntactic difference between caching and persistence of RDDs the two terms are often used interchangeably and I will follow the \"pattern\" here. RDDs can also be < > to remove RDD from a permanent storage like memory and/or disk. === [[cache]] Caching RDD -- cache Method [source, scala] \u00b6 cache(): this.type = persist() \u00b6 cache is a synonym of < > with storage:StorageLevel.md[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel storage:StorageLevel.md[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is core:ContextCleaner.md#registerRDDForCleanup[registered to ContextCleaner ] (if available) and ROOT:SparkContext.md#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method [source, scala] \u00b6 unpersist(blocking: Boolean = true): this.type \u00b6 When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls ROOT:SparkContext.md#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets storage:StorageLevel.md[ NONE storage level] as the current storage level.","title":"Caching and Persistence"},{"location":"rdd/spark-rdd-caching/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#cache-thistype-persist","text":"cache is a synonym of < > with storage:StorageLevel.md[ MEMORY_ONLY storage level]. === [[persist]] Persisting RDD -- persist Methods","title":"cache(): this.type = persist()"},{"location":"rdd/spark-rdd-caching/#source-scala_1","text":"persist(): this.type persist(newLevel: StorageLevel): this.type persist marks a RDD for persistence using newLevel storage:StorageLevel.md[storage level]. You can only change the storage level once or persist reports an UnsupportedOperationException : Cannot change storage level of an RDD after it was already assigned a level NOTE: You can pretend to change the storage level of an RDD with already-assigned storage level only if the storage level is the same as it is currently assigned. If the RDD is marked as persistent the first time, the RDD is core:ContextCleaner.md#registerRDDForCleanup[registered to ContextCleaner ] (if available) and ROOT:SparkContext.md#persistRDD[ SparkContext ]. The internal storageLevel attribute is set to the input newLevel storage level. === [[unpersist]] Unpersisting RDDs (Clearing Blocks) -- unpersist Method","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-caching/#unpersistblocking-boolean-true-thistype","text":"When called, unpersist prints the following INFO message to the logs: INFO [RddName]: Removing RDD [id] from persistence list It then calls ROOT:SparkContext.md#unpersist[SparkContext.unpersistRDD(id, blocking)] and sets storage:StorageLevel.md[ NONE storage level] as the current storage level.","title":"unpersist(blocking: Boolean = true): this.type"},{"location":"rdd/spark-rdd-lineage/","text":"== RDD Lineage -- Logical Execution Plan RDD Lineage (aka RDD operator graph or RDD dependency graph ) is a graph of all the parent RDDs of a RDD. It is built as a result of applying transformations to the RDD and creates a < >. NOTE: The execution DAG or physical execution plan is the scheduler:DAGScheduler.md[DAG of stages]. NOTE: The following diagram uses cartesian or zip for learning purposes only. You may use other operators to build a RDD graph. .RDD lineage image::rdd-lineage.png[align=\"center\"] The above RDD graph could be the result of the following series of transformations: val r00 = sc.parallelize(0 to 9) val r01 = sc.parallelize(0 to 90 by 10) val r10 = r00 cartesian r01 val r11 = r00.map(n => (n, n)) val r12 = r00 zip r01 val r13 = r01.keyBy(_ / 20) val r20 = Seq(r11, r12, r13).foldLeft(r10)(_ union _) A RDD lineage graph is hence a graph of what transformations need to be executed after an action has been called. You can learn about a RDD lineage graph using < > method. === [[logical-execution-plan]] Logical Execution Plan Logical Execution Plan starts with the earliest RDDs (those with no dependencies on other RDDs or reference cached data) and ends with the RDD that produces the result of the action that has been called to execute. NOTE: A logical plan, i.e. a DAG, is materialized and executed when ROOT:SparkContext.md#runJob[ SparkContext is requested to run a Spark job]. === [[toDebugString]] Getting RDD Lineage Graph -- toDebugString Method [source, scala] \u00b6 toDebugString: String \u00b6 You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence ROOT:SparkContext.md#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"RDD Lineage"},{"location":"rdd/spark-rdd-lineage/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-lineage/#todebugstring-string","text":"You can learn about a < > using toDebugString method. scala> val wordCount = sc.textFile(\"README.md\").flatMap(_.split(\"\\\\s+\")).map((_, 1)).reduceByKey(_ + _) wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24 scala> wordCount.toDebugString res13: String = (2) ShuffledRDD[21] at reduceByKey at <console>:24 [] +-(2) MapPartitionsRDD[20] at map at <console>:24 [] | MapPartitionsRDD[19] at flatMap at <console>:24 [] | README.md MapPartitionsRDD[18] at textFile at <console>:24 [] | README.md HadoopRDD[17] at textFile at <console>:24 [] toDebugString uses indentations to indicate a shuffle boundary. The numbers in round brackets show the level of parallelism at each stage, e.g. (2) in the above output. scala> wordCount.getNumPartitions res14: Int = 2 With < > property enabled, toDebugString is included when executing an action. $ ./bin/spark-shell --conf spark.logLineage=true scala> sc.textFile(\"README.md\", 4).count ... 15/10/17 14:46:42 INFO SparkContext: Starting job: count at <console>:25 15/10/17 14:46:42 INFO SparkContext: RDD's recursive dependencies: (4) MapPartitionsRDD[1] at textFile at <console>:25 [] | README.md HadoopRDD[0] at textFile at <console>:25 [] === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_logLineage]] spark.logLineage | false | When enabled (i.e. true ), executing an action (and hence ROOT:SparkContext.md#runJob[running a job]) will also print out the RDD lineage graph using < >. |===","title":"toDebugString: String"},{"location":"rdd/spark-rdd-operations/","text":"== Operators - Transformations and Actions RDDs have two types of operations: spark-rdd-transformations.md[transformations] and spark-rdd-actions.md[actions]. NOTE: Operators are also called operations . === Gotchas - things to watch for Even if you don't access it explicitly it cannot be referenced inside a closure as it is serialized and carried around across executors. See https://issues.apache.org/jira/browse/SPARK-5063","title":"Operators"},{"location":"rdd/spark-rdd-partitions/","text":"== Partitions and Partitioning === Introduction Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents. A partition (aka split ) is a logical chunk of a large distributed data set. [CAUTION] \u00b6 FIXME How does the number of partitions map to the number of tasks? How to verify it? How does the mapping between partitions and tasks correspond to data locality if any? \u00b6 Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning [TIP] \u00b6 Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?] https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark) \u00b6 By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation [source, scala] \u00b6 repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation [source, scala] \u00b6 coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T] \u00b6 The coalesce transformation is used to change the number of partitions. It can trigger spark-rdd-shuffle.md[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use spark-rdd-lineage.md#toDebugString[toDebugString] to check out the spark-rdd-lineage.md[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"Partitions and Partitioning"},{"location":"rdd/spark-rdd-partitions/#caution","text":"FIXME How does the number of partitions map to the number of tasks? How to verify it?","title":"[CAUTION]"},{"location":"rdd/spark-rdd-partitions/#how-does-the-mapping-between-partitions-and-tasks-correspond-to-data-locality-if-any","text":"Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons). Features: size number partitioning scheme node distribution repartitioning","title":"How does the mapping between partitions and tasks correspond to data locality if any?"},{"location":"rdd/spark-rdd-partitions/#tip","text":"Read the following documentations to learn what experts say on the topic: https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/performance_optimization/how_many_partitions_does_an_rdd_have.html[How Many Partitions Does An RDD Have?]","title":"[TIP]"},{"location":"rdd/spark-rdd-partitions/#httpssparkapacheorgdocslatesttuninghtmltuning-spark-the-official-documentation-of-spark","text":"By default, a partition is created for each HDFS partition, which by default is 64MB (from http://spark.apache.org/docs/latest/programming-guide.html#external-datasets[Spark's Programming Guide]). RDDs get partitioned automatically without programmer intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application. You use def getPartitions: Array[Partition] method on a RDD to know the set of partitions in this RDD. As noted in https://github.com/databricks/spark-knowledgebase/blob/master/performance_optimization/how_many_partitions_does_an_rdd_have.md#view-task-execution-against-partitions-using-the-ui[View Task Execution Against Partitions Using the UI]: When a stage executes, you can see the number of partitions for a given stage in the Spark UI. Start spark-shell and see it yourself! scala> sc.parallelize(1 to 100).count res0: Long = 100 When you execute the Spark job, i.e. sc.parallelize(1 to 100).count , you should see the following in http://localhost:4040/jobs[Spark shell application UI]. .The number of partition as Total tasks in UI image::spark-partitions-ui-stages.png[align=\"center\"] The reason for 8 Tasks in Total is that I'm on a 8-core laptop and by default the number of partitions is the number of all available cores. $ sysctl -n hw.ncpu 8 You can request for the minimum number of partitions, using the second input parameter to many transformations. scala> sc.parallelize(1 to 100, 2).count res1: Long = 100 .Total tasks in UI shows 2 partitions image::spark-partitions-ui-stages-2-partitions.png[align=\"center\"] You can always ask for the number of partitions using partitions method of a RDD: scala> val ints = sc.parallelize(1 to 100, 4) ints: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24 scala> ints.partitions.size res2: Int = 4 In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/fewer partitions allow work to be done in larger chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead. Increasing partitions count will make each partition to have less data (or not at all!) Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism[2-3x times that]). As far as choosing a \"good\" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed value by calling sc.defaultParallelism . Also, the number of partitions determines how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory of an executor. In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition) , the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever shuffle occurs. Repartitioning may cause shuffle to occur in some situations, but it is not guaranteed to occur in all cases. And it usually happens during action stage. When creating an RDD by reading a file using rdd = SparkContext().textFile(\"hdfs://.../file.txt\") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions. Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input parameter in the call like rdd = sc.textFile(\"hdfs://.../file.txt\", 400) , where 400 is the number of partitions. In this case, the partitioning makes for 400 splits that would be done by the Hadoop's TextInputFormat , not Spark and it would work much faster. It's also that the code spawns 400 concurrent tasks to try to load file.txt directly into 400 partitions. It will only work as described for uncompressed files. When using textFile with compressed files ( file.txt.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change the number of partitions you should do < >. Some operations, e.g. map , flatMap , filter , don't preserve partitioning. map , flatMap , filter operations apply a function to every partition. === [[repartitioning]][[repartition]] Repartitioning RDD -- repartition Transformation","title":"https://spark.apache.org/docs/latest/tuning.html[Tuning Spark] (the official documentation of Spark)"},{"location":"rdd/spark-rdd-partitions/#source-scala","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#repartitionnumpartitions-intimplicit-ord-orderingt-null-rddt","text":"repartition is < > with numPartitions and shuffle enabled. With the following computation you can see that repartition(5) causes 5 tasks to be started using NODE_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(5).count ... 15/10/07 08:10:00 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 7 (MapPartitionsRDD[19] at repartition at <console>:27) 15/10/07 08:10:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 5 tasks 15/10/07 08:10:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17, localhost, partition 0,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18, localhost, partition 1,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19, localhost, partition 2,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20, localhost, partition 3,NODE_LOCAL, 2089 bytes) 15/10/07 08:10:00 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21, localhost, partition 4,NODE_LOCAL, 2089 bytes) ... You can see a change after executing repartition(1) causes 2 tasks to be started using PROCESS_LOCAL spark-data-locality.md[data locality]. scala> lines.repartition(1).count ... 15/10/07 08:14:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[20] at repartition at <console>:27) 15/10/07 08:14:09 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks 15/10/07 08:14:09 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 22, localhost, partition 0,PROCESS_LOCAL, 2058 bytes) 15/10/07 08:14:09 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 23, localhost, partition 1,PROCESS_LOCAL, 2058 bytes) ... Please note that Spark disables splitting for compressed files and creates RDDs with only 1 partition. In such cases, it's helpful to use sc.textFile('demo.gz') and do repartitioning using rdd.repartition(100) as follows: rdd = sc.textFile('demo.gz') rdd = rdd.repartition(100) With the lines, you end up with rdd to be exactly 100 partitions of roughly equal in size. rdd.repartition(N) does a shuffle to split data to match N ** partitioning is done on round robin basis TIP: If partitioning scheme doesn't work for you, you can write your own custom partitioner. TIP: It's useful to get familiar with https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/TextInputFormat.html[Hadoop's TextInputFormat]. === [[coalesce]] coalesce Transformation","title":"repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-partitions/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-partitions/#coalescenumpartitions-int-shuffle-boolean-falseimplicit-ord-orderingt-null-rddt","text":"The coalesce transformation is used to change the number of partitions. It can trigger spark-rdd-shuffle.md[RDD shuffling] depending on the shuffle flag (disabled by default, i.e. false ). In the following sample, you parallelize a local 10-number sequence and coalesce it first without and then with shuffling (note the shuffle parameter being false and true , respectively). TIP: Use spark-rdd-lineage.md#toDebugString[toDebugString] to check out the spark-rdd-lineage.md[RDD lineage graph]. scala> val rdd = sc.parallelize(0 to 10, 8) rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24 scala> rdd.partitions.size res0: Int = 8 scala> rdd.coalesce(numPartitions=8, shuffle=false) // <1> res1: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at <console>:27 scala> res1.toDebugString res2: String = (8) CoalescedRDD[1] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] scala> rdd.coalesce(numPartitions=8, shuffle=true) res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at coalesce at <console>:27 scala> res3.toDebugString res4: String = (8) MapPartitionsRDD[5] at coalesce at <console>:27 [] | CoalescedRDD[4] at coalesce at <console>:27 [] | ShuffledRDD[3] at coalesce at <console>:27 [] +-(8) MapPartitionsRDD[2] at coalesce at <console>:27 [] | ParallelCollectionRDD[0] at parallelize at <console>:24 [] <1> shuffle is false by default and it's explicitly used here for demo purposes. Note the number of partitions that remains the same as the number of partitions in the source RDD rdd .","title":"coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T]"},{"location":"rdd/spark-rdd-shuffle/","text":"= RDD shuffling :url-spark-docs: https://spark.apache.org/docs/{spark-version } TIP: Read the official documentation about the topic {url-spark-docs}/rdd-programming-guide.html#shuffle-operations[Shuffle operations]. It is still better than this page. Shuffling is a process of spark-rdd-partitions.md[redistributing data across partitions] (aka repartitioning ) that may or may not cause moving data across JVM processes or even over the wire (between executors on separate machines). Shuffling is the process of data transfer between stages. TIP: Avoid shuffling at all cost. Think about ways to leverage existing partitions. Leverage partial aggregation to reduce data transfer. By default, shuffling doesn't change the number of partitions, but their content. Avoid groupByKey and use reduceByKey or combineByKey instead. ** groupByKey shuffles all the data, which is slow. ** reduceByKey shuffles only the results of sub-aggregations in each partition of the data. == Example - join PairRDD offers http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/programming-guide.html#JoinLink[join ] transformation that (quoting the official documentation): When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Let's have a look at an example and see how it works under the covers: scala> val kv = (0 to 5) zip Stream.continually(5) kv: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,5), (1,5), (2,5), (3,5), (4,5), (5,5)) scala> val kw = (0 to 5) zip Stream.continually(10) kw: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((0,10), (1,10), (2,10), (3,10), (4,10), (5,10)) scala> val kvR = sc.parallelize(kv) kvR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[3] at parallelize at <console>:26 scala> val kwR = sc.parallelize(kw) kwR: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:26 scala> val joined = kvR join kwR joined: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[10] at join at <console>:32 scala> joined.toDebugString res7: String = (8) MapPartitionsRDD[10] at join at <console>:32 [] | MapPartitionsRDD[9] at join at <console>:32 [] | CoGroupedRDD[8] at join at <console>:32 [] +-(8) ParallelCollectionRDD[3] at parallelize at <console>:26 [] +-(8) ParallelCollectionRDD[4] at parallelize at <console>:26 [] It doesn't look good when there is an \"angle\" between \"nodes\" in an operation graph. It appears before the join operation so shuffle is expected. Here is how the job of executing joined.count looks in Web UI. .Executing joined.count image::spark-shuffle-join-webui.png[align=\"center\"] The screenshot of Web UI shows 3 stages with two parallelize to Shuffle Write and count to Shuffle Read. It means shuffling has indeed happened. CAUTION: FIXME Just learnt about sc.range(0, 5) as a shorter version of sc.parallelize(0 to 5) join operation is one of the cogroup operations that uses defaultPartitioner , i.e. walks through spark-rdd-lineage.md[the RDD lineage graph] (sorted by the number of partitions decreasing) and picks the partitioner with positive number of output partitions. Otherwise, it checks ROOT:configuration-properties.md#spark.default.parallelism[spark.default.parallelism] configuration and if defined picks rdd:HashPartitioner.md[HashPartitioner] with the default parallelism of the scheduler:SchedulerBackend.md[SchedulerBackend]. join is almost CoGroupedRDD.mapValues . CAUTION: FIXME the default parallelism of scheduler backend","title":"Shuffling"},{"location":"rdd/spark-rdd-transformations/","text":"== Transformations -- Lazy Operations on RDD (to Create One or More RDDs) Transformations are lazy operations on an rdd:RDD.md[RDD] that create one or many new RDDs. // T and U are Scala types transformation: RDD[T] => RDD[U] transformation: RDD[T] => Seq[RDD[U]] In other words, transformations are functions that take an RDD as the input and produce one or many RDDs as the output. Transformations do not change the input RDD (since rdd:index.md#introduction[RDDs are immutable] and hence cannot be modified), but produce one or more new RDDs by applying the computations they represent. [[methods]] .(Subset of) RDD Transformations (Public API) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | aggregate a| [[aggregate]] [source, scala] \u00b6 aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]] [source, scala] \u00b6 barrier(): RDDBarrier[T] \u00b6 ( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]] [source, scala] \u00b6 cache(): this.type \u00b6 Persists the RDD with the storage:StorageLevel.md#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]] [source, scala] \u00b6 coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]] [source, scala] \u00b6 filter(f: T => Boolean): RDD[T] \u00b6 | flatMap a| [[flatMap]] [source, scala] \u00b6 flatMap U : RDD[U] \u00b6 | map a| [[map]] [source, scala] \u00b6 map U : RDD[U] \u00b6 | mapPartitions a| [[mapPartitions]] [source, scala] \u00b6 mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]] [source, scala] \u00b6 mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]] [source, scala] \u00b6 randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]] [source, scala] \u00b6 ++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]] [source, scala] \u00b6 persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a spark-rdd-lineage.md[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations. [source,scala] \u00b6 scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute spark-rdd-shuffle.md[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex [source, scala] \u00b6 zipWithIndex(): RDD[(T, Long)] \u00b6 zipWithIndex zips this RDD[T] with its element indices. [CAUTION] \u00b6 If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices. [source, scala] \u00b6 val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"Transformations"},{"location":"rdd/spark-rdd-transformations/#source-scala","text":"aggregate U ( seqOp: (U, T) => U, combOp: (U, U) => U): U | barrier a| [[barrier]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#barrier-rddbarriert","text":"( New in 2.4.0 ) Marks the current stage as a < > in < >, where Spark must launch all tasks together Internally, barrier creates a < > over the RDD | cache a| [[cache]]","title":"barrier(): RDDBarrier[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#cache-thistype","text":"Persists the RDD with the storage:StorageLevel.md#MEMORY_ONLY[MEMORY_ONLY] storage level Synonym of < > | coalesce a| [[coalesce]]","title":"cache(): this.type"},{"location":"rdd/spark-rdd-transformations/#source-scala_3","text":"coalesce( numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty) (implicit ord: Ordering[T] = null): RDD[T] | filter a| [[filter]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#filterf-t-boolean-rddt","text":"| flatMap a| [[flatMap]]","title":"filter(f: T =&gt; Boolean): RDD[T]"},{"location":"rdd/spark-rdd-transformations/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#flatmapu-rddu","text":"| map a| [[map]]","title":"flatMapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#mapu-rddu","text":"| mapPartitions a| [[mapPartitions]]","title":"mapU: RDD[U]"},{"location":"rdd/spark-rdd-transformations/#source-scala_7","text":"mapPartitions U : RDD[U] | mapPartitionsWithIndex a| [[mapPartitionsWithIndex]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_8","text":"mapPartitionsWithIndex U : RDD[U] | randomSplit a| [[randomSplit]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_9","text":"randomSplit( weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]] | union a| [[union]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_10","text":"++(other: RDD[T]): RDD[T] union(other: RDD[T]): RDD[T] | persist a| [[persist]]","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_11","text":"persist(): this.type persist(newLevel: StorageLevel): this.type |=== By applying transformations you incrementally build a spark-rdd-lineage.md[RDD lineage] with all the parent RDDs of the final RDD(s). Transformations are lazy, i.e. are not executed immediately. Only after calling an action are transformations executed. After executing a transformation, the result RDD(s) will always be different from their parents and can be smaller (e.g. filter , count , distinct , sample ), bigger (e.g. flatMap , union , cartesian ) or the same size (e.g. map ). CAUTION: There are transformations that may trigger jobs, e.g. sortBy , < >, etc. .From SparkContext by transformations to the result image::rdd-sparkcontext-transformations-action.png[align=\"center\"] Certain transformations can be pipelined which is an optimization that Spark uses to improve performance of computations.","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#sourcescala","text":"scala> val file = sc.textFile(\"README.md\") file: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[54] at textFile at :24 scala> val allWords = file.flatMap(_.split(\"\\W+\")) allWords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at flatMap at :26 scala> val words = allWords.filter(!_.isEmpty) words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[56] at filter at :28 scala> val pairs = words.map((_,1)) pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[57] at map at :30 scala> val reducedByKey = pairs.reduceByKey(_ + _) reducedByKey: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[59] at reduceByKey at :32 scala> val top10words = reducedByKey.takeOrdered(10)(Ordering[Int].reverse.on(_._2)) INFO SparkContext: Starting job: takeOrdered at :34 ... INFO DAGScheduler: Job 18 finished: takeOrdered at :34, took 0.074386 s top10words: Array[(String, Int)] = Array((the,21), (to,14), (Spark,13), (for,11), (and,10), (##,8), (a,8), (run,7), (can,6), (is,6)) There are two kinds of transformations: < > < > === [[narrow-transformations]] Narrow Transformations Narrow transformations are the result of map , filter and such that is from the data from a single partition only, i.e. it is self-sustained. An output RDD has partitions with records that originate from a single partition in the parent RDD. Only a limited subset of partitions used to calculate the result. Spark groups narrow transformations as a stage which is called pipelining . === [[wide-transformations]] Wide Transformations Wide transformations are the result of groupByKey and reduceByKey . The data required to compute the records in a single partition may reside in many partitions of the parent RDD. NOTE: Wide transformations are also called shuffle transformations as they may or may not depend on a shuffle. All of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy these operations, Spark must execute spark-rdd-shuffle.md[RDD shuffle], which transfers data across cluster and results in a new stage with a new set of partitions. === [[zipWithIndex]] zipWithIndex","title":"[source,scala]"},{"location":"rdd/spark-rdd-transformations/#source-scala_12","text":"","title":"[source, scala]"},{"location":"rdd/spark-rdd-transformations/#zipwithindex-rddt-long","text":"zipWithIndex zips this RDD[T] with its element indices.","title":"zipWithIndex(): RDD[(T, Long)]"},{"location":"rdd/spark-rdd-transformations/#caution","text":"If the number of partitions of the source RDD is greater than 1, it will submit an additional job to calculate start indices.","title":"[CAUTION]"},{"location":"rdd/spark-rdd-transformations/#source-scala_13","text":"val onePartition = sc.parallelize(0 to 9, 1) scala> onePartition.partitions.length res0: Int = 1 // no job submitted onePartition.zipWithIndex val eightPartitions = sc.parallelize(0 to 9, 8) scala> eightPartitions.partitions.length res1: Int = 8 // submits a job eightPartitions.zipWithIndex .Spark job submitted by zipWithIndex transformation image::spark-transformations-zipWithIndex-webui.png[align=\"center\"] ====","title":"[source, scala]"},{"location":"rest/","text":"= Status REST API -- Monitoring Spark Applications Using REST API Status REST API is a collection of REST endpoints under /api/v1 URI path in the spark-api-UIRoot.md[root containers for application UI information]: [[SparkUI]] spark-webui-SparkUI.md[SparkUI] - Application UI for an active Spark application (i.e. a Spark application that is still running) [[HistoryServer]] spark-history-server:HistoryServer.md[HistoryServer] - Application UI for active and completed Spark applications (i.e. Spark applications that are still running or have already finished) Status REST API uses spark-api-ApiRootResource.md[ApiRootResource] main resource class that registers /api/v1 URI < >. [[paths]] .URI Paths [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Path | Description | [[applications]] applications | [[ApplicationListResource]] Delegates to the spark-api-ApplicationListResource.md[ApplicationListResource] resource class | [[applications_appId]] applications/\\{appId} | [[OneApplicationResource]] Delegates to the spark-api-OneApplicationResource.md[OneApplicationResource] resource class | [[version]] version | Creates a VersionInfo with the current version of Spark |=== Status REST API uses the following components: https://jersey.github.io/[Jersey RESTful Web Services framework] with support for the https://github.com/jax-rs[Java API for RESTful Web Services] (JAX-RS API) https://www.eclipse.org/jetty/[Eclipse Jetty] as the lightweight HTTP server and the https://jcp.org/en/jsr/detail?id=369[Java Servlet] container","title":"Status REST API"},{"location":"rest/AbstractApplicationResource/","text":"== [[AbstractApplicationResource]] AbstractApplicationResource AbstractApplicationResource is a spark-api-BaseAppResource.md[BaseAppResource] with a set of < > that are common across < >. // start spark-shell $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 257 Content-Type: application/json Date: Tue, 05 Jun 2018 18:46:32 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-05T15:04:48.328GMT\", \"lastUpdatedEpoch\": 1528211088328, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-05T15:04:48.328GMT\", \"startTimeEpoch\": 1528211088328 } ], \"id\": \"local-1528211089216\", \"name\": \"Spark shell\" } ] $ http http://localhost:4040/api/v1/applications/local-1528211089216/storage/rdd HTTP/1.1 200 OK Content-Length: 3 Content-Type: application/json Date: Tue, 05 Jun 2018 18:48:00 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [] // Execute the following query in spark-shell spark.range(5).cache.count $ http http://localhost:4040/api/v1/applications/local-1528211089216/storage/rdd // output omitted for brevity [[implementations]] .AbstractApplicationResources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | AbstractApplicationResource | Description | spark-api-OneApplicationResource.md[OneApplicationResource] | [[OneApplicationResource]] Handles applications/appId requests | spark-api-OneApplicationAttemptResource.md[OneApplicationAttemptResource] | [[OneApplicationAttemptResource]] |=== [[paths]] .AbstractApplicationResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | allexecutors | GET | < > | environment | GET | < > | executors | GET | < > | jobs | GET | < > | jobs/{jobId: \\\\d+} | GET | < > | logs | GET | < > stages < > | storage/rdd/{rddId: \\\\d+} | GET | < > | [[storage_rdd]] storage/rdd | GET | < > |=== === [[rddList]] rddList Method [source, scala] \u00b6 rddList(): Seq[RDDStorageInfo] \u00b6 rddList ...FIXME NOTE: rddList is used when...FIXME === [[environmentInfo]] environmentInfo Method [source, scala] \u00b6 environmentInfo(): ApplicationEnvironmentInfo \u00b6 environmentInfo ...FIXME NOTE: environmentInfo is used when...FIXME === [[rddData]] rddData Method [source, scala] \u00b6 rddData(@PathParam(\"rddId\") rddId: Int): RDDStorageInfo \u00b6 rddData ...FIXME NOTE: rddData is used when...FIXME === [[allExecutorList]] allExecutorList Method [source, scala] \u00b6 allExecutorList(): Seq[ExecutorSummary] \u00b6 allExecutorList ...FIXME NOTE: allExecutorList is used when...FIXME === [[executorList]] executorList Method [source, scala] \u00b6 executorList(): Seq[ExecutorSummary] \u00b6 executorList ...FIXME NOTE: executorList is used when...FIXME === [[oneJob]] oneJob Method [source, scala] \u00b6 oneJob(@PathParam(\"jobId\") jobId: Int): JobData \u00b6 oneJob ...FIXME NOTE: oneJob is used when...FIXME === [[jobsList]] jobsList Method [source, scala] \u00b6 jobsList(@QueryParam(\"status\") statuses: JList[JobExecutionStatus]): Seq[JobData] \u00b6 jobsList ...FIXME NOTE: jobsList is used when...FIXME","title":"AbstractApplicationResource"},{"location":"rest/AbstractApplicationResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#rddlist-seqrddstorageinfo","text":"rddList ...FIXME NOTE: rddList is used when...FIXME === [[environmentInfo]] environmentInfo Method","title":"rddList(): Seq[RDDStorageInfo]"},{"location":"rest/AbstractApplicationResource/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#environmentinfo-applicationenvironmentinfo","text":"environmentInfo ...FIXME NOTE: environmentInfo is used when...FIXME === [[rddData]] rddData Method","title":"environmentInfo(): ApplicationEnvironmentInfo"},{"location":"rest/AbstractApplicationResource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#rdddatapathparamrddid-rddid-int-rddstorageinfo","text":"rddData ...FIXME NOTE: rddData is used when...FIXME === [[allExecutorList]] allExecutorList Method","title":"rddData(@PathParam(\"rddId\") rddId: Int): RDDStorageInfo"},{"location":"rest/AbstractApplicationResource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#allexecutorlist-seqexecutorsummary","text":"allExecutorList ...FIXME NOTE: allExecutorList is used when...FIXME === [[executorList]] executorList Method","title":"allExecutorList(): Seq[ExecutorSummary]"},{"location":"rest/AbstractApplicationResource/#source-scala_4","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#executorlist-seqexecutorsummary","text":"executorList ...FIXME NOTE: executorList is used when...FIXME === [[oneJob]] oneJob Method","title":"executorList(): Seq[ExecutorSummary]"},{"location":"rest/AbstractApplicationResource/#source-scala_5","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#onejobpathparamjobid-jobid-int-jobdata","text":"oneJob ...FIXME NOTE: oneJob is used when...FIXME === [[jobsList]] jobsList Method","title":"oneJob(@PathParam(\"jobId\") jobId: Int): JobData"},{"location":"rest/AbstractApplicationResource/#source-scala_6","text":"","title":"[source, scala]"},{"location":"rest/AbstractApplicationResource/#jobslistqueryparamstatus-statuses-jlistjobexecutionstatus-seqjobdata","text":"jobsList ...FIXME NOTE: jobsList is used when...FIXME","title":"jobsList(@QueryParam(\"status\") statuses: JList[JobExecutionStatus]): Seq[JobData]"},{"location":"rest/ApiRequestContext/","text":"== [[ApiRequestContext]] ApiRequestContext ApiRequestContext is the < > of...FIXME [[contract]] [source, scala] package org.apache.spark.status.api.v1 trait ApiRequestContext { // only required methods that have no implementation // the others follow @Context var servletContext: ServletContext = _ @Context var httpRequest: HttpServletRequest = _ } NOTE: ApiRequestContext is a private[v1] contract. .ApiRequestContext Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | httpRequest | [[httpRequest]] Java Servlets' HttpServletRequest Used when...FIXME | servletContext | [[servletContext]] Java Servlets' ServletContext Used when...FIXME |=== [[implementations]] .ApiRequestContexts [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ApiRequestContext | Description | spark-api-ApiRootResource.md[ApiRootResource] | [[ApiRootResource]] | ApiStreamingApp | [[ApiStreamingApp]] | spark-api-ApplicationListResource.md[ApplicationListResource] | [[ApplicationListResource]] | spark-api-BaseAppResource.md[BaseAppResource] | [[BaseAppResource]] | SecurityFilter | [[SecurityFilter]] |=== === [[uiRoot]] Getting Current UIRoot -- uiRoot Method [source, scala] \u00b6 uiRoot: UIRoot \u00b6 uiRoot simply requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#getUiRoot[get the current UIRoot] (for the given < >). NOTE: uiRoot is used when...FIXME","title":"ApiRequestContext"},{"location":"rest/ApiRequestContext/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/ApiRequestContext/#uiroot-uiroot","text":"uiRoot simply requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#getUiRoot[get the current UIRoot] (for the given < >). NOTE: uiRoot is used when...FIXME","title":"uiRoot: UIRoot"},{"location":"rest/ApiRootResource/","text":"== [[ApiRootResource]] ApiRootResource -- /api/v1 URI Handler ApiRootResource is the spark-api-ApiRequestContext.md[ApiRequestContext] for the /v1 URI path. ApiRootResource uses @Path(\"/v1\") annotation at the class level. It is a partial URI path template relative to the base URI of the server on which the resource is deployed, the context root of the application, and the URL pattern to which the JAX-RS runtime responds. TIP: Learn more about @Path annotation in https://docs.oracle.com/cd/E19798-01/821-1841/6nmq2cp26/index.html[The @Path Annotation and URI Path Templates]. ApiRootResource < > the /api/* context handler (with the REST resources and providers in org.apache.spark.status.api.v1 package). With the @Path(\"/v1\") annotation and after < > the /api/* context handler, ApiRootResource serves HTTP requests for < > under the /api/v1 URI paths for spark-webui-SparkUI.md#initialize[SparkUI] and spark-history-server:HistoryServer.md#initialize[HistoryServer]. ApiRootResource gives the metrics of a Spark application in JSON format (using JAX-RS API). // start spark-shell $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 257 Content-Type: application/json Date: Tue, 05 Jun 2018 18:36:16 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-05T15:04:48.328GMT\", \"lastUpdatedEpoch\": 1528211088328, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-05T15:04:48.328GMT\", \"startTimeEpoch\": 1528211088328 } ], \"id\": \"local-1528211089216\", \"name\": \"Spark shell\" } ] // Fixed in Spark 2.3.1 // https://issues.apache.org/jira/browse/SPARK-24188 $ http http://localhost:4040/api/v1/version HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 43 Content-Type: application/json Date: Thu, 14 Jun 2018 08:19:06 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent { \"spark\": \"2.3.1\" } [[paths]] .ApiRootResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description [[applications]] applications [[ApplicationListResource]] Delegates to the spark-api-ApplicationListResource.md[ApplicationListResource] resource class [[applications_appId]] applications/\\{appId} [[OneApplicationResource]] Delegates to the spark-api-OneApplicationResource.md[OneApplicationResource] resource class | [[version]] version | GET | Creates a VersionInfo with the current version of Spark |=== === [[getServletHandler]] Creating /api/* Context Handler -- getServletHandler Method [source, scala] \u00b6 getServletHandler(uiRoot: UIRoot): ServletContextHandler \u00b6 getServletHandler creates a Jetty ServletContextHandler for /api context path. NOTE: The Jetty ServletContextHandler created does not support HTTP sessions as REST API is stateless. getServletHandler creates a Jetty ServletHolder with the resources and providers in org.apache.spark.status.api.v1 package. It then registers the ServletHolder to serve /* context path (under the ServletContextHandler for /api ). getServletHandler requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#setUiRoot[setUiRoot] with the ServletContextHandler and the input spark-api-UIRoot.md[UIRoot]. NOTE: getServletHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and spark-history-server:HistoryServer.md#initialize[HistoryServer] are requested to initialize.","title":"ApiRootResource"},{"location":"rest/ApiRootResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/ApiRootResource/#getservlethandleruiroot-uiroot-servletcontexthandler","text":"getServletHandler creates a Jetty ServletContextHandler for /api context path. NOTE: The Jetty ServletContextHandler created does not support HTTP sessions as REST API is stateless. getServletHandler creates a Jetty ServletHolder with the resources and providers in org.apache.spark.status.api.v1 package. It then registers the ServletHolder to serve /* context path (under the ServletContextHandler for /api ). getServletHandler requests UIRootFromServletContext to spark-api-UIRootFromServletContext.md#setUiRoot[setUiRoot] with the ServletContextHandler and the input spark-api-UIRoot.md[UIRoot]. NOTE: getServletHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and spark-history-server:HistoryServer.md#initialize[HistoryServer] are requested to initialize.","title":"getServletHandler(uiRoot: UIRoot): ServletContextHandler"},{"location":"rest/ApplicationListResource/","text":"== [[ApplicationListResource]] ApplicationListResource -- applications URI Handler ApplicationListResource is a spark-api-ApiRequestContext.md[ApiRequestContext] that spark-api-ApiRootResource.md#applications[ApiRootResource] uses to handle < > URI path. [[paths]] .ApplicationListResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | [[root]] / | GET | < > |=== // start spark-shell // there should be a single Spark application -- the spark-shell itself $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 255 Content-Type: application/json Date: Wed, 06 Jun 2018 12:40:33 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-06T12:30:19.220GMT\", \"lastUpdatedEpoch\": 1528288219220, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-06T12:30:19.220GMT\", \"startTimeEpoch\": 1528288219220 } ], \"id\": \"local-1528288219790\", \"name\": \"Spark shell\" } ] === [[isAttemptInRange]] isAttemptInRange Internal Method [source, scala] \u00b6 isAttemptInRange( attempt: ApplicationAttemptInfo, minStartDate: SimpleDateParam, maxStartDate: SimpleDateParam, minEndDate: SimpleDateParam, maxEndDate: SimpleDateParam, anyRunning: Boolean): Boolean isAttemptInRange ...FIXME NOTE: isAttemptInRange is used exclusively when ApplicationListResource is requested to handle a < > HTTP request. === [[appList]] appList Method [source, scala] \u00b6 appList( @QueryParam(\"status\") status: JList[ApplicationStatus], @DefaultValue(\"2010-01-01\") @QueryParam(\"minDate\") minDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxDate\") maxDate: SimpleDateParam, @DefaultValue(\"2010-01-01\") @QueryParam(\"minEndDate\") minEndDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxEndDate\") maxEndDate: SimpleDateParam, @QueryParam(\"limit\") limit: Integer) : Iterator[ApplicationInfo] appList ...FIXME NOTE: appList is used when...FIXME","title":"ApplicationListResource"},{"location":"rest/ApplicationListResource/#source-scala","text":"isAttemptInRange( attempt: ApplicationAttemptInfo, minStartDate: SimpleDateParam, maxStartDate: SimpleDateParam, minEndDate: SimpleDateParam, maxEndDate: SimpleDateParam, anyRunning: Boolean): Boolean isAttemptInRange ...FIXME NOTE: isAttemptInRange is used exclusively when ApplicationListResource is requested to handle a < > HTTP request. === [[appList]] appList Method","title":"[source, scala]"},{"location":"rest/ApplicationListResource/#source-scala_1","text":"appList( @QueryParam(\"status\") status: JList[ApplicationStatus], @DefaultValue(\"2010-01-01\") @QueryParam(\"minDate\") minDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxDate\") maxDate: SimpleDateParam, @DefaultValue(\"2010-01-01\") @QueryParam(\"minEndDate\") minEndDate: SimpleDateParam, @DefaultValue(\"3000-01-01\") @QueryParam(\"maxEndDate\") maxEndDate: SimpleDateParam, @QueryParam(\"limit\") limit: Integer) : Iterator[ApplicationInfo] appList ...FIXME NOTE: appList is used when...FIXME","title":"[source, scala]"},{"location":"rest/BaseAppResource/","text":"== [[BaseAppResource]] BaseAppResource BaseAppResource is the contract of spark-api-ApiRequestContext.md[ApiRequestContexts] that can < > and use < > and < > path parameters in URI paths. [[path-params]] .BaseAppResource's Path Parameters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appId | [[appId]] @PathParam(\"appId\") Used when...FIXME | attemptId | [[attemptId]] @PathParam(\"attemptId\") Used when...FIXME |=== [[implementations]] .BaseAppResources [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | BaseAppResource | Description | spark-api-AbstractApplicationResource.md[AbstractApplicationResource] | [[AbstractApplicationResource]] | BaseStreamingAppResource | [[BaseStreamingAppResource]] | spark-api-StagesResource.md[StagesResource] | [[StagesResource]] |=== NOTE: BaseAppResource is a private[v1] contract. === [[withUI]] withUI Method [source, scala] \u00b6 withUI T : T \u00b6 withUI ...FIXME NOTE: withUI is used when...FIXME","title":"BaseAppResource"},{"location":"rest/BaseAppResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/BaseAppResource/#withuit-t","text":"withUI ...FIXME NOTE: withUI is used when...FIXME","title":"withUIT: T"},{"location":"rest/OneApplicationAttemptResource/","text":"== [[OneApplicationAttemptResource]] OneApplicationAttemptResource OneApplicationAttemptResource is a spark-api-AbstractApplicationResource.md[AbstractApplicationResource] (and so a spark-api-ApiRequestContext.md[ApiRequestContext] indirectly). OneApplicationAttemptResource is used when AbstractApplicationResource is requested to spark-api-AbstractApplicationResource.md#applicationAttempt[applicationAttempt]. [[paths]] .OneApplicationAttemptResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | [[root]] / | GET | < > |=== // start spark-shell // there should be a single Spark application -- the spark-shell itself // CAUTION: FIXME Demo of OneApplicationAttemptResource in Action === [[getAttempt]] getAttempt Method [source, scala] \u00b6 getAttempt(): ApplicationAttemptInfo \u00b6 getAttempt requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]) and finds the spark-api-BaseAppResource.md#attemptId[attemptId] among the available attempts. NOTE: spark-api-BaseAppResource.md#appId[appId] and spark-api-BaseAppResource.md#attemptId[attemptId] are path parameters. In the end, getAttempt returns the ApplicationAttemptInfo if available or reports a NotFoundException : unknown app [appId], attempt [attemptId]","title":"OneApplicationAttemptResource"},{"location":"rest/OneApplicationAttemptResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/OneApplicationAttemptResource/#getattempt-applicationattemptinfo","text":"getAttempt requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]) and finds the spark-api-BaseAppResource.md#attemptId[attemptId] among the available attempts. NOTE: spark-api-BaseAppResource.md#appId[appId] and spark-api-BaseAppResource.md#attemptId[attemptId] are path parameters. In the end, getAttempt returns the ApplicationAttemptInfo if available or reports a NotFoundException : unknown app [appId], attempt [attemptId]","title":"getAttempt(): ApplicationAttemptInfo"},{"location":"rest/OneApplicationResource/","text":"== [[OneApplicationResource]] OneApplicationResource -- applications/appId URI Handler OneApplicationResource is a spark-api-AbstractApplicationResource.md[AbstractApplicationResource] (and so a spark-api-ApiRequestContext.md[ApiRequestContext] indirectly) that spark-api-ApiRootResource.md#applications_appId[ApiRootResource] uses to handle < > URI path. [[paths]] .OneApplicationResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | [[root]] / | GET | < > |=== // start spark-shell // there should be a single Spark application -- the spark-shell itself $ http http://localhost:4040/api/v1/applications HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 255 Content-Type: application/json Date: Wed, 06 Jun 2018 12:40:33 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent [ { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-06T12:30:19.220GMT\", \"lastUpdatedEpoch\": 1528288219220, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-06T12:30:19.220GMT\", \"startTimeEpoch\": 1528288219220 } ], \"id\": \"local-1528288219790\", \"name\": \"Spark shell\" } ] $ http http://localhost:4040/api/v1/applications/local-1528288219790 HTTP/1.1 200 OK Content-Encoding: gzip Content-Length: 255 Content-Type: application/json Date: Wed, 06 Jun 2018 12:41:43 GMT Server: Jetty(9.3.z-SNAPSHOT) Vary: Accept-Encoding, User-Agent { \"attempts\": [ { \"appSparkVersion\": \"2.3.1-SNAPSHOT\", \"completed\": false, \"duration\": 0, \"endTime\": \"1969-12-31T23:59:59.999GMT\", \"endTimeEpoch\": -1, \"lastUpdated\": \"2018-06-06T12:30:19.220GMT\", \"lastUpdatedEpoch\": 1528288219220, \"sparkUser\": \"jacek\", \"startTime\": \"2018-06-06T12:30:19.220GMT\", \"startTimeEpoch\": 1528288219220 } ], \"id\": \"local-1528288219790\", \"name\": \"Spark shell\" } === [[getApp]] getApp Method [source, scala] \u00b6 getApp(): ApplicationInfo \u00b6 getApp requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]). In the end, getApp returns the ApplicationInfo if available or reports a NotFoundException : unknown app: [appId]","title":"OneApplicationResource"},{"location":"rest/OneApplicationResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/OneApplicationResource/#getapp-applicationinfo","text":"getApp requests the spark-api-ApiRequestContext.md#uiRoot[UIRoot] for the spark-api-UIRoot.md#getApplicationInfo[application info] (given the spark-api-BaseAppResource.md#appId[appId]). In the end, getApp returns the ApplicationInfo if available or reports a NotFoundException : unknown app: [appId]","title":"getApp(): ApplicationInfo"},{"location":"rest/StagesResource/","text":"== [[StagesResource]] StagesResource StagesResource is...FIXME [[paths]] .StagesResource's Paths [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Path | HTTP Method | Description | | GET | < > | {stageId: \\d+} | GET | < > | {stageId: \\d+}/{stageAttemptId: \\d+} | GET | < > | {stageId: \\d+}/{stageAttemptId: \\d+}/taskSummary | GET | < > | {stageId: \\d+}/{stageAttemptId: \\d+}/taskList | GET | < > |=== === [[stageList]] stageList Method [source, scala] \u00b6 stageList(@QueryParam(\"status\") statuses: JList[StageStatus]): Seq[StageData] \u00b6 stageList ...FIXME NOTE: stageList is used when...FIXME === [[stageData]] stageData Method [source, scala] \u00b6 stageData( @PathParam(\"stageId\") stageId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): Seq[StageData] stageData ...FIXME NOTE: stageData is used when...FIXME === [[oneAttemptData]] oneAttemptData Method [source, scala] \u00b6 oneAttemptData( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): StageData oneAttemptData ...FIXME NOTE: oneAttemptData is used when...FIXME === [[taskSummary]] taskSummary Method [source, scala] \u00b6 taskSummary( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0.05,0.25,0.5,0.75,0.95\") @QueryParam(\"quantiles\") quantileString: String) : TaskMetricDistributions taskSummary ...FIXME NOTE: taskSummary is used when...FIXME === [[taskList]] taskList Method [source, scala] \u00b6 taskList( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0\") @QueryParam(\"offset\") offset: Int, @DefaultValue(\"20\") @QueryParam(\"length\") length: Int, @DefaultValue(\"ID\") @QueryParam(\"sortBy\") sortBy: TaskSorting): Seq[TaskData] taskList ...FIXME NOTE: taskList is used when...FIXME","title":"StagesResource"},{"location":"rest/StagesResource/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/StagesResource/#stagelistqueryparamstatus-statuses-jliststagestatus-seqstagedata","text":"stageList ...FIXME NOTE: stageList is used when...FIXME === [[stageData]] stageData Method","title":"stageList(@QueryParam(\"status\") statuses: JList[StageStatus]): Seq[StageData]"},{"location":"rest/StagesResource/#source-scala_1","text":"stageData( @PathParam(\"stageId\") stageId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): Seq[StageData] stageData ...FIXME NOTE: stageData is used when...FIXME === [[oneAttemptData]] oneAttemptData Method","title":"[source, scala]"},{"location":"rest/StagesResource/#source-scala_2","text":"oneAttemptData( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @QueryParam(\"details\") @DefaultValue(\"true\") details: Boolean): StageData oneAttemptData ...FIXME NOTE: oneAttemptData is used when...FIXME === [[taskSummary]] taskSummary Method","title":"[source, scala]"},{"location":"rest/StagesResource/#source-scala_3","text":"taskSummary( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0.05,0.25,0.5,0.75,0.95\") @QueryParam(\"quantiles\") quantileString: String) : TaskMetricDistributions taskSummary ...FIXME NOTE: taskSummary is used when...FIXME === [[taskList]] taskList Method","title":"[source, scala]"},{"location":"rest/StagesResource/#source-scala_4","text":"taskList( @PathParam(\"stageId\") stageId: Int, @PathParam(\"stageAttemptId\") stageAttemptId: Int, @DefaultValue(\"0\") @QueryParam(\"offset\") offset: Int, @DefaultValue(\"20\") @QueryParam(\"length\") length: Int, @DefaultValue(\"ID\") @QueryParam(\"sortBy\") sortBy: TaskSorting): Seq[TaskData] taskList ...FIXME NOTE: taskList is used when...FIXME","title":"[source, scala]"},{"location":"rest/UIRoot/","text":"== [[UIRoot]] UIRoot -- Contract for Root Contrainers of Application UI Information UIRoot is the < > of the < >. [[contract]] [source, scala] package org.apache.spark.status.api.v1 trait UIRoot { // only required methods that have no implementation // the others follow def withSparkUI T (fn: SparkUI => T): T def getApplicationInfoList: Iterator[ApplicationInfo] def getApplicationInfo(appId: String): Option[ApplicationInfo] def securityManager: SecurityManager } NOTE: UIRoot is a private[spark] contract. .UIRoot Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | getApplicationInfo | [[getApplicationInfo]] Used when...FIXME | getApplicationInfoList | [[getApplicationInfoList]] Used when...FIXME | securityManager | [[securityManager]] Used when...FIXME | withSparkUI | [[withSparkUI]] Used exclusively when BaseAppResource is requested spark-api-BaseAppResource.md#withUI[withUI] |=== [[implementations]] .UIRoots [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | UIRoot | Description | spark-history-server:HistoryServer.md[HistoryServer] | [[HistoryServer]] Application UI for active and completed Spark applications (i.e. Spark applications that are still running or have already finished) | spark-webui-SparkUI.md[SparkUI] | [[SparkUI]] Application UI for an active Spark application (i.e. a Spark application that is still running) |=== === [[writeEventLogs]] writeEventLogs Method [source, scala] \u00b6 writeEventLogs(appId: String, attemptId: Option[String], zipStream: ZipOutputStream): Unit \u00b6 writeEventLogs ...FIXME NOTE: writeEventLogs is used when...FIXME","title":"UIRoot"},{"location":"rest/UIRoot/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/UIRoot/#writeeventlogsappid-string-attemptid-optionstring-zipstream-zipoutputstream-unit","text":"writeEventLogs ...FIXME NOTE: writeEventLogs is used when...FIXME","title":"writeEventLogs(appId: String, attemptId: Option[String], zipStream: ZipOutputStream): Unit"},{"location":"rest/UIRootFromServletContext/","text":"== [[UIRootFromServletContext]] UIRootFromServletContext UIRootFromServletContext manages the current < > object in a Jetty ContextHandler . [[attribute]] UIRootFromServletContext uses its canonical name for the context attribute that is used to < > or < > the current spark-api-UIRoot.md[UIRoot] object (in Jetty's ContextHandler ). NOTE: https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/server/handler/ContextHandler.html[ContextHandler ] is the environment for multiple Jetty Handlers , e.g. URI context path, class loader, static resource base. In essence, UIRootFromServletContext is simply a \"bridge\" between two worlds, Spark's spark-api-UIRoot.md[UIRoot] and Jetty's ContextHandler . === [[setUiRoot]] setUiRoot Method [source, scala] \u00b6 setUiRoot(contextHandler: ContextHandler, uiRoot: UIRoot): Unit \u00b6 setUiRoot ...FIXME NOTE: setUiRoot is used exclusively when ApiRootResource is requested to spark-api-ApiRootResource.md#getServletHandler[register /api/* context handler]. === [[getUiRoot]] getUiRoot Method [source, scala] \u00b6 getUiRoot(context: ServletContext): UIRoot \u00b6 getUiRoot ...FIXME NOTE: getUiRoot is used exclusively when ApiRequestContext is requested for the current spark-api-ApiRequestContext.md#uiRoot[UIRoot].","title":"UIRootFromServletContext"},{"location":"rest/UIRootFromServletContext/#source-scala","text":"","title":"[source, scala]"},{"location":"rest/UIRootFromServletContext/#setuirootcontexthandler-contexthandler-uiroot-uiroot-unit","text":"setUiRoot ...FIXME NOTE: setUiRoot is used exclusively when ApiRootResource is requested to spark-api-ApiRootResource.md#getServletHandler[register /api/* context handler]. === [[getUiRoot]] getUiRoot Method","title":"setUiRoot(contextHandler: ContextHandler, uiRoot: UIRoot): Unit"},{"location":"rest/UIRootFromServletContext/#source-scala_1","text":"","title":"[source, scala]"},{"location":"rest/UIRootFromServletContext/#getuirootcontext-servletcontext-uiroot","text":"getUiRoot ...FIXME NOTE: getUiRoot is used exclusively when ApiRequestContext is requested for the current spark-api-ApiRequestContext.md#uiRoot[UIRoot].","title":"getUiRoot(context: ServletContext): UIRoot"},{"location":"rpc/","text":"RPC System \u00b6 RPC System is a communication system of Spark services. The main abstractions are RpcEnv and RpcEndpoint .","title":"RPC System"},{"location":"rpc/#rpc-system","text":"RPC System is a communication system of Spark services. The main abstractions are RpcEnv and RpcEndpoint .","title":"RPC System"},{"location":"rpc/NettyRpcEnv/","text":"= NettyRpcEnv NettyRpcEnv is an rpc:RpcEnv.md[] using https://netty.io/[Netty ] ( \"an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers & clients\" ). == [[creating-instance]] Creating Instance NettyRpcEnv takes the following to be created: [[conf]] ROOT:SparkConf.md[] [[javaSerializerInstance]] JavaSerializerInstance [[host]] Host name [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores NettyRpcEnv is created when NettyRpcEnvFactory is requested to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv]. == [[streamManager]] NettyStreamManager NettyRpcEnv creates a rpc:NettyStreamManager.md[] when < >. NettyStreamManager is used for the following: Create a NettyRpcHandler for the < > As the rpc:RpcEnv.md#fileServer[RpcEnvFileServer] == [[transportContext]] TransportContext NettyRpcEnv creates a network:TransportContext.md[]. == [[startServer]] Starting Server [source,scala] \u00b6 startServer( bindAddress: String, port: Int): Unit startServer...FIXME startServer is used when NettyRpcEnvFactory is requested to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv] (in server mode). == [[deserialize]] deserialize Method [source,scala] \u00b6 deserialize T: ClassTag : T deserialize...FIXME deserialize is used when: RequestMessage utility is created NettyRpcEnv is requested to < >","title":"NettyRpcEnv"},{"location":"rpc/NettyRpcEnv/#sourcescala","text":"startServer( bindAddress: String, port: Int): Unit startServer...FIXME startServer is used when NettyRpcEnvFactory is requested to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv] (in server mode). == [[deserialize]] deserialize Method","title":"[source,scala]"},{"location":"rpc/NettyRpcEnv/#sourcescala_1","text":"deserialize T: ClassTag : T deserialize...FIXME deserialize is used when: RequestMessage utility is created NettyRpcEnv is requested to < >","title":"[source,scala]"},{"location":"rpc/NettyRpcEnvFactory/","text":"= NettyRpcEnvFactory NettyRpcEnvFactory is an rpc:RpcEnvFactory.md[] for a < >. == [[create]] Creating RpcEnv [source,scala] \u00b6 create( config: RpcEnvConfig): RpcEnv create creates a JavaSerializerInstance (using a JavaSerializer). NOTE: KryoSerializer is not supported. create creates a rpc:NettyRpcEnv.md[] with the JavaSerializerInstance. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#advertiseAddress[advertised address], rpc:RpcEnvConfig.md#securityManager[SecurityManager] and rpc:RpcEnvConfig.md#numUsableCores[number of CPU cores]. create returns the NettyRpcEnv unless the rpc:RpcEnvConfig.md#clientMode[clientMode] is turned off ( server mode ). In server mode, create attempts to start the NettyRpcEnv on a given port. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#port[port], rpc:RpcEnvConfig.md#bindAddress[bind address], and rpc:RpcEnvConfig.md#name[name]. With the port, the NettyRpcEnv is requested to rpc:NettyRpcEnv.md#startServer[start a server]. create is part of the rpc:RpcEnvFactory.md#create[RpcEnvFactory] abstraction.","title":"NettyRpcEnvFactory"},{"location":"rpc/NettyRpcEnvFactory/#sourcescala","text":"create( config: RpcEnvConfig): RpcEnv create creates a JavaSerializerInstance (using a JavaSerializer). NOTE: KryoSerializer is not supported. create creates a rpc:NettyRpcEnv.md[] with the JavaSerializerInstance. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#advertiseAddress[advertised address], rpc:RpcEnvConfig.md#securityManager[SecurityManager] and rpc:RpcEnvConfig.md#numUsableCores[number of CPU cores]. create returns the NettyRpcEnv unless the rpc:RpcEnvConfig.md#clientMode[clientMode] is turned off ( server mode ). In server mode, create attempts to start the NettyRpcEnv on a given port. create uses the given rpc:RpcEnvConfig.md[] for the rpc:RpcEnvConfig.md#port[port], rpc:RpcEnvConfig.md#bindAddress[bind address], and rpc:RpcEnvConfig.md#name[name]. With the port, the NettyRpcEnv is requested to rpc:NettyRpcEnv.md#startServer[start a server]. create is part of the rpc:RpcEnvFactory.md#create[RpcEnvFactory] abstraction.","title":"[source,scala]"},{"location":"rpc/NettyStreamManager/","text":"= NettyStreamManager NettyStreamManager is a network:StreamManager.md[]. == [[creating-instance]] Creating Instance NettyStreamManager takes the following to be created: [[rpcEnv]] rpc:NettyRpcEnv.md[] NettyStreamManager is created for rpc:NettyRpcEnv.md#streamManager[NettyRpcEnv]. == [[registerStream]] registerStream Method [source,java] \u00b6 long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"NettyStreamManager"},{"location":"rpc/NettyStreamManager/#sourcejava","text":"long registerStream( String appId, Iterator buffers) registerStream...FIXME registerStream is used when...FIXME","title":"[source,java]"},{"location":"rpc/RpcAddress/","text":"= RpcAddress RpcAddress is a logical address of an RPC system, with hostname and port. RpcAddress can be encoded as a Spark URL in the format of spark://host:port .","title":"RpcAddress"},{"location":"rpc/RpcEndpoint/","text":"= RpcEndpoint RpcEndpoint is a < > to define an RPC endpoint that can < > messages using callbacks , i.e. functions to execute when a message arrives. RpcEndpoint defines how to handle messages (what functions to execute given a message). RpcEndpoints register (with a name or uri) to RpcEnv to receive messages from rpc:RpcEndpointRef.md[RpcEndpointRefs]. [[contract]] [source, scala] package org.apache.spark.rpc trait RpcEndpoint { def onConnected(remoteAddress: RpcAddress): Unit def onDisconnected(remoteAddress: RpcAddress): Unit def onError(cause: Throwable): Unit def onNetworkError(cause: Throwable, remoteAddress: RpcAddress): Unit def onStart(): Unit def onStop(): Unit def receive: PartialFunction[Any, Unit] def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] val rpcEnv: RpcEnv } RpcEndpoint lives in rpc:index.md[RpcEnv] after being registered by a name. A RpcEndpoint can be registered to one and only one RpcEnv. The lifecycle of a RpcEndpoint is onStart , receive and onStop in sequence. receive can be called concurrently. TIP: If you want receive to be thread-safe, use < >. onError method is called for any exception thrown. .RpcEndpoint Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[receive]] receive | Receives and processes a message |=== NOTE: RpcEndpoint is a private[spark] contract. == [[onStart]] Activating RPC Endpoint (Just Before Handling Messages) CAUTION: FIXME == [[stop]] Stopping RpcEndpoint CAUTION: FIXME == [[ThreadSafeRpcEndpoint]] ThreadSafeRpcEndpoint ThreadSafeRpcEndpoint is an RpcEndpoint for endpoints that...FIXME","title":"RpcEndpoint"},{"location":"rpc/RpcEndpointAddress/","text":"= RpcEndpointAddress RpcEndpointAddress is a logical address of an endpoint in an RPC system, with < > and name . RpcEndpointAddress is in the format of spark://[name]@[rpcAddress.host]:[rpcAddress.port] .","title":"RpcEndpointAddress"},{"location":"rpc/RpcEndpointRef/","text":"= RpcEndpointRef -- Reference to RPC Endpoint :navtitle: RpcEndpointRef RpcEndpointRef is a reference to a rpc:RpcEndpoint.md[RpcEndpoint] in a rpc:index.md[RpcEnv]. RpcEndpointRef is a serializable entity and so you can send it over a network or save it for later use (it can however be deserialized using the owning RpcEnv only). A RpcEndpointRef has < > (a Spark URL), and a name. You can send asynchronous one-way messages to the corresponding RpcEndpoint using < > method. You can send a semi-synchronous message, i.e. \"subscribe\" to be notified when a response arrives, using ask method. You can also block the current calling thread for a response using askWithRetry method. spark.rpc.numRetries (default: 3 ) - the number of times to retry connection attempts. spark.rpc.retry.wait (default: 3s ) - the number of milliseconds to wait on each retry. It also uses rpc:index.md#endpoint-lookup-timeout[lookup timeouts]. == [[send]] send Method CAUTION: FIXME == [[askWithRetry]] askWithRetry Method CAUTION: FIXME","title":"RpcEndpointRef"},{"location":"rpc/RpcEnv/","text":"RpcEnv \u00b6 RpcEnv is an < > of < >. == [[implementations]] Available RpcEnvs rpc:NettyRpcEnv.md[] is the default and only known RpcEnv in Apache Spark. == [[contract]] Contract === [[address]] address Method [source,scala] \u00b6 address: RpcAddress \u00b6 rpc:RpcAddress.md[] of the RPC system === [[asyncSetupEndpointRefByURI]] asyncSetupEndpointRefByURI Method [source,scala] \u00b6 asyncSetupEndpointRefByURI( uri: String): Future[RpcEndpointRef] Sets up an RPC endpoing by URI (asynchronously) and returns rpc:RpcEndpointRef.md[] Used when: WorkerWatcher is created CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#onStart[onStart] RpcEnv is requested to < > === [[awaitTermination]] awaitTermination Method [source,scala] \u00b6 awaitTermination(): Unit \u00b6 Waits till the RPC system terminates Used when: SparkEnv is requested to core:SparkEnv.md#stop[stop] ClientApp is requested to start LocalSparkCluster is requested to stop (Spark Standalone) Master and Worker are launched CoarseGrainedExecutorBackend standalone application is launched === [[deserialize]] deserialize Method [source,scala] \u00b6 deserialize T : T Used when: PersistenceEngine is requested to readPersistedData NettyRpcEnv is requested to rpc:NettyRpcEnv.md#deserialize[deserialize] === [[endpointRef]] endpointRef Method [source,scala] \u00b6 endpointRef( endpoint: RpcEndpoint): RpcEndpointRef Used when RpcEndpoint is requested for the rpc:RpcEndpoint.md#self[RpcEndpointRef to itself] === [[fileServer]] RpcEnvFileServer [source,scala] \u00b6 fileServer: RpcEnvFileServer \u00b6 rpc:RpcEnvFileServer.md[] of the RPC system Used when ROOT:SparkContext.md[] is created (and registers the REPL's output directory) and requested to ROOT:SparkContext.md#addFile[addFile] or ROOT:SparkContext.md#addJar[addJar] === [[openChannel]] openChannel Method [source,scala] \u00b6 openChannel( uri: String): ReadableByteChannel Opens a channel to download a file from the given URI Used when: Utils utility is used to doFetchFile ExecutorClassLoader is requested to getClassFileInputStreamFromSparkRPC setupEndpoint \u00b6 setupEndpoint ( name : String , endpoint : RpcEndpoint ) : RpcEndpointRef Used when: SparkContext is created SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv] (and register the BlockManagerMaster, MapOutputTracker and OutputCommitCoordinator RPC endpoints on the driver) ClientApp is requested to start (and register the client RPC endpoint) StandaloneAppClient is requested to start (and register the AppClient RPC endpoint) (Spark Standalone) Master is requested to startRpcEnvAndEndpoint (and register the Master RPC endpoint) (Spark Standalone) Worker is requested to startRpcEnvAndEndpoint (and register the Worker RPC endpoint) DriverWrapper standalone application is launched (and registers the workerWatcher RPC endpoint) CoarseGrainedExecutorBackend standalone application is launched (and registers the Executor and WorkerWatcher RPC endpoints) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#maybeInitBarrierCoordinator[maybeInitBarrierCoordinator] CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#createDriverEndpointRef[createDriverEndpointRef] (and registers the CoarseGrainedScheduler RPC endpoint) LocalSchedulerBackend is requested to spark-local:spark-LocalSchedulerBackend.md#start[start] (and registers the LocalSchedulerBackendEndpoint RPC endpoint) storage:BlockManager.md#slaveEndpoint[BlockManager] is created (and registers the BlockManagerEndpoint RPC endpoint) (Spark on YARN) ApplicationMaster is requested to spark-on-yarn:spark-yarn-applicationmaster.md#createAllocator[createAllocator] (and registers the YarnAM RPC endpoint) (Spark on YARN) spark-on-yarn:spark-yarn-yarnschedulerbackend.md#yarnSchedulerEndpointRef[YarnSchedulerBackend] is created (and registers the YarnScheduler RPC endpoint) === [[setupEndpointRef]] setupEndpointRef Method [source,scala] \u00b6 setupEndpointRef( address: RpcAddress, endpointName: String): RpcEndpointRef setupEndpointRef creates an RpcEndpointAddress (for the given rpc:RpcAddress.md[] and endpoint name) and < >. setupEndpointRef is used when: ClientApp is requested to start ClientEndpoint is requested to tryRegisterAllMasters Worker is requested to tryRegisterAllMasters and reregisterWithMaster RpcUtils utility is used to rpc:RpcUtils.md#makeDriverRef[makeDriverRef] (Spark on YARN) ApplicationMaster is requested to spark-on-yarn:spark-yarn-applicationmaster.md#runDriver[runDriver] and spark-on-yarn:spark-yarn-applicationmaster.md#runExecutorLauncher[runExecutorLauncher] === [[setupEndpointRefByURI]] setupEndpointRefByURI Method [source,scala] \u00b6 setupEndpointRefByURI( uri: String): RpcEndpointRef setupEndpointRefByURI < > by the given URI and waits for the result or < >. setupEndpointRefByURI is used when: CoarseGrainedExecutorBackend standalone application is executor:CoarseGrainedExecutorBackend.md#run[launched] RpcEnv is requested to < > === [[shutdown]] shutdown Method [source,scala] \u00b6 shutdown(): Unit \u00b6 Shuts down the RPC system Used when: SparkEnv is requested to core:SparkEnv.md#stop[stop] LocalSparkCluster is requested to spark-standalone:spark-standalone-LocalSparkCluster.md#stop[stop] DriverWrapper is launched CoarseGrainedExecutorBackend is executor:CoarseGrainedExecutorBackend.md#run[launched] NettyRpcEnvFactory is requested to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv] (in server mode and failed to assign a port) === [[stop]] Stopping RpcEndpointRef [source,scala] \u00b6 stop( endpoint: RpcEndpointRef): Unit Used when: SparkContext is requested to ROOT:SparkContext.md#stop[stop] RpcEndpoint is requested to rpc:RpcEndpoint.md#stop[stop] BlockManager is requested to storage:BlockManager.md#stop[stop] == [[defaultLookupTimeout]] Default Endpoint Lookup Timeout RpcEnv uses the default lookup timeout for...FIXME When a remote endpoint is resolved, a local RPC environment connects to the remote one. It is called endpoint lookup . To configure the time needed for the endpoint lookup you can use the following settings. It is a prioritized list of lookup timeout properties (the higher on the list, the more important): ROOT:configuration-properties.md#spark.rpc.lookupTimeout[spark.rpc.lookupTimeout] < > Their value can be a number alone (seconds) or any number with time suffix, e.g. 50s , 100ms , or 250us . See < >. == [[creating-instance]] Creating Instance RpcEnv takes the following to be created: [[conf]] ROOT:SparkConf.md[] RpcEnv is created using < > utility. RpcEnv is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[create]] Creating RpcEnv [source,scala] \u00b6 create( name: String, host: String, port: Int, conf: SparkConf, securityManager: SecurityManager, clientMode: Boolean = false): RpcEnv // <1> create( name: String, bindAddress: String, advertiseAddress: String, port: Int, conf: SparkConf, securityManager: SecurityManager, numUsableCores: Int, clientMode: Boolean): RpcEnv <1> Uses 0 for numUsableCores create creates a rpc:NettyRpcEnvFactory.md[] and requests to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv] (with an rpc:RpcEnvConfig.md[] with all the given arguments). create is used when: SparkEnv utility is requested to core:SparkEnv.md#create[create a SparkEnv] (clientMode flag is turned on for executors and off for the driver) With clientMode flag turned on: ** (Spark on YARN) ApplicationMaster is requested to spark-on-yarn:spark-yarn-applicationmaster.md#runExecutorLauncher[runExecutorLauncher] (in client deploy mode with clientMode flag is turned on) ** ClientApp is requested to start ** (Spark Standalone) Master is requested to startRpcEnvAndEndpoint ** DriverWrapper standalone application is launched ** (Spark Standalone) Worker is requested to startRpcEnvAndEndpoint ** CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#run[run]","title":"RpcEnv"},{"location":"rpc/RpcEnv/#rpcenv","text":"RpcEnv is an < > of < >. == [[implementations]] Available RpcEnvs rpc:NettyRpcEnv.md[] is the default and only known RpcEnv in Apache Spark. == [[contract]] Contract === [[address]] address Method","title":"RpcEnv"},{"location":"rpc/RpcEnv/#sourcescala","text":"","title":"[source,scala]"},{"location":"rpc/RpcEnv/#address-rpcaddress","text":"rpc:RpcAddress.md[] of the RPC system === [[asyncSetupEndpointRefByURI]] asyncSetupEndpointRefByURI Method","title":"address: RpcAddress"},{"location":"rpc/RpcEnv/#sourcescala_1","text":"asyncSetupEndpointRefByURI( uri: String): Future[RpcEndpointRef] Sets up an RPC endpoing by URI (asynchronously) and returns rpc:RpcEndpointRef.md[] Used when: WorkerWatcher is created CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#onStart[onStart] RpcEnv is requested to < > === [[awaitTermination]] awaitTermination Method","title":"[source,scala]"},{"location":"rpc/RpcEnv/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"rpc/RpcEnv/#awaittermination-unit","text":"Waits till the RPC system terminates Used when: SparkEnv is requested to core:SparkEnv.md#stop[stop] ClientApp is requested to start LocalSparkCluster is requested to stop (Spark Standalone) Master and Worker are launched CoarseGrainedExecutorBackend standalone application is launched === [[deserialize]] deserialize Method","title":"awaitTermination(): Unit"},{"location":"rpc/RpcEnv/#sourcescala_3","text":"deserialize T : T Used when: PersistenceEngine is requested to readPersistedData NettyRpcEnv is requested to rpc:NettyRpcEnv.md#deserialize[deserialize] === [[endpointRef]] endpointRef Method","title":"[source,scala]"},{"location":"rpc/RpcEnv/#sourcescala_4","text":"endpointRef( endpoint: RpcEndpoint): RpcEndpointRef Used when RpcEndpoint is requested for the rpc:RpcEndpoint.md#self[RpcEndpointRef to itself] === [[fileServer]] RpcEnvFileServer","title":"[source,scala]"},{"location":"rpc/RpcEnv/#sourcescala_5","text":"","title":"[source,scala]"},{"location":"rpc/RpcEnv/#fileserver-rpcenvfileserver","text":"rpc:RpcEnvFileServer.md[] of the RPC system Used when ROOT:SparkContext.md[] is created (and registers the REPL's output directory) and requested to ROOT:SparkContext.md#addFile[addFile] or ROOT:SparkContext.md#addJar[addJar] === [[openChannel]] openChannel Method","title":"fileServer: RpcEnvFileServer"},{"location":"rpc/RpcEnv/#sourcescala_6","text":"openChannel( uri: String): ReadableByteChannel Opens a channel to download a file from the given URI Used when: Utils utility is used to doFetchFile ExecutorClassLoader is requested to getClassFileInputStreamFromSparkRPC","title":"[source,scala]"},{"location":"rpc/RpcEnv/#setupendpoint","text":"setupEndpoint ( name : String , endpoint : RpcEndpoint ) : RpcEndpointRef Used when: SparkContext is created SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv] (and register the BlockManagerMaster, MapOutputTracker and OutputCommitCoordinator RPC endpoints on the driver) ClientApp is requested to start (and register the client RPC endpoint) StandaloneAppClient is requested to start (and register the AppClient RPC endpoint) (Spark Standalone) Master is requested to startRpcEnvAndEndpoint (and register the Master RPC endpoint) (Spark Standalone) Worker is requested to startRpcEnvAndEndpoint (and register the Worker RPC endpoint) DriverWrapper standalone application is launched (and registers the workerWatcher RPC endpoint) CoarseGrainedExecutorBackend standalone application is launched (and registers the Executor and WorkerWatcher RPC endpoints) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#maybeInitBarrierCoordinator[maybeInitBarrierCoordinator] CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#createDriverEndpointRef[createDriverEndpointRef] (and registers the CoarseGrainedScheduler RPC endpoint) LocalSchedulerBackend is requested to spark-local:spark-LocalSchedulerBackend.md#start[start] (and registers the LocalSchedulerBackendEndpoint RPC endpoint) storage:BlockManager.md#slaveEndpoint[BlockManager] is created (and registers the BlockManagerEndpoint RPC endpoint) (Spark on YARN) ApplicationMaster is requested to spark-on-yarn:spark-yarn-applicationmaster.md#createAllocator[createAllocator] (and registers the YarnAM RPC endpoint) (Spark on YARN) spark-on-yarn:spark-yarn-yarnschedulerbackend.md#yarnSchedulerEndpointRef[YarnSchedulerBackend] is created (and registers the YarnScheduler RPC endpoint) === [[setupEndpointRef]] setupEndpointRef Method","title":" setupEndpoint"},{"location":"rpc/RpcEnv/#sourcescala_7","text":"setupEndpointRef( address: RpcAddress, endpointName: String): RpcEndpointRef setupEndpointRef creates an RpcEndpointAddress (for the given rpc:RpcAddress.md[] and endpoint name) and < >. setupEndpointRef is used when: ClientApp is requested to start ClientEndpoint is requested to tryRegisterAllMasters Worker is requested to tryRegisterAllMasters and reregisterWithMaster RpcUtils utility is used to rpc:RpcUtils.md#makeDriverRef[makeDriverRef] (Spark on YARN) ApplicationMaster is requested to spark-on-yarn:spark-yarn-applicationmaster.md#runDriver[runDriver] and spark-on-yarn:spark-yarn-applicationmaster.md#runExecutorLauncher[runExecutorLauncher] === [[setupEndpointRefByURI]] setupEndpointRefByURI Method","title":"[source,scala]"},{"location":"rpc/RpcEnv/#sourcescala_8","text":"setupEndpointRefByURI( uri: String): RpcEndpointRef setupEndpointRefByURI < > by the given URI and waits for the result or < >. setupEndpointRefByURI is used when: CoarseGrainedExecutorBackend standalone application is executor:CoarseGrainedExecutorBackend.md#run[launched] RpcEnv is requested to < > === [[shutdown]] shutdown Method","title":"[source,scala]"},{"location":"rpc/RpcEnv/#sourcescala_9","text":"","title":"[source,scala]"},{"location":"rpc/RpcEnv/#shutdown-unit","text":"Shuts down the RPC system Used when: SparkEnv is requested to core:SparkEnv.md#stop[stop] LocalSparkCluster is requested to spark-standalone:spark-standalone-LocalSparkCluster.md#stop[stop] DriverWrapper is launched CoarseGrainedExecutorBackend is executor:CoarseGrainedExecutorBackend.md#run[launched] NettyRpcEnvFactory is requested to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv] (in server mode and failed to assign a port) === [[stop]] Stopping RpcEndpointRef","title":"shutdown(): Unit"},{"location":"rpc/RpcEnv/#sourcescala_10","text":"stop( endpoint: RpcEndpointRef): Unit Used when: SparkContext is requested to ROOT:SparkContext.md#stop[stop] RpcEndpoint is requested to rpc:RpcEndpoint.md#stop[stop] BlockManager is requested to storage:BlockManager.md#stop[stop] == [[defaultLookupTimeout]] Default Endpoint Lookup Timeout RpcEnv uses the default lookup timeout for...FIXME When a remote endpoint is resolved, a local RPC environment connects to the remote one. It is called endpoint lookup . To configure the time needed for the endpoint lookup you can use the following settings. It is a prioritized list of lookup timeout properties (the higher on the list, the more important): ROOT:configuration-properties.md#spark.rpc.lookupTimeout[spark.rpc.lookupTimeout] < > Their value can be a number alone (seconds) or any number with time suffix, e.g. 50s , 100ms , or 250us . See < >. == [[creating-instance]] Creating Instance RpcEnv takes the following to be created: [[conf]] ROOT:SparkConf.md[] RpcEnv is created using < > utility. RpcEnv is an abstract class and cannot be created directly. It is created indirectly for the < >. == [[create]] Creating RpcEnv","title":"[source,scala]"},{"location":"rpc/RpcEnv/#sourcescala_11","text":"create( name: String, host: String, port: Int, conf: SparkConf, securityManager: SecurityManager, clientMode: Boolean = false): RpcEnv // <1> create( name: String, bindAddress: String, advertiseAddress: String, port: Int, conf: SparkConf, securityManager: SecurityManager, numUsableCores: Int, clientMode: Boolean): RpcEnv <1> Uses 0 for numUsableCores create creates a rpc:NettyRpcEnvFactory.md[] and requests to rpc:NettyRpcEnvFactory.md#create[create an RpcEnv] (with an rpc:RpcEnvConfig.md[] with all the given arguments). create is used when: SparkEnv utility is requested to core:SparkEnv.md#create[create a SparkEnv] (clientMode flag is turned on for executors and off for the driver) With clientMode flag turned on: ** (Spark on YARN) ApplicationMaster is requested to spark-on-yarn:spark-yarn-applicationmaster.md#runExecutorLauncher[runExecutorLauncher] (in client deploy mode with clientMode flag is turned on) ** ClientApp is requested to start ** (Spark Standalone) Master is requested to startRpcEnvAndEndpoint ** DriverWrapper standalone application is launched ** (Spark Standalone) Worker is requested to startRpcEnvAndEndpoint ** CoarseGrainedExecutorBackend is requested to executor:CoarseGrainedExecutorBackend.md#run[run]","title":"[source,scala]"},{"location":"rpc/RpcEnvConfig/","text":"= RpcEnvConfig :page-toclevels: -1 [[creating-instance]] RpcEnvConfig is a configuration of an rpc:RpcEnv.md[]: [[conf]] ROOT:SparkConf.md[] [[name]] System Name [[bindAddress]] Bind Address [[advertiseAddress]] Advertised Address [[port]] Port [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores < > RpcEnvConfig is created when RpcEnv utility is used to rpc:RpcEnv.md#create[create an RpcEnv] (using rpc:RpcEnvFactory.md[]). == [[clientMode]] Client Mode When an RPC Environment is initialized core:SparkEnv.md#createDriverEnv[as part of the initialization of the driver] or core:SparkEnv.md#createExecutorEnv[executors] (using RpcEnv.create ), clientMode is false for the driver and true for executors. Copied (almost verbatim) from https://issues.apache.org/jira/browse/SPARK-10997[SPARK-10997 Netty-based RPC env should support a \"client-only\" mode] and the https://github.com/apache/spark/commit/71d1c907dec446db566b19f912159fd8f46deb7d[commit ]: \"Client mode\" means the RPC env will not listen for incoming connections. This allows certain processes in the Spark stack (such as Executors or tha YARN client-mode AM) to act as pure clients when using the netty-based RPC backend, reducing the number of sockets Spark apps need to use and also the number of open ports. The AM connects to the driver in \"client mode\", and that connection is used for all driver -- AM communication, and so the AM is properly notified when the connection goes down. In \"general\", non-YARN case, clientMode flag is therefore enabled for executors and disabled for the driver. In Spark on YARN in spark-deploy-mode.md#client[ client deploy mode], clientMode flag is however enabled explicitly when Spark on YARN's spark-yarn-applicationmaster.md#runExecutorLauncher-sparkYarnAM[ApplicationMaster] creates the sparkYarnAM RPC Environment.","title":"RpcEnvConfig"},{"location":"rpc/RpcEnvFactory/","text":"= RpcEnvFactory RpcEnvFactory is an abstraction of < > to < >. == [[implementations]] Available RpcEnvFactories rpc:NettyRpcEnvFactory.md[] is the default and only known RpcEnvFactory in Apache Spark (as of https://github.com/apache/spark/commit/4f5a24d7e73104771f233af041eeba4f41675974[this commit]). == [[create]] Creating RpcEnv [source,scala] \u00b6 create( config: RpcEnvConfig): RpcEnv create is used when RpcEnv utility is requested to rpc:RpcEnv.md#create[create an RpcEnv].","title":"RpcEnvFactory"},{"location":"rpc/RpcEnvFactory/#sourcescala","text":"create( config: RpcEnvConfig): RpcEnv create is used when RpcEnv utility is requested to rpc:RpcEnv.md#create[create an RpcEnv].","title":"[source,scala]"},{"location":"rpc/RpcEnvFileServer/","text":"= RpcEnvFileServer RpcEnvFileServer is...FIXME","title":"RpcEnvFileServer"},{"location":"rpc/RpcUtils/","text":"= RpcUtils RpcUtils is an utility for...FIXME == [[makeDriverRef]] makeDriverRef Method [source,scala] \u00b6 makeDriverRef( name: String, conf: SparkConf, rpcEnv: RpcEnv): RpcEndpointRef makeDriverRef...FIXME makeDriverRef is used when: scheduler:spark-BarrierTaskContext.md#barrierCoordinator[BarrierTaskContext] is created SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv] (on executors) executor:Executor.md#heartbeatReceiverRef[Executor] is created","title":"RpcUtils"},{"location":"rpc/RpcUtils/#sourcescala","text":"makeDriverRef( name: String, conf: SparkConf, rpcEnv: RpcEnv): RpcEndpointRef makeDriverRef...FIXME makeDriverRef is used when: scheduler:spark-BarrierTaskContext.md#barrierCoordinator[BarrierTaskContext] is created SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv] (on executors) executor:Executor.md#heartbeatReceiverRef[Executor] is created","title":"[source,scala]"},{"location":"rpc/spark-rpc-netty/","text":"= Netty-based RpcEnv Netty-based RPC Environment is created by NettyRpcEnvFactory when rpc:index.md#settings[spark.rpc] is netty or org.apache.spark.rpc.netty.NettyRpcEnvFactory . NettyRpcEnv is only started on spark-driver.md[the driver]. See < >. The default port to listen to is 7077 . When NettyRpcEnv starts, the following INFO message is printed out in the logs: Successfully started service 'NettyRpcEnv' on port 0. == [[thread-pools]] Thread Pools === shuffle-server-ID EventLoopGroup uses a daemon thread pool called shuffle-server-ID , where ID is a unique integer for NioEventLoopGroup ( NIO ) or EpollEventLoopGroup ( EPOLL ) for the Shuffle server. CAUTION: FIXME Review Netty's NioEventLoopGroup . CAUTION: FIXME Where are SO_BACKLOG , SO_RCVBUF , SO_SNDBUF channel options used? === dispatcher-event-loop-ID NettyRpcEnv's Dispatcher uses the daemon fixed thread pool with < > threads. Thread names are formatted as dispatcher-event-loop-ID , where ID is a unique, sequentially assigned integer. It starts the message processing loop on all of the threads. === netty-rpc-env-timeout NettyRpcEnv uses the daemon single-thread scheduled thread pool netty-rpc-env-timeout . \"netty-rpc-env-timeout\" #87 daemon prio=5 os_prio=31 tid=0x00007f887775a000 nid=0xc503 waiting on condition [0x0000000123397000] === netty-rpc-connection-ID NettyRpcEnv uses the daemon cached thread pool with up to < > threads. Thread names are formatted as netty-rpc-connection-ID , where ID is a unique, sequentially assigned integer. == [[settings]] Settings The Netty-based implementation uses the following properties: spark.rpc.io.mode (default: NIO ) - NIO or EPOLL for low-level IO. NIO is always available, while EPOLL is only available on Linux. NIO uses io.netty.channel.nio.NioEventLoopGroup while EPOLL io.netty.channel.epoll.EpollEventLoopGroup . spark.shuffle.io.numConnectionsPerPeer always equals 1 spark.rpc.io.threads (default: 0 ; maximum: 8 ) - the number of threads to use for the Netty client and server thread pools. ** spark.shuffle.io.serverThreads (default: the value of spark.rpc.io.threads ) ** spark.shuffle.io.clientThreads (default: the value of spark.rpc.io.threads ) spark.rpc.netty.dispatcher.numThreads (default: the number of processors available to JVM) spark.rpc.connect.threads (default: 64 ) - used in cluster mode to communicate with a remote RPC endpoint spark.port.maxRetries (default: 16 or 100 for testing when spark.testing is set) controls the maximum number of binding attempts/retries to a port before giving up. == [[endpoints]] Endpoints endpoint-verifier ( RpcEndpointVerifier ) - a rpc:RpcEndpoint.md[RpcEndpoint] for remote RpcEnvs to query whether an RpcEndpoint exists or not. It uses Dispatcher that keeps track of registered endpoints and responds true / false to CheckExistence message. endpoint-verifier is used to check out whether a given endpoint exists or not before the endpoint's reference is given back to clients. One use case is when an spark-standalone.md#AppClient[AppClient connects to standalone Masters] before it registers the application it acts for. CAUTION: FIXME Who'd like to use endpoint-verifier and how? == Message Dispatcher A message dispatcher is responsible for routing RPC messages to the appropriate endpoint(s). It uses the daemon fixed thread pool dispatcher-event-loop with spark.rpc.netty.dispatcher.numThreads threads for dispatching messages. \"dispatcher-event-loop-0\" #26 daemon prio=5 os_prio=31 tid=0x00007f8877153800 nid=0x7103 waiting on condition [0x000000011f78b000]","title":"spark-rpc-netty"},{"location":"scheduler/","text":"Spark Scheduler \u00b6 Spark Scheduler is a core component of Apache Spark that is responsible for scheduling tasks for execution. Spark Scheduler uses the high-level stage-oriented DAGScheduler and the low-level task-oriented TaskScheduler . Resources \u00b6 Deep Dive into the Apache Spark Scheduler by Xingbo Jiang (Databricks)","title":"Spark Scheduler"},{"location":"scheduler/#spark-scheduler","text":"Spark Scheduler is a core component of Apache Spark that is responsible for scheduling tasks for execution. Spark Scheduler uses the high-level stage-oriented DAGScheduler and the low-level task-oriented TaskScheduler .","title":"Spark Scheduler"},{"location":"scheduler/#resources","text":"Deep Dive into the Apache Spark Scheduler by Xingbo Jiang (Databricks)","title":"Resources"},{"location":"scheduler/ActiveJob/","text":"ActiveJob \u00b6 A job (aka action job or active job ) is a top-level work item (computation) submitted to scheduler:DAGScheduler.md[DAGScheduler] to rdd:spark-rdd-actions.md[compute the result of an action] (or for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]). .RDD actions submit jobs to DAGScheduler image::action-job.png[align=\"center\"] Computing a job is equivalent to computing the partitions of the RDD the action has been executed upon. The number of partitions in a job depends on the type of a stage - scheduler:ResultStage.md[ResultStage] or scheduler:ShuffleMapStage.md[ShuffleMapStage]. A job starts with a single target RDD, but can ultimately include other RDDs that are all part of rdd:spark-rdd-lineage.md[RDD lineage]. The parent stages are the instances of scheduler:ShuffleMapStage.md[ShuffleMapStage]. .Computing a job is computing the partitions of an RDD image::rdd-job-partitions.png[align=\"center\"] NOTE: Note that not all partitions have always to be computed for scheduler:ResultStage.md[ResultStages] for actions like first() and lookup() . Internally, a job is represented by an instance of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[private[spark ] class org.apache.spark.scheduler.ActiveJob]. [CAUTION] \u00b6 FIXME * Where are instances of ActiveJob used? \u00b6 A job can be one of two logical types (that are only distinguished by an internal finalStage field of ActiveJob ): Map-stage job that computes the map output files for a scheduler:ShuffleMapStage.md[ShuffleMapStage] (for submitMapStage ) before any downstream stages are submitted. + It is also used for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling], to look at map output statistics before submitting later stages. Result job that computes a scheduler:ResultStage.md[ResultStage] to execute an action. Jobs track how many partitions have already been computed (using finished array of Boolean elements).","title":"ActiveJob"},{"location":"scheduler/ActiveJob/#activejob","text":"A job (aka action job or active job ) is a top-level work item (computation) submitted to scheduler:DAGScheduler.md[DAGScheduler] to rdd:spark-rdd-actions.md[compute the result of an action] (or for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]). .RDD actions submit jobs to DAGScheduler image::action-job.png[align=\"center\"] Computing a job is equivalent to computing the partitions of the RDD the action has been executed upon. The number of partitions in a job depends on the type of a stage - scheduler:ResultStage.md[ResultStage] or scheduler:ShuffleMapStage.md[ShuffleMapStage]. A job starts with a single target RDD, but can ultimately include other RDDs that are all part of rdd:spark-rdd-lineage.md[RDD lineage]. The parent stages are the instances of scheduler:ShuffleMapStage.md[ShuffleMapStage]. .Computing a job is computing the partitions of an RDD image::rdd-job-partitions.png[align=\"center\"] NOTE: Note that not all partitions have always to be computed for scheduler:ResultStage.md[ResultStages] for actions like first() and lookup() . Internally, a job is represented by an instance of https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[private[spark ] class org.apache.spark.scheduler.ActiveJob].","title":"ActiveJob"},{"location":"scheduler/ActiveJob/#caution","text":"FIXME","title":"[CAUTION]"},{"location":"scheduler/ActiveJob/#where-are-instances-of-activejob-used","text":"A job can be one of two logical types (that are only distinguished by an internal finalStage field of ActiveJob ): Map-stage job that computes the map output files for a scheduler:ShuffleMapStage.md[ShuffleMapStage] (for submitMapStage ) before any downstream stages are submitted. + It is also used for scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling], to look at map output statistics before submitting later stages. Result job that computes a scheduler:ResultStage.md[ResultStage] to execute an action. Jobs track how many partitions have already been computed (using finished array of Boolean elements).","title":"* Where are instances of ActiveJob used?"},{"location":"scheduler/BarrierTaskContext/","text":"== [[BarrierTaskContext]] BarrierTaskContext -- TaskContext for Barrier Tasks BarrierTaskContext is a concrete < > that is < > exclusively when Task is requested to scheduler:Task.md#run[run] and the task is scheduler:Task.md#isBarrier[isBarrier] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool) sometime in the future]). [[creating-instance]] [[taskContext]] BarrierTaskContext takes a single < > to be created. === [[barrierCoordinator]] RpcEndpointRef BarrierTaskContext creates an RpcEndpointRef for...FIXME","title":"BarrierTaskContext"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/","text":"DriverEndpoint \u2014 CoarseGrainedSchedulerBackend RPC Endpoint \u00b6 DriverEndpoint is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that acts as a < > for scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] to communicate with executor:CoarseGrainedExecutorBackend.md[]. DriverEndpoint < > when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#starts[starts]. DriverEndpoint uses scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] internal registry of all the executor:CoarseGrainedExecutorBackend.md#onStart[executors that registered with the driver]. An executor sends a < > message to inform that it wants to register. DriverEndpoint uses a < > called driver-revive-thread to < > (by emitting < > message every scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval]). [[messages]] .CoarseGrainedClusterMessages and Their Handlers (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | CoarseGrainedClusterMessage | Event Handler | When emitted? | [[KillExecutorsOnHost]] KillExecutorsOnHost | < > | CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#killExecutorsOnHost[kill all executors on a node]. | [[KillTask]] KillTask | < > | CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#killTask[kill a task]. | [[ReviveOffers]] ReviveOffers | < > a| Periodically (every scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval]) soon after DriverEndpoint < >. CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#reviveOffers[revive resource offers]. | [[RegisterExecutor]] RegisterExecutor | < > | CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#onStart[registers with the driver]. | [[StatusUpdate]] StatusUpdate | < > | CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#statusUpdate[sends task status updates to the driver]. |=== [[internal-properties]] .DriverEndpoint's Internal Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description [[addressToExecutorId]] addressToExecutorId Executor addresses (host and port) for executors. Set when an executor connects to register itself. See < > RPC message. [[executorsPendingLossReason]] executorsPendingLossReason [[reviveThread]] reviveThread === == [[disableExecutor]] disableExecutor Internal Method CAUTION: FIXME == [[KillExecutorsOnHost-handler]] KillExecutorsOnHost Handler CAUTION: FIXME == [[executorIsAlive]] executorIsAlive Internal Method CAUTION: FIXME == [[onStop]] onStop Callback CAUTION: FIXME == [[onDisconnected]] onDisconnected Callback When called, onDisconnected removes the worker from the internal < > (that effectively removes the worker from a cluster). While removing, it calls < > with the reason being SlaveLost and message: [options=\"wrap\"] \u00b6 Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages. \u00b6 NOTE: onDisconnected is called when a remote host is lost. == [[RemoveExecutor]] RemoveExecutor == [[RetrieveSparkProps]] RetrieveSparkProps == [[StopDriver]] StopDriver StopDriver message stops the RPC endpoint. == [[StopExecutors]] StopExecutors StopExecutors message is receive-reply and blocking. When received, the following INFO message appears in the logs: INFO Asking each executor to shut down It then sends a executor:CoarseGrainedExecutorBackend.md#StopExecutor[StopExecutor] message to every registered executor (from executorDataMap ). == [[onStart]] Scheduling Sending ReviveOffers Periodically -- onStart Callback [source, scala] \u00b6 onStart(): Unit \u00b6 NOTE: onStart is part of rpc:RpcEndpoint.md#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages. onStart schedules a periodic action to send < > immediately every scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval]. NOTE: scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval] defaults to 1s . == [[makeOffers]] Making Executor Resource Offers (for Launching Tasks) -- makeOffers Internal Method [source, scala] \u00b6 makeOffers(): Unit \u00b6 makeOffers first creates WorkerOffers for all < > (registered in the internal scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] cache). NOTE: WorkerOffer represents a resource offer with CPU cores available on an executor. makeOffers then scheduler:TaskSchedulerImpl.md#resourceOffers[requests TaskSchedulerImpl to generate tasks for the available WorkerOffers ] followed by < >. NOTE: makeOffers uses scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] that was given when scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[ CoarseGrainedSchedulerBackend was created]. NOTE: Tasks are described using spark-scheduler-TaskDescription.md[TaskDescription] that holds...FIXME NOTE: makeOffers is used when CoarseGrainedSchedulerBackend RPC endpoint (DriverEndpoint) handles < > or < > messages. == [[makeOffers-executorId]] Making Executor Resource Offer on Single Executor (for Launching Tasks) -- makeOffers Internal Method [source, scala] \u00b6 makeOffers(executorId: String): Unit \u00b6 makeOffers makes sure that the < executorId is alive>>. NOTE: makeOffers does nothing when the input executorId is registered as pending to be removed or got lost. makeOffers finds the executor data (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] registry) and creates a scheduler:TaskSchedulerImpl.md#WorkerOffer[WorkerOffer]. NOTE: WorkerOffer represents a resource offer with CPU cores available on an executor. makeOffers then scheduler:TaskSchedulerImpl.md#resourceOffers[requests TaskSchedulerImpl to generate tasks for the WorkerOffer ] followed by < > (on the executor). NOTE: makeOffers is used when CoarseGrainedSchedulerBackend RPC endpoint (DriverEndpoint) handles < > messages. == [[launchTasks]] Launching Tasks on Executors -- launchTasks Internal Method [source, scala] \u00b6 launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit \u00b6 launchTasks flattens (and hence \"destroys\" the structure of) the input tasks collection and takes one task at a time. Tasks are described using spark-scheduler-TaskDescription.md[TaskDescription]. NOTE: The input tasks collection contains one or more spark-scheduler-TaskDescription.md[TaskDescriptions] per executor (and the \"task partitioning\" per executor is of no use in launchTasks so it simply flattens the input data structure). launchTasks spark-scheduler-TaskDescription.md#encode[encodes the TaskDescription ] and makes sure that the encoded task's size is below the scheduler:CoarseGrainedSchedulerBackend.md#maxRpcMessageSize[maximum RPC message size]. NOTE: The scheduler:CoarseGrainedSchedulerBackend.md#maxRpcMessageSize[maximum RPC message size] is calculated when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[is created] and corresponds to scheduler:CoarseGrainedSchedulerBackend.md#spark.rpc.message.maxSize[spark.rpc.message.maxSize] Spark property (with maximum of 2047 MB). If the size of the encoded task is acceptable, launchTasks finds the ExecutorData of the executor that has been assigned to execute the task (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] internal registry) and decreases the executor's ROOT:configuration-properties.md#spark.task.cpus[available number of cores]. NOTE: ExecutorData tracks the number of free cores of an executor (as freeCores ). NOTE: The default task scheduler in Spark -- scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] -- uses ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] Spark property to control the number of tasks that can be scheduled per executor. You should see the following DEBUG message in the logs: DEBUG DriverEndpoint: Launching task [taskId] on executor id: [executorId] hostname: [executorHost]. In the end, launchTasks sends the (serialized) task to associated executor to launch the task (by sending a executor:CoarseGrainedExecutorBackend.md#LaunchTask[LaunchTask] message to the executor's RPC endpoint with the serialized task insize SerializableBuffer ). NOTE: ExecutorData tracks the rpc:RpcEndpointRef.md[RpcEndpointRef] of executors to send serialized tasks to (as executorEndpoint ). IMPORTANT: This is the moment in a task's lifecycle when the driver sends the serialized task to an assigned executor. In case the size of a serialized TaskDescription equals or exceeds the scheduler:CoarseGrainedSchedulerBackend.md#maxRpcMessageSize[maximum RPC message size], launchTasks finds the scheduler:TaskSetManager.md[TaskSetManager] (associated with the TaskDescription ) and scheduler:TaskSetManager.md#abort[aborts it] with the following message: [options=\"wrap\"] \u00b6 Serialized task [id]:[index] was [limit] bytes, which exceeds max allowed: spark.rpc.message.maxSize ([maxRpcMessageSize] bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values. \u00b6 NOTE: launchTasks uses the scheduler:TaskSchedulerImpl.md#taskIdToTaskSetManager[registry of active TaskSetManagers per task id] from < > that was given when < CoarseGrainedSchedulerBackend was created>>. NOTE: Scheduling in Spark relies on cores only (not memory), i.e. the number of tasks Spark can run on an executor is limited by the number of cores available only. When submitting a Spark application for execution both executor resources -- memory and cores -- can however be specified explicitly. It is the job of a cluster manager to monitor the memory and take action when its use exceeds what was assigned. NOTE: launchTasks is used when CoarseGrainedSchedulerBackend is requested to make resource offers on < > or < > executors. == [[creating-instance]] Creating DriverEndpoint Instance DriverEndpoint takes the following when created: [[rpcEnv]] rpc:index.md[RpcEnv] [[sparkProperties]] Collection of Spark properties and their values DriverEndpoint initializes the < >. == [[RegisterExecutor-handler]] RegisterExecutor Handler [source, scala] \u00b6 RegisterExecutor( executorId: String, executorRef: RpcEndpointRef, hostname: String, cores: Int, logUrls: Map[String, String]) extends CoarseGrainedClusterMessage NOTE: RegisterExecutor is sent when executor:CoarseGrainedExecutorBackend.md#onStart[ CoarseGrainedExecutorBackend (RPC Endpoint) is started]. .Executor registration (RegisterExecutor RPC message flow) image::CoarseGrainedSchedulerBackend-RegisterExecutor-event.png[align=\"center\"] When received, DriverEndpoint makes sure that no other scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executors were registered] under the input executorId and that the input hostname is not scheduler:TaskSchedulerImpl.md#nodeBlacklist[blacklisted]. NOTE: DriverEndpoint uses < > (for the list of blacklisted nodes) that was specified when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[was created]. If the requirements hold, you should see the following INFO message in the logs: INFO Registered executor [executorRef] ([address]) with ID [executorId] DriverEndpoint does the bookkeeping: Registers executorId (in < >) Adds cores (in scheduler:CoarseGrainedSchedulerBackend.md#totalCoreCount[totalCoreCount]) Increments scheduler:CoarseGrainedSchedulerBackend.md#totalRegisteredExecutors[totalRegisteredExecutors] Creates and registers ExecutorData for executorId (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap]) Updates scheduler:CoarseGrainedSchedulerBackend.md#currentExecutorIdCounter[currentExecutorIdCounter] if the input executorId is greater than the current value. If scheduler:CoarseGrainedSchedulerBackend.md#numPendingExecutors[numPendingExecutors] is greater than 0 , you should see the following DEBUG message in the logs and DriverEndpoint decrements numPendingExecutors . DEBUG Decremented number of pending executors ([numPendingExecutors] left) DriverEndpoint sends executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[RegisteredExecutor] message back (that is to confirm that the executor was registered successfully). NOTE: DriverEndpoint uses the input executorRef as the executor's rpc:RpcEndpointRef.md[RpcEndpointRef]. DriverEndpoint replies true (to acknowledge the message). DriverEndpoint then announces the new executor by posting ROOT:SparkListener.md#SparkListenerExecutorAdded[SparkListenerExecutorAdded] to scheduler:LiveListenerBus.md[] (with the current time, executor id, and ExecutorData ). In the end, DriverEndpoint < >. If however there was already another executor registered under the input executorId , DriverEndpoint sends executor:CoarseGrainedExecutorBackend.md#RegisterExecutorFailed[RegisterExecutorFailed] message back with the reason: Duplicate executor ID: [executorId] If however the input hostname is scheduler:TaskSchedulerImpl.md#nodeBlacklist[blacklisted], you should see the following INFO message in the logs: INFO Rejecting [executorId] as it has been blacklisted. DriverEndpoint sends executor:CoarseGrainedExecutorBackend.md#RegisterExecutorFailed[RegisterExecutorFailed] message back with the reason: Executor is blacklisted: [executorId] == [[StatusUpdate-handler]] StatusUpdate Handler [source, scala] \u00b6 StatusUpdate( executorId: String, taskId: Long, state: TaskState, data: SerializableBuffer) extends CoarseGrainedClusterMessage NOTE: StatusUpdate is sent when CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#statusUpdate[sends task status updates to the driver]. When StatusUpdate is received, DriverEndpoint requests the scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] to scheduler:TaskSchedulerImpl.md#statusUpdate[handle the task status update]. If the scheduler:Task.md#TaskState[task has finished], DriverEndpoint updates the number of cores available for work on the corresponding executor (registered in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap]). NOTE: DriverEndpoint uses TaskSchedulerImpl 's ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] as the number of cores that became available after the task has finished. DriverEndpoint < >. When DriverEndpoint found no executor (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap]), you should see the following WARN message in the logs: WARN Ignored task status update ([taskId] state [state]) from unknown executor with ID [executorId] == [[KillTask-handler]] KillTask Handler [source, scala] \u00b6 KillTask( taskId: Long, executor: String, interruptThread: Boolean) extends CoarseGrainedClusterMessage NOTE: KillTask is sent when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#killTask[kills a task]. When KillTask is received, DriverEndpoint finds executor (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] registry). If found, DriverEndpoint executor:CoarseGrainedExecutorBackend.md#KillTask[passes the message on to the executor] (using its registered RPC endpoint for CoarseGrainedExecutorBackend ). Otherwise, you should see the following WARN in the logs: WARN Attempted to kill task [taskId] for unknown executor [executor]. == [[removeExecutor]] Removing Executor from Internal Registries (and Notifying TaskSchedulerImpl and Posting SparkListenerExecutorRemoved) -- removeExecutor Internal Method [source, scala] \u00b6 removeExecutor(executorId: String, reason: ExecutorLossReason): Unit \u00b6 When removeExecutor is executed, you should see the following DEBUG message in the logs: DEBUG Asked to remove executor [executorId] with reason [reason] removeExecutor then tries to find the executorId executor (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] internal registry). If the executorId executor was found, removeExecutor removes the executor from the following registries: < > scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] < > scheduler:CoarseGrainedSchedulerBackend.md#executorsPendingToRemove[executorsPendingToRemove] removeExecutor decrements: scheduler:CoarseGrainedSchedulerBackend.md#totalCoreCount[totalCoreCount] by the executor's totalCores scheduler:CoarseGrainedSchedulerBackend.md#totalRegisteredExecutors[totalRegisteredExecutors] In the end, removeExecutor notifies TaskSchedulerImpl that an scheduler:TaskSchedulerImpl.md#executorLost[executor was lost]. NOTE: removeExecutor uses scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] that is specified when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[is created]. removeExecutor posts ROOT:SparkListener.md#SparkListenerExecutorRemoved[SparkListenerExecutorRemoved] to scheduler:LiveListenerBus.md[] (with the executorId executor). If however the executorId executor could not be found, removeExecutor storage:BlockManagerMaster.md#removeExecutorAsync[requests BlockManagerMaster to remove the executor asynchronously]. NOTE: removeExecutor uses SparkEnv core:SparkEnv.md#blockManager[to access the current BlockManager ] and then storage:BlockManager.md#master[BlockManagerMaster]. You should see the following INFO message in the logs: INFO Asked to remove non-existent executor [executorId] NOTE: removeExecutor is used when DriverEndpoint < RemoveExecutor message>> and < >. == [[removeWorker]] removeWorker Internal Method [source, scala] \u00b6 removeWorker( workerId: String, host: String, message: String): Unit removeWorker prints out the following DEBUG message to the logs: Asked to remove worker [workerId] with reason [message] In the end, removeWorker simply requests the scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] to scheduler:TaskSchedulerImpl.md#workerRemoved[workerRemoved]. NOTE: removeWorker is used exclusively when DriverEndpoint is requested to handle a < > event.","title":"DriverEndpoint"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#driverendpoint-coarsegrainedschedulerbackend-rpc-endpoint","text":"DriverEndpoint is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] that acts as a < > for scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] to communicate with executor:CoarseGrainedExecutorBackend.md[]. DriverEndpoint < > when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#starts[starts]. DriverEndpoint uses scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] internal registry of all the executor:CoarseGrainedExecutorBackend.md#onStart[executors that registered with the driver]. An executor sends a < > message to inform that it wants to register. DriverEndpoint uses a < > called driver-revive-thread to < > (by emitting < > message every scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval]). [[messages]] .CoarseGrainedClusterMessages and Their Handlers (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | CoarseGrainedClusterMessage | Event Handler | When emitted? | [[KillExecutorsOnHost]] KillExecutorsOnHost | < > | CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#killExecutorsOnHost[kill all executors on a node]. | [[KillTask]] KillTask | < > | CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#killTask[kill a task]. | [[ReviveOffers]] ReviveOffers | < > a| Periodically (every scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval]) soon after DriverEndpoint < >. CoarseGrainedSchedulerBackend is requested to scheduler:CoarseGrainedSchedulerBackend.md#reviveOffers[revive resource offers]. | [[RegisterExecutor]] RegisterExecutor | < > | CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#onStart[registers with the driver]. | [[StatusUpdate]] StatusUpdate | < > | CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#statusUpdate[sends task status updates to the driver]. |=== [[internal-properties]] .DriverEndpoint's Internal Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description [[addressToExecutorId]] addressToExecutorId Executor addresses (host and port) for executors. Set when an executor connects to register itself. See < > RPC message. [[executorsPendingLossReason]] executorsPendingLossReason [[reviveThread]] reviveThread === == [[disableExecutor]] disableExecutor Internal Method CAUTION: FIXME == [[KillExecutorsOnHost-handler]] KillExecutorsOnHost Handler CAUTION: FIXME == [[executorIsAlive]] executorIsAlive Internal Method CAUTION: FIXME == [[onStop]] onStop Callback CAUTION: FIXME == [[onDisconnected]] onDisconnected Callback When called, onDisconnected removes the worker from the internal < > (that effectively removes the worker from a cluster). While removing, it calls < > with the reason being SlaveLost and message:","title":"DriverEndpoint &mdash; CoarseGrainedSchedulerBackend RPC Endpoint"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#remote-rpc-client-disassociated-likely-due-to-containers-exceeding-thresholds-or-network-issues-check-driver-logs-for-warn-messages","text":"NOTE: onDisconnected is called when a remote host is lost. == [[RemoveExecutor]] RemoveExecutor == [[RetrieveSparkProps]] RetrieveSparkProps == [[StopDriver]] StopDriver StopDriver message stops the RPC endpoint. == [[StopExecutors]] StopExecutors StopExecutors message is receive-reply and blocking. When received, the following INFO message appears in the logs: INFO Asking each executor to shut down It then sends a executor:CoarseGrainedExecutorBackend.md#StopExecutor[StopExecutor] message to every registered executor (from executorDataMap ). == [[onStart]] Scheduling Sending ReviveOffers Periodically -- onStart Callback","title":"Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages."},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#onstart-unit","text":"NOTE: onStart is part of rpc:RpcEndpoint.md#onStart[RpcEndpoint contract] that is executed before a RPC endpoint starts accepting messages. onStart schedules a periodic action to send < > immediately every scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval]. NOTE: scheduler:CoarseGrainedSchedulerBackend.md#spark.scheduler.revive.interval[spark.scheduler.revive.interval] defaults to 1s . == [[makeOffers]] Making Executor Resource Offers (for Launching Tasks) -- makeOffers Internal Method","title":"onStart(): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#makeoffers-unit","text":"makeOffers first creates WorkerOffers for all < > (registered in the internal scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] cache). NOTE: WorkerOffer represents a resource offer with CPU cores available on an executor. makeOffers then scheduler:TaskSchedulerImpl.md#resourceOffers[requests TaskSchedulerImpl to generate tasks for the available WorkerOffers ] followed by < >. NOTE: makeOffers uses scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] that was given when scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[ CoarseGrainedSchedulerBackend was created]. NOTE: Tasks are described using spark-scheduler-TaskDescription.md[TaskDescription] that holds...FIXME NOTE: makeOffers is used when CoarseGrainedSchedulerBackend RPC endpoint (DriverEndpoint) handles < > or < > messages. == [[makeOffers-executorId]] Making Executor Resource Offer on Single Executor (for Launching Tasks) -- makeOffers Internal Method","title":"makeOffers(): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#makeoffersexecutorid-string-unit","text":"makeOffers makes sure that the < executorId is alive>>. NOTE: makeOffers does nothing when the input executorId is registered as pending to be removed or got lost. makeOffers finds the executor data (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] registry) and creates a scheduler:TaskSchedulerImpl.md#WorkerOffer[WorkerOffer]. NOTE: WorkerOffer represents a resource offer with CPU cores available on an executor. makeOffers then scheduler:TaskSchedulerImpl.md#resourceOffers[requests TaskSchedulerImpl to generate tasks for the WorkerOffer ] followed by < > (on the executor). NOTE: makeOffers is used when CoarseGrainedSchedulerBackend RPC endpoint (DriverEndpoint) handles < > messages. == [[launchTasks]] Launching Tasks on Executors -- launchTasks Internal Method","title":"makeOffers(executorId: String): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#launchtaskstasks-seqseqtaskdescription-unit","text":"launchTasks flattens (and hence \"destroys\" the structure of) the input tasks collection and takes one task at a time. Tasks are described using spark-scheduler-TaskDescription.md[TaskDescription]. NOTE: The input tasks collection contains one or more spark-scheduler-TaskDescription.md[TaskDescriptions] per executor (and the \"task partitioning\" per executor is of no use in launchTasks so it simply flattens the input data structure). launchTasks spark-scheduler-TaskDescription.md#encode[encodes the TaskDescription ] and makes sure that the encoded task's size is below the scheduler:CoarseGrainedSchedulerBackend.md#maxRpcMessageSize[maximum RPC message size]. NOTE: The scheduler:CoarseGrainedSchedulerBackend.md#maxRpcMessageSize[maximum RPC message size] is calculated when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[is created] and corresponds to scheduler:CoarseGrainedSchedulerBackend.md#spark.rpc.message.maxSize[spark.rpc.message.maxSize] Spark property (with maximum of 2047 MB). If the size of the encoded task is acceptable, launchTasks finds the ExecutorData of the executor that has been assigned to execute the task (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] internal registry) and decreases the executor's ROOT:configuration-properties.md#spark.task.cpus[available number of cores]. NOTE: ExecutorData tracks the number of free cores of an executor (as freeCores ). NOTE: The default task scheduler in Spark -- scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] -- uses ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] Spark property to control the number of tasks that can be scheduled per executor. You should see the following DEBUG message in the logs: DEBUG DriverEndpoint: Launching task [taskId] on executor id: [executorId] hostname: [executorHost]. In the end, launchTasks sends the (serialized) task to associated executor to launch the task (by sending a executor:CoarseGrainedExecutorBackend.md#LaunchTask[LaunchTask] message to the executor's RPC endpoint with the serialized task insize SerializableBuffer ). NOTE: ExecutorData tracks the rpc:RpcEndpointRef.md[RpcEndpointRef] of executors to send serialized tasks to (as executorEndpoint ). IMPORTANT: This is the moment in a task's lifecycle when the driver sends the serialized task to an assigned executor. In case the size of a serialized TaskDescription equals or exceeds the scheduler:CoarseGrainedSchedulerBackend.md#maxRpcMessageSize[maximum RPC message size], launchTasks finds the scheduler:TaskSetManager.md[TaskSetManager] (associated with the TaskDescription ) and scheduler:TaskSetManager.md#abort[aborts it] with the following message:","title":"launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#optionswrap_1","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#serialized-task-idindex-was-limit-bytes-which-exceeds-max-allowed-sparkrpcmessagemaxsize-maxrpcmessagesize-bytes-consider-increasing-sparkrpcmessagemaxsize-or-using-broadcast-variables-for-large-values","text":"NOTE: launchTasks uses the scheduler:TaskSchedulerImpl.md#taskIdToTaskSetManager[registry of active TaskSetManagers per task id] from < > that was given when < CoarseGrainedSchedulerBackend was created>>. NOTE: Scheduling in Spark relies on cores only (not memory), i.e. the number of tasks Spark can run on an executor is limited by the number of cores available only. When submitting a Spark application for execution both executor resources -- memory and cores -- can however be specified explicitly. It is the job of a cluster manager to monitor the memory and take action when its use exceeds what was assigned. NOTE: launchTasks is used when CoarseGrainedSchedulerBackend is requested to make resource offers on < > or < > executors. == [[creating-instance]] Creating DriverEndpoint Instance DriverEndpoint takes the following when created: [[rpcEnv]] rpc:index.md[RpcEnv] [[sparkProperties]] Collection of Spark properties and their values DriverEndpoint initializes the < >. == [[RegisterExecutor-handler]] RegisterExecutor Handler","title":"Serialized task [id]:[index] was [limit] bytes, which exceeds max allowed: spark.rpc.message.maxSize ([maxRpcMessageSize] bytes). Consider increasing spark.rpc.message.maxSize or using broadcast variables for large values."},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_4","text":"RegisterExecutor( executorId: String, executorRef: RpcEndpointRef, hostname: String, cores: Int, logUrls: Map[String, String]) extends CoarseGrainedClusterMessage NOTE: RegisterExecutor is sent when executor:CoarseGrainedExecutorBackend.md#onStart[ CoarseGrainedExecutorBackend (RPC Endpoint) is started]. .Executor registration (RegisterExecutor RPC message flow) image::CoarseGrainedSchedulerBackend-RegisterExecutor-event.png[align=\"center\"] When received, DriverEndpoint makes sure that no other scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executors were registered] under the input executorId and that the input hostname is not scheduler:TaskSchedulerImpl.md#nodeBlacklist[blacklisted]. NOTE: DriverEndpoint uses < > (for the list of blacklisted nodes) that was specified when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[was created]. If the requirements hold, you should see the following INFO message in the logs: INFO Registered executor [executorRef] ([address]) with ID [executorId] DriverEndpoint does the bookkeeping: Registers executorId (in < >) Adds cores (in scheduler:CoarseGrainedSchedulerBackend.md#totalCoreCount[totalCoreCount]) Increments scheduler:CoarseGrainedSchedulerBackend.md#totalRegisteredExecutors[totalRegisteredExecutors] Creates and registers ExecutorData for executorId (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap]) Updates scheduler:CoarseGrainedSchedulerBackend.md#currentExecutorIdCounter[currentExecutorIdCounter] if the input executorId is greater than the current value. If scheduler:CoarseGrainedSchedulerBackend.md#numPendingExecutors[numPendingExecutors] is greater than 0 , you should see the following DEBUG message in the logs and DriverEndpoint decrements numPendingExecutors . DEBUG Decremented number of pending executors ([numPendingExecutors] left) DriverEndpoint sends executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[RegisteredExecutor] message back (that is to confirm that the executor was registered successfully). NOTE: DriverEndpoint uses the input executorRef as the executor's rpc:RpcEndpointRef.md[RpcEndpointRef]. DriverEndpoint replies true (to acknowledge the message). DriverEndpoint then announces the new executor by posting ROOT:SparkListener.md#SparkListenerExecutorAdded[SparkListenerExecutorAdded] to scheduler:LiveListenerBus.md[] (with the current time, executor id, and ExecutorData ). In the end, DriverEndpoint < >. If however there was already another executor registered under the input executorId , DriverEndpoint sends executor:CoarseGrainedExecutorBackend.md#RegisterExecutorFailed[RegisterExecutorFailed] message back with the reason: Duplicate executor ID: [executorId] If however the input hostname is scheduler:TaskSchedulerImpl.md#nodeBlacklist[blacklisted], you should see the following INFO message in the logs: INFO Rejecting [executorId] as it has been blacklisted. DriverEndpoint sends executor:CoarseGrainedExecutorBackend.md#RegisterExecutorFailed[RegisterExecutorFailed] message back with the reason: Executor is blacklisted: [executorId] == [[StatusUpdate-handler]] StatusUpdate Handler","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_5","text":"StatusUpdate( executorId: String, taskId: Long, state: TaskState, data: SerializableBuffer) extends CoarseGrainedClusterMessage NOTE: StatusUpdate is sent when CoarseGrainedExecutorBackend executor:CoarseGrainedExecutorBackend.md#statusUpdate[sends task status updates to the driver]. When StatusUpdate is received, DriverEndpoint requests the scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] to scheduler:TaskSchedulerImpl.md#statusUpdate[handle the task status update]. If the scheduler:Task.md#TaskState[task has finished], DriverEndpoint updates the number of cores available for work on the corresponding executor (registered in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap]). NOTE: DriverEndpoint uses TaskSchedulerImpl 's ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] as the number of cores that became available after the task has finished. DriverEndpoint < >. When DriverEndpoint found no executor (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap]), you should see the following WARN message in the logs: WARN Ignored task status update ([taskId] state [state]) from unknown executor with ID [executorId] == [[KillTask-handler]] KillTask Handler","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_6","text":"KillTask( taskId: Long, executor: String, interruptThread: Boolean) extends CoarseGrainedClusterMessage NOTE: KillTask is sent when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#killTask[kills a task]. When KillTask is received, DriverEndpoint finds executor (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] registry). If found, DriverEndpoint executor:CoarseGrainedExecutorBackend.md#KillTask[passes the message on to the executor] (using its registered RPC endpoint for CoarseGrainedExecutorBackend ). Otherwise, you should see the following WARN in the logs: WARN Attempted to kill task [taskId] for unknown executor [executor]. == [[removeExecutor]] Removing Executor from Internal Registries (and Notifying TaskSchedulerImpl and Posting SparkListenerExecutorRemoved) -- removeExecutor Internal Method","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#removeexecutorexecutorid-string-reason-executorlossreason-unit","text":"When removeExecutor is executed, you should see the following DEBUG message in the logs: DEBUG Asked to remove executor [executorId] with reason [reason] removeExecutor then tries to find the executorId executor (in scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] internal registry). If the executorId executor was found, removeExecutor removes the executor from the following registries: < > scheduler:CoarseGrainedSchedulerBackend.md#executorDataMap[executorDataMap] < > scheduler:CoarseGrainedSchedulerBackend.md#executorsPendingToRemove[executorsPendingToRemove] removeExecutor decrements: scheduler:CoarseGrainedSchedulerBackend.md#totalCoreCount[totalCoreCount] by the executor's totalCores scheduler:CoarseGrainedSchedulerBackend.md#totalRegisteredExecutors[totalRegisteredExecutors] In the end, removeExecutor notifies TaskSchedulerImpl that an scheduler:TaskSchedulerImpl.md#executorLost[executor was lost]. NOTE: removeExecutor uses scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] that is specified when CoarseGrainedSchedulerBackend scheduler:CoarseGrainedSchedulerBackend.md#creating-instance[is created]. removeExecutor posts ROOT:SparkListener.md#SparkListenerExecutorRemoved[SparkListenerExecutorRemoved] to scheduler:LiveListenerBus.md[] (with the executorId executor). If however the executorId executor could not be found, removeExecutor storage:BlockManagerMaster.md#removeExecutorAsync[requests BlockManagerMaster to remove the executor asynchronously]. NOTE: removeExecutor uses SparkEnv core:SparkEnv.md#blockManager[to access the current BlockManager ] and then storage:BlockManager.md#master[BlockManagerMaster]. You should see the following INFO message in the logs: INFO Asked to remove non-existent executor [executorId] NOTE: removeExecutor is used when DriverEndpoint < RemoveExecutor message>> and < >. == [[removeWorker]] removeWorker Internal Method","title":"removeExecutor(executorId: String, reason: ExecutorLossReason): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend-DriverEndpoint/#source-scala_8","text":"removeWorker( workerId: String, host: String, message: String): Unit removeWorker prints out the following DEBUG message to the logs: Asked to remove worker [workerId] with reason [message] In the end, removeWorker simply requests the scheduler:CoarseGrainedSchedulerBackend.md#scheduler[TaskSchedulerImpl] to scheduler:TaskSchedulerImpl.md#workerRemoved[workerRemoved]. NOTE: removeWorker is used exclusively when DriverEndpoint is requested to handle a < > event.","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/","text":"CoarseGrainedSchedulerBackend \u00b6 CoarseGrainedSchedulerBackend is a SchedulerBackend . CoarseGrainedSchedulerBackend is an ExecutorAllocationClient . CoarseGrainedSchedulerBackend is responsible for requesting resources from a cluster manager for executors that it in turn uses to CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks] (on executor:CoarseGrainedExecutorBackend.md[]). CoarseGrainedSchedulerBackend holds executors for the duration of the Spark job rather than relinquishing executors whenever a task is done and asking the scheduler to launch a new executor for each new task. CoarseGrainedSchedulerBackend registers < > that executors use for RPC communication. NOTE: Active executors are executors that are not < > or CoarseGrainedSchedulerBackend-DriverEndpoint.md#executorsPendingLossReason[lost]. [[builtin-implementations]] .Built-In CoarseGrainedSchedulerBackends per Cluster Environment [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Cluster Environment | CoarseGrainedSchedulerBackend | Spark Standalone | spark-standalone-StandaloneSchedulerBackend.md[StandaloneSchedulerBackend] | Spark on YARN | yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend] | Spark on Mesos | spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.md[MesosCoarseGrainedSchedulerBackend] |=== NOTE: CoarseGrainedSchedulerBackend is only created indirectly through < >. [[internal-properties]] .CoarseGrainedSchedulerBackend's Internal Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description [[currentExecutorIdCounter]] currentExecutorIdCounter The last (highest) identifier of all < >. Used exclusively in yarn/spark-yarn-cluster-YarnSchedulerEndpoint.md#RetrieveLastAllocatedExecutorId[ YarnSchedulerEndpoint to respond to RetrieveLastAllocatedExecutorId message]. | [[createTime]] createTime | Current time | The time < CoarseGrainedSchedulerBackend was created>>. | [[defaultAskTimeout]] defaultAskTimeout | rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout] or rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s | Default timeout for blocking RPC messages ( aka ask messages). | [[driverEndpoint]] driverEndpoint | (uninitialized) a| rpc:RpcEndpointRef.md[RPC endpoint reference] to CoarseGrainedScheduler RPC endpoint (with scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint] as the message handler). Initialized when CoarseGrainedSchedulerBackend < >. Used when CoarseGrainedSchedulerBackend executes the following (asynchronously, i.e. on a separate thread): < > < > < > < > < > < > | [[executorDataMap]] executorDataMap | empty | Registry of ExecutorData by executor id. NOTE: ExecutorData holds an executor's endpoint reference, address, host, the number of free and total CPU cores, the URL of execution logs. Element added when scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RegisterExecutor[ DriverEndpoint receives RegisterExecutor message] and removed when scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RemoveExecutor[ DriverEndpoint receives RemoveExecutor message] or scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#onDisconnected[a remote host (with one or many executors) disconnects]. | [[executorsPendingToRemove]] executorsPendingToRemove | empty | Executors marked as removed but the confirmation from a cluster manager has not arrived yet. | [[hostToLocalTaskCount]] hostToLocalTaskCount | empty | Registry of hostnames and possible number of task running on them. | [[localityAwareTasks]] localityAwareTasks | 0 | Number of pending tasks...FIXME | [[maxRegisteredWaitingTimeMs]] maxRegisteredWaitingTimeMs | < > | | [[maxRpcMessageSize]] maxRpcMessageSize | < > but not greater than 2047 a| Maximum RPC message size in MB. When above 2047 MB you should see the following IllegalArgumentException : spark.rpc.message.maxSize should not be greater than 2047 MB | [[_minRegisteredRatio]] _minRegisteredRatio | < > | | [[numPendingExecutors]] numPendingExecutors | 0 | | [[totalCoreCount]] totalCoreCount | 0 | Total number of CPU cores, i.e. the sum of all the cores on all executors. | [[totalRegisteredExecutors]] totalRegisteredExecutors | 0 | Total number of registered executors |=== [TIP] \u00b6 Enable INFO or DEBUG logging level for org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend=DEBUG Refer to spark-logging.md[Logging]. \u00b6 == [[killExecutorsOnHost]] Killing All Executors on Node -- killExecutorsOnHost Method CAUTION: FIXME == [[makeOffers]] Making Fake Resource Offers on Executors -- makeOffers Internal Methods [source, scala] \u00b6 makeOffers(): Unit makeOffers(executorId: String): Unit makeOffers takes the active executors (out of the < > internal registry) and creates WorkerOffer resource offers for each (one per executor with the executor's id, host and free cores). CAUTION: Only free cores are considered in making offers. Memory is not! Why?! It then requests scheduler:TaskSchedulerImpl.md#resourceOffers[ TaskSchedulerImpl to process the resource offers] to create a collection of spark-scheduler-TaskDescription.md[TaskDescription] collections that it in turn uses to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks]. == [[creating-instance]] Creating CoarseGrainedSchedulerBackend Instance CoarseGrainedSchedulerBackend takes the following when created: . [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] . [[rpcEnv]] rpc:index.md[RpcEnv] CoarseGrainedSchedulerBackend initializes the < >. == [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method When called, getExecutorIds simply returns executor ids from the internal < > registry. NOTE: It is called when ROOT:SparkContext.md#getExecutorIds[SparkContext calculates executor ids]. == [[contract]] CoarseGrainedSchedulerBackend Contract [source, scala] \u00b6 class CoarseGrainedSchedulerBackend { def minRegisteredRatio: Double def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint def reset(): Unit def sufficientResourcesRegistered(): Boolean def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] def doKillExecutors(executorIds: Seq[String]): Future[Boolean] } NOTE: CoarseGrainedSchedulerBackend is a private[spark] contract. .FIXME Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[minRegisteredRatio]] minRegisteredRatio | Ratio between 0 and 1 (inclusive). Controlled by < >. | < > | FIXME | [[doRequestTotalExecutors]] doRequestTotalExecutors | FIXME | [[doKillExecutors]] doKillExecutors | FIXME | [[sufficientResourcesRegistered]] sufficientResourcesRegistered | Always positive, i.e. true , that means that sufficient resources are available. Used when CoarseGrainedSchedulerBackend < >. |=== It can < >. == [[numExistingExecutors]] numExistingExecutors Method CAUTION: FIXME == [[killExecutors]] killExecutors Methods CAUTION: FIXME == [[getDriverLogUrls]] getDriverLogUrls Method CAUTION: FIXME == [[applicationAttemptId]] applicationAttemptId Method CAUTION: FIXME == [[requestExecutors]] Requesting Additional Executors -- requestExecutors Method [source, scala] \u00b6 requestExecutors(numAdditionalExecutors: Int): Boolean \u00b6 requestExecutors is a \"decorator\" method that ultimately calls a cluster-specific < > method and returns whether the request was acknowledged or not (it is assumed false by default). NOTE: requestExecutors method is part of spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient Contract] that ROOT:SparkContext.md#requestExecutors[SparkContext uses for requesting additional executors] (as a part of a developer API for dynamic allocation of executors). When called, you should see the following INFO message followed by DEBUG message in the logs: INFO Requesting [numAdditionalExecutors] additional executor(s) from the cluster manager DEBUG Number of pending executors is now [numPendingExecutors] < > is increased by the input numAdditionalExecutors . requestExecutors < > (that reflects the current computation needs). The \"new executor total\" is a sum of the internal < > and < > decreased by the < >. If numAdditionalExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of additional executor(s) [numAdditionalExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one. == [[requestTotalExecutors]] Requesting Exact Number of Executors -- requestTotalExecutors Method [source, scala] \u00b6 requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a \"decorator\" method that ultimately calls a cluster-specific < > method and returns whether the request was acknowledged or not (it is assumed false by default). NOTE: requestTotalExecutors is part of spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient Contract] that ROOT:SparkContext.md#requestTotalExecutors[SparkContext uses for requesting the exact number of executors]. It sets the internal < > and < > registries. It then calculates the exact number of executors which is the input numExecutors and the < > decreased by the number of < >. If numExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of executor(s) [numExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one. == [[defaultParallelism]] Finding Default Level of Parallelism -- defaultParallelism Method [source, scala] \u00b6 defaultParallelism(): Int \u00b6 NOTE: defaultParallelism is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend Contract]. defaultParallelism is ROOT:configuration-properties.md#spark.default.parallelism[spark.default.parallelism] configuration property if defined. Otherwise, defaultParallelism is the maximum of < > or 2 . == [[killTask]] Killing Task -- killTask Method [source, scala] \u00b6 killTask(taskId: Long, executorId: String, interruptThread: Boolean): Unit \u00b6 NOTE: killTask is part of the scheduler:SchedulerBackend.md#killTask[SchedulerBackend contract]. killTask simply sends a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#KillTask[KillTask] message to < >. CAUTION: FIXME Image == [[stopExecutors]] Stopping All Executors -- stopExecutors Method stopExecutors sends a blocking < > message to < > (if already initialized). NOTE: It is called exclusively while CoarseGrainedSchedulerBackend is < >. You should see the following INFO message in the logs: INFO CoarseGrainedSchedulerBackend: Shutting down all executors == [[reset]] Reset State -- reset Method reset resets the internal state: Sets < > to 0 Clears executorsPendingToRemove Sends a blocking < > message to < > for every executor (in the internal executorDataMap ) to inform it about SlaveLost with the message: + Stale executor after cluster manager re-registered. reset is a method that is defined in CoarseGrainedSchedulerBackend , but used and overriden exclusively by yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend]. == [[removeExecutor]] Remove Executor -- removeExecutor Method [source, scala] \u00b6 removeExecutor(executorId: String, reason: ExecutorLossReason) \u00b6 removeExecutor sends a blocking < > message to < >. NOTE: It is called by subclasses spark-standalone.md#SparkDeploySchedulerBackend[SparkDeploySchedulerBackend], spark-mesos/spark-mesos.md#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend], and yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend]. == [[CoarseGrainedScheduler]] CoarseGrainedScheduler RPC Endpoint -- driverEndpoint When < >, it registers CoarseGrainedScheduler RPC endpoint to be the driver's communication endpoint. driverEndpoint is a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint]. NOTE: CoarseGrainedSchedulerBackend is created while ROOT:SparkContext.md#createTaskScheduler[SparkContext is being created] that in turn lives inside a spark-driver.md[Spark driver]. That explains the name driverEndpoint (at least partially). It is called standalone scheduler's driver endpoint internally. It tracks: It uses driver-revive-thread daemon single-thread thread pool for ...FIXME CAUTION: FIXME A potential issue with driverEndpoint.asInstanceOf[NettyRpcEndpointRef].toURI - doubles spark:// prefix. == [[start]] Starting CoarseGrainedSchedulerBackend (and Registering CoarseGrainedScheduler RPC Endpoint) -- start Method [source, scala] \u00b6 start(): Unit \u00b6 NOTE: start is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend contract]. start takes all spark. -prefixed properties and registers the < CoarseGrainedScheduler RPC endpoint>> (backed by scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint ThreadSafeRpcEndpoint]). .CoarseGrainedScheduler Endpoint image::CoarseGrainedScheduler-rpc-endpoint.png[align=\"center\"] NOTE: start uses < > to access the current ROOT:SparkContext.md[SparkContext] and in turn ROOT:SparkConf.md[SparkConf]. NOTE: start uses < > that was given when < CoarseGrainedSchedulerBackend was created>>. == [[isReady]] Checking If Sufficient Compute Resources Available Or Waiting Time Passed -- isReady Method [source, scala] \u00b6 isReady(): Boolean \u00b6 NOTE: isReady is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend contract]. isReady allows to delay task launching until < > or < > passes. Internally, isReady < >. NOTE: < > by default responds that sufficient resources are available. If the < >, you should see the following INFO message in the logs and isReady is positive. [options=\"wrap\"] \u00b6 INFO SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: [minRegisteredRatio] \u00b6 NOTE: < > is in the range 0 to 1 (uses < >) to denote the minimum ratio of registered resources to total expected resources before submitting tasks. If there are no sufficient resources available yet (the above requirement does not hold), isReady checks whether the time since < > passed < > to give a way to launch tasks (even when < > not being reached yet). You should see the following INFO message in the logs and isReady is positive. [options=\"wrap\"] \u00b6 INFO SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: maxRegisteredWaitingTimeMs \u00b6 Otherwise, when < > and < > has not elapsed, isReady is negative. == [[reviveOffers]] Reviving Resource Offers (by Posting ReviveOffers to CoarseGrainedSchedulerBackend RPC Endpoint) -- reviveOffers Method [source, scala] \u00b6 reviveOffers(): Unit \u00b6 NOTE: reviveOffers is part of the scheduler:SchedulerBackend.md#reviveOffers[SchedulerBackend contract]. reviveOffers simply sends a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#ReviveOffers[ReviveOffers] message to < CoarseGrainedSchedulerBackend RPC endpoint>>. .CoarseGrainedExecutorBackend Revives Offers image::CoarseGrainedExecutorBackend-reviveOffers.png[align=\"center\"] == [[stop]] Stopping CoarseGrainedSchedulerBackend (and Stopping Executors) -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend contract]. stop < > and < CoarseGrainedScheduler RPC endpoint>> (by sending a blocking scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StopDriver[StopDriver] message). In case of any Exception , stop reports a SparkException with the message: Error stopping standalone scheduler's driver endpoint == [[createDriverEndpointRef]] createDriverEndpointRef Method [source, scala] \u00b6 createDriverEndpointRef( properties: ArrayBuffer[(String, String)]): RpcEndpointRef createDriverEndpointRef < > and rpc:index.md#setupEndpoint[registers it] as CoarseGrainedScheduler . createDriverEndpointRef is used when CoarseGrainedSchedulerBackend is requested to < >. == [[createDriverEndpoint]] Creating DriverEndpoint -- createDriverEndpoint Method [source, scala] \u00b6 createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint \u00b6 createDriverEndpoint simply creates a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#creating-instance[DriverEndpoint]. NOTE: scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint] is the < CoarseGrainedSchedulerBackend >>. NOTE: The purpose of createDriverEndpoint is to allow YARN to use the custom YarnDriverEndpoint . NOTE: createDriverEndpoint is used when CoarseGrainedSchedulerBackend < >.","title":"CoarseGrainedSchedulerBackend"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#coarsegrainedschedulerbackend","text":"CoarseGrainedSchedulerBackend is a SchedulerBackend . CoarseGrainedSchedulerBackend is an ExecutorAllocationClient . CoarseGrainedSchedulerBackend is responsible for requesting resources from a cluster manager for executors that it in turn uses to CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks] (on executor:CoarseGrainedExecutorBackend.md[]). CoarseGrainedSchedulerBackend holds executors for the duration of the Spark job rather than relinquishing executors whenever a task is done and asking the scheduler to launch a new executor for each new task. CoarseGrainedSchedulerBackend registers < > that executors use for RPC communication. NOTE: Active executors are executors that are not < > or CoarseGrainedSchedulerBackend-DriverEndpoint.md#executorsPendingLossReason[lost]. [[builtin-implementations]] .Built-In CoarseGrainedSchedulerBackends per Cluster Environment [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Cluster Environment | CoarseGrainedSchedulerBackend | Spark Standalone | spark-standalone-StandaloneSchedulerBackend.md[StandaloneSchedulerBackend] | Spark on YARN | yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend] | Spark on Mesos | spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.md[MesosCoarseGrainedSchedulerBackend] |=== NOTE: CoarseGrainedSchedulerBackend is only created indirectly through < >. [[internal-properties]] .CoarseGrainedSchedulerBackend's Internal Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description [[currentExecutorIdCounter]] currentExecutorIdCounter The last (highest) identifier of all < >. Used exclusively in yarn/spark-yarn-cluster-YarnSchedulerEndpoint.md#RetrieveLastAllocatedExecutorId[ YarnSchedulerEndpoint to respond to RetrieveLastAllocatedExecutorId message]. | [[createTime]] createTime | Current time | The time < CoarseGrainedSchedulerBackend was created>>. | [[defaultAskTimeout]] defaultAskTimeout | rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout] or rpc:index.md#spark.network.timeout[spark.network.timeout] or 120s | Default timeout for blocking RPC messages ( aka ask messages). | [[driverEndpoint]] driverEndpoint | (uninitialized) a| rpc:RpcEndpointRef.md[RPC endpoint reference] to CoarseGrainedScheduler RPC endpoint (with scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint] as the message handler). Initialized when CoarseGrainedSchedulerBackend < >. Used when CoarseGrainedSchedulerBackend executes the following (asynchronously, i.e. on a separate thread): < > < > < > < > < > < > | [[executorDataMap]] executorDataMap | empty | Registry of ExecutorData by executor id. NOTE: ExecutorData holds an executor's endpoint reference, address, host, the number of free and total CPU cores, the URL of execution logs. Element added when scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RegisterExecutor[ DriverEndpoint receives RegisterExecutor message] and removed when scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#RemoveExecutor[ DriverEndpoint receives RemoveExecutor message] or scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#onDisconnected[a remote host (with one or many executors) disconnects]. | [[executorsPendingToRemove]] executorsPendingToRemove | empty | Executors marked as removed but the confirmation from a cluster manager has not arrived yet. | [[hostToLocalTaskCount]] hostToLocalTaskCount | empty | Registry of hostnames and possible number of task running on them. | [[localityAwareTasks]] localityAwareTasks | 0 | Number of pending tasks...FIXME | [[maxRegisteredWaitingTimeMs]] maxRegisteredWaitingTimeMs | < > | | [[maxRpcMessageSize]] maxRpcMessageSize | < > but not greater than 2047 a| Maximum RPC message size in MB. When above 2047 MB you should see the following IllegalArgumentException : spark.rpc.message.maxSize should not be greater than 2047 MB | [[_minRegisteredRatio]] _minRegisteredRatio | < > | | [[numPendingExecutors]] numPendingExecutors | 0 | | [[totalCoreCount]] totalCoreCount | 0 | Total number of CPU cores, i.e. the sum of all the cores on all executors. | [[totalRegisteredExecutors]] totalRegisteredExecutors | 0 | Total number of registered executors |===","title":"CoarseGrainedSchedulerBackend"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#tip","text":"Enable INFO or DEBUG logging level for org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend=DEBUG","title":"[TIP]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#refer-to-spark-loggingmdlogging","text":"== [[killExecutorsOnHost]] Killing All Executors on Node -- killExecutorsOnHost Method CAUTION: FIXME == [[makeOffers]] Making Fake Resource Offers on Executors -- makeOffers Internal Methods","title":"Refer to spark-logging.md[Logging]."},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala","text":"makeOffers(): Unit makeOffers(executorId: String): Unit makeOffers takes the active executors (out of the < > internal registry) and creates WorkerOffer resource offers for each (one per executor with the executor's id, host and free cores). CAUTION: Only free cores are considered in making offers. Memory is not! Why?! It then requests scheduler:TaskSchedulerImpl.md#resourceOffers[ TaskSchedulerImpl to process the resource offers] to create a collection of spark-scheduler-TaskDescription.md[TaskDescription] collections that it in turn uses to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks]. == [[creating-instance]] Creating CoarseGrainedSchedulerBackend Instance CoarseGrainedSchedulerBackend takes the following when created: . [[scheduler]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] . [[rpcEnv]] rpc:index.md[RpcEnv] CoarseGrainedSchedulerBackend initializes the < >. == [[getExecutorIds]] Getting Executor Ids -- getExecutorIds Method When called, getExecutorIds simply returns executor ids from the internal < > registry. NOTE: It is called when ROOT:SparkContext.md#getExecutorIds[SparkContext calculates executor ids]. == [[contract]] CoarseGrainedSchedulerBackend Contract","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_1","text":"class CoarseGrainedSchedulerBackend { def minRegisteredRatio: Double def createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint def reset(): Unit def sufficientResourcesRegistered(): Boolean def doRequestTotalExecutors(requestedTotal: Int): Future[Boolean] def doKillExecutors(executorIds: Seq[String]): Future[Boolean] } NOTE: CoarseGrainedSchedulerBackend is a private[spark] contract. .FIXME Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[minRegisteredRatio]] minRegisteredRatio | Ratio between 0 and 1 (inclusive). Controlled by < >. | < > | FIXME | [[doRequestTotalExecutors]] doRequestTotalExecutors | FIXME | [[doKillExecutors]] doKillExecutors | FIXME | [[sufficientResourcesRegistered]] sufficientResourcesRegistered | Always positive, i.e. true , that means that sufficient resources are available. Used when CoarseGrainedSchedulerBackend < >. |=== It can < >. == [[numExistingExecutors]] numExistingExecutors Method CAUTION: FIXME == [[killExecutors]] killExecutors Methods CAUTION: FIXME == [[getDriverLogUrls]] getDriverLogUrls Method CAUTION: FIXME == [[applicationAttemptId]] applicationAttemptId Method CAUTION: FIXME == [[requestExecutors]] Requesting Additional Executors -- requestExecutors Method","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#requestexecutorsnumadditionalexecutors-int-boolean","text":"requestExecutors is a \"decorator\" method that ultimately calls a cluster-specific < > method and returns whether the request was acknowledged or not (it is assumed false by default). NOTE: requestExecutors method is part of spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient Contract] that ROOT:SparkContext.md#requestExecutors[SparkContext uses for requesting additional executors] (as a part of a developer API for dynamic allocation of executors). When called, you should see the following INFO message followed by DEBUG message in the logs: INFO Requesting [numAdditionalExecutors] additional executor(s) from the cluster manager DEBUG Number of pending executors is now [numPendingExecutors] < > is increased by the input numAdditionalExecutors . requestExecutors < > (that reflects the current computation needs). The \"new executor total\" is a sum of the internal < > and < > decreased by the < >. If numAdditionalExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of additional executor(s) [numAdditionalExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one. == [[requestTotalExecutors]] Requesting Exact Number of Executors -- requestTotalExecutors Method","title":"requestExecutors(numAdditionalExecutors: Int): Boolean"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_3","text":"requestTotalExecutors( numExecutors: Int, localityAwareTasks: Int, hostToLocalTaskCount: Map[String, Int]): Boolean requestTotalExecutors is a \"decorator\" method that ultimately calls a cluster-specific < > method and returns whether the request was acknowledged or not (it is assumed false by default). NOTE: requestTotalExecutors is part of spark-service-ExecutorAllocationClient.md[ExecutorAllocationClient Contract] that ROOT:SparkContext.md#requestTotalExecutors[SparkContext uses for requesting the exact number of executors]. It sets the internal < > and < > registries. It then calculates the exact number of executors which is the input numExecutors and the < > decreased by the number of < >. If numExecutors is negative, a IllegalArgumentException is thrown: Attempted to request a negative number of executor(s) [numExecutors] from the cluster manager. Please specify a positive number! NOTE: It is a final method that no other scheduler backends could customize further. NOTE: The method is a synchronized block that makes multiple concurrent requests be handled in a serial fashion, i.e. one by one. == [[defaultParallelism]] Finding Default Level of Parallelism -- defaultParallelism Method","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#defaultparallelism-int","text":"NOTE: defaultParallelism is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend Contract]. defaultParallelism is ROOT:configuration-properties.md#spark.default.parallelism[spark.default.parallelism] configuration property if defined. Otherwise, defaultParallelism is the maximum of < > or 2 . == [[killTask]] Killing Task -- killTask Method","title":"defaultParallelism(): Int"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#killtasktaskid-long-executorid-string-interruptthread-boolean-unit","text":"NOTE: killTask is part of the scheduler:SchedulerBackend.md#killTask[SchedulerBackend contract]. killTask simply sends a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#KillTask[KillTask] message to < >. CAUTION: FIXME Image == [[stopExecutors]] Stopping All Executors -- stopExecutors Method stopExecutors sends a blocking < > message to < > (if already initialized). NOTE: It is called exclusively while CoarseGrainedSchedulerBackend is < >. You should see the following INFO message in the logs: INFO CoarseGrainedSchedulerBackend: Shutting down all executors == [[reset]] Reset State -- reset Method reset resets the internal state: Sets < > to 0 Clears executorsPendingToRemove Sends a blocking < > message to < > for every executor (in the internal executorDataMap ) to inform it about SlaveLost with the message: + Stale executor after cluster manager re-registered. reset is a method that is defined in CoarseGrainedSchedulerBackend , but used and overriden exclusively by yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend]. == [[removeExecutor]] Remove Executor -- removeExecutor Method","title":"killTask(taskId: Long, executorId: String, interruptThread: Boolean): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#removeexecutorexecutorid-string-reason-executorlossreason","text":"removeExecutor sends a blocking < > message to < >. NOTE: It is called by subclasses spark-standalone.md#SparkDeploySchedulerBackend[SparkDeploySchedulerBackend], spark-mesos/spark-mesos.md#CoarseMesosSchedulerBackend[CoarseMesosSchedulerBackend], and yarn/spark-yarn-yarnschedulerbackend.md[YarnSchedulerBackend]. == [[CoarseGrainedScheduler]] CoarseGrainedScheduler RPC Endpoint -- driverEndpoint When < >, it registers CoarseGrainedScheduler RPC endpoint to be the driver's communication endpoint. driverEndpoint is a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint]. NOTE: CoarseGrainedSchedulerBackend is created while ROOT:SparkContext.md#createTaskScheduler[SparkContext is being created] that in turn lives inside a spark-driver.md[Spark driver]. That explains the name driverEndpoint (at least partially). It is called standalone scheduler's driver endpoint internally. It tracks: It uses driver-revive-thread daemon single-thread thread pool for ...FIXME CAUTION: FIXME A potential issue with driverEndpoint.asInstanceOf[NettyRpcEndpointRef].toURI - doubles spark:// prefix. == [[start]] Starting CoarseGrainedSchedulerBackend (and Registering CoarseGrainedScheduler RPC Endpoint) -- start Method","title":"removeExecutor(executorId: String, reason: ExecutorLossReason)"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#start-unit","text":"NOTE: start is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend contract]. start takes all spark. -prefixed properties and registers the < CoarseGrainedScheduler RPC endpoint>> (backed by scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint ThreadSafeRpcEndpoint]). .CoarseGrainedScheduler Endpoint image::CoarseGrainedScheduler-rpc-endpoint.png[align=\"center\"] NOTE: start uses < > to access the current ROOT:SparkContext.md[SparkContext] and in turn ROOT:SparkConf.md[SparkConf]. NOTE: start uses < > that was given when < CoarseGrainedSchedulerBackend was created>>. == [[isReady]] Checking If Sufficient Compute Resources Available Or Waiting Time Passed -- isReady Method","title":"start(): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#isready-boolean","text":"NOTE: isReady is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend contract]. isReady allows to delay task launching until < > or < > passes. Internally, isReady < >. NOTE: < > by default responds that sufficient resources are available. If the < >, you should see the following INFO message in the logs and isReady is positive.","title":"isReady(): Boolean"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#info-schedulerbackend-is-ready-for-scheduling-beginning-after-reached-minregisteredresourcesratio-minregisteredratio","text":"NOTE: < > is in the range 0 to 1 (uses < >) to denote the minimum ratio of registered resources to total expected resources before submitting tasks. If there are no sufficient resources available yet (the above requirement does not hold), isReady checks whether the time since < > passed < > to give a way to launch tasks (even when < > not being reached yet). You should see the following INFO message in the logs and isReady is positive.","title":"INFO SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: [minRegisteredRatio]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#optionswrap_1","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#info-schedulerbackend-is-ready-for-scheduling-beginning-after-waiting-maxregisteredresourceswaitingtime-maxregisteredwaitingtimems","text":"Otherwise, when < > and < > has not elapsed, isReady is negative. == [[reviveOffers]] Reviving Resource Offers (by Posting ReviveOffers to CoarseGrainedSchedulerBackend RPC Endpoint) -- reviveOffers Method","title":"INFO SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: maxRegisteredWaitingTimeMs"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#reviveoffers-unit","text":"NOTE: reviveOffers is part of the scheduler:SchedulerBackend.md#reviveOffers[SchedulerBackend contract]. reviveOffers simply sends a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#ReviveOffers[ReviveOffers] message to < CoarseGrainedSchedulerBackend RPC endpoint>>. .CoarseGrainedExecutorBackend Revives Offers image::CoarseGrainedExecutorBackend-reviveOffers.png[align=\"center\"] == [[stop]] Stopping CoarseGrainedSchedulerBackend (and Stopping Executors) -- stop Method","title":"reviveOffers(): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#stop-unit","text":"NOTE: stop is part of the scheduler:SchedulerBackend.md#contract[SchedulerBackend contract]. stop < > and < CoarseGrainedScheduler RPC endpoint>> (by sending a blocking scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StopDriver[StopDriver] message). In case of any Exception , stop reports a SparkException with the message: Error stopping standalone scheduler's driver endpoint == [[createDriverEndpointRef]] createDriverEndpointRef Method","title":"stop(): Unit"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_11","text":"createDriverEndpointRef( properties: ArrayBuffer[(String, String)]): RpcEndpointRef createDriverEndpointRef < > and rpc:index.md#setupEndpoint[registers it] as CoarseGrainedScheduler . createDriverEndpointRef is used when CoarseGrainedSchedulerBackend is requested to < >. == [[createDriverEndpoint]] Creating DriverEndpoint -- createDriverEndpoint Method","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/CoarseGrainedSchedulerBackend/#createdriverendpointproperties-seqstring-string-driverendpoint","text":"createDriverEndpoint simply creates a scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#creating-instance[DriverEndpoint]. NOTE: scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md[DriverEndpoint] is the < CoarseGrainedSchedulerBackend >>. NOTE: The purpose of createDriverEndpoint is to allow YARN to use the custom YarnDriverEndpoint . NOTE: createDriverEndpoint is used when CoarseGrainedSchedulerBackend < >.","title":"createDriverEndpoint(properties: Seq[(String, String)]): DriverEndpoint"},{"location":"scheduler/DAGScheduler/","text":"DAGScheduler \u00b6 Note The introduction that follows was highly influenced by the scaladoc of org.apache.spark.scheduler.DAGScheduler . As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read the sources and only then read this and the related pages afterwards. Introduction \u00b6 DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling using Jobs and Stages . DAGScheduler transforms a logical execution plan ( RDD lineage of dependencies built using RDD transformations ) to a physical execution plan (using stages ). After an action has been called on an RDD , SparkContext hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as TaskSets for execution. DAGScheduler works solely on the driver and is created as part of SparkContext's initialization (right after TaskScheduler and SchedulerBackend are ready). DAGScheduler does three things in Spark: Computes an execution DAG (DAG of stages) for a job Determines the preferred locations to run each task on Handles failures due to shuffle output files being lost DAGScheduler computes a directed acyclic graph (DAG) of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to TaskScheduler . In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to TaskScheduler . DAGScheduler tracks which rdd/spark-rdd-caching.md[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what ShuffleMapStage.md[ShuffleMapStage]s have already produced output files (that are stored in BlockManager s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses SparkContext , TaskScheduler , LiveListenerBus.md[], MapOutputTracker.md[MapOutputTracker] and storage:BlockManager.md[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of rdd/index.md#actions[executing an action on a RDD] or calling SparkContext.runJob() method directly , it spawns parallel tasks to compute (partial) results per partition. Creating Instance \u00b6 DAGScheduler takes the following to be created: SparkContext TaskScheduler LiveListenerBus MapOutputTrackerMaster BlockManagerMaster SparkEnv Clock DAGScheduler is created when SparkContext is created. While being created, DAGScheduler requests the TaskScheduler to associate itself with and requests DAGScheduler Event Bus to start accepting events. DAGScheduler Event Bus \u00b6 DAGScheduler uses an event bus to process scheduling events on a separate thread (one by one and asynchronously). DAGScheduler requests the event bus to start right when created and stops it when requested to stop . DAGScheduler defines event-posting methods for posting DAGSchedulerEvent events to the event bus. TaskScheduler \u00b6 DAGScheduler is given a TaskScheduler when created . TaskScheduler is used for the following: Submitting missing tasks of a stage Handling task completion (CompletionEvent) Killing a task Failing a job and all other independent single-job stages Stopping itself Running Job \u00b6 runJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ) : Unit runJob submits a job and waits until a result is available. runJob prints out the following INFO message to the logs when the job has finished successfully: Job [jobId] finished: [callSite], took [time] s runJob prints out the following INFO message to the logs when the job has failed: Job [jobId] failed: [callSite], took [time] s runJob is used when SparkContext is requested to run a job . Submitting Job \u00b6 submitJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ) : JobWaiter [ U ] submitJob increments the nextJobId internal counter. submitJob creates a JobWaiter for the (number of) partitions and the given resultHandler function. submitJob requests the DAGSchedulerEventProcessLoop to post a JobSubmitted . In the end, submitJob returns the JobWaiter . For empty partitions (no partitions to compute), submitJob requests the LiveListenerBus to post a SparkListenerJobStart and SparkListenerJobEnd (with JobSucceeded result marker) events and returns a JobWaiter with no tasks to wait for. submitJob throws an IllegalArgumentException when the partitions indices are not among the partitions of the given RDD : Attempting to access a non-existent partition: [p]. Total number of partitions: [maxPartitions] submitJob is used when: SparkContext is requested to submit a job DAGScheduler is requested to run a job Partition Placement Preferences \u00b6 DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses TaskLocation that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the BlockManager s of the blocks. Initialized empty when DAGScheduler is created . Used when DAGScheduler is requested for the locations of the cache blocks of a RDD . ActiveJobs \u00b6 DAGScheduler tracks ActiveJob s: Adds a new ActiveJob when requested to handle JobSubmitted or MapStageSubmitted events Removes an ActiveJob when requested to clean up after an ActiveJob and independent stages . Removes all ActiveJobs when requested to doCancelAllJobs . DAGScheduler uses ActiveJobs registry when requested to handle JobGroupCancelled or TaskCompletion events, to cleanUpAfterSchedulerStop and to abort a stage . The number of ActiveJobs is available using job.activeJobs performance metric. Creating ResultStage for RDD \u00b6 createResultStage ( rdd : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], jobId : Int , callSite : CallSite ) : ResultStage createResultStage ...FIXME createResultStage is used when DAGScheduler is requested to handle a JobSubmitted event . Creating ShuffleMapStage for ShuffleDependency \u00b6 createShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], jobId : Int ) : ShuffleMapStage createShuffleMapStage creates a ShuffleMapStage for the given ShuffleDependency as follows: Stage ID is generated based on nextStageId internal counter RDD is taken from the given ShuffleDependency Number of tasks is the number of partitions of the RDD Parent RDDs MapOutputTrackerMaster createShuffleMapStage registers the ShuffleMapStage in the stageIdToStage and shuffleIdToMapStage internal registries. createShuffleMapStage updateJobIdStageIdMaps . createShuffleMapStage requests the MapOutputTrackerMaster to check whether it contains the shuffle ID or not . If not, createShuffleMapStage prints out the following INFO message to the logs and requests the MapOutputTrackerMaster to register the shuffle . Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId] createShuffleMapStage is used when DAGScheduler is requested to find or create a ShuffleMapStage for a given ShuffleDependency . Cleaning Up After Job and Independent Stages \u00b6 cleanupStateForJobAndIndependentStages ( job : ActiveJob ) : Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. ResultStage or ShuffleMapStage . cleanupStateForJobAndIndependentStages is used in handleTaskCompletion when a ResultTask has completed successfully , failJobAndIndependentStages and markMapStageJobAsFinished . Marking ShuffleMapStage Job Finished \u00b6 markMapStageJobAsFinished ( job : ActiveJob , stats : MapOutputStatistics ) : Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The job listener is notified about the 0 th task succeeded . The < job and independent stages are cleaned up>>. Ultimately, SparkListenerJobEnd is posted to LiveListenerBus (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in handleMapStageSubmitted and handleTaskCompletion . Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD \u00b6 getOrCreateParentStages ( rdd : RDD [ _ ], firstJobId : Int ) : List [ Stage ] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each ShuffleDependency . getOrCreateParentStages is used when DAGScheduler is requested to create a ShuffleMapStage or a ResultStage . Marking Stage Finished \u00b6 markStageAsFinished ( stage : Stage , errorMessage : Option [ String ] = None , willRetry : Boolean = false ) : Unit markStageAsFinished ...FIXME markStageAsFinished is used when...FIXME Finding or Creating ShuffleMapStage for ShuffleDependency \u00b6 getOrCreateShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], firstJobId : Int ) : ShuffleMapStage getOrCreateShuffleMapStage finds a ShuffleMapStage by the shuffleId of the given ShuffleDependency in the shuffleIdToMapStage internal registry and returns it if available. If not found, getOrCreateShuffleMapStage finds all the missing ancestor shuffle dependencies and creates the missing ShuffleMapStage stages (including one for the input ShuffleDependency ). getOrCreateShuffleMapStage is used when DAGScheduler is requested to find or create missing direct parent ShuffleMapStages of an RDD , find missing parent ShuffleMapStages for a stage , handle a MapStageSubmitted event , and check out stage dependency on a stage . Finding Missing ShuffleDependencies For RDD \u00b6 getMissingAncestorShuffleDependencies ( rdd : RDD [ _ ]) : Stack [ ShuffleDependency [ _ , _ , _ ]] getMissingAncestorShuffleDependencies finds all missing shuffle dependencies for the given RDD traversing its rdd/spark-rdd-lineage.md[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to find all ShuffleMapStage stages for a ShuffleDependency . Finding Direct Parent Shuffle Dependencies of RDD \u00b6 getShuffleDependencies ( rdd : RDD [ _ ]) : HashSet [ ShuffleDependency [ _ , _ , _ ]] getShuffleDependencies finds direct parent shuffle dependencies for the given RDD . Internally, getShuffleDependencies takes the direct rdd/index.md#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the dependency chain (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to find or create missing direct parent ShuffleMapStages (for ShuffleDependencies of a RDD) and find all missing shuffle dependencies for a given RDD . Failing Job and Independent Single-Job Stages \u00b6 failJobAndIndependentStages ( job : ActiveJob , failureReason : String , exception : Option [ Throwable ] = None ) : Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), TaskScheduler.md#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME Aborting Stage \u00b6 abortStage ( failedStage : Stage , reason : String , exception : Option [ Throwable ]) : Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's spark-scheduler-StageInfo.md[StageInfo]) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs: Ignoring failure of [failedStage] because all jobs depending on it are done abortStage is used when DAGScheduler is requested to handle a TaskSetFailed event , submit a stage , submit missing tasks of a stage , handle a TaskCompletion event . Checking Out Stage Dependency on Given Stage \u00b6 stageDependsOn ( stage : Stage , target : Stage ) : Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a NarrowDependency to a stack of RDDs to visit while for a ShuffleDependency it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to abort a stage . Submitting Waiting Child Stages for Execution \u00b6 submitWaitingChildStages ( parent : Stage ) : Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent Stage.md[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to submits missing tasks for a stage and handles a successful ShuffleMapTask completion . Submitting Stage (with Missing Parents) for Execution \u00b6 submitStage ( stage : Stage ) : Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to DAGSchedulerEventProcessLoop.md#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: < > (ResubmitFailedStages event) < > (CompletionEvent event) Handle < >, < > and < > events Stage Attempts \u00b6 A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple spark-scheduler-StageInfo.md[StageInfo] objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo . Preferred Locations \u00b6 DAGScheduler computes where to run each task in a stage based on the rdd/index.md#getPreferredLocations[preferred locations of its underlying RDDs], or < >. Adaptive Query Planning / Adaptive Scheduling \u00b6 See SPARK-9850 Adaptive execution in Spark for the design document. The work is currently in progress. DAGScheduler.submitMapStage method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages. ScheduledExecutorService daemon services \u00b6 DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a DAGSchedulerEventProcessLoop.md#ResubmitFailedStages[ResubmitFailedStages] event when DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory. Finding Missing Parent ShuffleMapStages For Stage \u00b6 getMissingParentStages ( stage : Stage ) : List [ Stage ] getMissingParentStages finds missing parent ShuffleMapStage s in the dependency graph of the input stage (using the breadth-first search algorithm ). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using Stage.md#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active storage:BlockManager.md[BlockManager]s on executors). getMissingParentStages traverses the rdd/index.md#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. ShuffleDependency or NarrowDependency . NOTE: ShuffleDependency and NarrowDependency are the main top-level Dependencies . For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: NarrowDependency is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: ShuffleDependency is a RDD dependency that represents a dependency on the output of a ShuffleMapStage , i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to submit a stage and handle JobSubmitted and MapStageSubmitted events. Submitting Missing Tasks of Stage \u00b6 submitMissingTasks ( stage : Stage , jobId : Int ) : Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given Stage for the missing partitions (partitions that need to be computed). submitMissingTasks adds the stage to the runningStages internal registry. submitMissingTasks notifies the OutputCommitCoordinator that stage execution started . \u00b6 submitMissingTasks determines preferred locations ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a new stage attempt . submitMissingTasks requests the LiveListenerBus to post a SparkListenerStageSubmitted event. submitMissingTasks uses the closure Serializer to serialize the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage ( ShuffleMapStage or ResultStage , respectively). submitMissingTasks creates a broadcast variable for the task binary. Note That shows how important Broadcast s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates tasks for every missing partition: ShuffleMapTasks for a ShuffleMapStage ResultTasks for a ResultStage If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to TaskScheduler.md#submitTasks[submit the tasks for execution] (as a new TaskSet.md[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to submit a stage for execution . Finding Preferred Locations for Missing Partitions \u00b6 getPreferredLocs ( rdd : RDD [ _ ], partition : Int ) : Seq [ TaskLocation ] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery) \u00b6 getCacheLocs ( rdd : RDD [ _ ]) : IndexedSeq [ Seq [ TaskLocation ]] getCacheLocs gives TaskLocations (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every TaskLocation collection (i.e. every entry in the result of getCacheLocs ) is exactly the number of blocks managed using storage:BlockManager.md[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its storage:StorageLevel.md[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs storage:BlockManagerMaster.md#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to TaskLocations with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as TaskLocation ) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using storage:BlockId.md#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses TaskLocation.md[TaskLocations] (with host and executor) while storage:BlockManagerMaster.md[BlockManagerMaster] uses storage:BlockManagerId.md[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to find missing parent MapStages and getPreferredLocsInternal . Finding Placement Preferences for RDD Partition (recursively) \u00b6 getPreferredLocsInternal ( rdd : RDD [ _ ], partition : Int , visited : HashSet [( RDD [ _ ] , Int )]) : Seq [ TaskLocation ] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal rdd/index.md#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent NarrowDependency and (recursively) finds TaskLocations . If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of TaskLocation.md[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the preferred locations for missing partitions . Stopping DAGScheduler \u00b6 stop () : Unit stop stops the internal dag-scheduler-message thread pool, dag-scheduler-event-loop , and TaskScheduler . stop is used when SparkContext is requested to stop . Updating Accumulators with Partial Values from Completed Tasks \u00b6 updateAccumulators ( event : CompletionEvent ) : Unit updateAccumulators merges the partial values of accumulators from a completed task into their \"source\" accumulators on the driver. NOTE: It is called by < >. For each spark-accumulators.md#AccumulableInfo[AccumulableInfo] in the CompletionEvent , a partial value from a task is obtained (from AccumulableInfo.update ) and added to the driver's accumulator (using Accumulable.++= method). For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new spark-accumulators.md#AccumulableInfo[AccumulableInfo] added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to handle a task completion . checkBarrierStageWithNumSlots \u00b6 checkBarrierStageWithNumSlots ( rdd : RDD [ _ ]) : Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages. Killing Task \u00b6 killTaskAttempt ( taskId : Long , interruptThread : Boolean , reason : String ) : Boolean killTaskAttempt requests the TaskScheduler to kill a task . killTaskAttempt is used when SparkContext is requested to kill a task . cleanUpAfterSchedulerStop \u00b6 cleanUpAfterSchedulerStop () : Unit cleanUpAfterSchedulerStop ...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to onStop . removeExecutorAndUnregisterOutputs \u00b6 removeExecutorAndUnregisterOutputs ( execId : String , fileLost : Boolean , hostToUnregisterOutputs : Option [ String ], maybeEpoch : Option [ Long ] = None ) : Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events. markMapStageJobsAsFinished \u00b6 markMapStageJobsAsFinished ( shuffleStage : ShuffleMapStage ) : Unit markMapStageJobsAsFinished ...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to submit missing tasks (of a ShuffleMapStage that has just been computed) and handle a task completion (of a ShuffleMapStage ). updateJobIdStageIdMaps \u00b6 updateJobIdStageIdMaps ( jobId : Int , stage : Stage ) : Unit updateJobIdStageIdMaps ...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create ShuffleMapStage and ResultStage stages. executorHeartbeatReceived \u00b6 executorHeartbeatReceived ( execId : String , // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates : Array [( Long , Int , Int , Seq [ AccumulableInfo ])], blockManagerId : BlockManagerId , // (stageId, stageAttemptId) -> metrics executorUpdates : mutable.Map [( Int , Int ) , ExecutorMetrics ]) : Boolean executorHeartbeatReceived posts a SparkListenerExecutorMetricsUpdate (to listenerBus ) and informs BlockManagerMaster that blockManagerId block manager is alive (by posting BlockManagerHeartbeat ). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to handle an executor heartbeat . postTaskEnd \u00b6 postTaskEnd ( event : CompletionEvent ) : Unit postTaskEnd ...FIXME postTaskEnd is used when DAGScheduler is requested to handle a task completion . Event Handlers \u00b6 AllJobsCancelled Event Handler \u00b6 doCancelAllJobs () : Unit doCancelAllJobs ...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an AllJobsCancelled event and onError . BeginEvent Event Handler \u00b6 handleBeginEvent ( task : Task [ _ ], taskInfo : TaskInfo ) : Unit handleBeginEvent ...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a BeginEvent event. Handling CompletionEvent \u00b6 handleTaskCompletion ( event : CompletionEvent ) : Unit handleTaskCompletion ...FIXME handleTaskCompletion is used when DAGSchedulerEventProcessLoop is requested to handle a CompletionEvent event. ExecutorAdded Event Handler \u00b6 handleExecutorAdded ( execId : String , host : String ) : Unit handleExecutorAdded ...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorAdded event. ExecutorLost Event Handler \u00b6 handleExecutorLost ( execId : String , workerLost : Boolean ) : Unit handleExecutorLost ...FIXME handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorLost event. GettingResultEvent Event Handler \u00b6 handleGetTaskResult ( taskInfo : TaskInfo ) : Unit handleGetTaskResult ...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a GettingResultEvent event. JobCancelled Event Handler \u00b6 handleJobCancellation ( jobId : Int , reason : Option [ String ]) : Unit handleJobCancellation ...FIXME handleJobCancellation is used when DAGScheduler is requested to handle a JobCancelled event, doCancelAllJobs , handleJobGroupCancelled , handleStageCancellation . JobGroupCancelled Event Handler \u00b6 handleJobGroupCancelled ( groupId : String ) : Unit handleJobGroupCancelled ...FIXME handleJobGroupCancelled is used when DAGScheduler is requested to handle JobGroupCancelled event. Handling JobSubmitted Event \u00b6 handleJobSubmitted ( jobId : Int , finalRDD : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], callSite : CallSite , listener : JobListener , properties : Properties ) : Unit handleJobSubmitted creates a ResultStage (as finalStage in the picture below) for the given RDD, func , partitions , jobId and callSite . handleJobSubmitted creates an ActiveJob for the ResultStage . handleJobSubmitted clears the internal cache of RDD partition locations . Important FIXME Why is this clearing here so important? handleJobSubmitted prints out the following INFO messages to the logs (with missingParentStages ): Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingParentStages] handleJobSubmitted registers the new ActiveJob in jobIdToActiveJob and activeJobs internal registries. handleJobSubmitted requests the ResultStage to associate itself with the ActiveJob . handleJobSubmitted uses the jobIdToStageIds internal registry to find all registered stages for the given jobId . handleJobSubmitted uses the stageIdToStage internal registry to request the Stages for the latestInfo . In the end, handleJobSubmitted posts a SparkListenerJobStart message to the LiveListenerBus and submits the ResultStage . handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a JobSubmitted event. MapStageSubmitted Event Handler \u00b6 handleMapStageSubmitted ( jobId : Int , dependency : ShuffleDependency [ _ , _ , _ ], callSite : CallSite , listener : JobListener , properties : Properties ) : Unit handleMapStageSubmitted ...FIXME handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a MapStageSubmitted event. ResubmitFailedStages Event Handler \u00b6 resubmitFailedStages () : Unit resubmitFailedStages ...FIXME resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a ResubmitFailedStages event. SpeculativeTaskSubmitted Event Handler \u00b6 handleSpeculativeTaskSubmitted () : Unit handleSpeculativeTaskSubmitted ...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a SpeculativeTaskSubmitted event. StageCancelled Event Handler \u00b6 handleStageCancellation () : Unit handleStageCancellation ...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a StageCancelled event. TaskSetFailed Event Handler \u00b6 handleTaskSetFailed () : Unit handleTaskSetFailed ...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a TaskSetFailed event. WorkerRemoved Event Handler \u00b6 handleWorkerRemoved ( workerId : String , host : String , message : String ) : Unit handleWorkerRemoved ...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a WorkerRemoved event. Internal Properties \u00b6 failedEpoch \u00b6 The lookup table of lost executors and the epoch of the event. failedStages \u00b6 Stages that failed due to fetch failures (when a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]). jobIdToActiveJob \u00b6 The lookup table of ActiveJob s per job id. jobIdToStageIds \u00b6 The lookup table of all stages per ActiveJob id metricsSource \u00b6 DAGSchedulerSource nextJobId Counter \u00b6 nextJobId : AtomicInteger nextJobId is a Java AtomicInteger for job IDs. nextJobId starts at 0 . Used when DAGScheduler is requested for numTotalJobs , to submitJob , runApproximateJob and submitMapStage . nextStageId \u00b6 The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >. runningStages \u00b6 The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added). shuffleIdToMapStage \u00b6 A lookup table of ShuffleMapStage s by ShuffleDependency stageIdToStage \u00b6 A lookup table of stages by stage ID Used when DAGScheduler creates a shuffle map stage , creates a result stage , cleans up job state and independent stages , is informed that a task is started , a taskset has failed , a job is submitted (to compute a ResultStage ) , a map stage was submitted , a task has completed or a stage was cancelled , updates accumulators , aborts a stage and fails a job and independent stages . waitingStages \u00b6 Stages with parents to be computed Event Posting Methods \u00b6 Posting AllJobsCancelled \u00b6 Posts an AllJobsCancelled Used when SparkContext is requested to cancel all running or scheduled Spark jobs Posting JobCancelled \u00b6 Posts a JobCancelled Used when SparkContext or JobWaiter are requested to cancel a Spark job Posting JobGroupCancelled \u00b6 Posts a JobGroupCancelled Used when SparkContext is requested to cancel a job group Posting StageCancelled \u00b6 Posts a StageCancelled Used when SparkContext is requested to cancel a stage Posting ExecutorAdded \u00b6 Posts an ExecutorAdded Used when TaskSchedulerImpl is requested to handle resource offers (and a new executor is found in the resource offers) Posting ExecutorLost \u00b6 Posts a ExecutorLost Used when TaskSchedulerImpl is requested to handle a task status update (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or executorLost Posting JobSubmitted \u00b6 Posts a JobSubmitted Used when SparkContext is requested to run an approximate job Posting SpeculativeTaskSubmitted \u00b6 Posts a SpeculativeTaskSubmitted Used when TaskSetManager is requested to checkAndSubmitSpeculatableTask Posting MapStageSubmitted \u00b6 Posts a MapStageSubmitted Used when SparkContext is requested to submit a MapStage for execution Posting CompletionEvent \u00b6 Posts a CompletionEvent Used when TaskSetManager is requested to handleSuccessfulTask , handleFailedTask , and executorLost Posting GettingResultEvent \u00b6 Posts a GettingResultEvent Used when TaskSetManager is requested to handle a task fetching result Posting TaskSetFailed \u00b6 Posts a TaskSetFailed Used when TaskSetManager is requested to abort Posting BeginEvent \u00b6 Posts a BeginEvent Used when TaskSetManager is requested to start a task Posting WorkerRemoved \u00b6 Posts a WorkerRemoved Used when TaskSchedulerImpl is requested to handle a removed worker event Logging \u00b6 Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL Refer to Logging .","title":"DAGScheduler"},{"location":"scheduler/DAGScheduler/#dagscheduler","text":"Note The introduction that follows was highly influenced by the scaladoc of org.apache.spark.scheduler.DAGScheduler . As DAGScheduler is a private class it does not appear in the official API documentation. You are strongly encouraged to read the sources and only then read this and the related pages afterwards.","title":"DAGScheduler"},{"location":"scheduler/DAGScheduler/#introduction","text":"DAGScheduler is the scheduling layer of Apache Spark that implements stage-oriented scheduling using Jobs and Stages . DAGScheduler transforms a logical execution plan ( RDD lineage of dependencies built using RDD transformations ) to a physical execution plan (using stages ). After an action has been called on an RDD , SparkContext hands over a logical plan to DAGScheduler that it in turn translates to a set of stages that are submitted as TaskSets for execution. DAGScheduler works solely on the driver and is created as part of SparkContext's initialization (right after TaskScheduler and SchedulerBackend are ready). DAGScheduler does three things in Spark: Computes an execution DAG (DAG of stages) for a job Determines the preferred locations to run each task on Handles failures due to shuffle output files being lost DAGScheduler computes a directed acyclic graph (DAG) of stages for each job, keeps track of which RDDs and stage outputs are materialized, and finds a minimal schedule to run jobs. It then submits stages to TaskScheduler . In addition to coming up with the execution DAG, DAGScheduler also determines the preferred locations to run each task on, based on the current cache status, and passes the information to TaskScheduler . DAGScheduler tracks which rdd/spark-rdd-caching.md[RDDs are cached (or persisted)] to avoid \"recomputing\" them, i.e. redoing the map side of a shuffle. DAGScheduler remembers what ShuffleMapStage.md[ShuffleMapStage]s have already produced output files (that are stored in BlockManager s). DAGScheduler is only interested in cache location coordinates, i.e. host and executor id, per partition of a RDD. Furthermore, it handles failures due to shuffle output files being lost, in which case old stages may need to be resubmitted. Failures within a stage that are not caused by shuffle file loss are handled by the TaskScheduler itself, which will retry each task a small number of times before cancelling the whole stage. DAGScheduler uses an event queue architecture in which a thread can post DAGSchedulerEvent events, e.g. a new job or stage being submitted, that DAGScheduler reads and executes sequentially. See the section < >. DAGScheduler runs stages in topological order. DAGScheduler uses SparkContext , TaskScheduler , LiveListenerBus.md[], MapOutputTracker.md[MapOutputTracker] and storage:BlockManager.md[BlockManager] for its services. However, at the very minimum, DAGScheduler takes a SparkContext only (and requests SparkContext for the other services). When DAGScheduler schedules a job as a result of rdd/index.md#actions[executing an action on a RDD] or calling SparkContext.runJob() method directly , it spawns parallel tasks to compute (partial) results per partition.","title":"Introduction"},{"location":"scheduler/DAGScheduler/#creating-instance","text":"DAGScheduler takes the following to be created: SparkContext TaskScheduler LiveListenerBus MapOutputTrackerMaster BlockManagerMaster SparkEnv Clock DAGScheduler is created when SparkContext is created. While being created, DAGScheduler requests the TaskScheduler to associate itself with and requests DAGScheduler Event Bus to start accepting events.","title":"Creating Instance"},{"location":"scheduler/DAGScheduler/#dagscheduler-event-bus","text":"DAGScheduler uses an event bus to process scheduling events on a separate thread (one by one and asynchronously). DAGScheduler requests the event bus to start right when created and stops it when requested to stop . DAGScheduler defines event-posting methods for posting DAGSchedulerEvent events to the event bus.","title":" DAGScheduler Event Bus"},{"location":"scheduler/DAGScheduler/#taskscheduler","text":"DAGScheduler is given a TaskScheduler when created . TaskScheduler is used for the following: Submitting missing tasks of a stage Handling task completion (CompletionEvent) Killing a task Failing a job and all other independent single-job stages Stopping itself","title":" TaskScheduler"},{"location":"scheduler/DAGScheduler/#running-job","text":"runJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ) : Unit runJob submits a job and waits until a result is available. runJob prints out the following INFO message to the logs when the job has finished successfully: Job [jobId] finished: [callSite], took [time] s runJob prints out the following INFO message to the logs when the job has failed: Job [jobId] failed: [callSite], took [time] s runJob is used when SparkContext is requested to run a job .","title":" Running Job"},{"location":"scheduler/DAGScheduler/#submitting-job","text":"submitJob [ T , U ]( rdd : RDD [ T ], func : ( TaskContext , Iterator [ T ]) => U , partitions : Seq [ Int ], callSite : CallSite , resultHandler : ( Int , U ) => Unit , properties : Properties ) : JobWaiter [ U ] submitJob increments the nextJobId internal counter. submitJob creates a JobWaiter for the (number of) partitions and the given resultHandler function. submitJob requests the DAGSchedulerEventProcessLoop to post a JobSubmitted . In the end, submitJob returns the JobWaiter . For empty partitions (no partitions to compute), submitJob requests the LiveListenerBus to post a SparkListenerJobStart and SparkListenerJobEnd (with JobSucceeded result marker) events and returns a JobWaiter with no tasks to wait for. submitJob throws an IllegalArgumentException when the partitions indices are not among the partitions of the given RDD : Attempting to access a non-existent partition: [p]. Total number of partitions: [maxPartitions] submitJob is used when: SparkContext is requested to submit a job DAGScheduler is requested to run a job","title":" Submitting Job"},{"location":"scheduler/DAGScheduler/#partition-placement-preferences","text":"DAGScheduler keeps track of block locations per RDD and partition. DAGScheduler uses TaskLocation that includes a host name and an executor id on that host (as ExecutorCacheTaskLocation ). The keys are RDDs (their ids) and the values are arrays indexed by partition numbers. Each entry is a set of block locations where a RDD partition is cached, i.e. the BlockManager s of the blocks. Initialized empty when DAGScheduler is created . Used when DAGScheduler is requested for the locations of the cache blocks of a RDD .","title":" Partition Placement Preferences"},{"location":"scheduler/DAGScheduler/#activejobs","text":"DAGScheduler tracks ActiveJob s: Adds a new ActiveJob when requested to handle JobSubmitted or MapStageSubmitted events Removes an ActiveJob when requested to clean up after an ActiveJob and independent stages . Removes all ActiveJobs when requested to doCancelAllJobs . DAGScheduler uses ActiveJobs registry when requested to handle JobGroupCancelled or TaskCompletion events, to cleanUpAfterSchedulerStop and to abort a stage . The number of ActiveJobs is available using job.activeJobs performance metric.","title":" ActiveJobs"},{"location":"scheduler/DAGScheduler/#creating-resultstage-for-rdd","text":"createResultStage ( rdd : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], jobId : Int , callSite : CallSite ) : ResultStage createResultStage ...FIXME createResultStage is used when DAGScheduler is requested to handle a JobSubmitted event .","title":" Creating ResultStage for RDD"},{"location":"scheduler/DAGScheduler/#creating-shufflemapstage-for-shuffledependency","text":"createShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], jobId : Int ) : ShuffleMapStage createShuffleMapStage creates a ShuffleMapStage for the given ShuffleDependency as follows: Stage ID is generated based on nextStageId internal counter RDD is taken from the given ShuffleDependency Number of tasks is the number of partitions of the RDD Parent RDDs MapOutputTrackerMaster createShuffleMapStage registers the ShuffleMapStage in the stageIdToStage and shuffleIdToMapStage internal registries. createShuffleMapStage updateJobIdStageIdMaps . createShuffleMapStage requests the MapOutputTrackerMaster to check whether it contains the shuffle ID or not . If not, createShuffleMapStage prints out the following INFO message to the logs and requests the MapOutputTrackerMaster to register the shuffle . Registering RDD [id] ([creationSite]) as input to shuffle [shuffleId] createShuffleMapStage is used when DAGScheduler is requested to find or create a ShuffleMapStage for a given ShuffleDependency .","title":" Creating ShuffleMapStage for ShuffleDependency"},{"location":"scheduler/DAGScheduler/#cleaning-up-after-job-and-independent-stages","text":"cleanupStateForJobAndIndependentStages ( job : ActiveJob ) : Unit cleanupStateForJobAndIndependentStages cleans up the state for job and any stages that are not part of any other job. cleanupStateForJobAndIndependentStages looks the job up in the internal < > registry. If no stages are found, the following ERROR is printed out to the logs: No stages registered for job [jobId] Oterwise, cleanupStateForJobAndIndependentStages uses < > registry to find the stages (the real objects not ids!). For each stage, cleanupStateForJobAndIndependentStages reads the jobs the stage belongs to. If the job does not belong to the jobs of the stage, the following ERROR is printed out to the logs: Job [jobId] not registered for stage [stageId] even though that stage was registered for the job If the job was the only job for the stage, the stage (and the stage id) gets cleaned up from the registries, i.e. < >, < >, < >, < > and < >. While removing from < >, you should see the following DEBUG message in the logs: Removing running stage [stageId] While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from waiting set. While removing from < >, you should see the following DEBUG message in the logs: Removing stage [stageId] from failed set. After all cleaning (using < > as the source registry), if the stage belonged to the one and only job , you should see the following DEBUG message in the logs: After removal of stage [stageId], remaining stages = [stageIdToStage.size] The job is removed from < >, < >, < > registries. The final stage of the job is removed, i.e. ResultStage or ShuffleMapStage . cleanupStateForJobAndIndependentStages is used in handleTaskCompletion when a ResultTask has completed successfully , failJobAndIndependentStages and markMapStageJobAsFinished .","title":" Cleaning Up After Job and Independent Stages"},{"location":"scheduler/DAGScheduler/#marking-shufflemapstage-job-finished","text":"markMapStageJobAsFinished ( job : ActiveJob , stats : MapOutputStatistics ) : Unit markMapStageJobAsFinished marks the active job finished and notifies Spark listeners. Internally, markMapStageJobAsFinished marks the zeroth partition finished and increases the number of tasks finished in job . The job listener is notified about the 0 th task succeeded . The < job and independent stages are cleaned up>>. Ultimately, SparkListenerJobEnd is posted to LiveListenerBus (as < >) for the job , the current time (in millis) and JobSucceeded job result. markMapStageJobAsFinished is used in handleMapStageSubmitted and handleTaskCompletion .","title":" Marking ShuffleMapStage Job Finished"},{"location":"scheduler/DAGScheduler/#finding-or-creating-missing-direct-parent-shufflemapstages-for-shuffledependencies-of-rdd","text":"getOrCreateParentStages ( rdd : RDD [ _ ], firstJobId : Int ) : List [ Stage ] getOrCreateParentStages < ShuffleDependencies >> of the input rdd and then < ShuffleMapStage stages>> for each ShuffleDependency . getOrCreateParentStages is used when DAGScheduler is requested to create a ShuffleMapStage or a ResultStage .","title":" Finding Or Creating Missing Direct Parent ShuffleMapStages (For ShuffleDependencies) of RDD"},{"location":"scheduler/DAGScheduler/#marking-stage-finished","text":"markStageAsFinished ( stage : Stage , errorMessage : Option [ String ] = None , willRetry : Boolean = false ) : Unit markStageAsFinished ...FIXME markStageAsFinished is used when...FIXME","title":" Marking Stage Finished"},{"location":"scheduler/DAGScheduler/#finding-or-creating-shufflemapstage-for-shuffledependency","text":"getOrCreateShuffleMapStage ( shuffleDep : ShuffleDependency [ _ , _ , _ ], firstJobId : Int ) : ShuffleMapStage getOrCreateShuffleMapStage finds a ShuffleMapStage by the shuffleId of the given ShuffleDependency in the shuffleIdToMapStage internal registry and returns it if available. If not found, getOrCreateShuffleMapStage finds all the missing ancestor shuffle dependencies and creates the missing ShuffleMapStage stages (including one for the input ShuffleDependency ). getOrCreateShuffleMapStage is used when DAGScheduler is requested to find or create missing direct parent ShuffleMapStages of an RDD , find missing parent ShuffleMapStages for a stage , handle a MapStageSubmitted event , and check out stage dependency on a stage .","title":" Finding or Creating ShuffleMapStage for ShuffleDependency"},{"location":"scheduler/DAGScheduler/#finding-missing-shuffledependencies-for-rdd","text":"getMissingAncestorShuffleDependencies ( rdd : RDD [ _ ]) : Stack [ ShuffleDependency [ _ , _ , _ ]] getMissingAncestorShuffleDependencies finds all missing shuffle dependencies for the given RDD traversing its rdd/spark-rdd-lineage.md[RDD lineage]. NOTE: A missing shuffle dependency of a RDD is a dependency not registered in < shuffleIdToMapStage internal registry>>. Internally, getMissingAncestorShuffleDependencies < >\u2009of the input RDD and collects the ones that are not registered in < shuffleIdToMapStage internal registry>>. It repeats the process for the RDDs of the parent shuffle dependencies. getMissingAncestorShuffleDependencies is used when DAGScheduler is requested to find all ShuffleMapStage stages for a ShuffleDependency .","title":" Finding Missing ShuffleDependencies For RDD"},{"location":"scheduler/DAGScheduler/#finding-direct-parent-shuffle-dependencies-of-rdd","text":"getShuffleDependencies ( rdd : RDD [ _ ]) : HashSet [ ShuffleDependency [ _ , _ , _ ]] getShuffleDependencies finds direct parent shuffle dependencies for the given RDD . Internally, getShuffleDependencies takes the direct rdd/index.md#dependencies[shuffle dependencies of the input RDD] and direct shuffle dependencies of all the parent non- ShuffleDependencies in the dependency chain (aka RDD lineage ). getShuffleDependencies is used when DAGScheduler is requested to find or create missing direct parent ShuffleMapStages (for ShuffleDependencies of a RDD) and find all missing shuffle dependencies for a given RDD .","title":" Finding Direct Parent Shuffle Dependencies of RDD"},{"location":"scheduler/DAGScheduler/#failing-job-and-independent-single-job-stages","text":"failJobAndIndependentStages ( job : ActiveJob , failureReason : String , exception : Option [ Throwable ] = None ) : Unit failJobAndIndependentStages fails the input job and all the stages that are only used by the job. Internally, failJobAndIndependentStages uses < jobIdToStageIds internal registry>> to look up the stages registered for the job. If no stages could be found, you should see the following ERROR message in the logs: No stages registered for job [id] Otherwise, for every stage, failJobAndIndependentStages finds the job ids the stage belongs to. If no stages could be found or the job is not referenced by the stages, you should see the following ERROR message in the logs: Job [id] not registered for stage [id] even though that stage was registered for the job Only when there is exactly one job registered for the stage and the stage is in RUNNING state (in runningStages internal registry), TaskScheduler.md#contract[ TaskScheduler is requested to cancel the stage's tasks] and < >. NOTE: failJobAndIndependentStages uses < >, < >, and < > internal registries. failJobAndIndependentStages is used when...FIXME","title":" Failing Job and Independent Single-Job Stages"},{"location":"scheduler/DAGScheduler/#aborting-stage","text":"abortStage ( failedStage : Stage , reason : String , exception : Option [ Throwable ]) : Unit abortStage is an internal method that finds all the active jobs that depend on the failedStage stage and fails them. Internally, abortStage looks the failedStage stage up in the internal < > registry and exits if there the stage was not registered earlier. If it was, abortStage finds all the active jobs (in the internal < > registry) with the < failedStage stage>>. At this time, the completionTime property (of the failed stage's spark-scheduler-StageInfo.md[StageInfo]) is assigned to the current time (millis). All the active jobs that depend on the failed stage (as calculated above) and the stages that do not belong to other jobs (aka independent stages ) are < > (with the failure reason being \"Job aborted due to stage failure: [reason]\" and the input exception ). If there are no jobs depending on the failed stage, you should see the following INFO message in the logs: Ignoring failure of [failedStage] because all jobs depending on it are done abortStage is used when DAGScheduler is requested to handle a TaskSetFailed event , submit a stage , submit missing tasks of a stage , handle a TaskCompletion event .","title":" Aborting Stage"},{"location":"scheduler/DAGScheduler/#checking-out-stage-dependency-on-given-stage","text":"stageDependsOn ( stage : Stage , target : Stage ) : Boolean stageDependsOn compares two stages and returns whether the stage depends on target stage (i.e. true ) or not (i.e. false ). NOTE: A stage A depends on stage B if B is among the ancestors of A . Internally, stageDependsOn walks through the graph of RDDs of the input stage . For every RDD in the RDD's dependencies (using RDD.dependencies ) stageDependsOn adds the RDD of a NarrowDependency to a stack of RDDs to visit while for a ShuffleDependency it < ShuffleMapStage stages for a ShuffleDependency >> for the dependency and the stage 's first job id that it later adds to a stack of RDDs to visit if the map stage is ready, i.e. all the partitions have shuffle outputs. After all the RDDs of the input stage are visited, stageDependsOn checks if the target 's RDD is among the RDDs of the stage , i.e. whether the stage depends on target stage. stageDependsOn is used when DAGScheduler is requested to abort a stage .","title":" Checking Out Stage Dependency on Given Stage"},{"location":"scheduler/DAGScheduler/#submitting-waiting-child-stages-for-execution","text":"submitWaitingChildStages ( parent : Stage ) : Unit submitWaitingChildStages submits for execution all waiting stages for which the input parent Stage.md[Stage] is the direct parent. NOTE: Waiting stages are the stages registered in < waitingStages internal registry>>. When executed, you should see the following TRACE messages in the logs: Checking if any dependencies of [parent] are now runnable running: [runningStages] waiting: [waitingStages] failed: [failedStages] submitWaitingChildStages finds child stages of the input parent stage, removes them from waitingStages internal registry, and < > one by one sorted by their job ids. submitWaitingChildStages is used when DAGScheduler is requested to submits missing tasks for a stage and handles a successful ShuffleMapTask completion .","title":" Submitting Waiting Child Stages for Execution"},{"location":"scheduler/DAGScheduler/#submitting-stage-with-missing-parents-for-execution","text":"submitStage ( stage : Stage ) : Unit submitStage submits the input stage or its missing parents (if there any stages not computed yet before the input stage could). NOTE: submitStage is also used to DAGSchedulerEventProcessLoop.md#resubmitFailedStages[resubmit failed stages]. submitStage recursively submits any missing parents of the stage . Internally, submitStage first finds the earliest-created job id that needs the stage . NOTE: A stage itself tracks the jobs (their ids) it belongs to (using the internal jobIds registry). The following steps depend on whether there is a job or not. If there are no jobs that require the stage , submitStage < > with the reason: No active job for stage [id] If however there is a job for the stage , you should see the following DEBUG message in the logs: submitStage([stage]) submitStage checks the status of the stage and continues when it was not recorded in < >, < > or < > internal registries. It simply exits otherwise. With the stage ready for submission, submitStage calculates the < stage >> (sorted by their job ids). You should see the following DEBUG message in the logs: missing: [missing] When the stage has no parent stages missing, you should see the following INFO message in the logs: Submitting [stage] ([stage.rdd]), which has no missing parents submitStage < stage >> (with the earliest-created job id) and finishes. If however there are missing parent stages for the stage , submitStage < >, and the stage is recorded in the internal < > registry. submitStage is used recursively for missing parents of the given stage and when DAGScheduler is requested for the following: < > (ResubmitFailedStages event) < > (CompletionEvent event) Handle < >, < > and < > events","title":" Submitting Stage (with Missing Parents) for Execution"},{"location":"scheduler/DAGScheduler/#stage-attempts","text":"A single stage can be re-executed in multiple attempts due to fault recovery. The number of attempts is configured (FIXME). If TaskScheduler reports that a task failed because a map output file from a previous stage was lost, the DAGScheduler resubmits the lost stage. This is detected through a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ CompletionEvent with FetchFailed ], or an < > event. DAGScheduler will wait a small amount of time to see whether other nodes or tasks fail, then resubmit TaskSets for any lost stage(s) that compute the missing tasks. Please note that tasks from the old attempts of a stage could still be running. A stage object tracks multiple spark-scheduler-StageInfo.md[StageInfo] objects to pass to Spark listeners or the web UI. The latest StageInfo for the most recent attempt for a stage is accessible through latestInfo .","title":" Stage Attempts"},{"location":"scheduler/DAGScheduler/#preferred-locations","text":"DAGScheduler computes where to run each task in a stage based on the rdd/index.md#getPreferredLocations[preferred locations of its underlying RDDs], or < >.","title":" Preferred Locations"},{"location":"scheduler/DAGScheduler/#adaptive-query-planning-adaptive-scheduling","text":"See SPARK-9850 Adaptive execution in Spark for the design document. The work is currently in progress. DAGScheduler.submitMapStage method is used for adaptive query planning, to run map stages and look at statistics about their outputs before submitting downstream stages.","title":" Adaptive Query Planning / Adaptive Scheduling"},{"location":"scheduler/DAGScheduler/#scheduledexecutorservice-daemon-services","text":"DAGScheduler uses the following ScheduledThreadPoolExecutors (with the policy of removing cancelled tasks from a work queue at time of cancellation): dag-scheduler-message - a daemon thread pool using j.u.c.ScheduledThreadPoolExecutor with core pool size 1 . It is used to post a DAGSchedulerEventProcessLoop.md#ResubmitFailedStages[ResubmitFailedStages] event when DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[ FetchFailed is reported]. They are created using ThreadUtils.newDaemonSingleThreadScheduledExecutor method that uses Guava DSL to instantiate a ThreadFactory.","title":"ScheduledExecutorService daemon services"},{"location":"scheduler/DAGScheduler/#finding-missing-parent-shufflemapstages-for-stage","text":"getMissingParentStages ( stage : Stage ) : List [ Stage ] getMissingParentStages finds missing parent ShuffleMapStage s in the dependency graph of the input stage (using the breadth-first search algorithm ). Internally, getMissingParentStages starts with the stage 's RDD and walks up the tree of all parent RDDs to find < >. NOTE: A Stage tracks the associated RDD using Stage.md#rdd[ rdd property]. NOTE: An uncached partition of a RDD is a partition that has Nil in the < > (which results in no RDD blocks in any of the active storage:BlockManager.md[BlockManager]s on executors). getMissingParentStages traverses the rdd/index.md#dependencies[parent dependencies of the RDD] and acts according to their type, i.e. ShuffleDependency or NarrowDependency . NOTE: ShuffleDependency and NarrowDependency are the main top-level Dependencies . For each NarrowDependency , getMissingParentStages simply marks the corresponding RDD to visit and moves on to a next dependency of a RDD or works on another unvisited parent RDD. NOTE: NarrowDependency is a RDD dependency that allows for pipelined execution. getMissingParentStages focuses on ShuffleDependency dependencies. NOTE: ShuffleDependency is a RDD dependency that represents a dependency on the output of a ShuffleMapStage , i.e. shuffle map stage . For each ShuffleDependency , getMissingParentStages < ShuffleMapStage stages>>. If the ShuffleMapStage is not available , it is added to the set of missing (map) stages. NOTE: A ShuffleMapStage is available when all its partitions are computed, i.e. results are available (as blocks). CAUTION: FIXME...IMAGE with ShuffleDependencies queried getMissingParentStages is used when DAGScheduler is requested to submit a stage and handle JobSubmitted and MapStageSubmitted events.","title":" Finding Missing Parent ShuffleMapStages For Stage"},{"location":"scheduler/DAGScheduler/#submitting-missing-tasks-of-stage","text":"submitMissingTasks ( stage : Stage , jobId : Int ) : Unit submitMissingTasks prints out the following DEBUG message to the logs: submitMissingTasks([stage]) submitMissingTasks requests the given Stage for the missing partitions (partitions that need to be computed). submitMissingTasks adds the stage to the runningStages internal registry. submitMissingTasks notifies the OutputCommitCoordinator that stage execution started .","title":" Submitting Missing Tasks of Stage"},{"location":"scheduler/DAGScheduler/#_1","text":"submitMissingTasks determines preferred locations ( task locality preferences ) of the missing partitions. submitMissingTasks requests the stage for a new stage attempt . submitMissingTasks requests the LiveListenerBus to post a SparkListenerStageSubmitted event. submitMissingTasks uses the closure Serializer to serialize the stage and create a so-called task binary. submitMissingTasks serializes the RDD (of the stage) and either the ShuffleDependency or the compute function based on the type of the stage ( ShuffleMapStage or ResultStage , respectively). submitMissingTasks creates a broadcast variable for the task binary. Note That shows how important Broadcast s are for Spark itself to distribute data among executors in a Spark application in the most efficient way. submitMissingTasks creates tasks for every missing partition: ShuffleMapTasks for a ShuffleMapStage ResultTasks for a ResultStage If there are tasks to submit for execution (i.e. there are missing partitions in the stage), submitMissingTasks prints out the following INFO message to the logs: Submitting [size] missing tasks from [stage] ([rdd]) (first 15 tasks are for partitions [partitionIds]) submitMissingTasks requests the < > to TaskScheduler.md#submitTasks[submit the tasks for execution] (as a new TaskSet.md[TaskSet]). With no tasks to submit for execution, submitMissingTasks < >. submitMissingTasks prints out the following DEBUG messages based on the type of the stage: Stage [stage] is actually done; (available: [isAvailable],available outputs: [numAvailableOutputs],partitions: [numPartitions]) or Stage [stage] is actually done; (partitions: [numPartitions]) for ShuffleMapStage and ResultStage , respectively. In the end, with no tasks to submit for execution, submitMissingTasks < > and exits. submitMissingTasks is used when DAGScheduler is requested to submit a stage for execution .","title":""},{"location":"scheduler/DAGScheduler/#finding-preferred-locations-for-missing-partitions","text":"getPreferredLocs ( rdd : RDD [ _ ], partition : Int ) : Seq [ TaskLocation ] getPreferredLocs is simply an alias for the internal (recursive) < >. getPreferredLocs is used when...FIXME","title":" Finding Preferred Locations for Missing Partitions"},{"location":"scheduler/DAGScheduler/#finding-blockmanagers-executors-for-cached-rdd-partitions-aka-block-location-discovery","text":"getCacheLocs ( rdd : RDD [ _ ]) : IndexedSeq [ Seq [ TaskLocation ]] getCacheLocs gives TaskLocations (block locations) for the partitions of the input rdd . getCacheLocs caches lookup results in < > internal registry. NOTE: The size of the collection from getCacheLocs is exactly the number of partitions in rdd RDD. NOTE: The size of every TaskLocation collection (i.e. every entry in the result of getCacheLocs ) is exactly the number of blocks managed using storage:BlockManager.md[BlockManagers] on executors. Internally, getCacheLocs finds rdd in the < > internal registry (of partition locations per RDD). If rdd is not in < > internal registry, getCacheLocs branches per its storage:StorageLevel.md[storage level]. For NONE storage level (i.e. no caching), the result is an empty locations (i.e. no location preference). For other non- NONE storage levels, getCacheLocs storage:BlockManagerMaster.md#getLocations-block-array[requests BlockManagerMaster for block locations] that are then mapped to TaskLocations with the hostname of the owning BlockManager for a block (of a partition) and the executor id. NOTE: getCacheLocs uses < > that was defined when < >. getCacheLocs records the computed block locations per partition (as TaskLocation ) in < > internal registry. NOTE: getCacheLocs requests locations from BlockManagerMaster using storage:BlockId.md#RDDBlockId[RDDBlockId] with the RDD id and the partition indices (which implies that the order of the partitions matters to request proper blocks). NOTE: DAGScheduler uses TaskLocation.md[TaskLocations] (with host and executor) while storage:BlockManagerMaster.md[BlockManagerMaster] uses storage:BlockManagerId.md[] (to track similar information, i.e. block locations). getCacheLocs is used when DAGScheduler is requested to find missing parent MapStages and getPreferredLocsInternal .","title":" Finding BlockManagers (Executors) for Cached RDD Partitions (aka Block Location Discovery)"},{"location":"scheduler/DAGScheduler/#finding-placement-preferences-for-rdd-partition-recursively","text":"getPreferredLocsInternal ( rdd : RDD [ _ ], partition : Int , visited : HashSet [( RDD [ _ ] , Int )]) : Seq [ TaskLocation ] getPreferredLocsInternal first < TaskLocations for the partition of the rdd >> (using < > internal cache) and returns them. Otherwise, if not found, getPreferredLocsInternal rdd/index.md#preferredLocations[requests rdd for the preferred locations of partition ] and returns them. NOTE: Preferred locations of the partitions of a RDD are also called placement preferences or locality preferences . Otherwise, if not found, getPreferredLocsInternal finds the first parent NarrowDependency and (recursively) finds TaskLocations . If all the attempts fail to yield any non-empty result, getPreferredLocsInternal returns an empty collection of TaskLocation.md[TaskLocations]. getPreferredLocsInternal is used when DAGScheduler is requested for the preferred locations for missing partitions .","title":" Finding Placement Preferences for RDD Partition (recursively)"},{"location":"scheduler/DAGScheduler/#stopping-dagscheduler","text":"stop () : Unit stop stops the internal dag-scheduler-message thread pool, dag-scheduler-event-loop , and TaskScheduler . stop is used when SparkContext is requested to stop .","title":" Stopping DAGScheduler"},{"location":"scheduler/DAGScheduler/#updating-accumulators-with-partial-values-from-completed-tasks","text":"updateAccumulators ( event : CompletionEvent ) : Unit updateAccumulators merges the partial values of accumulators from a completed task into their \"source\" accumulators on the driver. NOTE: It is called by < >. For each spark-accumulators.md#AccumulableInfo[AccumulableInfo] in the CompletionEvent , a partial value from a task is obtained (from AccumulableInfo.update ) and added to the driver's accumulator (using Accumulable.++= method). For named accumulators with the update value being a non-zero value, i.e. not Accumulable.zero : stage.latestInfo.accumulables for the AccumulableInfo.id is set CompletionEvent.taskInfo.accumulables has a new spark-accumulators.md#AccumulableInfo[AccumulableInfo] added. CAUTION: FIXME Where are Stage.latestInfo.accumulables and CompletionEvent.taskInfo.accumulables used? updateAccumulators is used when DAGScheduler is requested to handle a task completion .","title":" Updating Accumulators with Partial Values from Completed Tasks"},{"location":"scheduler/DAGScheduler/#checkbarrierstagewithnumslots","text":"checkBarrierStageWithNumSlots ( rdd : RDD [ _ ]) : Unit checkBarrierStageWithNumSlots...FIXME checkBarrierStageWithNumSlots is used when DAGScheduler is requested to create < > and < > stages.","title":" checkBarrierStageWithNumSlots"},{"location":"scheduler/DAGScheduler/#killing-task","text":"killTaskAttempt ( taskId : Long , interruptThread : Boolean , reason : String ) : Boolean killTaskAttempt requests the TaskScheduler to kill a task . killTaskAttempt is used when SparkContext is requested to kill a task .","title":" Killing Task"},{"location":"scheduler/DAGScheduler/#cleanupafterschedulerstop","text":"cleanUpAfterSchedulerStop () : Unit cleanUpAfterSchedulerStop ...FIXME cleanUpAfterSchedulerStop is used when DAGSchedulerEventProcessLoop is requested to onStop .","title":" cleanUpAfterSchedulerStop"},{"location":"scheduler/DAGScheduler/#removeexecutorandunregisteroutputs","text":"removeExecutorAndUnregisterOutputs ( execId : String , fileLost : Boolean , hostToUnregisterOutputs : Option [ String ], maybeEpoch : Option [ Long ] = None ) : Unit removeExecutorAndUnregisterOutputs...FIXME removeExecutorAndUnregisterOutputs is used when DAGScheduler is requested to handle < > (due to a fetch failure) and < > events.","title":" removeExecutorAndUnregisterOutputs"},{"location":"scheduler/DAGScheduler/#markmapstagejobsasfinished","text":"markMapStageJobsAsFinished ( shuffleStage : ShuffleMapStage ) : Unit markMapStageJobsAsFinished ...FIXME markMapStageJobsAsFinished is used when DAGScheduler is requested to submit missing tasks (of a ShuffleMapStage that has just been computed) and handle a task completion (of a ShuffleMapStage ).","title":" markMapStageJobsAsFinished"},{"location":"scheduler/DAGScheduler/#updatejobidstageidmaps","text":"updateJobIdStageIdMaps ( jobId : Int , stage : Stage ) : Unit updateJobIdStageIdMaps ...FIXME updateJobIdStageIdMaps is used when DAGScheduler is requested to create ShuffleMapStage and ResultStage stages.","title":" updateJobIdStageIdMaps"},{"location":"scheduler/DAGScheduler/#executorheartbeatreceived","text":"executorHeartbeatReceived ( execId : String , // (taskId, stageId, stageAttemptId, accumUpdates) accumUpdates : Array [( Long , Int , Int , Seq [ AccumulableInfo ])], blockManagerId : BlockManagerId , // (stageId, stageAttemptId) -> metrics executorUpdates : mutable.Map [( Int , Int ) , ExecutorMetrics ]) : Boolean executorHeartbeatReceived posts a SparkListenerExecutorMetricsUpdate (to listenerBus ) and informs BlockManagerMaster that blockManagerId block manager is alive (by posting BlockManagerHeartbeat ). executorHeartbeatReceived is used when TaskSchedulerImpl is requested to handle an executor heartbeat .","title":" executorHeartbeatReceived"},{"location":"scheduler/DAGScheduler/#posttaskend","text":"postTaskEnd ( event : CompletionEvent ) : Unit postTaskEnd ...FIXME postTaskEnd is used when DAGScheduler is requested to handle a task completion .","title":" postTaskEnd"},{"location":"scheduler/DAGScheduler/#event-handlers","text":"","title":"Event Handlers"},{"location":"scheduler/DAGScheduler/#alljobscancelled-event-handler","text":"doCancelAllJobs () : Unit doCancelAllJobs ...FIXME doCancelAllJobs is used when DAGSchedulerEventProcessLoop is requested to handle an AllJobsCancelled event and onError .","title":" AllJobsCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#beginevent-event-handler","text":"handleBeginEvent ( task : Task [ _ ], taskInfo : TaskInfo ) : Unit handleBeginEvent ...FIXME handleBeginEvent is used when DAGSchedulerEventProcessLoop is requested to handle a BeginEvent event.","title":" BeginEvent Event Handler"},{"location":"scheduler/DAGScheduler/#handling-completionevent","text":"handleTaskCompletion ( event : CompletionEvent ) : Unit handleTaskCompletion ...FIXME handleTaskCompletion is used when DAGSchedulerEventProcessLoop is requested to handle a CompletionEvent event.","title":" Handling CompletionEvent"},{"location":"scheduler/DAGScheduler/#executoradded-event-handler","text":"handleExecutorAdded ( execId : String , host : String ) : Unit handleExecutorAdded ...FIXME handleExecutorAdded is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorAdded event.","title":" ExecutorAdded Event Handler"},{"location":"scheduler/DAGScheduler/#executorlost-event-handler","text":"handleExecutorLost ( execId : String , workerLost : Boolean ) : Unit handleExecutorLost ...FIXME handleExecutorLost is used when DAGSchedulerEventProcessLoop is requested to handle an ExecutorLost event.","title":" ExecutorLost Event Handler"},{"location":"scheduler/DAGScheduler/#gettingresultevent-event-handler","text":"handleGetTaskResult ( taskInfo : TaskInfo ) : Unit handleGetTaskResult ...FIXME handleGetTaskResult is used when DAGSchedulerEventProcessLoop is requested to handle a GettingResultEvent event.","title":" GettingResultEvent Event Handler"},{"location":"scheduler/DAGScheduler/#jobcancelled-event-handler","text":"handleJobCancellation ( jobId : Int , reason : Option [ String ]) : Unit handleJobCancellation ...FIXME handleJobCancellation is used when DAGScheduler is requested to handle a JobCancelled event, doCancelAllJobs , handleJobGroupCancelled , handleStageCancellation .","title":" JobCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#jobgroupcancelled-event-handler","text":"handleJobGroupCancelled ( groupId : String ) : Unit handleJobGroupCancelled ...FIXME handleJobGroupCancelled is used when DAGScheduler is requested to handle JobGroupCancelled event.","title":" JobGroupCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#handling-jobsubmitted-event","text":"handleJobSubmitted ( jobId : Int , finalRDD : RDD [ _ ], func : ( TaskContext , Iterator [ _ ]) => _ , partitions : Array [ Int ], callSite : CallSite , listener : JobListener , properties : Properties ) : Unit handleJobSubmitted creates a ResultStage (as finalStage in the picture below) for the given RDD, func , partitions , jobId and callSite . handleJobSubmitted creates an ActiveJob for the ResultStage . handleJobSubmitted clears the internal cache of RDD partition locations . Important FIXME Why is this clearing here so important? handleJobSubmitted prints out the following INFO messages to the logs (with missingParentStages ): Got job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingParentStages] handleJobSubmitted registers the new ActiveJob in jobIdToActiveJob and activeJobs internal registries. handleJobSubmitted requests the ResultStage to associate itself with the ActiveJob . handleJobSubmitted uses the jobIdToStageIds internal registry to find all registered stages for the given jobId . handleJobSubmitted uses the stageIdToStage internal registry to request the Stages for the latestInfo . In the end, handleJobSubmitted posts a SparkListenerJobStart message to the LiveListenerBus and submits the ResultStage . handleJobSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a JobSubmitted event.","title":" Handling JobSubmitted Event"},{"location":"scheduler/DAGScheduler/#mapstagesubmitted-event-handler","text":"handleMapStageSubmitted ( jobId : Int , dependency : ShuffleDependency [ _ , _ , _ ], callSite : CallSite , listener : JobListener , properties : Properties ) : Unit handleMapStageSubmitted ...FIXME handleMapStageSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a MapStageSubmitted event.","title":" MapStageSubmitted Event Handler"},{"location":"scheduler/DAGScheduler/#resubmitfailedstages-event-handler","text":"resubmitFailedStages () : Unit resubmitFailedStages ...FIXME resubmitFailedStages is used when DAGSchedulerEventProcessLoop is requested to handle a ResubmitFailedStages event.","title":" ResubmitFailedStages Event Handler"},{"location":"scheduler/DAGScheduler/#speculativetasksubmitted-event-handler","text":"handleSpeculativeTaskSubmitted () : Unit handleSpeculativeTaskSubmitted ...FIXME handleSpeculativeTaskSubmitted is used when DAGSchedulerEventProcessLoop is requested to handle a SpeculativeTaskSubmitted event.","title":" SpeculativeTaskSubmitted Event Handler"},{"location":"scheduler/DAGScheduler/#stagecancelled-event-handler","text":"handleStageCancellation () : Unit handleStageCancellation ...FIXME handleStageCancellation is used when DAGSchedulerEventProcessLoop is requested to handle a StageCancelled event.","title":" StageCancelled Event Handler"},{"location":"scheduler/DAGScheduler/#tasksetfailed-event-handler","text":"handleTaskSetFailed () : Unit handleTaskSetFailed ...FIXME handleTaskSetFailed is used when DAGSchedulerEventProcessLoop is requested to handle a TaskSetFailed event.","title":" TaskSetFailed Event Handler"},{"location":"scheduler/DAGScheduler/#workerremoved-event-handler","text":"handleWorkerRemoved ( workerId : String , host : String , message : String ) : Unit handleWorkerRemoved ...FIXME handleWorkerRemoved is used when DAGSchedulerEventProcessLoop is requested to handle a WorkerRemoved event.","title":" WorkerRemoved Event Handler"},{"location":"scheduler/DAGScheduler/#internal-properties","text":"","title":"Internal Properties"},{"location":"scheduler/DAGScheduler/#failedepoch","text":"The lookup table of lost executors and the epoch of the event.","title":" failedEpoch"},{"location":"scheduler/DAGScheduler/#failedstages","text":"Stages that failed due to fetch failures (when a DAGSchedulerEventProcessLoop.md#handleTaskCompletion-FetchFailed[task fails with FetchFailed exception]).","title":" failedStages"},{"location":"scheduler/DAGScheduler/#jobidtoactivejob","text":"The lookup table of ActiveJob s per job id.","title":" jobIdToActiveJob"},{"location":"scheduler/DAGScheduler/#jobidtostageids","text":"The lookup table of all stages per ActiveJob id","title":" jobIdToStageIds"},{"location":"scheduler/DAGScheduler/#metricssource","text":"DAGSchedulerSource","title":" metricsSource"},{"location":"scheduler/DAGScheduler/#nextjobid-counter","text":"nextJobId : AtomicInteger nextJobId is a Java AtomicInteger for job IDs. nextJobId starts at 0 . Used when DAGScheduler is requested for numTotalJobs , to submitJob , runApproximateJob and submitMapStage .","title":" nextJobId Counter"},{"location":"scheduler/DAGScheduler/#nextstageid","text":"The next stage id counting from 0 . Used when DAGScheduler creates a < > and a < >. It is the key in < >.","title":" nextStageId"},{"location":"scheduler/DAGScheduler/#runningstages","text":"The set of stages that are currently \"running\". A stage is added when < > gets executed (without first checking if the stage has not already been added).","title":" runningStages"},{"location":"scheduler/DAGScheduler/#shuffleidtomapstage","text":"A lookup table of ShuffleMapStage s by ShuffleDependency","title":" shuffleIdToMapStage"},{"location":"scheduler/DAGScheduler/#stageidtostage","text":"A lookup table of stages by stage ID Used when DAGScheduler creates a shuffle map stage , creates a result stage , cleans up job state and independent stages , is informed that a task is started , a taskset has failed , a job is submitted (to compute a ResultStage ) , a map stage was submitted , a task has completed or a stage was cancelled , updates accumulators , aborts a stage and fails a job and independent stages .","title":" stageIdToStage"},{"location":"scheduler/DAGScheduler/#waitingstages","text":"Stages with parents to be computed","title":" waitingStages"},{"location":"scheduler/DAGScheduler/#event-posting-methods","text":"","title":"Event Posting Methods"},{"location":"scheduler/DAGScheduler/#posting-alljobscancelled","text":"Posts an AllJobsCancelled Used when SparkContext is requested to cancel all running or scheduled Spark jobs","title":" Posting AllJobsCancelled"},{"location":"scheduler/DAGScheduler/#posting-jobcancelled","text":"Posts a JobCancelled Used when SparkContext or JobWaiter are requested to cancel a Spark job","title":" Posting JobCancelled"},{"location":"scheduler/DAGScheduler/#posting-jobgroupcancelled","text":"Posts a JobGroupCancelled Used when SparkContext is requested to cancel a job group","title":" Posting JobGroupCancelled"},{"location":"scheduler/DAGScheduler/#posting-stagecancelled","text":"Posts a StageCancelled Used when SparkContext is requested to cancel a stage","title":" Posting StageCancelled"},{"location":"scheduler/DAGScheduler/#posting-executoradded","text":"Posts an ExecutorAdded Used when TaskSchedulerImpl is requested to handle resource offers (and a new executor is found in the resource offers)","title":" Posting ExecutorAdded"},{"location":"scheduler/DAGScheduler/#posting-executorlost","text":"Posts a ExecutorLost Used when TaskSchedulerImpl is requested to handle a task status update (and a task gets lost which is used to indicate that the executor got broken and hence should be considered lost) or executorLost","title":" Posting ExecutorLost"},{"location":"scheduler/DAGScheduler/#posting-jobsubmitted","text":"Posts a JobSubmitted Used when SparkContext is requested to run an approximate job","title":" Posting JobSubmitted"},{"location":"scheduler/DAGScheduler/#posting-speculativetasksubmitted","text":"Posts a SpeculativeTaskSubmitted Used when TaskSetManager is requested to checkAndSubmitSpeculatableTask","title":" Posting SpeculativeTaskSubmitted"},{"location":"scheduler/DAGScheduler/#posting-mapstagesubmitted","text":"Posts a MapStageSubmitted Used when SparkContext is requested to submit a MapStage for execution","title":" Posting MapStageSubmitted"},{"location":"scheduler/DAGScheduler/#posting-completionevent","text":"Posts a CompletionEvent Used when TaskSetManager is requested to handleSuccessfulTask , handleFailedTask , and executorLost","title":" Posting CompletionEvent"},{"location":"scheduler/DAGScheduler/#posting-gettingresultevent","text":"Posts a GettingResultEvent Used when TaskSetManager is requested to handle a task fetching result","title":" Posting GettingResultEvent"},{"location":"scheduler/DAGScheduler/#posting-tasksetfailed","text":"Posts a TaskSetFailed Used when TaskSetManager is requested to abort","title":" Posting TaskSetFailed"},{"location":"scheduler/DAGScheduler/#posting-beginevent","text":"Posts a BeginEvent Used when TaskSetManager is requested to start a task","title":" Posting BeginEvent"},{"location":"scheduler/DAGScheduler/#posting-workerremoved","text":"Posts a WorkerRemoved Used when TaskSchedulerImpl is requested to handle a removed worker event","title":" Posting WorkerRemoved"},{"location":"scheduler/DAGScheduler/#logging","text":"Enable ALL logging level for org.apache.spark.scheduler.DAGScheduler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.DAGScheduler=ALL Refer to Logging .","title":"Logging"},{"location":"scheduler/DAGSchedulerEvent/","text":"DAGSchedulerEvents \u00b6 == [[AllJobsCancelled]] AllJobsCancelled AllJobsCancelled event carries no extra information. Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelAllJobs[cancelAllJobs] Event handler: scheduler:DAGScheduler.md#doCancelAllJobs[doCancelAllJobs] == [[BeginEvent]] BeginEvent BeginEvent event carries the following: scheduler:Task.md[Task] scheduler:spark-scheduler-TaskInfo.md[TaskInfo] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskStarted[taskStarted] Event handler: scheduler:DAGScheduler.md#handleBeginEvent[handleBeginEvent] == [[CompletionEvent]] CompletionEvent CompletionEvent event carries the following: scheduler:Task.md[Task] Reason Result (value computed) Accumulator updates scheduler:spark-scheduler-TaskInfo.md[TaskInfo] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskEnded[taskEnded] Event handler: scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] == [[ExecutorAdded]] ExecutorAdded ExecutorAdded event carries the following: Executor ID Host name Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#executorAdded[executorAdded] Event handler: scheduler:DAGScheduler.md#handleExecutorAdded[handleExecutorAdded] == [[ExecutorLost]] ExecutorLost ExecutorLost event carries the following: Executor ID Reason Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#executorLost[executorLost] Event handler: scheduler:DAGScheduler.md#handleExecutorLost[handleExecutorLost] == [[GettingResultEvent]] GettingResultEvent GettingResultEvent event carries the following: scheduler:spark-scheduler-TaskInfo.md[TaskInfo] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskGettingResult[taskGettingResult] Event handler: scheduler:DAGScheduler.md#handleGetTaskResult[handleGetTaskResult] == [[JobCancelled]] JobCancelled JobCancelled event carries the following: Job ID Reason (optional) Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelJob[cancelJob] Event handler: scheduler:DAGScheduler.md#handleJobCancellation[handleJobCancellation] == [[JobGroupCancelled]] JobGroupCancelled JobGroupCancelled event carries the following: Group ID Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelJobGroup[cancelJobGroup] Event handler: scheduler:DAGScheduler.md#handleJobGroupCancelled[handleJobGroupCancelled] == [[JobSubmitted]] JobSubmitted JobSubmitted event carries the following: Job ID rdd:RDD.md[RDD] Partition function ( (TaskContext, Iterator[_]) => _ ) Partitions to compute CallSite JobListener to keep updated about the status of the stage execution Execution properties Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#submitJob[submit a job], scheduler:DAGScheduler.md#runApproximateJob[run an approximate job] and scheduler:DAGScheduler.md#handleJobSubmitted[handleJobSubmitted] Event handler: scheduler:DAGScheduler.md#handleJobSubmitted[handleJobSubmitted] == [[MapStageSubmitted]] MapStageSubmitted MapStageSubmitted event carries the following: Job ID ShuffleDependency CallSite JobListener Execution properties Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMapStage[submitMapStage] Event handler: scheduler:DAGScheduler.md#handleMapStageSubmitted[handleMapStageSubmitted] == [[ResubmitFailedStages]] ResubmitFailedStages ResubmitFailedStages event carries no extra information. Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] Event handler: scheduler:DAGScheduler.md#resubmitFailedStages[resubmitFailedStages] == [[SpeculativeTaskSubmitted]] SpeculativeTaskSubmitted SpeculativeTaskSubmitted event carries the following: scheduler:Task.md[Task] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#speculativeTaskSubmitted[speculativeTaskSubmitted] Event handler: scheduler:DAGScheduler.md#handleSpeculativeTaskSubmitted[handleSpeculativeTaskSubmitted] == [[StageCancelled]] StageCancelled StageCancelled event carries the following: Stage ID Reason (optional) Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelStage[cancelStage] Event handler: scheduler:DAGScheduler.md#handleStageCancellation[handleStageCancellation] == [[TaskSetFailed]] TaskSetFailed TaskSetFailed event carries the following: scheduler:TaskSet.md[TaskSet] Reason Exception (optional) Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskSetFailed[taskSetFailed] Event handler: scheduler:DAGScheduler.md#handleTaskSetFailed[handleTaskSetFailed] == [[WorkerRemoved]] WorkerRemoved WorkerRemoved event carries the following: Worked ID Host name Reason Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#workerRemoved[workerRemoved] Event handler: scheduler:DAGScheduler.md#handleWorkerRemoved[handleWorkerRemoved]","title":"DAGSchedulerEvent"},{"location":"scheduler/DAGSchedulerEvent/#dagschedulerevents","text":"== [[AllJobsCancelled]] AllJobsCancelled AllJobsCancelled event carries no extra information. Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelAllJobs[cancelAllJobs] Event handler: scheduler:DAGScheduler.md#doCancelAllJobs[doCancelAllJobs] == [[BeginEvent]] BeginEvent BeginEvent event carries the following: scheduler:Task.md[Task] scheduler:spark-scheduler-TaskInfo.md[TaskInfo] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskStarted[taskStarted] Event handler: scheduler:DAGScheduler.md#handleBeginEvent[handleBeginEvent] == [[CompletionEvent]] CompletionEvent CompletionEvent event carries the following: scheduler:Task.md[Task] Reason Result (value computed) Accumulator updates scheduler:spark-scheduler-TaskInfo.md[TaskInfo] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskEnded[taskEnded] Event handler: scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] == [[ExecutorAdded]] ExecutorAdded ExecutorAdded event carries the following: Executor ID Host name Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#executorAdded[executorAdded] Event handler: scheduler:DAGScheduler.md#handleExecutorAdded[handleExecutorAdded] == [[ExecutorLost]] ExecutorLost ExecutorLost event carries the following: Executor ID Reason Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#executorLost[executorLost] Event handler: scheduler:DAGScheduler.md#handleExecutorLost[handleExecutorLost] == [[GettingResultEvent]] GettingResultEvent GettingResultEvent event carries the following: scheduler:spark-scheduler-TaskInfo.md[TaskInfo] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskGettingResult[taskGettingResult] Event handler: scheduler:DAGScheduler.md#handleGetTaskResult[handleGetTaskResult] == [[JobCancelled]] JobCancelled JobCancelled event carries the following: Job ID Reason (optional) Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelJob[cancelJob] Event handler: scheduler:DAGScheduler.md#handleJobCancellation[handleJobCancellation] == [[JobGroupCancelled]] JobGroupCancelled JobGroupCancelled event carries the following: Group ID Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelJobGroup[cancelJobGroup] Event handler: scheduler:DAGScheduler.md#handleJobGroupCancelled[handleJobGroupCancelled] == [[JobSubmitted]] JobSubmitted JobSubmitted event carries the following: Job ID rdd:RDD.md[RDD] Partition function ( (TaskContext, Iterator[_]) => _ ) Partitions to compute CallSite JobListener to keep updated about the status of the stage execution Execution properties Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#submitJob[submit a job], scheduler:DAGScheduler.md#runApproximateJob[run an approximate job] and scheduler:DAGScheduler.md#handleJobSubmitted[handleJobSubmitted] Event handler: scheduler:DAGScheduler.md#handleJobSubmitted[handleJobSubmitted] == [[MapStageSubmitted]] MapStageSubmitted MapStageSubmitted event carries the following: Job ID ShuffleDependency CallSite JobListener Execution properties Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMapStage[submitMapStage] Event handler: scheduler:DAGScheduler.md#handleMapStageSubmitted[handleMapStageSubmitted] == [[ResubmitFailedStages]] ResubmitFailedStages ResubmitFailedStages event carries no extra information. Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] Event handler: scheduler:DAGScheduler.md#resubmitFailedStages[resubmitFailedStages] == [[SpeculativeTaskSubmitted]] SpeculativeTaskSubmitted SpeculativeTaskSubmitted event carries the following: scheduler:Task.md[Task] Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#speculativeTaskSubmitted[speculativeTaskSubmitted] Event handler: scheduler:DAGScheduler.md#handleSpeculativeTaskSubmitted[handleSpeculativeTaskSubmitted] == [[StageCancelled]] StageCancelled StageCancelled event carries the following: Stage ID Reason (optional) Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#cancelStage[cancelStage] Event handler: scheduler:DAGScheduler.md#handleStageCancellation[handleStageCancellation] == [[TaskSetFailed]] TaskSetFailed TaskSetFailed event carries the following: scheduler:TaskSet.md[TaskSet] Reason Exception (optional) Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#taskSetFailed[taskSetFailed] Event handler: scheduler:DAGScheduler.md#handleTaskSetFailed[handleTaskSetFailed] == [[WorkerRemoved]] WorkerRemoved WorkerRemoved event carries the following: Worked ID Host name Reason Posted when DAGScheduler is requested to scheduler:DAGScheduler.md#workerRemoved[workerRemoved] Event handler: scheduler:DAGScheduler.md#handleWorkerRemoved[handleWorkerRemoved]","title":"DAGSchedulerEvents"},{"location":"scheduler/DAGSchedulerEventProcessLoop/","text":"= [[DAGSchedulerEventProcessLoop]] DAGSchedulerEventProcessLoop DAGSchedulerEventProcessLoop is an event processing thread to handle scheduler:DAGSchedulerEvent.md[DAGSchedulerEvents] asynchronously and serially (one by one). DAGSchedulerEventProcessLoop is registered under the name of dag-scheduler-event-loop . The purpose of the DAGSchedulerEventProcessLoop is to have a separate thread to process events asynchronously alongside scheduler:DAGScheduler.md[DAGScheduler]. [[dagScheduler]] When created, DAGSchedulerEventProcessLoop gets the reference to the owning scheduler:DAGScheduler.md[DAGScheduler] that it uses to call event handler methods on. DAGSchedulerEventProcessLoop uses {java-javadoc-url}/java/util/concurrent/LinkedBlockingDeque.html[java.util.concurrent.LinkedBlockingDeque] blocking deque that grows indefinitely (up to {java-javadoc-url}/java/lang/Integer.html#MAX_VALUE[Integer.MAX_VALUE] events). .DAGSchedulerEvents and Event Handlers [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | DAGSchedulerEvent | Event Handler | Trigger | < > | | DAGScheduler was requested to scheduler:DAGScheduler.md#cancelAllJobs[cancel all running or waiting jobs]. | < > | < > | scheduler:TaskSetManager.md[TaskSetManager] informs DAGScheduler that a task is starting (through scheduler:DAGScheduler.md#taskStarted[taskStarted]). | [[CompletionEvent]] CompletionEvent | a| Posted exclusively when DAGScheduler is requested to < > Event handler: < > CompletionEvent holds the following: [[CompletionEvent-task]] scheduler:Task.md[Task] [[CompletionEvent-reason]] TaskEndReason [[CompletionEvent-result]] Result of executing the task [[CompletionEvent-accumUpdates]] < > [[CompletionEvent-taskInfo]] < > | < > | < > | DAGScheduler was informed (through scheduler:DAGScheduler.md#executorAdded[executorAdded]) that an executor was spun up on a host. | [[ExecutorLost]] ExecutorLost | < > | Posted to notify scheduler:DAGScheduler.md#executorLost[DAGScheduler that an executor was lost]. ExecutorLost conveys the following information: execId ExecutorLossReason NOTE: The input filesLost for < > is enabled when ExecutorLossReason is SlaveLost with workerLost enabled (it is disabled by default). NOTE: < > is also called when DAGScheduler is informed that a < FetchFailed exception>>. | < > | | scheduler:TaskSetManager.md[TaskSetManager] informs DAGScheduler (through scheduler:DAGScheduler.md#taskGettingResult[taskGettingResult]) that a task has completed and results are being fetched remotely. | < > | < > | DAGScheduler was requested to scheduler:DAGScheduler.md#cancelJob[cancel a job]. | < > | < > | DAGScheduler was requested to scheduler:DAGScheduler.md#cancelJobGroup[cancel a job group]. | [[MapStageSubmitted]] MapStageSubmitted | < > | Posted to inform DAGScheduler that ROOT:SparkContext.md#submitMapStage[ SparkContext submitted a MapStage for execution] (through scheduler:DAGScheduler.md#submitMapStage[submitMapStage]). MapStageSubmitted conveys the following information: A job identifier (as jobId ) The ShuffleDependency A CallSite (as callSite ) The JobListener to inform about the status of the stage. Properties of the execution | < > | < > | DAGScheduler was informed that a scheduler:DAGScheduler.md#handleTaskCompletion-FetchFailed[task has failed due to FetchFailed exception]. | < > | < > | DAGScheduler was requested to scheduler:DAGScheduler.md#cancelStage[cancel a stage]. | < > | < > | DAGScheduler was requested to scheduler:DAGScheduler.md#taskSetFailed[cancel a TaskSet ] |=== == [[GettingResultEvent]] GettingResultEvent Event and handleGetTaskResult Handler [source, scala] \u00b6 GettingResultEvent(taskInfo: TaskInfo) extends DAGSchedulerEvent \u00b6 GettingResultEvent is a DAGSchedulerEvent that triggers < > (on a separate thread). NOTE: GettingResultEvent is posted to inform DAGScheduler (through scheduler:DAGScheduler.md#taskGettingResult[taskGettingResult]) that a scheduler:TaskSetManager.md#handleTaskGettingResult[task fetches results]. === [[handleGetTaskResult]] handleGetTaskResult Handler [source, scala] \u00b6 handleGetTaskResult(taskInfo: TaskInfo): Unit \u00b6 handleGetTaskResult merely posts ROOT:SparkListener.md#SparkListenerTaskGettingResult[SparkListenerTaskGettingResult] (to scheduler:DAGScheduler.md#listenerBus[ LiveListenerBus Event Bus]). == [[BeginEvent]] BeginEvent Event and handleBeginEvent Handler [source, scala] \u00b6 BeginEvent(task: Task[_], taskInfo: TaskInfo) extends DAGSchedulerEvent \u00b6 BeginEvent is a DAGSchedulerEvent that triggers < > (on a separate thread). NOTE: BeginEvent is posted to inform DAGScheduler (through scheduler:DAGScheduler.md#taskStarted[taskStarted]) that a scheduler:TaskSetManager.md#resourceOffer[ TaskSetManager starts a task]. == [[JobGroupCancelled]] JobGroupCancelled Event and handleJobGroupCancelled Handler [source, scala] \u00b6 JobGroupCancelled(groupId: String) extends DAGSchedulerEvent \u00b6 JobGroupCancelled is a DAGSchedulerEvent that triggers < > (on a separate thread). NOTE: JobGroupCancelled is posted when DAGScheduler is informed (through scheduler:DAGScheduler.md#cancelJobGroup[cancelJobGroup]) that ROOT:SparkContext.md#cancelJobGroup[ SparkContext was requested to cancel a job group]. === [[handleJobGroupCancelled]] handleJobGroupCancelled Handler [source, scala] \u00b6 handleJobGroupCancelled(groupId: String): Unit \u00b6 handleJobGroupCancelled finds active jobs in a group and cancels them. Internally, handleJobGroupCancelled computes all the active jobs (registered in the internal scheduler:DAGScheduler.md#activeJobs[collection of active jobs]) that have spark.jobGroup.id scheduling property set to groupId . handleJobGroupCancelled then < > in the group one by one and the cancellation reason: \"part of cancelled job group [groupId]\". == [[handleMapStageSubmitted]] Getting Notified that ShuffleDependency Was Submitted -- handleMapStageSubmitted Handler [source, scala] \u00b6 handleMapStageSubmitted( jobId: Int, dependency: ShuffleDependency[_, _, _], callSite: CallSite, listener: JobListener, properties: Properties): Unit . MapStageSubmitted Event Handling image::scheduler-handlemapstagesubmitted.png[align=\"center\"] handleMapStageSubmitted scheduler:DAGScheduler.md#getOrCreateShuffleMapStage[finds or creates a new ShuffleMapStage ] for the input ShuffleDependency and jobId . handleMapStageSubmitted creates an spark-scheduler-ActiveJob.md[ActiveJob] (with the input jobId , callSite , listener and properties , and the ShuffleMapStage ). handleMapStageSubmitted scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations]. CAUTION: FIXME Why is this clearing here so important? You should see the following INFO messages in the logs: Got map stage job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingStages] handleMapStageSubmitted registers the new job in scheduler:DAGScheduler.md#jobIdToActiveJob[jobIdToActiveJob] and scheduler:DAGScheduler.md#activeJobs[activeJobs] internal registries, and scheduler:ShuffleMapStage.md#addActiveJob[with the final ShuffleMapStage ]. NOTE: ShuffleMapStage can have multiple ActiveJob s registered. handleMapStageSubmitted scheduler:DAGScheduler.md#jobIdToStageIds[finds all the registered stages for the input jobId ] and collects scheduler:Stage.md#latestInfo[their latest StageInfo ]. In the end, handleMapStageSubmitted posts ROOT:SparkListener.md#SparkListenerJobStart[SparkListenerJobStart] message to scheduler:LiveListenerBus.md[] and scheduler:DAGScheduler.md#submitStage[submits the ShuffleMapStage ]. In case the scheduler:ShuffleMapStage.md#isAvailable[ ShuffleMapStage could be available] already, handleMapStageSubmitted scheduler:DAGScheduler.md#markMapStageJobAsFinished[marks the job finished]. NOTE: DAGScheduler scheduler:MapOutputTracker.md#getStatistics[requests MapOutputTrackerMaster for statistics for ShuffleDependency ] that it uses for handleMapStageSubmitted. NOTE: MapOutputTrackerMaster is passed in when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. When handleMapStageSubmitted could not find or create a ShuffleMapStage , you should see the following WARN message in the logs. WARN Creating new stage failed due to exception - job: [id] handleMapStageSubmitted notifies listener about the job failure and exits. NOTE: MapStageSubmitted event processing is very similar to < > events. [TIP] \u00b6 The difference between < > and < >: handleMapStageSubmitted has a ShuffleDependency among the input parameters while handleJobSubmitted has finalRDD , func , and partitions . handleMapStageSubmitted initializes finalStage as getShuffleMapStage(dependency, jobId) while handleJobSubmitted as finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite) handleMapStageSubmitted INFO logs Got map stage job %s (%s) with %d output partitions with dependency.rdd.partitions.length while handleJobSubmitted does Got job %s (%s) with %d output partitions with partitions.length . FIXME: Could the above be cut to ActiveJob.numPartitions ? handleMapStageSubmitted adds a new job with finalStage.addActiveJob(job) while handleJobSubmitted sets with finalStage.setActiveJob(job) . handleMapStageSubmitted checks if the final stage has already finished, tells the listener and removes it using the code: + [source, scala] if (finalStage.isAvailable) { markMapStageJobAsFinished(job, mapOutputTracker.getStatistics(dependency)) } ==== == [[resubmitFailedStages]] resubmitFailedStages Handler [source, scala] \u00b6 resubmitFailedStages(): Unit \u00b6 resubmitFailedStages iterates over the internal scheduler:DAGScheduler.md#failedStages[collection of failed stages] and scheduler:DAGScheduler.md#submitStage[submits] them. NOTE: resubmitFailedStages does nothing when there are no scheduler:DAGScheduler.md#failedStages[failed stages reported]. You should see the following INFO message in the logs: INFO Resubmitting failed stages resubmitFailedStages scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations] first. It then makes a copy of the scheduler:DAGScheduler.md#failedStages[collection of failed stages] so DAGScheduler can track failed stages afresh. NOTE: At this point DAGScheduler has no failed stages reported. The previously-reported failed stages are sorted by the corresponding job ids in incremental order and scheduler:DAGScheduler.md#submitStage[resubmitted]. == [[handleExecutorLost]] Getting Notified that Executor Is Lost -- handleExecutorLost Handler [source, scala] \u00b6 handleExecutorLost( execId: String, filesLost: Boolean, maybeEpoch: Option[Long] = None): Unit handleExecutorLost checks whether the input optional maybeEpoch is defined and if not requests the scheduler:MapOutputTracker.md#getEpoch[current epoch from MapOutputTrackerMaster ]. NOTE: MapOutputTrackerMaster is passed in (as mapOutputTracker ) when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. CAUTION: FIXME When is maybeEpoch passed in? .DAGScheduler.handleExecutorLost image::dagscheduler-handleExecutorLost.png[align=\"center\"] Recurring ExecutorLost events lead to the following repeating DEBUG message in the logs: DEBUG Additional executor lost message for [execId] (epoch [currentEpoch]) NOTE: handleExecutorLost handler uses DAGScheduler 's failedEpoch and FIXME internal registries. Otherwise, when the executor execId is not in the scheduler:DAGScheduler.md#failedEpoch[list of executor lost] or the executor failure's epoch is smaller than the input maybeEpoch , the executor's lost event is recorded in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry]. CAUTION: FIXME Describe the case above in simpler non-technical words. Perhaps change the order, too. You should see the following INFO message in the logs: INFO Executor lost: [execId] (epoch [epoch]) storage:BlockManagerMaster.md#removeExecutor[ BlockManagerMaster is requested to remove the lost executor execId ]. CAUTION: FIXME Review what's filesLost . handleExecutorLost exits unless the ExecutorLost event was for a map output fetch operation (and the input filesLost is true ) or deploy:ExternalShuffleService.md[external shuffle service] is not used. In such a case, you should see the following INFO message in the logs: INFO Shuffle files lost for executor: [execId] (epoch [epoch]) handleExecutorLost walks over all scheduler:ShuffleMapStage.md[ShuffleMapStage]s in scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] and do the following (in order): ShuffleMapStage.removeOutputsOnExecutor(execId) is called scheduler:MapOutputTrackerMaster.md#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs(shuffleId, stage.outputLocInMapOutputTrackerFormat(), changeEpoch = true)] is called. In case scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] has no shuffles registered, scheduler:MapOutputTrackerMaster.md#incrementEpoch[ MapOutputTrackerMaster is requested to increment epoch]. Ultimatelly, DAGScheduler scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations]. == [[handleJobCancellation]] handleJobCancellation Handler [source, scala] \u00b6 handleJobCancellation(jobId: Int, reason: String = \"\") \u00b6 handleJobCancellation first makes sure that the input jobId has been registered earlier (using scheduler:DAGScheduler.md#jobIdToStageIds[jobIdToStageIds] internal registry). If the input jobId is not known to DAGScheduler, you should see the following DEBUG message in the logs: DEBUG DAGScheduler: Trying to cancel unregistered job [jobId] Otherwise, handleJobCancellation scheduler:DAGScheduler.md#failJobAndIndependentStages[fails the active job and all independent stages] (by looking up the active job using scheduler:DAGScheduler.md#jobIdToActiveJob[jobIdToActiveJob]) with failure reason: Job [jobId] cancelled [reason] == [[handleTaskCompletion]] Getting Notified That Task Has Finished -- handleTaskCompletion Handler [source, scala] \u00b6 handleTaskCompletion(event: CompletionEvent): Unit \u00b6 .DAGScheduler and CompletionEvent image::dagscheduler-tasksetmanager.png[align=\"center\"] NOTE: CompletionEvent holds contextual information about the completed task. . CompletionEvent Properties [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Property | Description | task | Completed scheduler:Task.md[Task] instance for a stage, partition and stage attempt. | reason | TaskEndReason ...FIXME | result | Result of the task | accumUpdates | spark-accumulators.md[Accumulators] with...FIXME | taskInfo | spark-scheduler-TaskInfo.md[TaskInfo] |=== handleTaskCompletion starts by scheduler:OutputCommitCoordinator.md#taskCompleted[notifying OutputCommitCoordinator that a task completed]. handleTaskCompletion executor:TaskMetrics.md#fromAccumulators[re-creates TaskMetrics ] (using < accumUpdates accumulators of the input event >>). NOTE: executor:TaskMetrics.md[] can be empty when the task has failed. handleTaskCompletion announces task completion application-wide (by posting a ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] to scheduler:LiveListenerBus.md[]). handleTaskCompletion checks the stage of the task out in the scheduler:DAGScheduler.md#stageIdToStage[ stageIdToStage internal registry] and if not found, it simply exits. handleTaskCompletion branches off per TaskEndReason (as event.reason ). . handleTaskCompletion Branches per TaskEndReason [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskEndReason | Description | < > | Acts according to the type of the task that completed, i.e. < > and < >. < > < > | ExceptionFailure | scheduler:DAGScheduler.md#updateAccumulators[Updates accumulators] (with partial values from the task). | ExecutorLostFailure | Does nothing | TaskCommitDenied | Does nothing | TaskKilled | Does nothing | TaskResultLost | Does nothing | UnknownReason | Does nothing |=== === [[handleTaskCompletion-Success]] Handling Successful Task Completion When a task has finished successfully (i.e. Success end reason), handleTaskCompletion marks the partition as no longer pending (i.e. the partition the task worked on is removed from pendingPartitions of the stage). NOTE: A Stage tracks its own pending partitions using scheduler:Stage.md#pendingPartitions[ pendingPartitions property]. handleTaskCompletion branches off given the type of the task that completed, i.e. < > and < >. ==== [[handleTaskCompletion-Success-ResultTask]] Handling Successful ResultTask Completion For scheduler:ResultTask.md[ResultTask], the stage is assumed a scheduler:ResultStage.md[ResultStage]. handleTaskCompletion finds the ActiveJob associated with the ResultStage . NOTE: scheduler:ResultStage.md[ResultStage] tracks the optional ActiveJob as scheduler:ResultStage.md#activeJob[ activeJob property]. There could only be one active job for a ResultStage . If there is no job for the ResultStage , you should see the following INFO message in the logs: Ignoring result from [task] because its job has finished Otherwise, when the ResultStage has a ActiveJob , handleTaskCompletion checks the status of the partition output for the partition the ResultTask ran for. NOTE: ActiveJob tracks task completions in finished property with flags for every partition in a stage. When the flag for a partition is enabled (i.e. true ), it is assumed that the partition has been computed (and no results from any ResultTask are expected and hence simply ignored). CAUTION: FIXME Describe why could a partition has more ResultTask running. handleTaskCompletion ignores the CompletionEvent when the partition has already been marked as completed for the stage and simply exits. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The partition for the ActiveJob (of the ResultStage ) is marked as computed and the number of partitions calculated increased. NOTE: ActiveJob tracks what partitions have already been computed and their number. If the ActiveJob has finished (when the number of partitions computed is exactly the number of partitions in a stage) handleTaskCompletion does the following (in order): scheduler:DAGScheduler.md#markStageAsFinished[Marks ResultStage computed]. scheduler:DAGScheduler.md#cleanupStateForJobAndIndependentStages[Cleans up after ActiveJob and independent stages]. Announces the job completion application-wide (by posting a ROOT:SparkListener.md#SparkListenerJobEnd[SparkListenerJobEnd] to scheduler:LiveListenerBus.md[]). In the end, handleTaskCompletion notifies JobListener of the ActiveJob that the task succeeded . NOTE: A task succeeded notification holds the output index and the result. When the notification throws an exception (because it runs user code), handleTaskCompletion notifies JobListener about the failure (wrapping it inside a SparkDriverExecutionException exception). ==== [[handleTaskCompletion-Success-ShuffleMapTask]] Handling Successful ShuffleMapTask Completion For scheduler:ShuffleMapTask.md[ShuffleMapTask], the stage is assumed a scheduler:ShuffleMapStage.md[ShuffleMapStage]. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The task's result is assumed scheduler:MapStatus.md[MapStatus] that knows the executor where the task has finished. You should see the following DEBUG message in the logs: DEBUG DAGScheduler: ShuffleMapTask finished on [execId] If the executor is registered in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry] and the epoch of the completed task is not greater than that of the executor (as in failedEpoch registry), you should see the following INFO message in the logs: INFO DAGScheduler: Ignoring possibly bogus [task] completion from executor [executorId] Otherwise, handleTaskCompletion scheduler:ShuffleMapStage.md#addOutputLoc[registers the MapStatus result for the partition with the stage] (of the completed task). handleTaskCompletion does more processing only if the ShuffleMapStage is registered as still running (in scheduler:DAGScheduler.md#runningStages[ runningStages internal registry]) and the scheduler:Stage.md#pendingPartitions[ ShuffleMapStage stage has no pending partitions to compute]. The ShuffleMapStage is < >. You should see the following INFO messages in the logs: INFO DAGScheduler: looking for newly runnable stages INFO DAGScheduler: running: [runningStages] INFO DAGScheduler: waiting: [waitingStages] INFO DAGScheduler: failed: [failedStages] handleTaskCompletion scheduler:MapOutputTrackerMaster.md#registerMapOutputs[registers the shuffle map outputs of the ShuffleDependency with MapOutputTrackerMaster ] (with the epoch incremented) and scheduler:DAGScheduler.md#clearCacheLocs[clears internal cache of the stage's RDD block locations]. NOTE: scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] is given when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. If the scheduler:ShuffleMapStage.md#isAvailable[ ShuffleMapStage stage is ready], all scheduler:ShuffleMapStage.md#mapStageJobs[active jobs of the stage] (aka map-stage jobs ) are scheduler:DAGScheduler.md#markMapStageJobAsFinished[marked as finished] (with scheduler:MapOutputTrackerMaster.md#getStatistics[ MapOutputStatistics from MapOutputTrackerMaster for the ShuffleDependency ]). NOTE: A ShuffleMapStage stage is ready (aka available ) when all partitions have shuffle outputs, i.e. when their tasks have completed. Eventually, handleTaskCompletion scheduler:DAGScheduler.md#submitWaitingChildStages[submits waiting child stages (of the ready ShuffleMapStage )]. If however the ShuffleMapStage is not ready, you should see the following INFO message in the logs: INFO DAGScheduler: Resubmitting [shuffleStage] ([shuffleStage.name]) because some of its tasks had failed: [missingPartitions] In the end, handleTaskCompletion scheduler:DAGScheduler.md#submitStage[submits the ShuffleMapStage for execution]. === [[handleTaskCompletion-Resubmitted]] TaskEndReason: Resubmitted For Resubmitted case, you should see the following INFO message in the logs: INFO Resubmitted [task], so marking it as still running The task (by task.partitionId ) is added to the collection of pending partitions of the stage (using stage.pendingPartitions ). TIP: A stage knows how many partitions are yet to be calculated. A task knows about the partition id for which it was launched. === [[handleTaskCompletion-FetchFailed]] Task Failed with FetchFailed Exception -- TaskEndReason: FetchFailed [source, scala] \u00b6 FetchFailed( bmAddress: BlockManagerId, shuffleId: Int, mapId: Int, reduceId: Int, message: String) extends TaskFailedReason . FetchFailed Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | bmAddress | storage:BlockManagerId.md[] | shuffleId | Used when... | mapId | Used when... | reduceId | Used when... | failureMessage | Used when... |=== NOTE: A task knows about the id of the stage it belongs to. When FetchFailed happens, stageIdToStage is used to access the failed stage (using task.stageId and the task is available in event in handleTaskCompletion(event: CompletionEvent) ). shuffleToMapStage is used to access the map stage (using shuffleId ). If failedStage.latestInfo.attemptId != task.stageAttemptId , you should see the following INFO in the logs: INFO Ignoring fetch failure from [task] as it's from [failedStage] attempt [task.stageAttemptId] and there is a more recent attempt for that stage (attempt ID [failedStage.latestInfo.attemptId]) running CAUTION: FIXME What does failedStage.latestInfo.attemptId != task.stageAttemptId mean? And the case finishes. Otherwise, the case continues. If the failed stage is in runningStages , the following INFO message shows in the logs: INFO Marking [failedStage] ([failedStage.name]) as failed due to a fetch failure from [mapStage] ([mapStage.name]) markStageAsFinished(failedStage, Some(failureMessage)) is called. CAUTION: FIXME What does markStageAsFinished do? If the failed stage is not in runningStages , the following DEBUG message shows in the logs: DEBUG Received fetch failure from [task], but its from [failedStage] which is no longer running When disallowStageRetryForTest is set, abortStage(failedStage, \"Fetch failure will not retry stage due to testing config\", None) is called. CAUTION: FIXME Describe disallowStageRetryForTest and abortStage . If the scheduler:Stage.md#failedOnFetchAndShouldAbort[number of fetch failed attempts for the stage exceeds the allowed number], the scheduler:DAGScheduler.md#abortStage[failed stage is aborted] with the reason: [failedStage] ([name]) has failed the maximum allowable number of times: 4. Most recent failure reason: [failureMessage] If there are no failed stages reported (scheduler:DAGScheduler.md#failedStages[DAGScheduler.failedStages] is empty), the following INFO shows in the logs: INFO Resubmitting [mapStage] ([mapStage.name]) and [failedStage] ([failedStage.name]) due to fetch failure And the following code is executed: messageScheduler.schedule( new Runnable { override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages) }, DAGScheduler.RESUBMIT_TIMEOUT, TimeUnit.MILLISECONDS) CAUTION: FIXME What does the above code do? For all the cases, the failed stage and map stages are both added to the internal scheduler:DAGScheduler.md#failedStages[registry of failed stages]. If mapId (in the FetchFailed object for the case) is provided, the map stage output is cleaned up (as it is broken) using mapStage.removeOutputLoc(mapId, bmAddress) and scheduler:MapOutputTracker.md#unregisterMapOutput[MapOutputTrackerMaster.unregisterMapOutput(shuffleId, mapId, bmAddress)] methods. CAUTION: FIXME What does mapStage.removeOutputLoc do? If BlockManagerId (as bmAddress in the FetchFailed object) is defined, handleTaskCompletion < > (with filesLost enabled and maybeEpoch from the scheduler:Task.md#epoch[Task] that completed).","title":"DAGSchedulerEventProcessLoop"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#gettingresulteventtaskinfo-taskinfo-extends-dagschedulerevent","text":"GettingResultEvent is a DAGSchedulerEvent that triggers < > (on a separate thread). NOTE: GettingResultEvent is posted to inform DAGScheduler (through scheduler:DAGScheduler.md#taskGettingResult[taskGettingResult]) that a scheduler:TaskSetManager.md#handleTaskGettingResult[task fetches results]. === [[handleGetTaskResult]] handleGetTaskResult Handler","title":"GettingResultEvent(taskInfo: TaskInfo) extends DAGSchedulerEvent"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#handlegettaskresulttaskinfo-taskinfo-unit","text":"handleGetTaskResult merely posts ROOT:SparkListener.md#SparkListenerTaskGettingResult[SparkListenerTaskGettingResult] (to scheduler:DAGScheduler.md#listenerBus[ LiveListenerBus Event Bus]). == [[BeginEvent]] BeginEvent Event and handleBeginEvent Handler","title":"handleGetTaskResult(taskInfo: TaskInfo): Unit"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#begineventtask-task_-taskinfo-taskinfo-extends-dagschedulerevent","text":"BeginEvent is a DAGSchedulerEvent that triggers < > (on a separate thread). NOTE: BeginEvent is posted to inform DAGScheduler (through scheduler:DAGScheduler.md#taskStarted[taskStarted]) that a scheduler:TaskSetManager.md#resourceOffer[ TaskSetManager starts a task]. == [[JobGroupCancelled]] JobGroupCancelled Event and handleJobGroupCancelled Handler","title":"BeginEvent(task: Task[_], taskInfo: TaskInfo) extends DAGSchedulerEvent"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#jobgroupcancelledgroupid-string-extends-dagschedulerevent","text":"JobGroupCancelled is a DAGSchedulerEvent that triggers < > (on a separate thread). NOTE: JobGroupCancelled is posted when DAGScheduler is informed (through scheduler:DAGScheduler.md#cancelJobGroup[cancelJobGroup]) that ROOT:SparkContext.md#cancelJobGroup[ SparkContext was requested to cancel a job group]. === [[handleJobGroupCancelled]] handleJobGroupCancelled Handler","title":"JobGroupCancelled(groupId: String) extends DAGSchedulerEvent"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#handlejobgroupcancelledgroupid-string-unit","text":"handleJobGroupCancelled finds active jobs in a group and cancels them. Internally, handleJobGroupCancelled computes all the active jobs (registered in the internal scheduler:DAGScheduler.md#activeJobs[collection of active jobs]) that have spark.jobGroup.id scheduling property set to groupId . handleJobGroupCancelled then < > in the group one by one and the cancellation reason: \"part of cancelled job group [groupId]\". == [[handleMapStageSubmitted]] Getting Notified that ShuffleDependency Was Submitted -- handleMapStageSubmitted Handler","title":"handleJobGroupCancelled(groupId: String): Unit"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_5","text":"handleMapStageSubmitted( jobId: Int, dependency: ShuffleDependency[_, _, _], callSite: CallSite, listener: JobListener, properties: Properties): Unit . MapStageSubmitted Event Handling image::scheduler-handlemapstagesubmitted.png[align=\"center\"] handleMapStageSubmitted scheduler:DAGScheduler.md#getOrCreateShuffleMapStage[finds or creates a new ShuffleMapStage ] for the input ShuffleDependency and jobId . handleMapStageSubmitted creates an spark-scheduler-ActiveJob.md[ActiveJob] (with the input jobId , callSite , listener and properties , and the ShuffleMapStage ). handleMapStageSubmitted scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations]. CAUTION: FIXME Why is this clearing here so important? You should see the following INFO messages in the logs: Got map stage job [id] ([callSite]) with [number] output partitions Final stage: [stage] ([name]) Parents of final stage: [parents] Missing parents: [missingStages] handleMapStageSubmitted registers the new job in scheduler:DAGScheduler.md#jobIdToActiveJob[jobIdToActiveJob] and scheduler:DAGScheduler.md#activeJobs[activeJobs] internal registries, and scheduler:ShuffleMapStage.md#addActiveJob[with the final ShuffleMapStage ]. NOTE: ShuffleMapStage can have multiple ActiveJob s registered. handleMapStageSubmitted scheduler:DAGScheduler.md#jobIdToStageIds[finds all the registered stages for the input jobId ] and collects scheduler:Stage.md#latestInfo[their latest StageInfo ]. In the end, handleMapStageSubmitted posts ROOT:SparkListener.md#SparkListenerJobStart[SparkListenerJobStart] message to scheduler:LiveListenerBus.md[] and scheduler:DAGScheduler.md#submitStage[submits the ShuffleMapStage ]. In case the scheduler:ShuffleMapStage.md#isAvailable[ ShuffleMapStage could be available] already, handleMapStageSubmitted scheduler:DAGScheduler.md#markMapStageJobAsFinished[marks the job finished]. NOTE: DAGScheduler scheduler:MapOutputTracker.md#getStatistics[requests MapOutputTrackerMaster for statistics for ShuffleDependency ] that it uses for handleMapStageSubmitted. NOTE: MapOutputTrackerMaster is passed in when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. When handleMapStageSubmitted could not find or create a ShuffleMapStage , you should see the following WARN message in the logs. WARN Creating new stage failed due to exception - job: [id] handleMapStageSubmitted notifies listener about the job failure and exits. NOTE: MapStageSubmitted event processing is very similar to < > events.","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#tip","text":"The difference between < > and < >: handleMapStageSubmitted has a ShuffleDependency among the input parameters while handleJobSubmitted has finalRDD , func , and partitions . handleMapStageSubmitted initializes finalStage as getShuffleMapStage(dependency, jobId) while handleJobSubmitted as finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite) handleMapStageSubmitted INFO logs Got map stage job %s (%s) with %d output partitions with dependency.rdd.partitions.length while handleJobSubmitted does Got job %s (%s) with %d output partitions with partitions.length . FIXME: Could the above be cut to ActiveJob.numPartitions ? handleMapStageSubmitted adds a new job with finalStage.addActiveJob(job) while handleJobSubmitted sets with finalStage.setActiveJob(job) . handleMapStageSubmitted checks if the final stage has already finished, tells the listener and removes it using the code: + [source, scala] if (finalStage.isAvailable) { markMapStageJobAsFinished(job, mapOutputTracker.getStatistics(dependency)) } ==== == [[resubmitFailedStages]] resubmitFailedStages Handler","title":"[TIP]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#resubmitfailedstages-unit","text":"resubmitFailedStages iterates over the internal scheduler:DAGScheduler.md#failedStages[collection of failed stages] and scheduler:DAGScheduler.md#submitStage[submits] them. NOTE: resubmitFailedStages does nothing when there are no scheduler:DAGScheduler.md#failedStages[failed stages reported]. You should see the following INFO message in the logs: INFO Resubmitting failed stages resubmitFailedStages scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations] first. It then makes a copy of the scheduler:DAGScheduler.md#failedStages[collection of failed stages] so DAGScheduler can track failed stages afresh. NOTE: At this point DAGScheduler has no failed stages reported. The previously-reported failed stages are sorted by the corresponding job ids in incremental order and scheduler:DAGScheduler.md#submitStage[resubmitted]. == [[handleExecutorLost]] Getting Notified that Executor Is Lost -- handleExecutorLost Handler","title":"resubmitFailedStages(): Unit"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_7","text":"handleExecutorLost( execId: String, filesLost: Boolean, maybeEpoch: Option[Long] = None): Unit handleExecutorLost checks whether the input optional maybeEpoch is defined and if not requests the scheduler:MapOutputTracker.md#getEpoch[current epoch from MapOutputTrackerMaster ]. NOTE: MapOutputTrackerMaster is passed in (as mapOutputTracker ) when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. CAUTION: FIXME When is maybeEpoch passed in? .DAGScheduler.handleExecutorLost image::dagscheduler-handleExecutorLost.png[align=\"center\"] Recurring ExecutorLost events lead to the following repeating DEBUG message in the logs: DEBUG Additional executor lost message for [execId] (epoch [currentEpoch]) NOTE: handleExecutorLost handler uses DAGScheduler 's failedEpoch and FIXME internal registries. Otherwise, when the executor execId is not in the scheduler:DAGScheduler.md#failedEpoch[list of executor lost] or the executor failure's epoch is smaller than the input maybeEpoch , the executor's lost event is recorded in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry]. CAUTION: FIXME Describe the case above in simpler non-technical words. Perhaps change the order, too. You should see the following INFO message in the logs: INFO Executor lost: [execId] (epoch [epoch]) storage:BlockManagerMaster.md#removeExecutor[ BlockManagerMaster is requested to remove the lost executor execId ]. CAUTION: FIXME Review what's filesLost . handleExecutorLost exits unless the ExecutorLost event was for a map output fetch operation (and the input filesLost is true ) or deploy:ExternalShuffleService.md[external shuffle service] is not used. In such a case, you should see the following INFO message in the logs: INFO Shuffle files lost for executor: [execId] (epoch [epoch]) handleExecutorLost walks over all scheduler:ShuffleMapStage.md[ShuffleMapStage]s in scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] and do the following (in order): ShuffleMapStage.removeOutputsOnExecutor(execId) is called scheduler:MapOutputTrackerMaster.md#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs(shuffleId, stage.outputLocInMapOutputTrackerFormat(), changeEpoch = true)] is called. In case scheduler:DAGScheduler.md#shuffleToMapStage[DAGScheduler's shuffleToMapStage internal registry] has no shuffles registered, scheduler:MapOutputTrackerMaster.md#incrementEpoch[ MapOutputTrackerMaster is requested to increment epoch]. Ultimatelly, DAGScheduler scheduler:DAGScheduler.md#clearCacheLocs[clears the internal cache of RDD partition locations]. == [[handleJobCancellation]] handleJobCancellation Handler","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#handlejobcancellationjobid-int-reason-string","text":"handleJobCancellation first makes sure that the input jobId has been registered earlier (using scheduler:DAGScheduler.md#jobIdToStageIds[jobIdToStageIds] internal registry). If the input jobId is not known to DAGScheduler, you should see the following DEBUG message in the logs: DEBUG DAGScheduler: Trying to cancel unregistered job [jobId] Otherwise, handleJobCancellation scheduler:DAGScheduler.md#failJobAndIndependentStages[fails the active job and all independent stages] (by looking up the active job using scheduler:DAGScheduler.md#jobIdToActiveJob[jobIdToActiveJob]) with failure reason: Job [jobId] cancelled [reason] == [[handleTaskCompletion]] Getting Notified That Task Has Finished -- handleTaskCompletion Handler","title":"handleJobCancellation(jobId: Int, reason: String = \"\")"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#handletaskcompletionevent-completionevent-unit","text":".DAGScheduler and CompletionEvent image::dagscheduler-tasksetmanager.png[align=\"center\"] NOTE: CompletionEvent holds contextual information about the completed task. . CompletionEvent Properties [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Property | Description | task | Completed scheduler:Task.md[Task] instance for a stage, partition and stage attempt. | reason | TaskEndReason ...FIXME | result | Result of the task | accumUpdates | spark-accumulators.md[Accumulators] with...FIXME | taskInfo | spark-scheduler-TaskInfo.md[TaskInfo] |=== handleTaskCompletion starts by scheduler:OutputCommitCoordinator.md#taskCompleted[notifying OutputCommitCoordinator that a task completed]. handleTaskCompletion executor:TaskMetrics.md#fromAccumulators[re-creates TaskMetrics ] (using < accumUpdates accumulators of the input event >>). NOTE: executor:TaskMetrics.md[] can be empty when the task has failed. handleTaskCompletion announces task completion application-wide (by posting a ROOT:SparkListener.md#SparkListenerTaskEnd[SparkListenerTaskEnd] to scheduler:LiveListenerBus.md[]). handleTaskCompletion checks the stage of the task out in the scheduler:DAGScheduler.md#stageIdToStage[ stageIdToStage internal registry] and if not found, it simply exits. handleTaskCompletion branches off per TaskEndReason (as event.reason ). . handleTaskCompletion Branches per TaskEndReason [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskEndReason | Description | < > | Acts according to the type of the task that completed, i.e. < > and < >. < > < > | ExceptionFailure | scheduler:DAGScheduler.md#updateAccumulators[Updates accumulators] (with partial values from the task). | ExecutorLostFailure | Does nothing | TaskCommitDenied | Does nothing | TaskKilled | Does nothing | TaskResultLost | Does nothing | UnknownReason | Does nothing |=== === [[handleTaskCompletion-Success]] Handling Successful Task Completion When a task has finished successfully (i.e. Success end reason), handleTaskCompletion marks the partition as no longer pending (i.e. the partition the task worked on is removed from pendingPartitions of the stage). NOTE: A Stage tracks its own pending partitions using scheduler:Stage.md#pendingPartitions[ pendingPartitions property]. handleTaskCompletion branches off given the type of the task that completed, i.e. < > and < >. ==== [[handleTaskCompletion-Success-ResultTask]] Handling Successful ResultTask Completion For scheduler:ResultTask.md[ResultTask], the stage is assumed a scheduler:ResultStage.md[ResultStage]. handleTaskCompletion finds the ActiveJob associated with the ResultStage . NOTE: scheduler:ResultStage.md[ResultStage] tracks the optional ActiveJob as scheduler:ResultStage.md#activeJob[ activeJob property]. There could only be one active job for a ResultStage . If there is no job for the ResultStage , you should see the following INFO message in the logs: Ignoring result from [task] because its job has finished Otherwise, when the ResultStage has a ActiveJob , handleTaskCompletion checks the status of the partition output for the partition the ResultTask ran for. NOTE: ActiveJob tracks task completions in finished property with flags for every partition in a stage. When the flag for a partition is enabled (i.e. true ), it is assumed that the partition has been computed (and no results from any ResultTask are expected and hence simply ignored). CAUTION: FIXME Describe why could a partition has more ResultTask running. handleTaskCompletion ignores the CompletionEvent when the partition has already been marked as completed for the stage and simply exits. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The partition for the ActiveJob (of the ResultStage ) is marked as computed and the number of partitions calculated increased. NOTE: ActiveJob tracks what partitions have already been computed and their number. If the ActiveJob has finished (when the number of partitions computed is exactly the number of partitions in a stage) handleTaskCompletion does the following (in order): scheduler:DAGScheduler.md#markStageAsFinished[Marks ResultStage computed]. scheduler:DAGScheduler.md#cleanupStateForJobAndIndependentStages[Cleans up after ActiveJob and independent stages]. Announces the job completion application-wide (by posting a ROOT:SparkListener.md#SparkListenerJobEnd[SparkListenerJobEnd] to scheduler:LiveListenerBus.md[]). In the end, handleTaskCompletion notifies JobListener of the ActiveJob that the task succeeded . NOTE: A task succeeded notification holds the output index and the result. When the notification throws an exception (because it runs user code), handleTaskCompletion notifies JobListener about the failure (wrapping it inside a SparkDriverExecutionException exception). ==== [[handleTaskCompletion-Success-ShuffleMapTask]] Handling Successful ShuffleMapTask Completion For scheduler:ShuffleMapTask.md[ShuffleMapTask], the stage is assumed a scheduler:ShuffleMapStage.md[ShuffleMapStage]. handleTaskCompletion scheduler:DAGScheduler.md#updateAccumulators[updates accumulators]. The task's result is assumed scheduler:MapStatus.md[MapStatus] that knows the executor where the task has finished. You should see the following DEBUG message in the logs: DEBUG DAGScheduler: ShuffleMapTask finished on [execId] If the executor is registered in scheduler:DAGScheduler.md#failedEpoch[ failedEpoch internal registry] and the epoch of the completed task is not greater than that of the executor (as in failedEpoch registry), you should see the following INFO message in the logs: INFO DAGScheduler: Ignoring possibly bogus [task] completion from executor [executorId] Otherwise, handleTaskCompletion scheduler:ShuffleMapStage.md#addOutputLoc[registers the MapStatus result for the partition with the stage] (of the completed task). handleTaskCompletion does more processing only if the ShuffleMapStage is registered as still running (in scheduler:DAGScheduler.md#runningStages[ runningStages internal registry]) and the scheduler:Stage.md#pendingPartitions[ ShuffleMapStage stage has no pending partitions to compute]. The ShuffleMapStage is < >. You should see the following INFO messages in the logs: INFO DAGScheduler: looking for newly runnable stages INFO DAGScheduler: running: [runningStages] INFO DAGScheduler: waiting: [waitingStages] INFO DAGScheduler: failed: [failedStages] handleTaskCompletion scheduler:MapOutputTrackerMaster.md#registerMapOutputs[registers the shuffle map outputs of the ShuffleDependency with MapOutputTrackerMaster ] (with the epoch incremented) and scheduler:DAGScheduler.md#clearCacheLocs[clears internal cache of the stage's RDD block locations]. NOTE: scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] is given when scheduler:DAGScheduler.md#creating-instance[DAGScheduler is created]. If the scheduler:ShuffleMapStage.md#isAvailable[ ShuffleMapStage stage is ready], all scheduler:ShuffleMapStage.md#mapStageJobs[active jobs of the stage] (aka map-stage jobs ) are scheduler:DAGScheduler.md#markMapStageJobAsFinished[marked as finished] (with scheduler:MapOutputTrackerMaster.md#getStatistics[ MapOutputStatistics from MapOutputTrackerMaster for the ShuffleDependency ]). NOTE: A ShuffleMapStage stage is ready (aka available ) when all partitions have shuffle outputs, i.e. when their tasks have completed. Eventually, handleTaskCompletion scheduler:DAGScheduler.md#submitWaitingChildStages[submits waiting child stages (of the ready ShuffleMapStage )]. If however the ShuffleMapStage is not ready, you should see the following INFO message in the logs: INFO DAGScheduler: Resubmitting [shuffleStage] ([shuffleStage.name]) because some of its tasks had failed: [missingPartitions] In the end, handleTaskCompletion scheduler:DAGScheduler.md#submitStage[submits the ShuffleMapStage for execution]. === [[handleTaskCompletion-Resubmitted]] TaskEndReason: Resubmitted For Resubmitted case, you should see the following INFO message in the logs: INFO Resubmitted [task], so marking it as still running The task (by task.partitionId ) is added to the collection of pending partitions of the stage (using stage.pendingPartitions ). TIP: A stage knows how many partitions are yet to be calculated. A task knows about the partition id for which it was launched. === [[handleTaskCompletion-FetchFailed]] Task Failed with FetchFailed Exception -- TaskEndReason: FetchFailed","title":"handleTaskCompletion(event: CompletionEvent): Unit"},{"location":"scheduler/DAGSchedulerEventProcessLoop/#source-scala_10","text":"FetchFailed( bmAddress: BlockManagerId, shuffleId: Int, mapId: Int, reduceId: Int, message: String) extends TaskFailedReason . FetchFailed Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | bmAddress | storage:BlockManagerId.md[] | shuffleId | Used when... | mapId | Used when... | reduceId | Used when... | failureMessage | Used when... |=== NOTE: A task knows about the id of the stage it belongs to. When FetchFailed happens, stageIdToStage is used to access the failed stage (using task.stageId and the task is available in event in handleTaskCompletion(event: CompletionEvent) ). shuffleToMapStage is used to access the map stage (using shuffleId ). If failedStage.latestInfo.attemptId != task.stageAttemptId , you should see the following INFO in the logs: INFO Ignoring fetch failure from [task] as it's from [failedStage] attempt [task.stageAttemptId] and there is a more recent attempt for that stage (attempt ID [failedStage.latestInfo.attemptId]) running CAUTION: FIXME What does failedStage.latestInfo.attemptId != task.stageAttemptId mean? And the case finishes. Otherwise, the case continues. If the failed stage is in runningStages , the following INFO message shows in the logs: INFO Marking [failedStage] ([failedStage.name]) as failed due to a fetch failure from [mapStage] ([mapStage.name]) markStageAsFinished(failedStage, Some(failureMessage)) is called. CAUTION: FIXME What does markStageAsFinished do? If the failed stage is not in runningStages , the following DEBUG message shows in the logs: DEBUG Received fetch failure from [task], but its from [failedStage] which is no longer running When disallowStageRetryForTest is set, abortStage(failedStage, \"Fetch failure will not retry stage due to testing config\", None) is called. CAUTION: FIXME Describe disallowStageRetryForTest and abortStage . If the scheduler:Stage.md#failedOnFetchAndShouldAbort[number of fetch failed attempts for the stage exceeds the allowed number], the scheduler:DAGScheduler.md#abortStage[failed stage is aborted] with the reason: [failedStage] ([name]) has failed the maximum allowable number of times: 4. Most recent failure reason: [failureMessage] If there are no failed stages reported (scheduler:DAGScheduler.md#failedStages[DAGScheduler.failedStages] is empty), the following INFO shows in the logs: INFO Resubmitting [mapStage] ([mapStage.name]) and [failedStage] ([failedStage.name]) due to fetch failure And the following code is executed: messageScheduler.schedule( new Runnable { override def run(): Unit = eventProcessLoop.post(ResubmitFailedStages) }, DAGScheduler.RESUBMIT_TIMEOUT, TimeUnit.MILLISECONDS) CAUTION: FIXME What does the above code do? For all the cases, the failed stage and map stages are both added to the internal scheduler:DAGScheduler.md#failedStages[registry of failed stages]. If mapId (in the FetchFailed object for the case) is provided, the map stage output is cleaned up (as it is broken) using mapStage.removeOutputLoc(mapId, bmAddress) and scheduler:MapOutputTracker.md#unregisterMapOutput[MapOutputTrackerMaster.unregisterMapOutput(shuffleId, mapId, bmAddress)] methods. CAUTION: FIXME What does mapStage.removeOutputLoc do? If BlockManagerId (as bmAddress in the FetchFailed object) is defined, handleTaskCompletion < > (with filesLost enabled and maybeEpoch from the scheduler:Task.md#epoch[Task] that completed).","title":"[source, scala]"},{"location":"scheduler/ExternalClusterManager/","text":"ExternalClusterManager \u00b6 ExternalClusterManager is an abstraction of pluggable cluster managers that can create a SchedulerBackend and TaskScheduler for a given master URL (when SparkContext is created). Note The support for pluggable cluster managers was introduced in SPARK-13904 Add support for pluggable cluster manager . ExternalClusterManager can be registered using the java.util.ServiceLoader mechanism (with service markers under META-INF/services directory). Contract \u00b6 Checking Support for Master URL \u00b6 canCreate ( masterURL : String ) : Boolean Checks whether this cluster manager instance can create scheduler components for a given master URL Used when SparkContext is created (and requested for a cluster manager ) Creating SchedulerBackend \u00b6 createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend Creates a SchedulerBackend for a given SparkContext , master URL, and TaskScheduler . Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler ) Creating TaskScheduler \u00b6 createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler Creates a TaskScheduler for a given SparkContext and master URL Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler ) Initializing Scheduling Components \u00b6 initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit Initializes the TaskScheduler and SchedulerBackend Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler ) Implementations \u00b6 KubernetesClusterManager MesosClusterManager YarnClusterManager","title":"ExternalClusterManager"},{"location":"scheduler/ExternalClusterManager/#externalclustermanager","text":"ExternalClusterManager is an abstraction of pluggable cluster managers that can create a SchedulerBackend and TaskScheduler for a given master URL (when SparkContext is created). Note The support for pluggable cluster managers was introduced in SPARK-13904 Add support for pluggable cluster manager . ExternalClusterManager can be registered using the java.util.ServiceLoader mechanism (with service markers under META-INF/services directory).","title":"ExternalClusterManager"},{"location":"scheduler/ExternalClusterManager/#contract","text":"","title":"Contract"},{"location":"scheduler/ExternalClusterManager/#checking-support-for-master-url","text":"canCreate ( masterURL : String ) : Boolean Checks whether this cluster manager instance can create scheduler components for a given master URL Used when SparkContext is created (and requested for a cluster manager )","title":" Checking Support for Master URL"},{"location":"scheduler/ExternalClusterManager/#creating-schedulerbackend","text":"createSchedulerBackend ( sc : SparkContext , masterURL : String , scheduler : TaskScheduler ) : SchedulerBackend Creates a SchedulerBackend for a given SparkContext , master URL, and TaskScheduler . Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler )","title":" Creating SchedulerBackend"},{"location":"scheduler/ExternalClusterManager/#creating-taskscheduler","text":"createTaskScheduler ( sc : SparkContext , masterURL : String ) : TaskScheduler Creates a TaskScheduler for a given SparkContext and master URL Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler )","title":" Creating TaskScheduler"},{"location":"scheduler/ExternalClusterManager/#initializing-scheduling-components","text":"initialize ( scheduler : TaskScheduler , backend : SchedulerBackend ) : Unit Initializes the TaskScheduler and SchedulerBackend Used when SparkContext is created (and requested for a SchedulerBackend and TaskScheduler )","title":" Initializing Scheduling Components"},{"location":"scheduler/ExternalClusterManager/#implementations","text":"KubernetesClusterManager MesosClusterManager YarnClusterManager","title":"Implementations"},{"location":"scheduler/FIFOSchedulableBuilder/","text":"== FIFOSchedulableBuilder - SchedulableBuilder for FIFO Scheduling Mode FIFOSchedulableBuilder is a < > that holds a single spark-scheduler-Pool.md[Pool] (that is given when < FIFOSchedulableBuilder is created>>). NOTE: FIFOSchedulableBuilder is the scheduler:TaskSchedulerImpl.md#creating-instance[default SchedulableBuilder for TaskSchedulerImpl ]. NOTE: When FIFOSchedulableBuilder is created, the TaskSchedulerImpl passes its own rootPool (a part of scheduler:TaskScheduler.md#contract[TaskScheduler Contract]). FIFOSchedulableBuilder obeys the < > as follows: < > does nothing. addTaskSetManager spark-scheduler-Pool.md#addSchedulable[passes the input Schedulable to the one and only rootPool Pool (using addSchedulable )] and completely disregards the properties of the Schedulable. === [[creating-instance]] Creating FIFOSchedulableBuilder Instance FIFOSchedulableBuilder takes the following when created: [[rootPool]] rootPool spark-scheduler-Pool.md[Pool]","title":"FIFOSchedulableBuilder"},{"location":"scheduler/FairSchedulableBuilder/","text":"== [[FairSchedulableBuilder]] FairSchedulableBuilder -- SchedulableBuilder for FAIR Scheduling Mode FairSchedulableBuilder is a < > that is < > exclusively for scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] for FAIR scheduling mode (when ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR ). [[creating-instance]] FairSchedulableBuilder takes the following to be created: [[rootPool]] < > [[conf]] ROOT:SparkConf.md[] Once < >, TaskSchedulerImpl requests the FairSchedulableBuilder to < >. [[DEFAULT_SCHEDULER_FILE]] FairSchedulableBuilder uses the pools defined in an < > that is assumed to be the value of the ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property or the default fairscheduler.xml (that is < >). TIP: Use conf/fairscheduler.xml.template as a template for the < >. [[DEFAULT_POOL_NAME]] FairSchedulableBuilder always has the default pool defined (and < > unless done in the < >). [[FAIR_SCHEDULER_PROPERTIES]] [[spark.scheduler.pool]] FairSchedulableBuilder uses spark.scheduler.pool local property for the name of the pool to use when requested to < > (default: < >). NOTE: Use spark-sparkcontext-local-properties.md#setLocalProperty[SparkContext.setLocalProperty] to set properties per thread (aka local properties ) to group jobs in logical groups, e.g. to allow FairSchedulableBuilder to use spark.scheduler.pool property and to group jobs from different threads to be submitted for execution on a non-< > pool. [source, scala] \u00b6 scala> :type sc org.apache.spark.SparkContext sc.setLocalProperty(\"spark.scheduler.pool\", \"production\") // whatever is executed afterwards is submitted to production pool \u00b6 [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.scheduler.FairSchedulableBuilder logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.FairSchedulableBuilder=ALL Refer to < >. \u00b6 === [[allocations-file]] Allocation Pools Configuration File The allocation pools configuration file is an XML file. The default conf/fairscheduler.xml.template is as follows: [source, xml] \u00b6 FAIR 1 2 FIFO 2 3 TIP: The top-level element's name allocations can be anything. Spark does not insist on allocations and accepts any name. === [[buildPools]] Building (Tree of) Pools of Schedulables -- buildPools Method [source, scala] \u00b6 buildPools(): Unit \u00b6 NOTE: buildPools is part of the < > to build a tree of < >. buildPools < > if available and then < >. buildPools prints out the following INFO message to the logs when the configuration file (per the ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property) could be read: Creating Fair Scheduler pools from [file] buildPools prints out the following INFO message to the logs when the ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property was not used to define the configuration file and the < > is used instead: Creating Fair Scheduler pools from default file: [DEFAULT_SCHEDULER_FILE] When neither ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property nor the < > could be used, buildPools prints out the following WARN message to the logs: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in [DEFAULT_SCHEDULER_FILE] or set spark.scheduler.allocation.file to a file that contains the configuration. === [[addTaskSetManager]] addTaskSetManager Method [source, scala] \u00b6 addTaskSetManager(manager: Schedulable, properties: Properties): Unit \u00b6 NOTE: addTaskSetManager is part of the < > to register a new < > with the < > addTaskSetManager finds the pool by name (in the given Properties ) under the < > property or defaults to the < > pool if undefined. addTaskSetManager then requests the < > to < >. Unless found, addTaskSetManager creates a new < > with the < > (as if the < > pool were used) and requests the < > to < >. In the end, addTaskSetManager prints out the following WARN message to the logs: A job was submitted with scheduler pool [poolName], which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain [poolName]. Created [poolName] with default configuration (schedulingMode: [mode], minShare: [minShare], weight: [weight]) addTaskSetManager then requests the pool (found or newly-created) to < > the given < >. In the end, addTaskSetManager prints out the following INFO message to the logs: Added task set [name] tasks to pool [poolName] === [[buildDefaultPool]] Registering Default Pool -- buildDefaultPool Method [source, scala] \u00b6 buildDefaultPool(): Unit \u00b6 buildDefaultPool requests the < > to < > (one with the < > name). Unless already available, buildDefaultPool creates a < > with the following: < > pool name FIFO scheduling mode 0 for the initial minimum share 1 for the initial weight In the end, buildDefaultPool requests the < > to < > followed by the INFO message in the logs: Created default pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildDefaultPool is used exclusively when FairSchedulableBuilder is requested to < >. === [[buildFairSchedulerPool]] Building Pools from XML Allocations File -- buildFairSchedulerPool Internal Method [source, scala] \u00b6 buildFairSchedulerPool( is: InputStream, fileName: String): Unit buildFairSchedulerPool starts by loading the XML file from the given InputStream . For every pool element, buildFairSchedulerPool creates a < > with the following: Pool name per name attribute Scheduling mode per schedulingMode element (case-insensitive with FIFO as the default) Initial minimum share per minShare element (default: 0 ) Initial weight per weight element (default: 1 ) In the end, buildFairSchedulerPool requests the < > to < > followed by the INFO message in the logs: Created pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildFairSchedulerPool is used exclusively when FairSchedulableBuilder is requested to < >.","title":"FairSchedulableBuilder"},{"location":"scheduler/FairSchedulableBuilder/#source-scala","text":"scala> :type sc org.apache.spark.SparkContext sc.setLocalProperty(\"spark.scheduler.pool\", \"production\")","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#whatever-is-executed-afterwards-is-submitted-to-production-pool","text":"[[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.scheduler.FairSchedulableBuilder logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.FairSchedulableBuilder=ALL","title":"// whatever is executed afterwards is submitted to production pool"},{"location":"scheduler/FairSchedulableBuilder/#refer-to","text":"=== [[allocations-file]] Allocation Pools Configuration File The allocation pools configuration file is an XML file. The default conf/fairscheduler.xml.template is as follows:","title":"Refer to &lt;&gt;."},{"location":"scheduler/FairSchedulableBuilder/#source-xml","text":"FAIR 1 2 FIFO 2 3 TIP: The top-level element's name allocations can be anything. Spark does not insist on allocations and accepts any name. === [[buildPools]] Building (Tree of) Pools of Schedulables -- buildPools Method","title":"[source, xml]"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#buildpools-unit","text":"NOTE: buildPools is part of the < > to build a tree of < >. buildPools < > if available and then < >. buildPools prints out the following INFO message to the logs when the configuration file (per the ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property) could be read: Creating Fair Scheduler pools from [file] buildPools prints out the following INFO message to the logs when the ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property was not used to define the configuration file and the < > is used instead: Creating Fair Scheduler pools from default file: [DEFAULT_SCHEDULER_FILE] When neither ROOT:configuration-properties.md#spark.scheduler.allocation.file[spark.scheduler.allocation.file] configuration property nor the < > could be used, buildPools prints out the following WARN message to the logs: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in [DEFAULT_SCHEDULER_FILE] or set spark.scheduler.allocation.file to a file that contains the configuration. === [[addTaskSetManager]] addTaskSetManager Method","title":"buildPools(): Unit"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#addtasksetmanagermanager-schedulable-properties-properties-unit","text":"NOTE: addTaskSetManager is part of the < > to register a new < > with the < > addTaskSetManager finds the pool by name (in the given Properties ) under the < > property or defaults to the < > pool if undefined. addTaskSetManager then requests the < > to < >. Unless found, addTaskSetManager creates a new < > with the < > (as if the < > pool were used) and requests the < > to < >. In the end, addTaskSetManager prints out the following WARN message to the logs: A job was submitted with scheduler pool [poolName], which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain [poolName]. Created [poolName] with default configuration (schedulingMode: [mode], minShare: [minShare], weight: [weight]) addTaskSetManager then requests the pool (found or newly-created) to < > the given < >. In the end, addTaskSetManager prints out the following INFO message to the logs: Added task set [name] tasks to pool [poolName] === [[buildDefaultPool]] Registering Default Pool -- buildDefaultPool Method","title":"addTaskSetManager(manager: Schedulable, properties: Properties): Unit"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/FairSchedulableBuilder/#builddefaultpool-unit","text":"buildDefaultPool requests the < > to < > (one with the < > name). Unless already available, buildDefaultPool creates a < > with the following: < > pool name FIFO scheduling mode 0 for the initial minimum share 1 for the initial weight In the end, buildDefaultPool requests the < > to < > followed by the INFO message in the logs: Created default pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildDefaultPool is used exclusively when FairSchedulableBuilder is requested to < >. === [[buildFairSchedulerPool]] Building Pools from XML Allocations File -- buildFairSchedulerPool Internal Method","title":"buildDefaultPool(): Unit"},{"location":"scheduler/FairSchedulableBuilder/#source-scala_4","text":"buildFairSchedulerPool( is: InputStream, fileName: String): Unit buildFairSchedulerPool starts by loading the XML file from the given InputStream . For every pool element, buildFairSchedulerPool creates a < > with the following: Pool name per name attribute Scheduling mode per schedulingMode element (case-insensitive with FIFO as the default) Initial minimum share per minShare element (default: 0 ) Initial weight per weight element (default: 1 ) In the end, buildFairSchedulerPool requests the < > to < > followed by the INFO message in the logs: Created pool: [name], schedulingMode: [mode], minShare: [minShare], weight: [weight] NOTE: buildFairSchedulerPool is used exclusively when FairSchedulableBuilder is requested to < >.","title":"[source, scala]"},{"location":"scheduler/JobListener/","text":"JobListener \u00b6 JobListener is an abstraction of listeners that listen for job completion or failure events (after submitting a job to the DAGScheduler ). Contract \u00b6 taskSucceeded \u00b6 taskSucceeded ( index : Int , result : Any ) : Unit Used when DAGScheduler is requested to handleTaskCompletion or markMapStageJobAsFinished jobFailed \u00b6 jobFailed ( exception : Exception ) : Unit Used when DAGScheduler is requested to cleanUpAfterSchedulerStop , handleJobSubmitted , handleMapStageSubmitted , handleTaskCompletion or failJobAndIndependentStages Implementations \u00b6 ApproximateActionListener JobWaiter","title":"JobListener"},{"location":"scheduler/JobListener/#joblistener","text":"JobListener is an abstraction of listeners that listen for job completion or failure events (after submitting a job to the DAGScheduler ).","title":"JobListener"},{"location":"scheduler/JobListener/#contract","text":"","title":"Contract"},{"location":"scheduler/JobListener/#tasksucceeded","text":"taskSucceeded ( index : Int , result : Any ) : Unit Used when DAGScheduler is requested to handleTaskCompletion or markMapStageJobAsFinished","title":" taskSucceeded"},{"location":"scheduler/JobListener/#jobfailed","text":"jobFailed ( exception : Exception ) : Unit Used when DAGScheduler is requested to cleanUpAfterSchedulerStop , handleJobSubmitted , handleMapStageSubmitted , handleTaskCompletion or failJobAndIndependentStages","title":" jobFailed"},{"location":"scheduler/JobListener/#implementations","text":"ApproximateActionListener JobWaiter","title":"Implementations"},{"location":"scheduler/JobWaiter/","text":"JobWaiter \u00b6 JobWaiter is a JobListener to listen to task events and to know when all have finished successfully or not . Creating Instance \u00b6 JobWaiter takes the following to be created: DAGScheduler Job ID Total number of tasks Result Handler Function ( (Int, T) => Unit ) JobWaiter is created when DAGScheduler is requested to submit a job or a map stage . Scala Promise \u00b6 jobPromise : Promise [ Unit ] jobPromise is a Scala Promise that is completed when all tasks have finished successfully or failed with an exception . taskSucceeded \u00b6 taskSucceeded ( index : Int , result : Any ) : Unit taskSucceeded executes the Result Handler Function with the given index and result . taskSucceeded marks the waiter finished successfully when all tasks have finished. taskSucceeded is part of the JobListener abstraction. jobFailed \u00b6 jobFailed ( exception : Exception ) : Unit jobFailed marks the waiter failed. jobFailed is part of the JobListener abstraction.","title":"JobWaiter"},{"location":"scheduler/JobWaiter/#jobwaiter","text":"JobWaiter is a JobListener to listen to task events and to know when all have finished successfully or not .","title":"JobWaiter"},{"location":"scheduler/JobWaiter/#creating-instance","text":"JobWaiter takes the following to be created: DAGScheduler Job ID Total number of tasks Result Handler Function ( (Int, T) => Unit ) JobWaiter is created when DAGScheduler is requested to submit a job or a map stage .","title":"Creating Instance"},{"location":"scheduler/JobWaiter/#scala-promise","text":"jobPromise : Promise [ Unit ] jobPromise is a Scala Promise that is completed when all tasks have finished successfully or failed with an exception .","title":" Scala Promise"},{"location":"scheduler/JobWaiter/#tasksucceeded","text":"taskSucceeded ( index : Int , result : Any ) : Unit taskSucceeded executes the Result Handler Function with the given index and result . taskSucceeded marks the waiter finished successfully when all tasks have finished. taskSucceeded is part of the JobListener abstraction.","title":" taskSucceeded"},{"location":"scheduler/JobWaiter/#jobfailed","text":"jobFailed ( exception : Exception ) : Unit jobFailed marks the waiter failed. jobFailed is part of the JobListener abstraction.","title":" jobFailed"},{"location":"scheduler/LiveListenerBus/","text":"= LiveListenerBus LiveListenerBus is an event bus to announce application-wide events in a Spark application. It asynchronously passes < > to registered ROOT:SparkListener.md[]s (based on ROOT:configuration-properties.md#spark.extraListeners[spark.extraListeners] configuration property). .LiveListenerBus, SparkListenerEvents, and Senders image::spark-sparklistener-event-senders.png[align=\"center\"] LiveListenerBus is a single-JVM spark-SparkListenerBus.md[SparkListenerBus] that uses < >. Emitters are supposed to use < post method to post SparkListenerEvent events>>. NOTE: The event queue is http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html[java.util.concurrent.LinkedBlockingQueue ] with capacity of 10000 SparkListenerEvent events. == [[creating-instance]] Creating Instance LiveListenerBus takes the following to be created: [[conf]] ROOT:SparkConf.md[] LiveListenerBus is created (and < >) when SparkContext is requested to ROOT:SparkContext.md#listenerBus[initialize]. == [[start]] Starting LiveListenerBus [source, scala] \u00b6 start( sc: SparkContext): Unit start starts < >. Internally, it saves the input SparkContext for later use and starts < >. It makes sure that it only happens when LiveListenerBus has not been started before (i.e. started is disabled). If however LiveListenerBus has already been started, a IllegalStateException is thrown: [name] already started! == [[post]] Posting SparkListenerEvent Event [source, scala] \u00b6 post( event: SparkListenerEvent): Unit post puts the input event onto the internal eventQueue queue and releases the internal eventLock semaphore. If the event placement was not successful (and it could happen since it is tapped at 10000 events) < > method is called. The event publishing is only possible when stopped flag has been enabled. CAUTION: FIXME Who's enabling the stopped flag and when/why? If LiveListenerBus has been stopped, the following ERROR appears in the logs: [name] has already stopped! Dropping event [event] == [[onDropEvent]] Event Dropped Callback [source, scala] \u00b6 onDropEvent( event: SparkListenerEvent): Unit onDropEvent is called when no further events can be added to the internal eventQueue queue (while < >). It simply prints out the following ERROR message to the logs and ensures that it happens only once. ERROR Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. NOTE: It uses the internal logDroppedEvent atomic variable to track the state. == [[stop]] Stopping LiveListenerBus [source, scala] \u00b6 stop(): Unit \u00b6 stop releases the internal eventLock semaphore and waits until < > dies. It can only happen after all events were posted (and polling eventQueue gives nothing). It checks that started flag is enabled (i.e. true ) and throws a IllegalStateException otherwise. Attempted to stop [name] that has not yet started! stopped flag is enabled. == [[listenerThread]] listenerThread for Event Polling LiveListenerBus uses ROOT:spark-SparkListenerBus.md[] single-daemon thread that ensures that the polling events from the event queue is only after < > and only one event at a time. CAUTION: FIXME There is some logic around no events in the queue. == [[addToStatusQueue]] Registering SparkListenerInterface with Application Status Queue [source, scala] \u00b6 addToStatusQueue( listener: SparkListenerInterface): Unit addToStatusQueue simply < > to < > queue. addToStatusQueue is used when...FIXME == [[addToQueue]] Registering SparkListenerInterface with Queue [source, scala] \u00b6 addToQueue( listener: SparkListenerInterface, queue: String): Unit addToQueue...FIXME addToQueue is used when...FIXME","title":"LiveListenerBus"},{"location":"scheduler/LiveListenerBus/#source-scala","text":"start( sc: SparkContext): Unit start starts < >. Internally, it saves the input SparkContext for later use and starts < >. It makes sure that it only happens when LiveListenerBus has not been started before (i.e. started is disabled). If however LiveListenerBus has already been started, a IllegalStateException is thrown: [name] already started! == [[post]] Posting SparkListenerEvent Event","title":"[source, scala]"},{"location":"scheduler/LiveListenerBus/#source-scala_1","text":"post( event: SparkListenerEvent): Unit post puts the input event onto the internal eventQueue queue and releases the internal eventLock semaphore. If the event placement was not successful (and it could happen since it is tapped at 10000 events) < > method is called. The event publishing is only possible when stopped flag has been enabled. CAUTION: FIXME Who's enabling the stopped flag and when/why? If LiveListenerBus has been stopped, the following ERROR appears in the logs: [name] has already stopped! Dropping event [event] == [[onDropEvent]] Event Dropped Callback","title":"[source, scala]"},{"location":"scheduler/LiveListenerBus/#source-scala_2","text":"onDropEvent( event: SparkListenerEvent): Unit onDropEvent is called when no further events can be added to the internal eventQueue queue (while < >). It simply prints out the following ERROR message to the logs and ensures that it happens only once. ERROR Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler. NOTE: It uses the internal logDroppedEvent atomic variable to track the state. == [[stop]] Stopping LiveListenerBus","title":"[source, scala]"},{"location":"scheduler/LiveListenerBus/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/LiveListenerBus/#stop-unit","text":"stop releases the internal eventLock semaphore and waits until < > dies. It can only happen after all events were posted (and polling eventQueue gives nothing). It checks that started flag is enabled (i.e. true ) and throws a IllegalStateException otherwise. Attempted to stop [name] that has not yet started! stopped flag is enabled. == [[listenerThread]] listenerThread for Event Polling LiveListenerBus uses ROOT:spark-SparkListenerBus.md[] single-daemon thread that ensures that the polling events from the event queue is only after < > and only one event at a time. CAUTION: FIXME There is some logic around no events in the queue. == [[addToStatusQueue]] Registering SparkListenerInterface with Application Status Queue","title":"stop(): Unit"},{"location":"scheduler/LiveListenerBus/#source-scala_4","text":"addToStatusQueue( listener: SparkListenerInterface): Unit addToStatusQueue simply < > to < > queue. addToStatusQueue is used when...FIXME == [[addToQueue]] Registering SparkListenerInterface with Queue","title":"[source, scala]"},{"location":"scheduler/LiveListenerBus/#source-scala_5","text":"addToQueue( listener: SparkListenerInterface, queue: String): Unit addToQueue...FIXME addToQueue is used when...FIXME","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/","text":"MapOutputTracker \u00b6 MapOutputTracker is a base abstraction of < > that can < > and < >. MapOutputTracker is available using SparkEnv (on the driver and executors). SparkEnv . get . mapOutputTracker == [[extensions]] MapOutputTrackers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MapOutputTracker | Description | scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] | [[MapOutputTrackerMaster]] Runs on the driver | scheduler:MapOutputTrackerWorker.md[MapOutputTrackerWorker] | [[MapOutputTrackerWorker]] Runs on executors |=== == [[creating-instance]][[conf]] Creating Instance MapOutputTracker takes a single ROOT:SparkConf.md[SparkConf] to be created. == [[trackerEndpoint]][[ENDPOINT_NAME]] MapOutputTracker RPC Endpoint trackerEndpoint is a rpc:RpcEndpointRef.md[RpcEndpointRef] of the MapOutputTracker RPC endpoint. trackerEndpoint is initialized (registered or looked up) when SparkEnv is core:SparkEnv.md#create[created] for the driver and executors. trackerEndpoint is used to < >. trackerEndpoint is cleared ( null ) when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#stop[stop]. == [[serializeMapStatuses]] serializeMapStatuses Utility [source, scala] \u00b6 serializeMapStatuses( statuses: Array[MapStatus], broadcastManager: BroadcastManager, isLocal: Boolean, minBroadcastSize: Int): (Array[Byte], Broadcast[Array[Byte]]) serializeMapStatuses serializes the given array of map output locations into an efficient byte format (to send to reduce tasks). serializeMapStatuses compresses the serialized bytes using GZIP. They are supposed to be pretty compressible because many map outputs will be on the same hostname. Internally, serializeMapStatuses creates a Java {java-javadoc-url}/java/io/ByteArrayOutputStream.html[ByteArrayOutputStream]. serializeMapStatuses writes out 0 (direct) first. serializeMapStatuses creates a Java {java-javadoc-url}/java/util/zip/GZIPOutputStream.html[GZIPOutputStream] (with the ByteArrayOutputStream created) and writes out the given statuses array. serializeMapStatuses decides whether to return the output array (of the output stream) or use a broadcast variable based on the size of the byte array. If the size of the result byte array is the given minBroadcastSize threshold or bigger, serializeMapStatuses requests the input BroadcastManager to core:BroadcastManager.md#newBroadcast[create a broadcast variable]. serializeMapStatuses resets the ByteArrayOutputStream and starts over. serializeMapStatuses writes out 1 (broadcast) first. serializeMapStatuses creates a new Java GZIPOutputStream (with the ByteArrayOutputStream created) and writes out the broadcast variable. serializeMapStatuses prints out the following INFO message to the logs: [source,plaintext] \u00b6 Broadcast mapstatuses size = [length], actual size = [length] \u00b6 serializeMapStatuses is used when ShuffleStatus is requested to scheduler:ShuffleStatus.md#serializedMapStatus[serialize shuffle map output statuses]. == [[deserializeMapStatuses]] deserializeMapStatuses Utility [source, scala] \u00b6 deserializeMapStatuses( bytes: Array[Byte]): Array[MapStatus] deserializeMapStatuses...FIXME deserializeMapStatuses is used when...FIXME == [[sendTracker]] sendTracker Utility [source, scala] \u00b6 sendTracker( message: Any): Unit sendTracker...FIXME sendTracker is used when...FIXME == [[getMapSizesByExecutorId]] Finding Locations with Blocks and Sizes [source, scala] \u00b6 getMapSizesByExecutorId( shuffleId: Int, startPartition: Int, endPartition: Int): Seq[(BlockManagerId, Seq[(BlockId, Long)])] getMapSizesByExecutorId returns a collection of storage:BlockManagerId.md[]s with their blocks and sizes. When executed, you should see the following DEBUG message in the logs: Fetching outputs for shuffle [id], partitions [startPartition]-[endPartition] getMapSizesByExecutorId < > for the input shuffleId . NOTE: getMapSizesByExecutorId gets the map outputs for all the partitions (despite the method's signature). In the end, getMapSizesByExecutorId < > (as MapStatuses ) into the collection of storage:BlockManagerId.md[]s with their blocks and sizes. getMapSizesByExecutorId is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task]. == [[unregisterShuffle]] Deregistering Map Output Status Information of Shuffle Stage [source, scala] \u00b6 unregisterShuffle( shuffleId: Int): Unit Deregisters map output status information for the given shuffle stage Used when: ContextCleaner is requested for core:ContextCleaner.md#doCleanupShuffle[shuffle cleanup] BlockManagerSlaveEndpoint is requested to storage:BlockManagerSlaveEndpoint.md#RemoveShuffle[remove a shuffle] == [[stop]] Stopping MapOutputTracker [source, scala] \u00b6 stop(): Unit \u00b6 stop does nothing at all. stop is used when SparkEnv is requested to core:SparkEnv.md#stop[stop] (and stops all the services, incl. MapOutputTracker). == [[convertMapStatuses]] Converting MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes [source, scala] \u00b6 convertMapStatuses( shuffleId: Int, startPartition: Int, endPartition: Int, statuses: Array[MapStatus]): Seq[(BlockManagerId, Seq[(BlockId, Long)])] convertMapStatuses iterates over the input statuses array (of scheduler:MapStatus.md[MapStatus] entries indexed by map id) and creates a collection of storage:BlockManagerId.md[]s (for each MapStatus entry) with a storage:BlockId.md#ShuffleBlockId[ShuffleBlockId] (with the input shuffleId , a mapId , and partition ranging from the input startPartition and endPartition ) and scheduler:MapStatus.md#getSizeForBlock[estimated size for the reduce block] for every status and partitions. For any empty MapStatus , you should see the following ERROR message in the logs: Missing an output location for shuffle [id] And convertMapStatuses throws a MetadataFetchFailedException (with shuffleId , startPartition , and the above error message). convertMapStatuses is used when < BlockManagerId s with their ShuffleBlockId s and sizes>>. == [[askTracker]] Sending Blocking Messages To trackerEndpoint RpcEndpointRef [source, scala] \u00b6 askTracker T : T \u00b6 askTracker rpc:RpcEndpointRef.md#askWithRetry[sends the message ] to < > and waits for a result. When an exception happens, you should see the following ERROR message in the logs and askTracker throws a SparkException . Error communicating with MapOutputTracker askTracker is used when MapOutputTracker < ShuffleDependency remotely>> and < >. == [[epoch]][[epochLock]] Epoch Starts from 0 when < >. Can be < > (on MapOutputTrackerWorkers ) or scheduler:MapOutputTrackerMaster.md#incrementEpoch[incremented] (on the driver's MapOutputTrackerMaster ).","title":"MapOutputTracker"},{"location":"scheduler/MapOutputTracker/#mapoutputtracker","text":"MapOutputTracker is a base abstraction of < > that can < > and < >. MapOutputTracker is available using SparkEnv (on the driver and executors). SparkEnv . get . mapOutputTracker == [[extensions]] MapOutputTrackers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MapOutputTracker | Description | scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] | [[MapOutputTrackerMaster]] Runs on the driver | scheduler:MapOutputTrackerWorker.md[MapOutputTrackerWorker] | [[MapOutputTrackerWorker]] Runs on executors |=== == [[creating-instance]][[conf]] Creating Instance MapOutputTracker takes a single ROOT:SparkConf.md[SparkConf] to be created. == [[trackerEndpoint]][[ENDPOINT_NAME]] MapOutputTracker RPC Endpoint trackerEndpoint is a rpc:RpcEndpointRef.md[RpcEndpointRef] of the MapOutputTracker RPC endpoint. trackerEndpoint is initialized (registered or looked up) when SparkEnv is core:SparkEnv.md#create[created] for the driver and executors. trackerEndpoint is used to < >. trackerEndpoint is cleared ( null ) when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#stop[stop]. == [[serializeMapStatuses]] serializeMapStatuses Utility","title":"MapOutputTracker"},{"location":"scheduler/MapOutputTracker/#source-scala","text":"serializeMapStatuses( statuses: Array[MapStatus], broadcastManager: BroadcastManager, isLocal: Boolean, minBroadcastSize: Int): (Array[Byte], Broadcast[Array[Byte]]) serializeMapStatuses serializes the given array of map output locations into an efficient byte format (to send to reduce tasks). serializeMapStatuses compresses the serialized bytes using GZIP. They are supposed to be pretty compressible because many map outputs will be on the same hostname. Internally, serializeMapStatuses creates a Java {java-javadoc-url}/java/io/ByteArrayOutputStream.html[ByteArrayOutputStream]. serializeMapStatuses writes out 0 (direct) first. serializeMapStatuses creates a Java {java-javadoc-url}/java/util/zip/GZIPOutputStream.html[GZIPOutputStream] (with the ByteArrayOutputStream created) and writes out the given statuses array. serializeMapStatuses decides whether to return the output array (of the output stream) or use a broadcast variable based on the size of the byte array. If the size of the result byte array is the given minBroadcastSize threshold or bigger, serializeMapStatuses requests the input BroadcastManager to core:BroadcastManager.md#newBroadcast[create a broadcast variable]. serializeMapStatuses resets the ByteArrayOutputStream and starts over. serializeMapStatuses writes out 1 (broadcast) first. serializeMapStatuses creates a new Java GZIPOutputStream (with the ByteArrayOutputStream created) and writes out the broadcast variable. serializeMapStatuses prints out the following INFO message to the logs:","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"scheduler/MapOutputTracker/#broadcast-mapstatuses-size-length-actual-size-length","text":"serializeMapStatuses is used when ShuffleStatus is requested to scheduler:ShuffleStatus.md#serializedMapStatus[serialize shuffle map output statuses]. == [[deserializeMapStatuses]] deserializeMapStatuses Utility","title":"Broadcast mapstatuses size = [length], actual size = [length]"},{"location":"scheduler/MapOutputTracker/#source-scala_1","text":"deserializeMapStatuses( bytes: Array[Byte]): Array[MapStatus] deserializeMapStatuses...FIXME deserializeMapStatuses is used when...FIXME == [[sendTracker]] sendTracker Utility","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#source-scala_2","text":"sendTracker( message: Any): Unit sendTracker...FIXME sendTracker is used when...FIXME == [[getMapSizesByExecutorId]] Finding Locations with Blocks and Sizes","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#source-scala_3","text":"getMapSizesByExecutorId( shuffleId: Int, startPartition: Int, endPartition: Int): Seq[(BlockManagerId, Seq[(BlockId, Long)])] getMapSizesByExecutorId returns a collection of storage:BlockManagerId.md[]s with their blocks and sizes. When executed, you should see the following DEBUG message in the logs: Fetching outputs for shuffle [id], partitions [startPartition]-[endPartition] getMapSizesByExecutorId < > for the input shuffleId . NOTE: getMapSizesByExecutorId gets the map outputs for all the partitions (despite the method's signature). In the end, getMapSizesByExecutorId < > (as MapStatuses ) into the collection of storage:BlockManagerId.md[]s with their blocks and sizes. getMapSizesByExecutorId is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task]. == [[unregisterShuffle]] Deregistering Map Output Status Information of Shuffle Stage","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#source-scala_4","text":"unregisterShuffle( shuffleId: Int): Unit Deregisters map output status information for the given shuffle stage Used when: ContextCleaner is requested for core:ContextCleaner.md#doCleanupShuffle[shuffle cleanup] BlockManagerSlaveEndpoint is requested to storage:BlockManagerSlaveEndpoint.md#RemoveShuffle[remove a shuffle] == [[stop]] Stopping MapOutputTracker","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#stop-unit","text":"stop does nothing at all. stop is used when SparkEnv is requested to core:SparkEnv.md#stop[stop] (and stops all the services, incl. MapOutputTracker). == [[convertMapStatuses]] Converting MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes","title":"stop(): Unit"},{"location":"scheduler/MapOutputTracker/#source-scala_6","text":"convertMapStatuses( shuffleId: Int, startPartition: Int, endPartition: Int, statuses: Array[MapStatus]): Seq[(BlockManagerId, Seq[(BlockId, Long)])] convertMapStatuses iterates over the input statuses array (of scheduler:MapStatus.md[MapStatus] entries indexed by map id) and creates a collection of storage:BlockManagerId.md[]s (for each MapStatus entry) with a storage:BlockId.md#ShuffleBlockId[ShuffleBlockId] (with the input shuffleId , a mapId , and partition ranging from the input startPartition and endPartition ) and scheduler:MapStatus.md#getSizeForBlock[estimated size for the reduce block] for every status and partitions. For any empty MapStatus , you should see the following ERROR message in the logs: Missing an output location for shuffle [id] And convertMapStatuses throws a MetadataFetchFailedException (with shuffleId , startPartition , and the above error message). convertMapStatuses is used when < BlockManagerId s with their ShuffleBlockId s and sizes>>. == [[askTracker]] Sending Blocking Messages To trackerEndpoint RpcEndpointRef","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTracker/#asktrackert-t","text":"askTracker rpc:RpcEndpointRef.md#askWithRetry[sends the message ] to < > and waits for a result. When an exception happens, you should see the following ERROR message in the logs and askTracker throws a SparkException . Error communicating with MapOutputTracker askTracker is used when MapOutputTracker < ShuffleDependency remotely>> and < >. == [[epoch]][[epochLock]] Epoch Starts from 0 when < >. Can be < > (on MapOutputTrackerWorkers ) or scheduler:MapOutputTrackerMaster.md#incrementEpoch[incremented] (on the driver's MapOutputTrackerMaster ).","title":"askTrackerT: T"},{"location":"scheduler/MapOutputTrackerMaster/","text":"MapOutputTrackerMaster \u00b6 MapOutputTrackerMaster is a MapOutputTracker for the driver. MapOutputTrackerMaster is the source of truth of shuffle map output locations . Creating Instance \u00b6 MapOutputTrackerMaster takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] < > [[isLocal]] isLocal flag (whether MapOutputTrackerMaster runs in local or on a cluster) MapOutputTrackerMaster starts < > on the < >. == [[BroadcastManager]][[broadcastManager]] MapOutputTrackerMaster and BroadcastManager MapOutputTrackerMaster is given a core:BroadcastManager.md[] to be created. == [[shuffleStatuses]] Shuffle Map Output Status Registry MapOutputTrackerMaster uses an internal registry of ShuffleStatus.md[ShuffleStatuses] by shuffle stages. MapOutputTrackerMaster adds a new shuffle when requested to < > (when DAGScheduler is requested to create a ShuffleMapStage for a ShuffleDependency ). MapOutputTrackerMaster uses the registry when requested for the following: < > < > < > < >, < >, < >, < >, < >, < >, < >, < >, < >, < > MapOutputTrackerMaster removes ( clears ) all shuffles when requested to < >. == [[configuration-properties]] Configuration Properties MapOutputTrackerMaster uses the following configuration properties: [[spark.shuffle.mapOutput.minSizeForBroadcast]][[minSizeForBroadcast]] ROOT:configuration-properties.md#spark.shuffle.mapOutput.minSizeForBroadcast[spark.shuffle.mapOutput.minSizeForBroadcast] [[spark.shuffle.mapOutput.dispatcher.numThreads]] ROOT:configuration-properties.md#spark.shuffle.mapOutput.dispatcher.numThreads[spark.shuffle.mapOutput.dispatcher.numThreads] [[spark.shuffle.reduceLocality.enabled]][[shuffleLocalityEnabled]] ROOT:configuration-properties.md#spark.shuffle.reduceLocality.enabled[spark.shuffle.reduceLocality.enabled] == [[SHUFFLE_PREF_MAP_THRESHOLD]][[SHUFFLE_PREF_REDUCE_THRESHOLD]] Map and Reduce Task Thresholds for Preferred Locations MapOutputTrackerMaster defines 1000 (tasks) as the hardcoded threshold of the number of map and reduce tasks when requested to < > with < >. == [[REDUCER_PREF_LOCS_FRACTION]] Map Output Threshold for Preferred Location of Reduce Tasks MapOutputTrackerMaster defines 0.2 as the fraction of total map output that must be at a location for it to considered as a preferred location for a reduce task. Making this larger will focus on fewer locations where most data can be read locally, but may lead to more delay in scheduling if those locations are busy. MapOutputTrackerMaster uses the fraction when requested for the < >. == [[mapOutputRequests]][[GetMapOutputMessage]] GetMapOutputMessage Queue MapOutputTrackerMaster uses a blocking queue (a Java {java-javadoc-url}/java/util/concurrent/LinkedBlockingQueue.html[LinkedBlockingQueue]) for requests for map output statuses. [source,scala] \u00b6 GetMapOutputMessage(shuffleId: Int, context: RpcCallContext) \u00b6 GetMapOutputMessage holds the shuffle ID and the RpcCallContext of the caller. A new GetMapOutputMessage is added to the queue when MapOutputTrackerMaster is requested to < >. MapOutputTrackerMaster uses < > to process GetMapOutputMessages. == [[MessageLoop]][[run]] MessageLoop Dispatcher Thread MessageLoop is a thread of execution to handle < > until a PoisonPill marker message arrives (posted when < >). MessageLoop takes a GetMapOutputMessage and prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Handling request to send map output locations for shuffle [shuffleId] to [hostPort] \u00b6 MessageLoop then finds the ShuffleStatus.md[ShuffleStatus] by the shuffle ID in the < > internal registry and replies back (to the RPC client) with a ShuffleStatus.md#serializedMapStatus[serialized map output status] (with the < > and < > configuration property). MessageLoop threads run on the < >. == [[threadpool]] map-output-dispatcher Thread Pool [source, scala] \u00b6 threadpool: ThreadPoolExecutor \u00b6 threadpool is a daemon fixed thread pool registered with map-output-dispatcher thread name prefix. threadpool uses ROOT:configuration-properties.md#spark.shuffle.mapOutput.dispatcher.numThreads[spark.shuffle.mapOutput.dispatcher.numThreads] configuration property for the number of < > to process received GetMapOutputMessage messages. The dispatcher threads are started immediately when < >. The thread pool is shut down when MapOutputTrackerMaster is requested to < >. == [[epoch]][[getEpoch]] Epoch Number MapOutputTrackerMaster uses an epoch number to...FIXME getEpoch is used when: DAGScheduler is requested to DAGScheduler.md#removeExecutorAndUnregisterOutputs[removeExecutorAndUnregisterOutputs] TaskSetManager.md[TaskSetManager] is created (and sets the epoch to tasks) == [[post]] Enqueueing GetMapOutputMessage [source, scala] \u00b6 post( message: GetMapOutputMessage): Unit post simply adds the input GetMapOutputMessage to the < > internal queue. post is used when MapOutputTrackerMasterEndpoint is requested to MapOutputTrackerMasterEndpoint.md#GetMapOutputStatuses[handle a GetMapOutputStatuses message]. == [[stop]] Stopping MapOutputTrackerMaster [source, scala] \u00b6 stop(): Unit \u00b6 stop...FIXME stop is part of the MapOutputTracker.md#stop[MapOutputTracker] abstraction. == [[unregisterMapOutput]] Unregistering Shuffle Map Output [source, scala] \u00b6 unregisterMapOutput( shuffleId: Int, mapId: Int, bmAddress: BlockManagerId): Unit unregisterMapOutput...FIXME unregisterMapOutput is used when DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a task completion (due to a fetch failure)]. == [[getPreferredLocationsForShuffle]] Computing Preferred Locations (with Most Shuffle Map Outputs) [source, scala] \u00b6 getPreferredLocationsForShuffle( dep: ShuffleDependency[_, _, _], partitionId: Int): Seq[String] getPreferredLocationsForShuffle computes the locations (storage:BlockManager.md[BlockManagers]) with the most shuffle map outputs for the input ShuffleDependency and Partition . getPreferredLocationsForShuffle computes the locations when all of the following are met: < > configuration property is enabled The number of \"map\" partitions (of the ../rdd/ShuffleDependency.md#rdd[RDD] of the input ShuffleDependency ) is below < > The number of \"reduce\" partitions (of the Partitioner of the input ShuffleDependency ) is below < > NOTE: getPreferredLocationsForShuffle is simply < > with a guard condition. Internally, getPreferredLocationsForShuffle checks whether < spark.shuffle.reduceLocality.enabled Spark property>> is enabled (it is by default) with the number of partitions of the RDD of the input ShuffleDependency and partitions in the partitioner of the input ShuffleDependency both being less than 1000 . NOTE: The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are 1000 and are not configurable. If the condition holds, getPreferredLocationsForShuffle < > for the input ShuffleDependency and partitionId (with the number of partitions in the partitioner of the input ShuffleDependency and 0.2 ) and returns the hosts of the preferred BlockManagers . NOTE: 0.2 is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable. getPreferredLocationsForShuffle is used when ../rdd/ShuffledRDD.md#getPreferredLocations[ShuffledRDD] and Spark SQL's ShuffledRowRDD are requested for preferred locations of a partition. == [[incrementEpoch]] Incrementing Epoch [source, scala] \u00b6 incrementEpoch(): Unit \u00b6 incrementEpoch increments the internal MapOutputTracker.md#epoch[epoch]. incrementEpoch prints out the following DEBUG message to the logs: Increasing epoch to [epoch] incrementEpoch is used when: MapOutputTrackerMaster is requested to < >, < >, < > and < > DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a ShuffleMapTask completion] (of a ShuffleMapStage) == [[containsShuffle]] Checking Availability of Shuffle Map Output Status [source, scala] \u00b6 containsShuffle( shuffleId: Int): Boolean containsShuffle checks if the input shuffleId is registered in the < > or MapOutputTracker.md#mapStatuses[mapStatuses] internal caches. containsShuffle is used when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[create a createShuffleMapStage] (for a ShuffleDependency ). == [[registerShuffle]] Registering Shuffle [source, scala] \u00b6 registerShuffle( shuffleId: Int, numMaps: Int): Unit registerShuffle adds the input shuffle ID and the number of partitions (as a ShuffleStatus.md[ShuffleStatus]) to < > internal registry. If the shuffle ID has already been registered, registerShuffle throws an IllegalArgumentException: Shuffle ID [shuffleId] registered twice registerShuffle is used when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[create a ShuffleMapStage] (for a ShuffleDependency ). == [[registerMapOutputs]] Registering Map Outputs for Shuffle (Possibly with Epoch Change) [source, scala] \u00b6 registerMapOutputs( shuffleId: Int, statuses: Array[MapStatus], changeEpoch: Boolean = false): Unit registerMapOutputs registers the input statuses (as the shuffle map output) with the input shuffleId in the MapOutputTracker.md#mapStatuses[mapStatuses] internal cache. registerMapOutputs < > if the input changeEpoch is enabled (it is not by default). registerMapOutputs is used when DAGScheduler handles DAGSchedulerEventProcessLoop.md#handleTaskCompletion-Success-ShuffleMapTask[successful ShuffleMapTask completion] and DAGSchedulerEventProcessLoop.md#handleExecutorLost[executor lost events]. == [[getSerializedMapOutputStatuses]] Finding Serialized Map Output Statuses (And Possibly Broadcasting Them) [source, scala] \u00b6 getSerializedMapOutputStatuses( shuffleId: Int): Array[Byte] getSerializedMapOutputStatuses < > for the input shuffleId . If found, getSerializedMapOutputStatuses returns the cached serialized map statuses. Otherwise, getSerializedMapOutputStatuses acquires the < > for shuffleId and < > again since some other thread could not update the < > internal cache. getSerializedMapOutputStatuses returns the serialized map statuses if found. If not, getSerializedMapOutputStatuses MapOutputTracker.md#serializeMapStatuses[serializes the local array of MapStatuses ] (from < >). You should see the following INFO message in the logs: Size of output statuses for shuffle [shuffleId] is [bytes] bytes getSerializedMapOutputStatuses saves the serialized map output statuses in < > internal cache if the < > has not changed in the meantime. getSerializedMapOutputStatuses also saves its broadcast version in < > internal cache. If the < > has changed in the meantime, the serialized map output statuses and their broadcast version are not saved, and you should see the following INFO message in the logs: Epoch changed, not caching! getSerializedMapOutputStatuses < >. getSerializedMapOutputStatuses returns the serialized map statuses. getSerializedMapOutputStatuses is used when < GetMapOutputMessage requests>> and DAGScheduler.md#createShuffleMapStage[ DAGScheduler creates ShuffleMapStage for ShuffleDependency ] (copying the shuffle map output locations from previous jobs to avoid unnecessarily regenerating data). === [[checkCachedStatuses]] Finding Cached Serialized Map Statuses [source, scala] \u00b6 checkCachedStatuses(): Boolean \u00b6 checkCachedStatuses is an internal helper method that < > uses to do some bookkeeping (when the < > and < > differ) and set local statuses , retBytes and epochGotten (that getSerializedMapOutputStatuses uses). Internally, checkCachedStatuses acquires the MapOutputTracker.md#epochLock[ epochLock lock] and checks the status of < > to < cacheEpoch >>. If epoch is younger (i.e. greater), checkCachedStatuses clears < > internal cache, < > and sets cacheEpoch to be epoch . checkCachedStatuses gets the serialized map output statuses for the shuffleId (of the owning < >). When the serialized map output status is found, checkCachedStatuses saves it in a local retBytes and returns true . When not found, you should see the following DEBUG message in the logs: cached status not found for : [shuffleId] checkCachedStatuses uses MapOutputTracker.md#mapStatuses[mapStatuses] internal cache to get map output statuses for the shuffleId (of the owning < >) or falls back to an empty array and sets it to a local statuses . checkCachedStatuses sets the local epochGotten to the current < > and returns false . == [[registerMapOutput]] Registering Shuffle Map Output [source, scala] \u00b6 registerMapOutput( shuffleId: Int, mapId: Int, status: MapStatus): Unit registerMapOutput finds the ShuffleStatus.md[ShuffleStatus] by the given shuffle ID and ShuffleStatus.md#addMapOutput[adds the given MapStatus]: The given mapId is the Task.md#partitionId[partitionId] of the ShuffleMapTask.md[ShuffleMapTask] that finished. The given shuffleId is the shuffleId of the ShuffleDependency of the ShuffleMapStage (for which the ShuffleMapTask completed) registerMapOutput is used when DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a ShuffleMapTask completion]. == [[getStatistics]] Calculating Shuffle Map Output Statistics [source, scala] \u00b6 getStatistics( dep: ShuffleDependency[_, _, _]): MapOutputStatistics getStatistics...FIXME getStatistics is used when DAGScheduler is requested to DAGScheduler.md#handleMapStageSubmitted[handle a ShuffleMapStage submission] (and the stage has finished) and DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished]. == [[unregisterAllMapOutput]] Deregistering All Map Outputs of Shuffle Stage [source, scala] \u00b6 unregisterAllMapOutput( shuffleId: Int): Unit unregisterAllMapOutput...FIXME unregisterAllMapOutput is used when DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a task completion (due to a fetch failure)]. == [[unregisterShuffle]] Deregistering Shuffle [source, scala] \u00b6 unregisterShuffle( shuffleId: Int): Unit unregisterShuffle...FIXME unregisterShuffle is part of the MapOutputTracker.md#unregisterShuffle[MapOutputTracker] abstraction. == [[removeOutputsOnHost]] Deregistering Shuffle Outputs Associated with Host [source, scala] \u00b6 removeOutputsOnHost( host: String): Unit removeOutputsOnHost...FIXME removeOutputsOnHost is used when DAGScheduler is requested to DAGScheduler.md#removeExecutorAndUnregisterOutputs[removeExecutorAndUnregisterOutputs] and DAGScheduler.md#handleWorkerRemoved[handle a worker removal]. == [[removeOutputsOnExecutor]] Deregistering Shuffle Outputs Associated with Executor [source, scala] \u00b6 removeOutputsOnExecutor( execId: String): Unit removeOutputsOnExecutor...FIXME removeOutputsOnExecutor is used when DAGScheduler is requested to DAGScheduler.md#removeExecutorAndUnregisterOutputs[removeExecutorAndUnregisterOutputs]. == [[getNumAvailableOutputs]] Number of Partitions with Shuffle Map Outputs Available [source, scala] \u00b6 getNumAvailableOutputs( shuffleId: Int): Int getNumAvailableOutputs...FIXME getNumAvailableOutputs is used when ShuffleMapStage is requested for the ShuffleMapStage.md#numAvailableOutputs[number of partitions with shuffle outputs available]. == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions( shuffleId: Int): Option[Seq[Int]] findMissingPartitions...FIXME findMissingPartitions is used when ShuffleMapStage is requested for ShuffleMapStage.md#findMissingPartitions[missing partitions]. == [[getMapSizesByExecutorId]] Finding Locations with Blocks and Sizes [source, scala] \u00b6 getMapSizesByExecutorId( shuffleId: Int, startPartition: Int, endPartition: Int): Iterator[(BlockManagerId, Seq[(BlockId, Long)])] getMapSizesByExecutorId...FIXME getMapSizesByExecutorId is part of the MapOutputTracker.md#getMapSizesByExecutorId[MapOutputTracker] abstraction. == [[getLocationsWithLargestOutputs]] Finding Locations with Largest Number of Shuffle Map Outputs [source, scala] \u00b6 getLocationsWithLargestOutputs( shuffleId: Int, reducerId: Int, numReducers: Int, fractionThreshold: Double): Option[Array[BlockManagerId]] getLocationsWithLargestOutputs returns storage:BlockManagerId.md[]s with the largest size (of all the shuffle blocks they manage) above the input fractionThreshold (given the total size of all the shuffle blocks for the shuffle across all storage:BlockManager.md[BlockManagers]). NOTE: getLocationsWithLargestOutputs may return no BlockManagerId if their shuffle blocks do not total up above the input fractionThreshold . NOTE: The input numReducers is not used. Internally, getLocationsWithLargestOutputs queries the < > internal cache for the input shuffleId . [NOTE] \u00b6 One entry in mapStatuses internal cache is a MapStatus.md[MapStatus] array indexed by partition id. MapStatus includes MapStatus.md#contract[information about the BlockManager (as BlockManagerId ) and estimated size of the reduce blocks]. \u00b6 getLocationsWithLargestOutputs iterates over the MapStatus array and builds an interim mapping between storage:BlockManagerId.md[] and the cumulative sum of shuffle blocks across storage:BlockManager.md[BlockManagers]. getLocationsWithLargestOutputs is used when MapOutputTrackerMaster is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.MapOutputTrackerMaster=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"MapOutputTrackerMaster"},{"location":"scheduler/MapOutputTrackerMaster/#mapoutputtrackermaster","text":"MapOutputTrackerMaster is a MapOutputTracker for the driver. MapOutputTrackerMaster is the source of truth of shuffle map output locations .","title":"MapOutputTrackerMaster"},{"location":"scheduler/MapOutputTrackerMaster/#creating-instance","text":"MapOutputTrackerMaster takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] < > [[isLocal]] isLocal flag (whether MapOutputTrackerMaster runs in local or on a cluster) MapOutputTrackerMaster starts < > on the < >. == [[BroadcastManager]][[broadcastManager]] MapOutputTrackerMaster and BroadcastManager MapOutputTrackerMaster is given a core:BroadcastManager.md[] to be created. == [[shuffleStatuses]] Shuffle Map Output Status Registry MapOutputTrackerMaster uses an internal registry of ShuffleStatus.md[ShuffleStatuses] by shuffle stages. MapOutputTrackerMaster adds a new shuffle when requested to < > (when DAGScheduler is requested to create a ShuffleMapStage for a ShuffleDependency ). MapOutputTrackerMaster uses the registry when requested for the following: < > < > < > < >, < >, < >, < >, < >, < >, < >, < >, < >, < > MapOutputTrackerMaster removes ( clears ) all shuffles when requested to < >. == [[configuration-properties]] Configuration Properties MapOutputTrackerMaster uses the following configuration properties: [[spark.shuffle.mapOutput.minSizeForBroadcast]][[minSizeForBroadcast]] ROOT:configuration-properties.md#spark.shuffle.mapOutput.minSizeForBroadcast[spark.shuffle.mapOutput.minSizeForBroadcast] [[spark.shuffle.mapOutput.dispatcher.numThreads]] ROOT:configuration-properties.md#spark.shuffle.mapOutput.dispatcher.numThreads[spark.shuffle.mapOutput.dispatcher.numThreads] [[spark.shuffle.reduceLocality.enabled]][[shuffleLocalityEnabled]] ROOT:configuration-properties.md#spark.shuffle.reduceLocality.enabled[spark.shuffle.reduceLocality.enabled] == [[SHUFFLE_PREF_MAP_THRESHOLD]][[SHUFFLE_PREF_REDUCE_THRESHOLD]] Map and Reduce Task Thresholds for Preferred Locations MapOutputTrackerMaster defines 1000 (tasks) as the hardcoded threshold of the number of map and reduce tasks when requested to < > with < >. == [[REDUCER_PREF_LOCS_FRACTION]] Map Output Threshold for Preferred Location of Reduce Tasks MapOutputTrackerMaster defines 0.2 as the fraction of total map output that must be at a location for it to considered as a preferred location for a reduce task. Making this larger will focus on fewer locations where most data can be read locally, but may lead to more delay in scheduling if those locations are busy. MapOutputTrackerMaster uses the fraction when requested for the < >. == [[mapOutputRequests]][[GetMapOutputMessage]] GetMapOutputMessage Queue MapOutputTrackerMaster uses a blocking queue (a Java {java-javadoc-url}/java/util/concurrent/LinkedBlockingQueue.html[LinkedBlockingQueue]) for requests for map output statuses.","title":"Creating Instance"},{"location":"scheduler/MapOutputTrackerMaster/#sourcescala","text":"","title":"[source,scala]"},{"location":"scheduler/MapOutputTrackerMaster/#getmapoutputmessageshuffleid-int-context-rpccallcontext","text":"GetMapOutputMessage holds the shuffle ID and the RpcCallContext of the caller. A new GetMapOutputMessage is added to the queue when MapOutputTrackerMaster is requested to < >. MapOutputTrackerMaster uses < > to process GetMapOutputMessages. == [[MessageLoop]][[run]] MessageLoop Dispatcher Thread MessageLoop is a thread of execution to handle < > until a PoisonPill marker message arrives (posted when < >). MessageLoop takes a GetMapOutputMessage and prints out the following DEBUG message to the logs:","title":"GetMapOutputMessage(shuffleId: Int, context: RpcCallContext)"},{"location":"scheduler/MapOutputTrackerMaster/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"scheduler/MapOutputTrackerMaster/#handling-request-to-send-map-output-locations-for-shuffle-shuffleid-to-hostport","text":"MessageLoop then finds the ShuffleStatus.md[ShuffleStatus] by the shuffle ID in the < > internal registry and replies back (to the RPC client) with a ShuffleStatus.md#serializedMapStatus[serialized map output status] (with the < > and < > configuration property). MessageLoop threads run on the < >. == [[threadpool]] map-output-dispatcher Thread Pool","title":"Handling request to send map output locations for shuffle [shuffleId] to [hostPort]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#threadpool-threadpoolexecutor","text":"threadpool is a daemon fixed thread pool registered with map-output-dispatcher thread name prefix. threadpool uses ROOT:configuration-properties.md#spark.shuffle.mapOutput.dispatcher.numThreads[spark.shuffle.mapOutput.dispatcher.numThreads] configuration property for the number of < > to process received GetMapOutputMessage messages. The dispatcher threads are started immediately when < >. The thread pool is shut down when MapOutputTrackerMaster is requested to < >. == [[epoch]][[getEpoch]] Epoch Number MapOutputTrackerMaster uses an epoch number to...FIXME getEpoch is used when: DAGScheduler is requested to DAGScheduler.md#removeExecutorAndUnregisterOutputs[removeExecutorAndUnregisterOutputs] TaskSetManager.md[TaskSetManager] is created (and sets the epoch to tasks) == [[post]] Enqueueing GetMapOutputMessage","title":"threadpool: ThreadPoolExecutor"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_1","text":"post( message: GetMapOutputMessage): Unit post simply adds the input GetMapOutputMessage to the < > internal queue. post is used when MapOutputTrackerMasterEndpoint is requested to MapOutputTrackerMasterEndpoint.md#GetMapOutputStatuses[handle a GetMapOutputStatuses message]. == [[stop]] Stopping MapOutputTrackerMaster","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#stop-unit","text":"stop...FIXME stop is part of the MapOutputTracker.md#stop[MapOutputTracker] abstraction. == [[unregisterMapOutput]] Unregistering Shuffle Map Output","title":"stop(): Unit"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_3","text":"unregisterMapOutput( shuffleId: Int, mapId: Int, bmAddress: BlockManagerId): Unit unregisterMapOutput...FIXME unregisterMapOutput is used when DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a task completion (due to a fetch failure)]. == [[getPreferredLocationsForShuffle]] Computing Preferred Locations (with Most Shuffle Map Outputs)","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_4","text":"getPreferredLocationsForShuffle( dep: ShuffleDependency[_, _, _], partitionId: Int): Seq[String] getPreferredLocationsForShuffle computes the locations (storage:BlockManager.md[BlockManagers]) with the most shuffle map outputs for the input ShuffleDependency and Partition . getPreferredLocationsForShuffle computes the locations when all of the following are met: < > configuration property is enabled The number of \"map\" partitions (of the ../rdd/ShuffleDependency.md#rdd[RDD] of the input ShuffleDependency ) is below < > The number of \"reduce\" partitions (of the Partitioner of the input ShuffleDependency ) is below < > NOTE: getPreferredLocationsForShuffle is simply < > with a guard condition. Internally, getPreferredLocationsForShuffle checks whether < spark.shuffle.reduceLocality.enabled Spark property>> is enabled (it is by default) with the number of partitions of the RDD of the input ShuffleDependency and partitions in the partitioner of the input ShuffleDependency both being less than 1000 . NOTE: The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are 1000 and are not configurable. If the condition holds, getPreferredLocationsForShuffle < > for the input ShuffleDependency and partitionId (with the number of partitions in the partitioner of the input ShuffleDependency and 0.2 ) and returns the hosts of the preferred BlockManagers . NOTE: 0.2 is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable. getPreferredLocationsForShuffle is used when ../rdd/ShuffledRDD.md#getPreferredLocations[ShuffledRDD] and Spark SQL's ShuffledRowRDD are requested for preferred locations of a partition. == [[incrementEpoch]] Incrementing Epoch","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#incrementepoch-unit","text":"incrementEpoch increments the internal MapOutputTracker.md#epoch[epoch]. incrementEpoch prints out the following DEBUG message to the logs: Increasing epoch to [epoch] incrementEpoch is used when: MapOutputTrackerMaster is requested to < >, < >, < > and < > DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a ShuffleMapTask completion] (of a ShuffleMapStage) == [[containsShuffle]] Checking Availability of Shuffle Map Output Status","title":"incrementEpoch(): Unit"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_6","text":"containsShuffle( shuffleId: Int): Boolean containsShuffle checks if the input shuffleId is registered in the < > or MapOutputTracker.md#mapStatuses[mapStatuses] internal caches. containsShuffle is used when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[create a createShuffleMapStage] (for a ShuffleDependency ). == [[registerShuffle]] Registering Shuffle","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_7","text":"registerShuffle( shuffleId: Int, numMaps: Int): Unit registerShuffle adds the input shuffle ID and the number of partitions (as a ShuffleStatus.md[ShuffleStatus]) to < > internal registry. If the shuffle ID has already been registered, registerShuffle throws an IllegalArgumentException: Shuffle ID [shuffleId] registered twice registerShuffle is used when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[create a ShuffleMapStage] (for a ShuffleDependency ). == [[registerMapOutputs]] Registering Map Outputs for Shuffle (Possibly with Epoch Change)","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_8","text":"registerMapOutputs( shuffleId: Int, statuses: Array[MapStatus], changeEpoch: Boolean = false): Unit registerMapOutputs registers the input statuses (as the shuffle map output) with the input shuffleId in the MapOutputTracker.md#mapStatuses[mapStatuses] internal cache. registerMapOutputs < > if the input changeEpoch is enabled (it is not by default). registerMapOutputs is used when DAGScheduler handles DAGSchedulerEventProcessLoop.md#handleTaskCompletion-Success-ShuffleMapTask[successful ShuffleMapTask completion] and DAGSchedulerEventProcessLoop.md#handleExecutorLost[executor lost events]. == [[getSerializedMapOutputStatuses]] Finding Serialized Map Output Statuses (And Possibly Broadcasting Them)","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_9","text":"getSerializedMapOutputStatuses( shuffleId: Int): Array[Byte] getSerializedMapOutputStatuses < > for the input shuffleId . If found, getSerializedMapOutputStatuses returns the cached serialized map statuses. Otherwise, getSerializedMapOutputStatuses acquires the < > for shuffleId and < > again since some other thread could not update the < > internal cache. getSerializedMapOutputStatuses returns the serialized map statuses if found. If not, getSerializedMapOutputStatuses MapOutputTracker.md#serializeMapStatuses[serializes the local array of MapStatuses ] (from < >). You should see the following INFO message in the logs: Size of output statuses for shuffle [shuffleId] is [bytes] bytes getSerializedMapOutputStatuses saves the serialized map output statuses in < > internal cache if the < > has not changed in the meantime. getSerializedMapOutputStatuses also saves its broadcast version in < > internal cache. If the < > has changed in the meantime, the serialized map output statuses and their broadcast version are not saved, and you should see the following INFO message in the logs: Epoch changed, not caching! getSerializedMapOutputStatuses < >. getSerializedMapOutputStatuses returns the serialized map statuses. getSerializedMapOutputStatuses is used when < GetMapOutputMessage requests>> and DAGScheduler.md#createShuffleMapStage[ DAGScheduler creates ShuffleMapStage for ShuffleDependency ] (copying the shuffle map output locations from previous jobs to avoid unnecessarily regenerating data). === [[checkCachedStatuses]] Finding Cached Serialized Map Statuses","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#checkcachedstatuses-boolean","text":"checkCachedStatuses is an internal helper method that < > uses to do some bookkeeping (when the < > and < > differ) and set local statuses , retBytes and epochGotten (that getSerializedMapOutputStatuses uses). Internally, checkCachedStatuses acquires the MapOutputTracker.md#epochLock[ epochLock lock] and checks the status of < > to < cacheEpoch >>. If epoch is younger (i.e. greater), checkCachedStatuses clears < > internal cache, < > and sets cacheEpoch to be epoch . checkCachedStatuses gets the serialized map output statuses for the shuffleId (of the owning < >). When the serialized map output status is found, checkCachedStatuses saves it in a local retBytes and returns true . When not found, you should see the following DEBUG message in the logs: cached status not found for : [shuffleId] checkCachedStatuses uses MapOutputTracker.md#mapStatuses[mapStatuses] internal cache to get map output statuses for the shuffleId (of the owning < >) or falls back to an empty array and sets it to a local statuses . checkCachedStatuses sets the local epochGotten to the current < > and returns false . == [[registerMapOutput]] Registering Shuffle Map Output","title":"checkCachedStatuses(): Boolean"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_11","text":"registerMapOutput( shuffleId: Int, mapId: Int, status: MapStatus): Unit registerMapOutput finds the ShuffleStatus.md[ShuffleStatus] by the given shuffle ID and ShuffleStatus.md#addMapOutput[adds the given MapStatus]: The given mapId is the Task.md#partitionId[partitionId] of the ShuffleMapTask.md[ShuffleMapTask] that finished. The given shuffleId is the shuffleId of the ShuffleDependency of the ShuffleMapStage (for which the ShuffleMapTask completed) registerMapOutput is used when DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a ShuffleMapTask completion]. == [[getStatistics]] Calculating Shuffle Map Output Statistics","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_12","text":"getStatistics( dep: ShuffleDependency[_, _, _]): MapOutputStatistics getStatistics...FIXME getStatistics is used when DAGScheduler is requested to DAGScheduler.md#handleMapStageSubmitted[handle a ShuffleMapStage submission] (and the stage has finished) and DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished]. == [[unregisterAllMapOutput]] Deregistering All Map Outputs of Shuffle Stage","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_13","text":"unregisterAllMapOutput( shuffleId: Int): Unit unregisterAllMapOutput...FIXME unregisterAllMapOutput is used when DAGScheduler is requested to DAGScheduler.md#handleTaskCompletion[handle a task completion (due to a fetch failure)]. == [[unregisterShuffle]] Deregistering Shuffle","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_14","text":"unregisterShuffle( shuffleId: Int): Unit unregisterShuffle...FIXME unregisterShuffle is part of the MapOutputTracker.md#unregisterShuffle[MapOutputTracker] abstraction. == [[removeOutputsOnHost]] Deregistering Shuffle Outputs Associated with Host","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_15","text":"removeOutputsOnHost( host: String): Unit removeOutputsOnHost...FIXME removeOutputsOnHost is used when DAGScheduler is requested to DAGScheduler.md#removeExecutorAndUnregisterOutputs[removeExecutorAndUnregisterOutputs] and DAGScheduler.md#handleWorkerRemoved[handle a worker removal]. == [[removeOutputsOnExecutor]] Deregistering Shuffle Outputs Associated with Executor","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_16","text":"removeOutputsOnExecutor( execId: String): Unit removeOutputsOnExecutor...FIXME removeOutputsOnExecutor is used when DAGScheduler is requested to DAGScheduler.md#removeExecutorAndUnregisterOutputs[removeExecutorAndUnregisterOutputs]. == [[getNumAvailableOutputs]] Number of Partitions with Shuffle Map Outputs Available","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_17","text":"getNumAvailableOutputs( shuffleId: Int): Int getNumAvailableOutputs...FIXME getNumAvailableOutputs is used when ShuffleMapStage is requested for the ShuffleMapStage.md#numAvailableOutputs[number of partitions with shuffle outputs available]. == [[findMissingPartitions]] Finding Missing Partitions","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_18","text":"findMissingPartitions( shuffleId: Int): Option[Seq[Int]] findMissingPartitions...FIXME findMissingPartitions is used when ShuffleMapStage is requested for ShuffleMapStage.md#findMissingPartitions[missing partitions]. == [[getMapSizesByExecutorId]] Finding Locations with Blocks and Sizes","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_19","text":"getMapSizesByExecutorId( shuffleId: Int, startPartition: Int, endPartition: Int): Iterator[(BlockManagerId, Seq[(BlockId, Long)])] getMapSizesByExecutorId...FIXME getMapSizesByExecutorId is part of the MapOutputTracker.md#getMapSizesByExecutorId[MapOutputTracker] abstraction. == [[getLocationsWithLargestOutputs]] Finding Locations with Largest Number of Shuffle Map Outputs","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#source-scala_20","text":"getLocationsWithLargestOutputs( shuffleId: Int, reducerId: Int, numReducers: Int, fractionThreshold: Double): Option[Array[BlockManagerId]] getLocationsWithLargestOutputs returns storage:BlockManagerId.md[]s with the largest size (of all the shuffle blocks they manage) above the input fractionThreshold (given the total size of all the shuffle blocks for the shuffle across all storage:BlockManager.md[BlockManagers]). NOTE: getLocationsWithLargestOutputs may return no BlockManagerId if their shuffle blocks do not total up above the input fractionThreshold . NOTE: The input numReducers is not used. Internally, getLocationsWithLargestOutputs queries the < > internal cache for the input shuffleId .","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMaster/#note","text":"One entry in mapStatuses internal cache is a MapStatus.md[MapStatus] array indexed by partition id.","title":"[NOTE]"},{"location":"scheduler/MapOutputTrackerMaster/#mapstatus-includes-mapstatusmdcontractinformation-about-the-blockmanager-as-blockmanagerid-and-estimated-size-of-the-reduce-blocks","text":"getLocationsWithLargestOutputs iterates over the MapStatus array and builds an interim mapping between storage:BlockManagerId.md[] and the cumulative sum of shuffle blocks across storage:BlockManager.md[BlockManagers]. getLocationsWithLargestOutputs is used when MapOutputTrackerMaster is requested for the < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerMaster logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"MapStatus includes MapStatus.md#contract[information about the BlockManager (as BlockManagerId) and estimated size of the reduce blocks]."},{"location":"scheduler/MapOutputTrackerMaster/#source","text":"","title":"[source]"},{"location":"scheduler/MapOutputTrackerMaster/#log4jloggerorgapachesparkmapoutputtrackermasterall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.MapOutputTrackerMaster=ALL"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/","text":"= [[MapOutputTrackerMasterEndpoint]] MapOutputTrackerMasterEndpoint MapOutputTrackerMasterEndpoint is a rpc:RpcEndpoint.md[RpcEndpoint] for scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] to < > the following messages: < > < > == [[creating-instance]] Creating Instance MapOutputTrackerMasterEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[tracker]] scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] [[conf]] ROOT:SparkConf.md[SparkConf] While being created, MapOutputTrackerMasterEndpoint prints out the following DEBUG message to the logs: init == [[messages]][[receiveAndReply]] Messages === [[GetMapOutputStatuses]] GetMapOutputStatuses [source, scala] \u00b6 GetMapOutputStatuses(shuffleId: Int) \u00b6 When received, MapOutputTrackerMasterEndpoint prints out the following INFO message to the logs: [source,plaintext] \u00b6 Asked to send map output locations for shuffle [shuffleId] to [hostPort] \u00b6 MapOutputTrackerMasterEndpoint requests the < > to scheduler:MapOutputTrackerMaster.md#post[post a GetMapOutputMessage]. GetMapOutputStatuses is posted when MapOutputTrackerWorker is requested for scheduler:MapOutputTrackerWorker.md#getStatuses[shuffle map outputs for a given shuffle ID]. === [[StopMapOutputTracker]] StopMapOutputTracker [source, scala] \u00b6 StopMapOutputTracker \u00b6 When StopMapOutputTracker arrives, you should see the following INFO message in the logs: INFO MapOutputTrackerMasterEndpoint stopped! MapOutputTrackerMasterEndpoint confirms the request (by replying true ) and rpc:RpcEndpoint.md#stop[stops itself] (and stops accepting messages). StopMapOutputTracker is posted when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#stop[stop]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.MapOutputTrackerMasterEndpoint=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"MapOutputTrackerMasterEndpoint"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#getmapoutputstatusesshuffleid-int","text":"When received, MapOutputTrackerMasterEndpoint prints out the following INFO message to the logs:","title":"GetMapOutputStatuses(shuffleId: Int)"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#asked-to-send-map-output-locations-for-shuffle-shuffleid-to-hostport","text":"MapOutputTrackerMasterEndpoint requests the < > to scheduler:MapOutputTrackerMaster.md#post[post a GetMapOutputMessage]. GetMapOutputStatuses is posted when MapOutputTrackerWorker is requested for scheduler:MapOutputTrackerWorker.md#getStatuses[shuffle map outputs for a given shuffle ID]. === [[StopMapOutputTracker]] StopMapOutputTracker","title":"Asked to send map output locations for shuffle [shuffleId] to [hostPort]"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#stopmapoutputtracker","text":"When StopMapOutputTracker arrives, you should see the following INFO message in the logs: INFO MapOutputTrackerMasterEndpoint stopped! MapOutputTrackerMasterEndpoint confirms the request (by replying true ) and rpc:RpcEndpoint.md#stop[stops itself] (and stops accepting messages). StopMapOutputTracker is posted when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#stop[stop]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"StopMapOutputTracker"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#source","text":"","title":"[source]"},{"location":"scheduler/MapOutputTrackerMasterEndpoint/#log4jloggerorgapachesparkmapoutputtrackermasterendpointall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.MapOutputTrackerMasterEndpoint=ALL"},{"location":"scheduler/MapOutputTrackerWorker/","text":"= [[MapOutputTrackerWorker]] MapOutputTrackerWorker MapOutputTrackerWorker is the scheduler:MapOutputTracker.md[MapOutputTracker] for executors. MapOutputTrackerWorker uses Java's thread-safe https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap ] for scheduler:MapOutputTracker.md#mapStatuses[ mapStatuses internal cache] and any lookup cache miss triggers a fetch from the driver's scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster]. == [[getStatuses]] Finding Shuffle Map Outputs [source, scala] \u00b6 getStatuses( shuffleId: Int): Array[MapStatus] getStatuses finds scheduler:MapStatus.md[MapStatuses] for the input shuffleId in the < > internal cache and, when not available, fetches them from a remote scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] (using RPC). Internally, getStatuses first queries the < mapStatuses internal cache>> and returns the map outputs if found. If not found (in the mapStatuses internal cache), you should see the following INFO message in the logs: Don't have map outputs for shuffle [id], fetching them If some other process fetches the map outputs for the shuffleId (as recorded in fetching internal registry), getStatuses waits until it is done. When no other process fetches the map outputs, getStatuses registers the input shuffleId in fetching internal registry (of shuffle map outputs being fetched). You should see the following INFO message in the logs: Doing the fetch; tracker endpoint = [trackerEndpoint] getStatuses sends a GetMapOutputStatuses RPC remote message for the input shuffleId to the trackerEndpoint expecting a Array[Byte] . NOTE: getStatuses requests shuffle map outputs remotely within a timeout and with retries. Refer to rpc:RpcEndpointRef.md[RpcEndpointRef]. getStatuses < > and records the result in the < mapStatuses internal cache>>. You should see the following INFO message in the logs: Got the output locations getStatuses removes the input shuffleId from fetching internal registry. You should see the following DEBUG message in the logs: Fetching map output statuses for shuffle [id] took [time] ms If getStatuses could not find the map output locations for the input shuffleId (locally and remotely), you should see the following ERROR message in the logs and throws a MetadataFetchFailedException . Missing all output locations for shuffle [id] NOTE: getStatuses is used when MapOutputTracker < > and < ShuffleDependency >>. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerWorker logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.MapOutputTrackerWorker=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"MapOutputTrackerWorker"},{"location":"scheduler/MapOutputTrackerWorker/#source-scala","text":"getStatuses( shuffleId: Int): Array[MapStatus] getStatuses finds scheduler:MapStatus.md[MapStatuses] for the input shuffleId in the < > internal cache and, when not available, fetches them from a remote scheduler:MapOutputTrackerMaster.md[MapOutputTrackerMaster] (using RPC). Internally, getStatuses first queries the < mapStatuses internal cache>> and returns the map outputs if found. If not found (in the mapStatuses internal cache), you should see the following INFO message in the logs: Don't have map outputs for shuffle [id], fetching them If some other process fetches the map outputs for the shuffleId (as recorded in fetching internal registry), getStatuses waits until it is done. When no other process fetches the map outputs, getStatuses registers the input shuffleId in fetching internal registry (of shuffle map outputs being fetched). You should see the following INFO message in the logs: Doing the fetch; tracker endpoint = [trackerEndpoint] getStatuses sends a GetMapOutputStatuses RPC remote message for the input shuffleId to the trackerEndpoint expecting a Array[Byte] . NOTE: getStatuses requests shuffle map outputs remotely within a timeout and with retries. Refer to rpc:RpcEndpointRef.md[RpcEndpointRef]. getStatuses < > and records the result in the < mapStatuses internal cache>>. You should see the following INFO message in the logs: Got the output locations getStatuses removes the input shuffleId from fetching internal registry. You should see the following DEBUG message in the logs: Fetching map output statuses for shuffle [id] took [time] ms If getStatuses could not find the map output locations for the input shuffleId (locally and remotely), you should see the following ERROR message in the logs and throws a MetadataFetchFailedException . Missing all output locations for shuffle [id] NOTE: getStatuses is used when MapOutputTracker < > and < ShuffleDependency >>. == [[logging]] Logging Enable ALL logging level for org.apache.spark.MapOutputTrackerWorker logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"scheduler/MapOutputTrackerWorker/#source","text":"","title":"[source]"},{"location":"scheduler/MapOutputTrackerWorker/#log4jloggerorgapachesparkmapoutputtrackerworkerall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.MapOutputTrackerWorker=ALL"},{"location":"scheduler/MapStatus/","text":"= MapStatus MapStatus is an < > of < > that are metadata of shuffle map outputs with < > and < >. MapStatus is < > as the result of scheduler:ShuffleMapTask.md#runTask[executing a ShuffleMapTask] (after a shuffle:ShuffleManager.md#getWriter[ShuffleWriter] has shuffle:ShuffleWriter.md#stop[finished writing partition records successfully]). After a scheduler:ShuffleMapTask.md#runTask[ShuffleMapTask has finished execution successfully], DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] (of the ShuffleMapTask ) that requests the scheduler:DAGScheduler.md#mapOutputTracker[MapOutputTrackerMaster] to scheduler:MapOutputTrackerMaster.md#registerMapOutput[register the MapStatus]. == [[contract]] Contract [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | getSizeForBlock a| [[getSizeForBlock]] [source, scala] \u00b6 getSizeForBlock(reduceId: Int): Long \u00b6 Estimated size for the reduce block (in bytes) Used when: MapOutputTrackerMaster is requested for a scheduler:MapOutputTrackerMaster.md#getStatistics[MapOutputStatistics] and scheduler:MapOutputTrackerMaster.md#getLocationsWithLargestOutputs[locations with largest number of shuffle map outputs] MapOutputTracker object is requested to scheduler:MapOutputTracker.md#convertMapStatuses[convert MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes] | location a| [[location]] [source, scala] \u00b6 location: BlockManagerId \u00b6 Block location, i.e. the < > where a ShuffleMapTask ran and the result is stored. Used when: DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] (of a ShuffleMapTask ) ShuffleStatus is requested to removeMapOutput and removeOutputsByFilter MapOutputTrackerMaster is requested for scheduler:MapOutputTrackerMaster.md#getLocationsWithLargestOutputs[locations with largest number of shuffle map outputs] MapOutputTracker object is requested to scheduler:MapOutputTracker.md#convertMapStatuses[convert MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes] |=== == [[implementations]] MapStatuses [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MapStatus | Description | CompressedMapStatus | [[CompressedMapStatus]] Default MapStatus that compresses the < > to 8 bits ( Byte ) for efficient reporting | HighlyCompressedMapStatus | [[HighlyCompressedMapStatus]] Stores the average size of non-empty blocks, and a compressed bitmap for tracking which blocks are empty. Used when the number of partitions is above the < > threshold |=== == [[minPartitionsToUseHighlyCompressMapStatus]] Minimum Number of Partitions Threshold MapStatus object uses ROOT:configuration-properties.md#spark.shuffle.minNumPartitionsToHighlyCompress[spark.shuffle.minNumPartitionsToHighlyCompress] internal configuration property for the minimum number of partitions threshold to create a < > when requested to < >. == [[apply]] Creating MapStatus [source, scala] \u00b6 apply( loc: BlockManagerId, uncompressedSizes: Array[Long]): MapStatus apply creates a concrete < > per the size of the given uncompressedSizes array: < > when above the < > threshold < > otherwise apply is used when: SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records (into shuffle partitioned file in disk store)] BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records (into one single shuffle block data file)] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[close the internal resources and write out merged spill files]","title":"MapStatus"},{"location":"scheduler/MapStatus/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/MapStatus/#getsizeforblockreduceid-int-long","text":"Estimated size for the reduce block (in bytes) Used when: MapOutputTrackerMaster is requested for a scheduler:MapOutputTrackerMaster.md#getStatistics[MapOutputStatistics] and scheduler:MapOutputTrackerMaster.md#getLocationsWithLargestOutputs[locations with largest number of shuffle map outputs] MapOutputTracker object is requested to scheduler:MapOutputTracker.md#convertMapStatuses[convert MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes] | location a| [[location]]","title":"getSizeForBlock(reduceId: Int): Long"},{"location":"scheduler/MapStatus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/MapStatus/#location-blockmanagerid","text":"Block location, i.e. the < > where a ShuffleMapTask ran and the result is stored. Used when: DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] (of a ShuffleMapTask ) ShuffleStatus is requested to removeMapOutput and removeOutputsByFilter MapOutputTrackerMaster is requested for scheduler:MapOutputTrackerMaster.md#getLocationsWithLargestOutputs[locations with largest number of shuffle map outputs] MapOutputTracker object is requested to scheduler:MapOutputTracker.md#convertMapStatuses[convert MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes] |=== == [[implementations]] MapStatuses [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MapStatus | Description | CompressedMapStatus | [[CompressedMapStatus]] Default MapStatus that compresses the < > to 8 bits ( Byte ) for efficient reporting | HighlyCompressedMapStatus | [[HighlyCompressedMapStatus]] Stores the average size of non-empty blocks, and a compressed bitmap for tracking which blocks are empty. Used when the number of partitions is above the < > threshold |=== == [[minPartitionsToUseHighlyCompressMapStatus]] Minimum Number of Partitions Threshold MapStatus object uses ROOT:configuration-properties.md#spark.shuffle.minNumPartitionsToHighlyCompress[spark.shuffle.minNumPartitionsToHighlyCompress] internal configuration property for the minimum number of partitions threshold to create a < > when requested to < >. == [[apply]] Creating MapStatus","title":"location: BlockManagerId"},{"location":"scheduler/MapStatus/#source-scala_2","text":"apply( loc: BlockManagerId, uncompressedSizes: Array[Long]): MapStatus apply creates a concrete < > per the size of the given uncompressedSizes array: < > when above the < > threshold < > otherwise apply is used when: SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records (into shuffle partitioned file in disk store)] BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records (into one single shuffle block data file)] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[close the internal resources and write out merged spill files]","title":"[source, scala]"},{"location":"scheduler/OutputCommitCoordinator/","text":"OutputCommitCoordinator \u00b6 OutputCommitCoordinator is used to coordinate < > by means of commit locks (using the internal < > registry). [[result-commits]] Result commits are the outputs of running tasks (and a running task is described by a task attempt for a partition in a stage). TIP: A partition (of a stage) is unlocked when it is marked as -1 in < authorizedCommittersByStage internal registry>>. From the scaladoc (it's a private[spark] class so no way to find it https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala[outside the code]): Authority that decides whether tasks can commit output to HDFS. Uses a \"first committer wins\" policy. OutputCommitCoordinator is instantiated in both the drivers and executors. On executors, it is configured with a reference to the driver's OutputCommitCoordinatorEndpoint, so requests to commit output will be forwarded to the driver's OutputCommitCoordinator. The most interesting piece is in... This class was introduced in https://issues.apache.org/jira/browse/SPARK-4879[SPARK-4879 ]; see that JIRA issue (and the associated pull requests) for an extensive design discussion. [[authorized-committers]] Authorized committers are task attempts (per partition and stage) that can...FIXME == [[stop]] stop Method CAUTION: FIXME == [[stageStart]] Stage Execution Started Notification CAUTION: FIXME == [[taskCompleted]] taskCompleted Method [source, scala] \u00b6 taskCompleted( stage: StageId, partition: PartitionId, attemptNumber: TaskAttemptNumber, reason: TaskEndReason): Unit taskCompleted marks the partition (in the stage ) completed (and hence a result committed), but only when the attemptNumber is amongst < > per stage (for the partition ). Internally, taskCompleted first finds < > for the stage . For task completions with no stage registered in < authorizedCommittersByStage internal registry>>, you should see the following DEBUG message in the logs and taskCompleted simply exits. DEBUG OutputCommitCoordinator: Ignoring task completion for completed stage For the reason being Success taskCompleted does nothing and exits. For the reason being TaskCommitDenied , you should see the following INFO message in the logs and taskCompleted exits. INFO OutputCommitCoordinator: Task was denied committing, stage: [stage], partition: [partition], attempt: [attemptNumber] NOTE: For no stage registered or reason being Success or TaskCommitDenied , taskCompleted does nothing (important). For task completion reasons other than Success or TaskCommitDenied and attemptNumber amongst < >, taskCompleted < partition unlocked>>. NOTE: A task attempt can never be -1 . When the lock for partition is cleared, You should see the following DEBUG message in the logs: DEBUG OutputCommitCoordinator: Authorized committer (attemptNumber=[attemptNumber], stage=[stage], partition=[partition]) failed; clearing lock NOTE: taskCompleted is executed only when scheduler:DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler informs that a task has completed]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.OutputCommitCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.scheduler.OutputCommitCoordinator=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[authorizedCommittersByStage]] authorizedCommittersByStage | Tracks commit locks for task attempts for a partition in a stage. Used in < > to authorize task completions to...FIXME |===","title":"OutputCommitCoordinator"},{"location":"scheduler/OutputCommitCoordinator/#outputcommitcoordinator","text":"OutputCommitCoordinator is used to coordinate < > by means of commit locks (using the internal < > registry). [[result-commits]] Result commits are the outputs of running tasks (and a running task is described by a task attempt for a partition in a stage). TIP: A partition (of a stage) is unlocked when it is marked as -1 in < authorizedCommittersByStage internal registry>>. From the scaladoc (it's a private[spark] class so no way to find it https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala[outside the code]): Authority that decides whether tasks can commit output to HDFS. Uses a \"first committer wins\" policy. OutputCommitCoordinator is instantiated in both the drivers and executors. On executors, it is configured with a reference to the driver's OutputCommitCoordinatorEndpoint, so requests to commit output will be forwarded to the driver's OutputCommitCoordinator. The most interesting piece is in... This class was introduced in https://issues.apache.org/jira/browse/SPARK-4879[SPARK-4879 ]; see that JIRA issue (and the associated pull requests) for an extensive design discussion. [[authorized-committers]] Authorized committers are task attempts (per partition and stage) that can...FIXME == [[stop]] stop Method CAUTION: FIXME == [[stageStart]] Stage Execution Started Notification CAUTION: FIXME == [[taskCompleted]] taskCompleted Method","title":"OutputCommitCoordinator"},{"location":"scheduler/OutputCommitCoordinator/#source-scala","text":"taskCompleted( stage: StageId, partition: PartitionId, attemptNumber: TaskAttemptNumber, reason: TaskEndReason): Unit taskCompleted marks the partition (in the stage ) completed (and hence a result committed), but only when the attemptNumber is amongst < > per stage (for the partition ). Internally, taskCompleted first finds < > for the stage . For task completions with no stage registered in < authorizedCommittersByStage internal registry>>, you should see the following DEBUG message in the logs and taskCompleted simply exits. DEBUG OutputCommitCoordinator: Ignoring task completion for completed stage For the reason being Success taskCompleted does nothing and exits. For the reason being TaskCommitDenied , you should see the following INFO message in the logs and taskCompleted exits. INFO OutputCommitCoordinator: Task was denied committing, stage: [stage], partition: [partition], attempt: [attemptNumber] NOTE: For no stage registered or reason being Success or TaskCommitDenied , taskCompleted does nothing (important). For task completion reasons other than Success or TaskCommitDenied and attemptNumber amongst < >, taskCompleted < partition unlocked>>. NOTE: A task attempt can never be -1 . When the lock for partition is cleared, You should see the following DEBUG message in the logs: DEBUG OutputCommitCoordinator: Authorized committer (attemptNumber=[attemptNumber], stage=[stage], partition=[partition]) failed; clearing lock NOTE: taskCompleted is executed only when scheduler:DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler informs that a task has completed]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.OutputCommitCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"scheduler/OutputCommitCoordinator/#source","text":"","title":"[source]"},{"location":"scheduler/OutputCommitCoordinator/#log4jloggerorgapachesparkscheduleroutputcommitcoordinatorall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[authorizedCommittersByStage]] authorizedCommittersByStage | Tracks commit locks for task attempts for a partition in a stage. Used in < > to authorize task completions to...FIXME |===","title":"log4j.logger.org.apache.spark.scheduler.OutputCommitCoordinator=ALL"},{"location":"scheduler/Pool/","text":"== [[Pool]] Schedulable Pool Pool is a scheduler:spark-scheduler-Schedulable.md[Schedulable] entity that represents a tree of scheduler:TaskSetManager.md[TaskSetManagers], i.e. it contains a collection of TaskSetManagers or the Pools thereof. A Pool has a mandatory name, a spark-scheduler-SchedulingMode.md[scheduling mode], initial minShare and weight that are defined when it is created. NOTE: An instance of Pool is created when scheduler:TaskSchedulerImpl.md#initialize[TaskSchedulerImpl is initialized]. NOTE: The scheduler:TaskScheduler.md#contract[TaskScheduler Contract] and spark-scheduler-Schedulable.md#contract[Schedulable Contract] both require that their entities have rootPool of type Pool . === [[increaseRunningTasks]] increaseRunningTasks Method CAUTION: FIXME === [[decreaseRunningTasks]] decreaseRunningTasks Method CAUTION: FIXME === [[taskSetSchedulingAlgorithm]] taskSetSchedulingAlgorithm Attribute Using the spark-scheduler-SchedulingMode.md[scheduling mode] (given when a Pool object is created), Pool selects < > and sets taskSetSchedulingAlgorithm : < > for FIFO scheduling mode. < > for FAIR scheduling mode. It throws an IllegalArgumentException when unsupported scheduling mode is passed on: Unsupported spark.scheduler.mode: [schedulingMode] TIP: Read about the scheduling modes in spark-scheduler-SchedulingMode.md[SchedulingMode]. NOTE: taskSetSchedulingAlgorithm is used in < >. === [[getSortedTaskSetQueue]] Getting TaskSetManagers Sorted -- getSortedTaskSetQueue Method NOTE: getSortedTaskSetQueue is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. getSortedTaskSetQueue sorts all the spark-scheduler-Schedulable.md[Schedulables] in spark-scheduler-Schedulable.md#contract[schedulableQueue] queue by a < > (from the internal < >). NOTE: It is called when scheduler:TaskSchedulerImpl.md#resourceOffers[ TaskSchedulerImpl processes executor resource offers]. === [[schedulableNameToSchedulable]] Schedulables by Name -- schedulableNameToSchedulable Registry [source, scala] \u00b6 schedulableNameToSchedulable = new ConcurrentHashMap[String, Schedulable] \u00b6 schedulableNameToSchedulable is a lookup table of spark-scheduler-Schedulable.md[Schedulable] objects by their names. Beside the obvious usage in the housekeeping methods like addSchedulable , removeSchedulable , getSchedulableByName from the spark-scheduler-Schedulable.md#contract[Schedulable Contract], it is exclusively used in ROOT:SparkContext.md#getPoolForName[SparkContext.getPoolForName]. === [[addSchedulable]] addSchedulable Method NOTE: addSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. addSchedulable adds a Schedulable to the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. More importantly, it sets the Schedulable entity's spark-scheduler-Schedulable.md#contract[parent] to itself. === [[removeSchedulable]] removeSchedulable Method NOTE: removeSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. removeSchedulable removes a Schedulable from the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. NOTE: removeSchedulable is the opposite to < addSchedulable method>>. === [[SchedulingAlgorithm]] SchedulingAlgorithm SchedulingAlgorithm is the interface for a sorting algorithm to sort spark-scheduler-Schedulable.md[Schedulables]. There are currently two SchedulingAlgorithms : < > for FIFO scheduling mode. < > for FAIR scheduling mode. ==== [[FIFOSchedulingAlgorithm]] FIFOSchedulingAlgorithm FIFOSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their priority first and, when equal, by their stageId . NOTE: priority and stageId are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. CAUTION: FIXME A picture is worth a thousand words. How to picture the algorithm? ==== [[FairSchedulingAlgorithm]] FairSchedulingAlgorithm FairSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their minShare , runningTasks , and weight . NOTE: minShare , runningTasks , and weight are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. .FairSchedulingAlgorithm image::spark-pool-FairSchedulingAlgorithm.png[align=\"center\"] For each input Schedulable , minShareRatio is computed as runningTasks by minShare (but at least 1 ) while taskToWeightRatio is runningTasks by weight . === [[getSchedulableByName]] Finding Schedulable by Name -- getSchedulableByName Method [source, scala] \u00b6 getSchedulableByName(schedulableName: String): Schedulable \u00b6 NOTE: getSchedulableByName is part of the < > to find a < > by name. getSchedulableByName ...FIXME","title":"Pool"},{"location":"scheduler/Pool/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Pool/#schedulablenametoschedulable-new-concurrenthashmapstring-schedulable","text":"schedulableNameToSchedulable is a lookup table of spark-scheduler-Schedulable.md[Schedulable] objects by their names. Beside the obvious usage in the housekeeping methods like addSchedulable , removeSchedulable , getSchedulableByName from the spark-scheduler-Schedulable.md#contract[Schedulable Contract], it is exclusively used in ROOT:SparkContext.md#getPoolForName[SparkContext.getPoolForName]. === [[addSchedulable]] addSchedulable Method NOTE: addSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. addSchedulable adds a Schedulable to the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. More importantly, it sets the Schedulable entity's spark-scheduler-Schedulable.md#contract[parent] to itself. === [[removeSchedulable]] removeSchedulable Method NOTE: removeSchedulable is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. removeSchedulable removes a Schedulable from the spark-scheduler-Schedulable.md#contract[schedulableQueue] and < >. NOTE: removeSchedulable is the opposite to < addSchedulable method>>. === [[SchedulingAlgorithm]] SchedulingAlgorithm SchedulingAlgorithm is the interface for a sorting algorithm to sort spark-scheduler-Schedulable.md[Schedulables]. There are currently two SchedulingAlgorithms : < > for FIFO scheduling mode. < > for FAIR scheduling mode. ==== [[FIFOSchedulingAlgorithm]] FIFOSchedulingAlgorithm FIFOSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their priority first and, when equal, by their stageId . NOTE: priority and stageId are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. CAUTION: FIXME A picture is worth a thousand words. How to picture the algorithm? ==== [[FairSchedulingAlgorithm]] FairSchedulingAlgorithm FairSchedulingAlgorithm is a scheduling algorithm that compares Schedulables by their minShare , runningTasks , and weight . NOTE: minShare , runningTasks , and weight are part of spark-scheduler-Schedulable.md#contract[Schedulable Contract]. .FairSchedulingAlgorithm image::spark-pool-FairSchedulingAlgorithm.png[align=\"center\"] For each input Schedulable , minShareRatio is computed as runningTasks by minShare (but at least 1 ) while taskToWeightRatio is runningTasks by weight . === [[getSchedulableByName]] Finding Schedulable by Name -- getSchedulableByName Method","title":"schedulableNameToSchedulable = new ConcurrentHashMap[String, Schedulable]"},{"location":"scheduler/Pool/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Pool/#getschedulablebynameschedulablename-string-schedulable","text":"NOTE: getSchedulableByName is part of the < > to find a < > by name. getSchedulableByName ...FIXME","title":"getSchedulableByName(schedulableName: String): Schedulable"},{"location":"scheduler/ResultStage/","text":"ResultStage \u00b6 ResultStage is the final stage in a job that applies a function on one or many partitions of the target RDD to compute the result of an action. The partitions are given as a collection of partition ids ( partitions ) and the function func: (TaskContext, Iterator[_]) => _ . == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 NOTE: findMissingPartitions is part of the scheduler:Stage.md#findMissingPartitions[Stage] abstraction. findMissingPartitions...FIXME .ResultStage.findMissingPartitions and ActiveJob image::resultstage-findMissingPartitions.png[align=\"center\"] In the above figure, partitions 1 and 2 are not finished ( F is false while T is true). == [[func]] func Property CAUTION: FIXME == [[setActiveJob]] setActiveJob Method CAUTION: FIXME == [[removeActiveJob]] removeActiveJob Method CAUTION: FIXME == [[activeJob]] activeJob Method [source, scala] \u00b6 activeJob: Option[ActiveJob] \u00b6 activeJob returns the optional ActiveJob associated with a ResultStage . CAUTION: FIXME When/why would that be NONE (empty)?","title":"ResultStage"},{"location":"scheduler/ResultStage/#resultstage","text":"ResultStage is the final stage in a job that applies a function on one or many partitions of the target RDD to compute the result of an action. The partitions are given as a collection of partition ids ( partitions ) and the function func: (TaskContext, Iterator[_]) => _ . == [[findMissingPartitions]] Finding Missing Partitions","title":"ResultStage"},{"location":"scheduler/ResultStage/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/ResultStage/#findmissingpartitions-seqint","text":"NOTE: findMissingPartitions is part of the scheduler:Stage.md#findMissingPartitions[Stage] abstraction. findMissingPartitions...FIXME .ResultStage.findMissingPartitions and ActiveJob image::resultstage-findMissingPartitions.png[align=\"center\"] In the above figure, partitions 1 and 2 are not finished ( F is false while T is true). == [[func]] func Property CAUTION: FIXME == [[setActiveJob]] setActiveJob Method CAUTION: FIXME == [[removeActiveJob]] removeActiveJob Method CAUTION: FIXME == [[activeJob]] activeJob Method","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/ResultStage/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/ResultStage/#activejob-optionactivejob","text":"activeJob returns the optional ActiveJob associated with a ResultStage . CAUTION: FIXME When/why would that be NONE (empty)?","title":"activeJob: Option[ActiveJob]"},{"location":"scheduler/ResultTask/","text":"== [[ResultTask]] ResultTask -- Task to Compute Result for ResultStage ResultTask is a scheduler:Task.md[Task] that < >. < ResultTask is created>> exclusively when scheduler:DAGScheduler.md#submitMissingTasks[ DAGScheduler submits missing tasks for a ResultStage ]. ResultTask is created with a < > with the RDD and the function to execute it on and the < >. [[internal-registries]] .ResultTask's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[preferredLocs]] preferredLocs | Collection of scheduler:TaskLocation.md[TaskLocations]. Corresponds directly to unique entries in < > with the only rule that when locs is not defined, it is empty, and no task location preferences are defined. Initialized when < ResultTask is created>>. Used exclusively when ResultTask is requested for < >. |=== === [[creating-instance]] Creating ResultTask Instance ResultTask takes the following when created: stageId -- the stage the task is executed for stageAttemptId -- the stage attempt id [[taskBinary]] ROOT:Broadcast.md[] with the serialized task (as Array[Byte] ). The broadcast contains of a serialized pair of RDD and the function to execute. [[partition]] spark-rdd-Partition.md[Partition] to compute [[locs]] Collection of scheduler:TaskLocation.md[TaskLocations], i.e. preferred locations (executors) to execute the task on [[outputId]] outputId [[localProperties]] local Properties [[serializedTaskMetrics]] The stage's serialized executor:TaskMetrics.md[] (as Array[Byte] ) [[jobId]] (optional) spark-scheduler-ActiveJob.md[Job] id [[appId]] (optional) Application id [[appAttemptId]] (optional) Application attempt id ResultTask initializes the < >. === [[preferredLocations]] preferredLocations Method [source, scala] \u00b6 preferredLocations: Seq[TaskLocation] \u00b6 NOTE: preferredLocations is part of scheduler:Task.md#contract[Task contract]. preferredLocations simply returns < > internal property. === [[runTask]] Deserialize RDD and Function (From Broadcast) and Execute Function (on RDD Partition) -- runTask Method [source, scala] \u00b6 runTask(context: TaskContext): U \u00b6 NOTE: U is the type of a result as defined when < ResultTask is created>>. runTask deserializes a RDD and a function from the < > and then executes the function (on the records from the RDD < >). NOTE: runTask is part of scheduler:Task.md#contract[Task contract] to run a task. Internally, runTask starts by tracking the time required to deserialize a RDD and a function to execute. runTask serializer:Serializer.md#newInstance[creates a new closure Serializer ]. NOTE: runTask uses core:SparkEnv.md#closureSerializer[ SparkEnv to access the current closure Serializer ]. runTask serializer:Serializer.md#deserialize[requests the closure Serializer to deserialize an RDD and the function to execute] (from < > broadcast). NOTE: < > broadcast is defined when < ResultTask is created>>. runTask records scheduler:Task.md#_executorDeserializeTime[_executorDeserializeTime] and scheduler:Task.md#_executorDeserializeCpuTime[_executorDeserializeCpuTime] properties. In the end, runTask executes the function (passing in the input context and the rdd:index.md#iterator[records from partition of the RDD]). NOTE: partition to use to access the records in a deserialized RDD is defined when < ResultTask was created>>.","title":"ResultTask"},{"location":"scheduler/ResultTask/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/ResultTask/#preferredlocations-seqtasklocation","text":"NOTE: preferredLocations is part of scheduler:Task.md#contract[Task contract]. preferredLocations simply returns < > internal property. === [[runTask]] Deserialize RDD and Function (From Broadcast) and Execute Function (on RDD Partition) -- runTask Method","title":"preferredLocations: Seq[TaskLocation]"},{"location":"scheduler/ResultTask/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/ResultTask/#runtaskcontext-taskcontext-u","text":"NOTE: U is the type of a result as defined when < ResultTask is created>>. runTask deserializes a RDD and a function from the < > and then executes the function (on the records from the RDD < >). NOTE: runTask is part of scheduler:Task.md#contract[Task contract] to run a task. Internally, runTask starts by tracking the time required to deserialize a RDD and a function to execute. runTask serializer:Serializer.md#newInstance[creates a new closure Serializer ]. NOTE: runTask uses core:SparkEnv.md#closureSerializer[ SparkEnv to access the current closure Serializer ]. runTask serializer:Serializer.md#deserialize[requests the closure Serializer to deserialize an RDD and the function to execute] (from < > broadcast). NOTE: < > broadcast is defined when < ResultTask is created>>. runTask records scheduler:Task.md#_executorDeserializeTime[_executorDeserializeTime] and scheduler:Task.md#_executorDeserializeCpuTime[_executorDeserializeCpuTime] properties. In the end, runTask executes the function (passing in the input context and the rdd:index.md#iterator[records from partition of the RDD]). NOTE: partition to use to access the records in a deserialized RDD is defined when < ResultTask was created>>.","title":"runTask(context: TaskContext): U"},{"location":"scheduler/Schedulable/","text":"== [[Schedulable]] Schedulable Contract -- Schedulable Entities Schedulable is the < > of < > that manages the < > and can < >. [[contract]] .Schedulable Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | addSchedulable a| [[addSchedulable]] [source, scala] \u00b6 addSchedulable(schedulable: Schedulable): Unit \u00b6 Registers a < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > | checkSpeculatableTasks a| [[checkSpeculatableTasks]] [source, scala] \u00b6 checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean \u00b6 Used when...FIXME | executorLost a| [[executorLost]] [source, scala] \u00b6 executorLost( executorId: String, host: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: Pool is requested to < > TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | getSchedulableByName a| [[getSchedulableByName]] [source, scala] \u00b6 getSchedulableByName(name: String): Schedulable \u00b6 Finds a < > by name Used when...FIXME | getSortedTaskSetQueue a| [[getSortedTaskSetQueue]] [source, scala] \u00b6 getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] \u00b6 Builds a collection of scheduler:TaskSetManager.md[TaskSetManagers] sorted by < > Used when: Pool is requested to < > (recursively) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] | minShare a| [[minShare]] [source, scala] \u00b6 minShare: Int \u00b6 Used when...FIXME | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 Used when...FIXME | parent a| [[parent]] [source, scala] \u00b6 parent: Pool \u00b6 Used when...FIXME | priority a| [[priority]] [source, scala] \u00b6 priority: Int \u00b6 Used when...FIXME | removeSchedulable a| [[removeSchedulable]] [source, scala] \u00b6 removeSchedulable(schedulable: Schedulable): Unit \u00b6 Used when...FIXME | runningTasks a| [[runningTasks]] [source, scala] \u00b6 runningTasks: Int \u00b6 Used when...FIXME | schedulableQueue a| [[schedulableQueue]] [source, scala] \u00b6 schedulableQueue: ConcurrentLinkedQueue[Schedulable] \u00b6 Queue of < > (as https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentLinkedQueue.html[ConcurrentLinkedQueue ]) Used when: SparkContext is requested to ROOT:SparkContext.md#getAllPools[getAllPools] Pool is requested to < >, < >, < >, < >, < >, and < > | schedulingMode a| [[schedulingMode]] [source, scala] \u00b6 schedulingMode: SchedulingMode \u00b6 < > Used when: Pool is < > web UI's PoolTable is requested to render a page with pools ( poolRow ) | stageId a| [[stageId]] [source, scala] \u00b6 stageId: Int \u00b6 Used when...FIXME | weight a| [[weight]] [source, scala] \u00b6 weight: Int \u00b6 Used when...FIXME |=== [[implementations]] .Schedulables [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Schedulable | Description | < > | [[Pool]] Pool of < > (i.e. a recursive data structure for prioritizing task sets) | scheduler:TaskSetManager.md[TaskSetManager] | [[TaskSetManager]] Manages scheduling of tasks of a scheduler:TaskSet.md[TaskSet] |===","title":"Schedulable"},{"location":"scheduler/Schedulable/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#addschedulableschedulable-schedulable-unit","text":"Registers a < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > | checkSpeculatableTasks a| [[checkSpeculatableTasks]]","title":"addSchedulable(schedulable: Schedulable): Unit"},{"location":"scheduler/Schedulable/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#checkspeculatabletasksmintimetospeculation-int-boolean","text":"Used when...FIXME | executorLost a| [[executorLost]]","title":"checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean"},{"location":"scheduler/Schedulable/#source-scala_2","text":"executorLost( executorId: String, host: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: Pool is requested to < > TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | getSchedulableByName a| [[getSchedulableByName]]","title":"[source, scala]"},{"location":"scheduler/Schedulable/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#getschedulablebynamename-string-schedulable","text":"Finds a < > by name Used when...FIXME | getSortedTaskSetQueue a| [[getSortedTaskSetQueue]]","title":"getSchedulableByName(name: String): Schedulable"},{"location":"scheduler/Schedulable/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#getsortedtasksetqueue-arraybuffertasksetmanager","text":"Builds a collection of scheduler:TaskSetManager.md[TaskSetManagers] sorted by < > Used when: Pool is requested to < > (recursively) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers] | minShare a| [[minShare]]","title":"getSortedTaskSetQueue: ArrayBuffer[TaskSetManager]"},{"location":"scheduler/Schedulable/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#minshare-int","text":"Used when...FIXME | name a| [[name]]","title":"minShare: Int"},{"location":"scheduler/Schedulable/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#name-string","text":"Used when...FIXME | parent a| [[parent]]","title":"name: String"},{"location":"scheduler/Schedulable/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#parent-pool","text":"Used when...FIXME | priority a| [[priority]]","title":"parent: Pool"},{"location":"scheduler/Schedulable/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#priority-int","text":"Used when...FIXME | removeSchedulable a| [[removeSchedulable]]","title":"priority: Int"},{"location":"scheduler/Schedulable/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#removeschedulableschedulable-schedulable-unit","text":"Used when...FIXME | runningTasks a| [[runningTasks]]","title":"removeSchedulable(schedulable: Schedulable): Unit"},{"location":"scheduler/Schedulable/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#runningtasks-int","text":"Used when...FIXME | schedulableQueue a| [[schedulableQueue]]","title":"runningTasks: Int"},{"location":"scheduler/Schedulable/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#schedulablequeue-concurrentlinkedqueueschedulable","text":"Queue of < > (as https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentLinkedQueue.html[ConcurrentLinkedQueue ]) Used when: SparkContext is requested to ROOT:SparkContext.md#getAllPools[getAllPools] Pool is requested to < >, < >, < >, < >, < >, and < > | schedulingMode a| [[schedulingMode]]","title":"schedulableQueue: ConcurrentLinkedQueue[Schedulable]"},{"location":"scheduler/Schedulable/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#schedulingmode-schedulingmode","text":"< > Used when: Pool is < > web UI's PoolTable is requested to render a page with pools ( poolRow ) | stageId a| [[stageId]]","title":"schedulingMode: SchedulingMode"},{"location":"scheduler/Schedulable/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#stageid-int","text":"Used when...FIXME | weight a| [[weight]]","title":"stageId: Int"},{"location":"scheduler/Schedulable/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/Schedulable/#weight-int","text":"Used when...FIXME |=== [[implementations]] .Schedulables [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Schedulable | Description | < > | [[Pool]] Pool of < > (i.e. a recursive data structure for prioritizing task sets) | scheduler:TaskSetManager.md[TaskSetManager] | [[TaskSetManager]] Manages scheduling of tasks of a scheduler:TaskSet.md[TaskSet] |===","title":"weight: Int"},{"location":"scheduler/SchedulableBuilder/","text":"== [[SchedulableBuilder]] SchedulableBuilder Contract -- Builders of Schedulable Pools SchedulableBuilder is the < > of < > that manage a < >, which is to < > and < >. SchedulableBuilder is a private[spark] Scala trait that is used exclusively by scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] (the default Spark scheduler). When requested to scheduler:TaskSchedulerImpl.md#initialize[initialize], TaskSchedulerImpl uses the ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property (default: FIFO ) to select one of the < >. [[contract]] .SchedulableBuilder Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | addTaskSetManager a| [[addTaskSetManager]] [source, scala] \u00b6 addTaskSetManager(manager: Schedulable, properties: Properties): Unit \u00b6 Registers a new < > with the < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#submitTasks[submit tasks (of TaskSet) for execution] (and registers a new scheduler:TaskSetManager.md[TaskSetManager] for the TaskSet ) | buildPools a| [[buildPools]] [source, scala] \u00b6 buildPools(): Unit \u00b6 Builds a tree of < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] (and creates a scheduler:TaskSchedulerImpl.md#schedulableBuilder[SchedulableBuilder] per ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property) | rootPool a| [[rootPool]] [source, scala] \u00b6 rootPool: Pool \u00b6 Root (top-level) < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > |=== [[implementations]] .SchedulableBuilders [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | SchedulableBuilder | Description | < > | [[FairSchedulableBuilder]] Used when the ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR | < > | [[FIFOSchedulableBuilder]] Default SchedulableBuilder that is used when the ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FIFO (default) |===","title":"SchedulableBuilder"},{"location":"scheduler/SchedulableBuilder/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulableBuilder/#addtasksetmanagermanager-schedulable-properties-properties-unit","text":"Registers a new < > with the < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#submitTasks[submit tasks (of TaskSet) for execution] (and registers a new scheduler:TaskSetManager.md[TaskSetManager] for the TaskSet ) | buildPools a| [[buildPools]]","title":"addTaskSetManager(manager: Schedulable, properties: Properties): Unit"},{"location":"scheduler/SchedulableBuilder/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulableBuilder/#buildpools-unit","text":"Builds a tree of < > Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] (and creates a scheduler:TaskSchedulerImpl.md#schedulableBuilder[SchedulableBuilder] per ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property) | rootPool a| [[rootPool]]","title":"buildPools(): Unit"},{"location":"scheduler/SchedulableBuilder/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulableBuilder/#rootpool-pool","text":"Root (top-level) < > Used when: FIFOSchedulableBuilder is requested to < > FairSchedulableBuilder is requested to < >, < >, and < > |=== [[implementations]] .SchedulableBuilders [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | SchedulableBuilder | Description | < > | [[FairSchedulableBuilder]] Used when the ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FAIR | < > | [[FIFOSchedulableBuilder]] Default SchedulableBuilder that is used when the ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property is FIFO (default) |===","title":"rootPool: Pool"},{"location":"scheduler/SchedulerBackend/","text":"SchedulerBackend \u00b6 SchedulerBackend is an abstraction of < > that can < > from cluster managers. SchedulerBackend abstraction allows TaskSchedulerImpl to use variety of cluster managers (with their own resource offers and task scheduling modes). Note Being a scheduler backend system assumes a Apache Mesos -like scheduling model in which \"an application\" gets resource offers as machines become available so it is possible to launch tasks on them. Once required resource allocation is obtained, the scheduler backend can start executors. == [[implementations]] Direct Implementations and Extensions [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | SchedulerBackend | Description | scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] | [[CoarseGrainedSchedulerBackend]] Base SchedulerBackend for coarse-grained scheduling systems | spark-local:spark-LocalSchedulerBackend.md[LocalSchedulerBackend] | [[LocalSchedulerBackend]] Spark local | MesosFineGrainedSchedulerBackend | [[MesosFineGrainedSchedulerBackend]] Fine-grained scheduling system for Apache Mesos |=== == [[start]] Starting SchedulerBackend [source, scala] \u00b6 start(): Unit \u00b6 Starts the SchedulerBackend Used when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#start[start] == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | applicationAttemptId a| [[applicationAttemptId]] [source, scala] \u00b6 applicationAttemptId(): Option[String] \u00b6 Execution attempt ID of the Spark application Default: None (undefined) Used exclusively when TaskSchedulerImpl is requested for the scheduler:TaskSchedulerImpl.md#applicationAttemptId[execution attempt ID of a Spark application] | applicationId a| [[applicationId]][[appId]] [source, scala] \u00b6 applicationId(): String \u00b6 Unique identifier of the Spark Application Default: spark-application-[currentTimeMillis] Used exclusively when TaskSchedulerImpl is requested for the scheduler:TaskSchedulerImpl.md#applicationId[unique identifier of a Spark application] | defaultParallelism a| [[defaultParallelism]] [source, scala] \u00b6 defaultParallelism(): Int \u00b6 Default parallelism , i.e. a hint for the number of tasks in stages while sizing jobs Used exclusively when TaskSchedulerImpl is requested for the scheduler:TaskSchedulerImpl.md#defaultParallelism[default parallelism] | getDriverLogUrls a| [[getDriverLogUrls]] [source, scala] \u00b6 getDriverLogUrls: Option[Map[String, String]] \u00b6 Driver log URLs Default: None (undefined) Used exclusively when SparkContext is requested to ROOT:SparkContext.md#postApplicationStart[postApplicationStart] | isReady a| [[isReady]] [source, scala] \u00b6 isReady(): Boolean \u00b6 Controls whether the scheduler:SchedulerBackend.md[SchedulerBackend] is ready ( true ) or not ( false ) Default: true Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#waitBackendReady[wait until scheduling backend is ready] | killTask a| [[killTask]] [source, scala] \u00b6 killTask( taskId: Long, executorId: String, interruptThread: Boolean, reason: String): Unit Kills a given task Default: Throws an UnsupportedOperationException Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#killTaskAttempt[killTaskAttempt] and scheduler:TaskSchedulerImpl.md#killAllTaskAttempts[killAllTaskAttempts] TaskSetManager is requested to scheduler:TaskSetManager.md#handleSuccessfulTask[handle a successful task attempt] | maxNumConcurrentTasks a| [[maxNumConcurrentTasks]] [source, scala] \u00b6 maxNumConcurrentTasks(): Int \u00b6 Maximum number of concurrent tasks that can be launched now Used exclusively when SparkContext is requested to ROOT:SparkContext.md#maxNumConcurrentTasks[maxNumConcurrentTasks] | reviveOffers a| [[reviveOffers]] [source, scala] \u00b6 reviveOffers(): Unit \u00b6 Handles resource allocation offers (from the scheduling system) Used when TaskSchedulerImpl is requested to: scheduler:TaskSchedulerImpl.md#submitTasks[Submit tasks (from a TaskSet)] scheduler:TaskSchedulerImpl.md#statusUpdate[Handle a task status update] scheduler:TaskSchedulerImpl.md#handleFailedTask[Notify the TaskSetManager that a task has failed] scheduler:TaskSchedulerImpl.md#checkSpeculatableTasks[Check for speculatable tasks] scheduler:TaskSchedulerImpl.md#executorLost[Handle a lost executor event] | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Stops the SchedulerBackend Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#stop[stop] MesosCoarseGrainedSchedulerBackend is requested to < > |===","title":"SchedulerBackend"},{"location":"scheduler/SchedulerBackend/#schedulerbackend","text":"SchedulerBackend is an abstraction of < > that can < > from cluster managers. SchedulerBackend abstraction allows TaskSchedulerImpl to use variety of cluster managers (with their own resource offers and task scheduling modes). Note Being a scheduler backend system assumes a Apache Mesos -like scheduling model in which \"an application\" gets resource offers as machines become available so it is possible to launch tasks on them. Once required resource allocation is obtained, the scheduler backend can start executors. == [[implementations]] Direct Implementations and Extensions [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | SchedulerBackend | Description | scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] | [[CoarseGrainedSchedulerBackend]] Base SchedulerBackend for coarse-grained scheduling systems | spark-local:spark-LocalSchedulerBackend.md[LocalSchedulerBackend] | [[LocalSchedulerBackend]] Spark local | MesosFineGrainedSchedulerBackend | [[MesosFineGrainedSchedulerBackend]] Fine-grained scheduling system for Apache Mesos |=== == [[start]] Starting SchedulerBackend","title":"SchedulerBackend"},{"location":"scheduler/SchedulerBackend/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#start-unit","text":"Starts the SchedulerBackend Used when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#start[start] == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | applicationAttemptId a| [[applicationAttemptId]]","title":"start(): Unit"},{"location":"scheduler/SchedulerBackend/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#applicationattemptid-optionstring","text":"Execution attempt ID of the Spark application Default: None (undefined) Used exclusively when TaskSchedulerImpl is requested for the scheduler:TaskSchedulerImpl.md#applicationAttemptId[execution attempt ID of a Spark application] | applicationId a| [[applicationId]][[appId]]","title":"applicationAttemptId(): Option[String]"},{"location":"scheduler/SchedulerBackend/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#applicationid-string","text":"Unique identifier of the Spark Application Default: spark-application-[currentTimeMillis] Used exclusively when TaskSchedulerImpl is requested for the scheduler:TaskSchedulerImpl.md#applicationId[unique identifier of a Spark application] | defaultParallelism a| [[defaultParallelism]]","title":"applicationId(): String"},{"location":"scheduler/SchedulerBackend/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#defaultparallelism-int","text":"Default parallelism , i.e. a hint for the number of tasks in stages while sizing jobs Used exclusively when TaskSchedulerImpl is requested for the scheduler:TaskSchedulerImpl.md#defaultParallelism[default parallelism] | getDriverLogUrls a| [[getDriverLogUrls]]","title":"defaultParallelism(): Int"},{"location":"scheduler/SchedulerBackend/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#getdriverlogurls-optionmapstring-string","text":"Driver log URLs Default: None (undefined) Used exclusively when SparkContext is requested to ROOT:SparkContext.md#postApplicationStart[postApplicationStart] | isReady a| [[isReady]]","title":"getDriverLogUrls: Option[Map[String, String]]"},{"location":"scheduler/SchedulerBackend/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#isready-boolean","text":"Controls whether the scheduler:SchedulerBackend.md[SchedulerBackend] is ready ( true ) or not ( false ) Default: true Used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#waitBackendReady[wait until scheduling backend is ready] | killTask a| [[killTask]]","title":"isReady(): Boolean"},{"location":"scheduler/SchedulerBackend/#source-scala_6","text":"killTask( taskId: Long, executorId: String, interruptThread: Boolean, reason: String): Unit Kills a given task Default: Throws an UnsupportedOperationException Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#killTaskAttempt[killTaskAttempt] and scheduler:TaskSchedulerImpl.md#killAllTaskAttempts[killAllTaskAttempts] TaskSetManager is requested to scheduler:TaskSetManager.md#handleSuccessfulTask[handle a successful task attempt] | maxNumConcurrentTasks a| [[maxNumConcurrentTasks]]","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#maxnumconcurrenttasks-int","text":"Maximum number of concurrent tasks that can be launched now Used exclusively when SparkContext is requested to ROOT:SparkContext.md#maxNumConcurrentTasks[maxNumConcurrentTasks] | reviveOffers a| [[reviveOffers]]","title":"maxNumConcurrentTasks(): Int"},{"location":"scheduler/SchedulerBackend/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#reviveoffers-unit","text":"Handles resource allocation offers (from the scheduling system) Used when TaskSchedulerImpl is requested to: scheduler:TaskSchedulerImpl.md#submitTasks[Submit tasks (from a TaskSet)] scheduler:TaskSchedulerImpl.md#statusUpdate[Handle a task status update] scheduler:TaskSchedulerImpl.md#handleFailedTask[Notify the TaskSetManager that a task has failed] scheduler:TaskSchedulerImpl.md#checkSpeculatableTasks[Check for speculatable tasks] scheduler:TaskSchedulerImpl.md#executorLost[Handle a lost executor event] | stop a| [[stop]]","title":"reviveOffers(): Unit"},{"location":"scheduler/SchedulerBackend/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/SchedulerBackend/#stop-unit","text":"Stops the SchedulerBackend Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#stop[stop] MesosCoarseGrainedSchedulerBackend is requested to < > |===","title":"stop(): Unit"},{"location":"scheduler/SchedulingMode/","text":"== [[SchedulingMode]] Scheduling Mode -- spark.scheduler.mode Spark Property Scheduling Mode (aka order task policy or scheduling policy or scheduling order ) defines a policy to sort tasks in order for execution. The scheduling mode schedulingMode attribute is part of the scheduler:TaskScheduler.md#schedulingMode[TaskScheduler Contract]. The only implementation of the TaskScheduler contract in Spark -- scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] -- uses ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] setting to configure schedulingMode that is merely used to set up the scheduler:TaskScheduler.md#rootPool[rootPool] attribute (with FIFO being the default). It happens when scheduler:TaskSchedulerImpl.md#initialize[ TaskSchedulerImpl is initialized]. There are three acceptable scheduling modes: [[FIFO]] FIFO with no pools but a single top-level unnamed pool with elements being scheduler:TaskSetManager.md[TaskSetManager] objects; lower priority gets scheduler:spark-scheduler-Schedulable.md[Schedulable] sooner or earlier stage wins. [[FAIR]] FAIR with a scheduler:spark-scheduler-FairSchedulableBuilder.md#buildPools[hierarchy of Schedulable (sub)pools] with the scheduler:TaskScheduler.md#rootPool[rootPool] at the top. [[NONE]] NONE (not used) NOTE: Out of three possible SchedulingMode policies only FIFO and FAIR modes are supported by scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl]. [NOTE] \u00b6 After the root pool is initialized, the scheduling mode is no longer relevant (since the spark-scheduler-Schedulable.md[Schedulable] that represents the root pool is fully set up). The root pool is later used when scheduler:TaskSchedulerImpl.md#submitTasks[ TaskSchedulerImpl submits tasks (as TaskSets ) for execution]. \u00b6 NOTE: The scheduler:TaskScheduler.md#rootPool[root pool] is a Schedulable . Refer to spark-scheduler-Schedulable.md[Schedulable]. === [[fair-scheduling-sparkui]] Monitoring FAIR Scheduling Mode using Spark UI CAUTION: FIXME Describe me...","title":"SchedulingMode"},{"location":"scheduler/SchedulingMode/#note","text":"After the root pool is initialized, the scheduling mode is no longer relevant (since the spark-scheduler-Schedulable.md[Schedulable] that represents the root pool is fully set up).","title":"[NOTE]"},{"location":"scheduler/SchedulingMode/#the-root-pool-is-later-used-when-schedulertaskschedulerimplmdsubmittaskstaskschedulerimpl-submits-tasks-as-tasksets-for-execution","text":"NOTE: The scheduler:TaskScheduler.md#rootPool[root pool] is a Schedulable . Refer to spark-scheduler-Schedulable.md[Schedulable]. === [[fair-scheduling-sparkui]] Monitoring FAIR Scheduling Mode using Spark UI CAUTION: FIXME Describe me...","title":"The root pool is later used when scheduler:TaskSchedulerImpl.md#submitTasks[TaskSchedulerImpl submits tasks (as TaskSets) for execution]."},{"location":"scheduler/ShuffleMapStage/","text":"ShuffleMapStage \u00b6 ShuffleMapStage ( shuffle map stage or simply map stage ) is one of the two types of stages in a physical execution DAG (beside a ResultStage ). NOTE: The logical DAG or logical execution plan is the RDD lineage . ShuffleMapStage corresponds to (and is associated with) a < >. ShuffleMapStage is created when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[plan a ShuffleDependency for execution]. ShuffleMapStage can also be DAGScheduler.md#submitMapStage[submitted independently as a Spark job] for DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. ShuffleMapStage is an input for the other following stages in the DAG of stages and is also called a shuffle dependency's map side . Creating Instance \u00b6 ShuffleMapStage takes the following to be created: [[id]] Stage ID [[rdd]] RDD of the < > [[numTasks]] Number of tasks [[parents]] Parent Stage.md[stages] [[firstJobId]] ID of the ActiveJob that created it [[callSite]] CallSite [[shuffleDep]] ShuffleDependency [[mapOutputTrackerMaster]] MapOutputTrackerMaster == [[_mapStageJobs]][[mapStageJobs]][[addActiveJob]][[removeActiveJob]] Jobs Registry ShuffleMapStage keeps track of jobs that were submitted to execute it independently (if any). The registry is used when DAGScheduler is requested to DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished] (FIXME: when DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler is notified that a ShuffleMapTask has finished successfully] and the task made ShuffleMapStage completed and so marks any map-stage jobs waiting on this stage as finished). A new job is registered ( added ) when DAGScheduler is DAGScheduler.md#handleMapStageSubmitted[notified that a ShuffleDependency was submitted for execution (as a MapStageSubmitted event)]. An active job is deregistered ( removed ) when DAGScheduler is requested to DAGScheduler.md#cleanupStateForJobAndIndependentStages[clean up after a job and independent stages]. == [[isAvailable]][[numAvailableOutputs]] ShuffleMapStage is Available (Fully Computed) When executed, a ShuffleMapStage saves map output files (for reduce tasks). When all < > have shuffle map outputs available, ShuffleMapStage is considered available ( done or ready ). ShuffleMapStage is asked about its availability when DAGScheduler is requested for DAGScheduler.md#getMissingParentStages[missing parent map stages for a stage], DAGScheduler.md#handleMapStageSubmitted[handleMapStageSubmitted], DAGScheduler.md#submitMissingTasks[submitMissingTasks], DAGScheduler.md#handleTaskCompletion[handleTaskCompletion], DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished], DAGScheduler.md#stageDependsOn[stageDependsOn]. ShuffleMapStage uses the < > for the MapOutputTrackerMaster.md#getNumAvailableOutputs[number of partitions with shuffle map outputs available] (of the < > by the shuffle ID). == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 findMissingPartitions requests the < > for the MapOutputTrackerMaster.md#findMissingPartitions[missing partitions] (of the < > by the shuffle ID) and returns them. If MapOutputTrackerMaster does not track the ShuffleDependency yet, findMissingPartitions simply returns all the Stage.md#numPartitions[partitions] as missing. findMissingPartitions is part of the Stage.md#findMissingPartitions[Stage] abstraction. == [[stage-sharing]] ShuffleMapStage Sharing A ShuffleMapStage can be shared across multiple jobs, if these jobs reuse the same RDDs. .Skipped Stages are already-computed ShuffleMapStages image::dagscheduler-webui-skipped-stages.png[align=\"center\"] [source, scala] \u00b6 val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey() // <1> rdd.count // <2> rdd.count // <3> <1> Shuffle at sortByKey() <2> Submits a job with two stages with two being executed <3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed","title":"ShuffleMapStage"},{"location":"scheduler/ShuffleMapStage/#shufflemapstage","text":"ShuffleMapStage ( shuffle map stage or simply map stage ) is one of the two types of stages in a physical execution DAG (beside a ResultStage ). NOTE: The logical DAG or logical execution plan is the RDD lineage . ShuffleMapStage corresponds to (and is associated with) a < >. ShuffleMapStage is created when DAGScheduler is requested to DAGScheduler.md#createShuffleMapStage[plan a ShuffleDependency for execution]. ShuffleMapStage can also be DAGScheduler.md#submitMapStage[submitted independently as a Spark job] for DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. ShuffleMapStage is an input for the other following stages in the DAG of stages and is also called a shuffle dependency's map side .","title":"ShuffleMapStage"},{"location":"scheduler/ShuffleMapStage/#creating-instance","text":"ShuffleMapStage takes the following to be created: [[id]] Stage ID [[rdd]] RDD of the < > [[numTasks]] Number of tasks [[parents]] Parent Stage.md[stages] [[firstJobId]] ID of the ActiveJob that created it [[callSite]] CallSite [[shuffleDep]] ShuffleDependency [[mapOutputTrackerMaster]] MapOutputTrackerMaster == [[_mapStageJobs]][[mapStageJobs]][[addActiveJob]][[removeActiveJob]] Jobs Registry ShuffleMapStage keeps track of jobs that were submitted to execute it independently (if any). The registry is used when DAGScheduler is requested to DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished] (FIXME: when DAGSchedulerEventProcessLoop.md#handleTaskCompletion[ DAGScheduler is notified that a ShuffleMapTask has finished successfully] and the task made ShuffleMapStage completed and so marks any map-stage jobs waiting on this stage as finished). A new job is registered ( added ) when DAGScheduler is DAGScheduler.md#handleMapStageSubmitted[notified that a ShuffleDependency was submitted for execution (as a MapStageSubmitted event)]. An active job is deregistered ( removed ) when DAGScheduler is requested to DAGScheduler.md#cleanupStateForJobAndIndependentStages[clean up after a job and independent stages]. == [[isAvailable]][[numAvailableOutputs]] ShuffleMapStage is Available (Fully Computed) When executed, a ShuffleMapStage saves map output files (for reduce tasks). When all < > have shuffle map outputs available, ShuffleMapStage is considered available ( done or ready ). ShuffleMapStage is asked about its availability when DAGScheduler is requested for DAGScheduler.md#getMissingParentStages[missing parent map stages for a stage], DAGScheduler.md#handleMapStageSubmitted[handleMapStageSubmitted], DAGScheduler.md#submitMissingTasks[submitMissingTasks], DAGScheduler.md#handleTaskCompletion[handleTaskCompletion], DAGScheduler.md#markMapStageJobsAsFinished[markMapStageJobsAsFinished], DAGScheduler.md#stageDependsOn[stageDependsOn]. ShuffleMapStage uses the < > for the MapOutputTrackerMaster.md#getNumAvailableOutputs[number of partitions with shuffle map outputs available] (of the < > by the shuffle ID). == [[findMissingPartitions]] Finding Missing Partitions","title":"Creating Instance"},{"location":"scheduler/ShuffleMapStage/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/ShuffleMapStage/#findmissingpartitions-seqint","text":"findMissingPartitions requests the < > for the MapOutputTrackerMaster.md#findMissingPartitions[missing partitions] (of the < > by the shuffle ID) and returns them. If MapOutputTrackerMaster does not track the ShuffleDependency yet, findMissingPartitions simply returns all the Stage.md#numPartitions[partitions] as missing. findMissingPartitions is part of the Stage.md#findMissingPartitions[Stage] abstraction. == [[stage-sharing]] ShuffleMapStage Sharing A ShuffleMapStage can be shared across multiple jobs, if these jobs reuse the same RDDs. .Skipped Stages are already-computed ShuffleMapStages image::dagscheduler-webui-skipped-stages.png[align=\"center\"]","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/ShuffleMapStage/#source-scala_1","text":"val rdd = sc.parallelize(0 to 5).map((_,1)).sortByKey() // <1> rdd.count // <2> rdd.count // <3> <1> Shuffle at sortByKey() <2> Submits a job with two stages with two being executed <3> Intentionally repeat the last action that submits a new job with two stages with one being shared as already-being-computed","title":"[source, scala]"},{"location":"scheduler/ShuffleMapTask/","text":"ShuffleMapTask \u00b6 ShuffleMapTask is one of the two types of tasks that, when < >, writes the result of executing a < > over the records (of a < >) to the shuffle:ShuffleManager.md[shuffle system] and returns a MapStatus.md[MapStatus] (information about the storage:BlockManager.md[BlockManager] and estimated size of the result shuffle blocks). Creating Instance \u00b6 ShuffleMapTask takes the following to be created: [[stageId]] Stage ID [[stageAttemptId]] Stage attempt ID < > [[partition]] Partition [[locs]] TaskLocations [[localProperties]] Task-specific local properties [[serializedTaskMetrics]] Serialized task metrics ( Array[Byte] ) [[jobId]] Optional job ID (default: None ) [[appId]] Optional application ID (default: None ) [[appAttemptId]] Optional application attempt ID (default: None ) [[isBarrier]] isBarrier flag (default: false ) ShuffleMapTask is created when DAGScheduler is requested to DAGScheduler.md#submitMissingTasks[submit tasks for all missing partitions of a ShuffleMapStage]. == [[taskBinary]] Broadcast Variable and Serialized Task Binary ShuffleMapTask is given a ROOT:Broadcast.md[] with a reference to a serialized task binary ( Broadcast[Array[Byte]] ). < > expects that the serialized task binary is a tuple of an ../rdd/RDD.md[RDD] and a ShuffleDependency . == [[runTask]] Running Task [source, scala] \u00b6 runTask( context: TaskContext): MapStatus runTask writes the result ( records ) of executing the < > over the records (in the < >) to the shuffle:ShuffleManager.md[shuffle system] and returns a MapStatus.md[MapStatus] (with the storage:BlockManager.md[BlockManager] and an estimated size of the result shuffle blocks). Internally, runTask requests the core:SparkEnv.md[SparkEnv] for the new instance of core:SparkEnv.md#closureSerializer[closure serializer] and requests it to serializer:Serializer.md#deserialize[deserialize] the < > (into a tuple of a ../rdd/RDD.md[RDD] and a ShuffleDependency ). runTask measures the Task.md#_executorDeserializeTime[thread] and Task.md#_executorDeserializeCpuTime[CPU] deserialization times. runTask requests the core:SparkEnv.md[SparkEnv] for the core:SparkEnv.md#shuffleManager[ShuffleManager] and requests it for a shuffle:ShuffleManager.md#getWriter[ShuffleWriter] (for the ShuffleHandle , the RDD partition , and the TaskContext ). runTask then requests the < > for the ../rdd/RDD.md#iterator[records] (of the < >) that the ShuffleWriter is requested to shuffle:ShuffleWriter.md#write[write out] (to the shuffle system). In the end, runTask requests the ShuffleWriter to shuffle:ShuffleWriter.md#stop[stop] (with the success flag on) and returns the MapStatus.md[shuffle map output status]. NOTE: This is the moment in Task 's lifecycle (and its corresponding RDD) when a ../rdd/index.md#iterator[RDD partition is computed] and in turn becomes a sequence of records (i.e. real data) on an executor. In case of any exceptions, runTask requests the ShuffleWriter to shuffle:ShuffleWriter.md#stop[stop] (with the success flag off) and (re)throws the exception. runTask may also print out the following DEBUG message to the logs when the ShuffleWriter could not be shuffle:ShuffleWriter.md#stop[stopped]. [source,plaintext] \u00b6 Could not stop writer \u00b6 runTask is part of Task.md#runTask[Task] abstraction. == [[preferredLocations]] preferredLocations Method [source, scala] \u00b6 preferredLocations: Seq[TaskLocation] \u00b6 preferredLocations simply returns the < > internal property. preferredLocations is part of Task.md#preferredLocations[Task] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.ShuffleMapTask logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.scheduler.ShuffleMapTask=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[preferredLocs]] Preferred Locations TaskLocation.md[TaskLocations] that are the unique entries in the given < > with the only rule that when locs is not defined, it is empty, and no task location preferences are defined. Initialized when ShuffleMapTask is < > Used exclusively when ShuffleMapTask is requested for the < >","title":"ShuffleMapTask"},{"location":"scheduler/ShuffleMapTask/#shufflemaptask","text":"ShuffleMapTask is one of the two types of tasks that, when < >, writes the result of executing a < > over the records (of a < >) to the shuffle:ShuffleManager.md[shuffle system] and returns a MapStatus.md[MapStatus] (information about the storage:BlockManager.md[BlockManager] and estimated size of the result shuffle blocks).","title":"ShuffleMapTask"},{"location":"scheduler/ShuffleMapTask/#creating-instance","text":"ShuffleMapTask takes the following to be created: [[stageId]] Stage ID [[stageAttemptId]] Stage attempt ID < > [[partition]] Partition [[locs]] TaskLocations [[localProperties]] Task-specific local properties [[serializedTaskMetrics]] Serialized task metrics ( Array[Byte] ) [[jobId]] Optional job ID (default: None ) [[appId]] Optional application ID (default: None ) [[appAttemptId]] Optional application attempt ID (default: None ) [[isBarrier]] isBarrier flag (default: false ) ShuffleMapTask is created when DAGScheduler is requested to DAGScheduler.md#submitMissingTasks[submit tasks for all missing partitions of a ShuffleMapStage]. == [[taskBinary]] Broadcast Variable and Serialized Task Binary ShuffleMapTask is given a ROOT:Broadcast.md[] with a reference to a serialized task binary ( Broadcast[Array[Byte]] ). < > expects that the serialized task binary is a tuple of an ../rdd/RDD.md[RDD] and a ShuffleDependency . == [[runTask]] Running Task","title":"Creating Instance"},{"location":"scheduler/ShuffleMapTask/#source-scala","text":"runTask( context: TaskContext): MapStatus runTask writes the result ( records ) of executing the < > over the records (in the < >) to the shuffle:ShuffleManager.md[shuffle system] and returns a MapStatus.md[MapStatus] (with the storage:BlockManager.md[BlockManager] and an estimated size of the result shuffle blocks). Internally, runTask requests the core:SparkEnv.md[SparkEnv] for the new instance of core:SparkEnv.md#closureSerializer[closure serializer] and requests it to serializer:Serializer.md#deserialize[deserialize] the < > (into a tuple of a ../rdd/RDD.md[RDD] and a ShuffleDependency ). runTask measures the Task.md#_executorDeserializeTime[thread] and Task.md#_executorDeserializeCpuTime[CPU] deserialization times. runTask requests the core:SparkEnv.md[SparkEnv] for the core:SparkEnv.md#shuffleManager[ShuffleManager] and requests it for a shuffle:ShuffleManager.md#getWriter[ShuffleWriter] (for the ShuffleHandle , the RDD partition , and the TaskContext ). runTask then requests the < > for the ../rdd/RDD.md#iterator[records] (of the < >) that the ShuffleWriter is requested to shuffle:ShuffleWriter.md#write[write out] (to the shuffle system). In the end, runTask requests the ShuffleWriter to shuffle:ShuffleWriter.md#stop[stop] (with the success flag on) and returns the MapStatus.md[shuffle map output status]. NOTE: This is the moment in Task 's lifecycle (and its corresponding RDD) when a ../rdd/index.md#iterator[RDD partition is computed] and in turn becomes a sequence of records (i.e. real data) on an executor. In case of any exceptions, runTask requests the ShuffleWriter to shuffle:ShuffleWriter.md#stop[stop] (with the success flag off) and (re)throws the exception. runTask may also print out the following DEBUG message to the logs when the ShuffleWriter could not be shuffle:ShuffleWriter.md#stop[stopped].","title":"[source, scala]"},{"location":"scheduler/ShuffleMapTask/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"scheduler/ShuffleMapTask/#could-not-stop-writer","text":"runTask is part of Task.md#runTask[Task] abstraction. == [[preferredLocations]] preferredLocations Method","title":"Could not stop writer"},{"location":"scheduler/ShuffleMapTask/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/ShuffleMapTask/#preferredlocations-seqtasklocation","text":"preferredLocations simply returns the < > internal property. preferredLocations is part of Task.md#preferredLocations[Task] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.ShuffleMapTask logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"preferredLocations: Seq[TaskLocation]"},{"location":"scheduler/ShuffleMapTask/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"scheduler/ShuffleMapTask/#log4jloggerorgapachesparkschedulershufflemaptaskall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[preferredLocs]] Preferred Locations TaskLocation.md[TaskLocations] that are the unique entries in the given < > with the only rule that when locs is not defined, it is empty, and no task location preferences are defined. Initialized when ShuffleMapTask is < > Used exclusively when ShuffleMapTask is requested for the < >","title":"log4j.logger.org.apache.spark.scheduler.ShuffleMapTask=ALL"},{"location":"scheduler/ShuffleStatus/","text":"= [[ShuffleStatus]] ShuffleStatus ShuffleStatus is a registry of shuffle map outputs (of a shuffle stage). ShuffleStatus is managed by scheduler:MapOutputTrackerMaster.md#shuffleStatuses[MapOutputTrackerMaster] to keep track of shuffle map outputs across shuffle stages. == [[creating-instance]][[numPartitions]] Creating Instance ShuffleStatus takes a single number of partitions to be created. == [[addMapOutput]] Registering Shuffle Map Output [source, scala] \u00b6 addMapOutput( mapId: Int, status: MapStatus): Unit addMapOutput...FIXME addMapOutput is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#registerMapOutput[register a shuffle map output]. == [[removeMapOutput]] Deregistering Shuffle Map Output [source, scala] \u00b6 removeMapOutput( mapId: Int, bmAddress: BlockManagerId): Unit removeMapOutput...FIXME removeMapOutput is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#unregisterMapOutput[unregister a shuffle map output]. == [[serializedMapStatus]] Serializing Shuffle Map Output Statuses [source, scala] \u00b6 serializedMapStatus( broadcastManager: BroadcastManager, isLocal: Boolean, minBroadcastSize: Int): Array[Byte] serializedMapStatus...FIXME serializedMapStatus is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#run[send the map output locations of a shuffle] (on the MessageLoop dispatcher thread). == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 findMissingPartitions...FIXME findMissingPartitions is used when MapOutputTrackerMaster is requested for scheduler:MapOutputTrackerMaster.md#findMissingPartitions[missing partitions (that need to be computed)]. == [[invalidateSerializedMapOutputStatusCache]] Invalidating Serialized Map Output Status Cache [source, scala] \u00b6 invalidateSerializedMapOutputStatusCache(): Unit \u00b6 invalidateSerializedMapOutputStatusCache...FIXME invalidateSerializedMapOutputStatusCache is used when: ShuffleStatus is requested to < >, < >, < > MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#unregisterShuffle[unregister a shuffle] == [[removeOutputsByFilter]] Deregistering Shuffle Map Outputs by Filter [source, scala] \u00b6 removeOutputsByFilter( f: (BlockManagerId) => Boolean): Unit removeOutputsByFilter...FIXME removeOutputsByFilter is used when: ShuffleStatus is requested to < >, < > MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#unregisterAllMapOutput[unregister all map outputs of a given shuffle stage] == [[removeOutputsOnExecutor]] Deregistering Shuffle Map Outputs Associated with Executor [source, scala] \u00b6 removeOutputsOnExecutor( execId: String): Unit removeOutputsOnExecutor...FIXME removeOutputsOnExecutor is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#removeOutputsOnExecutor[delete shuffle outputs associated with an executor]. == [[removeOutputsOnHost]] Deregistering Shuffle Map Outputs Associated with Host [source, scala] \u00b6 removeOutputsOnHost( host: String): Unit removeOutputsOnHost...FIXME removeOutputsOnHost is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#removeOutputsOnHost[delete shuffle outputs associated with a host].","title":"ShuffleStatus"},{"location":"scheduler/ShuffleStatus/#source-scala","text":"addMapOutput( mapId: Int, status: MapStatus): Unit addMapOutput...FIXME addMapOutput is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#registerMapOutput[register a shuffle map output]. == [[removeMapOutput]] Deregistering Shuffle Map Output","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#source-scala_1","text":"removeMapOutput( mapId: Int, bmAddress: BlockManagerId): Unit removeMapOutput...FIXME removeMapOutput is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#unregisterMapOutput[unregister a shuffle map output]. == [[serializedMapStatus]] Serializing Shuffle Map Output Statuses","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#source-scala_2","text":"serializedMapStatus( broadcastManager: BroadcastManager, isLocal: Boolean, minBroadcastSize: Int): Array[Byte] serializedMapStatus...FIXME serializedMapStatus is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#run[send the map output locations of a shuffle] (on the MessageLoop dispatcher thread). == [[findMissingPartitions]] Finding Missing Partitions","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#findmissingpartitions-seqint","text":"findMissingPartitions...FIXME findMissingPartitions is used when MapOutputTrackerMaster is requested for scheduler:MapOutputTrackerMaster.md#findMissingPartitions[missing partitions (that need to be computed)]. == [[invalidateSerializedMapOutputStatusCache]] Invalidating Serialized Map Output Status Cache","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/ShuffleStatus/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#invalidateserializedmapoutputstatuscache-unit","text":"invalidateSerializedMapOutputStatusCache...FIXME invalidateSerializedMapOutputStatusCache is used when: ShuffleStatus is requested to < >, < >, < > MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#unregisterShuffle[unregister a shuffle] == [[removeOutputsByFilter]] Deregistering Shuffle Map Outputs by Filter","title":"invalidateSerializedMapOutputStatusCache(): Unit"},{"location":"scheduler/ShuffleStatus/#source-scala_5","text":"removeOutputsByFilter( f: (BlockManagerId) => Boolean): Unit removeOutputsByFilter...FIXME removeOutputsByFilter is used when: ShuffleStatus is requested to < >, < > MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#unregisterAllMapOutput[unregister all map outputs of a given shuffle stage] == [[removeOutputsOnExecutor]] Deregistering Shuffle Map Outputs Associated with Executor","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#source-scala_6","text":"removeOutputsOnExecutor( execId: String): Unit removeOutputsOnExecutor...FIXME removeOutputsOnExecutor is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#removeOutputsOnExecutor[delete shuffle outputs associated with an executor]. == [[removeOutputsOnHost]] Deregistering Shuffle Map Outputs Associated with Host","title":"[source, scala]"},{"location":"scheduler/ShuffleStatus/#source-scala_7","text":"removeOutputsOnHost( host: String): Unit removeOutputsOnHost...FIXME removeOutputsOnHost is used when MapOutputTrackerMaster is requested to scheduler:MapOutputTrackerMaster.md#removeOutputsOnHost[delete shuffle outputs associated with a host].","title":"[source, scala]"},{"location":"scheduler/Stage/","text":"Stage \u00b6 Stage is a physical unit of execution. It is a step in a physical execution plan. A stage is a set of parallel tasks -- one task per partition (of an RDD that computes partial results of a function executed as part of a Spark job). In other words, a Spark job is a computation with that computation sliced into stages. A stage is uniquely identified by id . When a stage is created, scheduler:DAGScheduler.md[DAGScheduler] increments internal counter nextStageId to track the number of scheduler:DAGScheduler.md#submitStage[stage submissions]. [[rdd]] A stage can only work on the partitions of a single RDD (identified by rdd ), but can be associated with many other dependent parent stages (via internal field parents ), with the boundary of a stage marked by shuffle dependencies. Submitting a stage can therefore trigger execution of a series of dependent parent stages (refer to scheduler:DAGScheduler.md#runJob[RDDs, Job Execution, Stages, and Partitions]). Finally, every stage has a firstJobId that is the id of the job that submitted the stage. There are two types of stages: scheduler:ShuffleMapStage.md[ShuffleMapStage] is an intermediate stage (in the execution DAG) that produces data for other stage(s). It writes map output files for a shuffle. It can also be the final stage in a job in scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. scheduler:ResultStage.md[ResultStage] is the final stage that executes rdd:index.md#actions[a Spark action] in a user program by running a function on an RDD. When a job is submitted, a new stage is created with the parent scheduler:ShuffleMapStage.md[ShuffleMapStage] linked -- they can be created from scratch or linked to, i.e. shared, if other jobs use them already. A stage tracks the jobs (their ids) it belongs to (using the internal jobIds registry). DAGScheduler splits up a job into a collection of stages. Each stage contains a sequence of rdd:index.md[narrow transformations] that can be completed without rdd:spark-rdd-shuffle.md[shuffling] the entire data set, separated at shuffle boundaries , i.e. where shuffle occurs. Stages are thus a result of breaking the RDD graph at shuffle boundaries. Shuffle boundaries introduce a barrier where stages/tasks must wait for the previous stage to finish before they fetch map outputs. RDD operations with rdd:index.md[narrow dependencies], like map() and filter() , are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages, i.e. one to write a set of map output files, and another to read those files after a barrier. In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the RDD.compute() functions of various RDDs, e.g. MappedRDD , FilteredRDD , etc. At some point of time in a stage's life, every partition of the stage gets transformed into a task - scheduler:ShuffleMapTask.md[ShuffleMapTask] or scheduler:ResultTask.md[ResultTask] for scheduler:ShuffleMapStage.md[ShuffleMapStage] and scheduler:ResultStage.md[ResultStage], respectively. Partitions are computed in jobs, and result stages may not always need to compute all partitions in their target RDD, e.g. for actions like first() and lookup() . DAGScheduler prints the following INFO message when there are tasks to submit: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[86] at reduceByKey at <console>:24) There is also the following DEBUG message with pending partitions: New pending partitions: Set(0) Tasks are later submitted to scheduler:TaskScheduler.md[Task Scheduler] (via taskScheduler.submitTasks ). When no tasks in a stage can be submitted, the following DEBUG message shows in the logs: FIXME == [[findMissingPartitions]] Finding Missing Partitions [source, scala] \u00b6 findMissingPartitions(): Seq[Int] \u00b6 findMissingPartitions gives the partition ids that are missing and need to be computed. findMissingPartitions is used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submitMissingTasks] and scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion]. == [[failedOnFetchAndShouldAbort]] failedOnFetchAndShouldAbort Method Stage.failedOnFetchAndShouldAbort(stageAttemptId: Int): Boolean checks whether the number of fetch failed attempts (using fetchFailedAttemptIds ) exceeds the number of consecutive failures allowed for a given stage (that should then be aborted) NOTE: The number of consecutive failures for a stage is not configurable. == [[latestInfo]] Getting StageInfo For Most Recent Attempt [source, scala] \u00b6 latestInfo: StageInfo \u00b6 latestInfo simply returns the <<_latestInfo, most recent StageInfo >> (i.e. makes it accessible). == [[makeNewStageAttempt]] Creating New Stage Attempt [source, scala] \u00b6 makeNewStageAttempt( numPartitionsToCompute: Int, taskLocalityPreferences: Seq[Seq[TaskLocation]] = Seq.empty): Unit makeNewStageAttempt executor:TaskMetrics.md[creates a new TaskMetrics ] and executor:TaskMetrics.md#register[registers the internal accumulators (using the RDD's SparkContext )]. NOTE: makeNewStageAttempt uses < > that was defined when < Stage was created>>. makeNewStageAttempt sets <<_latestInfo, _latestInfo>> to be a scheduler:spark-scheduler-StageInfo.md#fromStage[ StageInfo from the current stage] (with < >, numPartitionsToCompute , and taskLocalityPreferences ). makeNewStageAttempt increments < > counter. makeNewStageAttempt is used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit the missing tasks of a stage]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[details]] details | Long description of the stage Used when...FIXME | [[fetchFailedAttemptIds]] fetchFailedAttemptIds | FIXME Used when...FIXME | [[jobIds]] jobIds | Set of spark-scheduler-ActiveJob.md[jobs] the stage belongs to. Used when...FIXME | [[name]] name | Name of the stage Used when...FIXME | [[nextAttemptId]] nextAttemptId | The ID for the next attempt of the stage. Used when...FIXME | [[numPartitions]] numPartitions | Number of partitions Used when...FIXME | [[pendingPartitions]] pendingPartitions | Set of pending spark-rdd-partitions.md[partitions] Used when...FIXME | [[_latestInfo]] _latestInfo | Internal cache with...FIXME Used when...FIXME |===","title":"Stage"},{"location":"scheduler/Stage/#stage","text":"Stage is a physical unit of execution. It is a step in a physical execution plan. A stage is a set of parallel tasks -- one task per partition (of an RDD that computes partial results of a function executed as part of a Spark job). In other words, a Spark job is a computation with that computation sliced into stages. A stage is uniquely identified by id . When a stage is created, scheduler:DAGScheduler.md[DAGScheduler] increments internal counter nextStageId to track the number of scheduler:DAGScheduler.md#submitStage[stage submissions]. [[rdd]] A stage can only work on the partitions of a single RDD (identified by rdd ), but can be associated with many other dependent parent stages (via internal field parents ), with the boundary of a stage marked by shuffle dependencies. Submitting a stage can therefore trigger execution of a series of dependent parent stages (refer to scheduler:DAGScheduler.md#runJob[RDDs, Job Execution, Stages, and Partitions]). Finally, every stage has a firstJobId that is the id of the job that submitted the stage. There are two types of stages: scheduler:ShuffleMapStage.md[ShuffleMapStage] is an intermediate stage (in the execution DAG) that produces data for other stage(s). It writes map output files for a shuffle. It can also be the final stage in a job in scheduler:DAGScheduler.md#adaptive-query-planning[Adaptive Query Planning / Adaptive Scheduling]. scheduler:ResultStage.md[ResultStage] is the final stage that executes rdd:index.md#actions[a Spark action] in a user program by running a function on an RDD. When a job is submitted, a new stage is created with the parent scheduler:ShuffleMapStage.md[ShuffleMapStage] linked -- they can be created from scratch or linked to, i.e. shared, if other jobs use them already. A stage tracks the jobs (their ids) it belongs to (using the internal jobIds registry). DAGScheduler splits up a job into a collection of stages. Each stage contains a sequence of rdd:index.md[narrow transformations] that can be completed without rdd:spark-rdd-shuffle.md[shuffling] the entire data set, separated at shuffle boundaries , i.e. where shuffle occurs. Stages are thus a result of breaking the RDD graph at shuffle boundaries. Shuffle boundaries introduce a barrier where stages/tasks must wait for the previous stage to finish before they fetch map outputs. RDD operations with rdd:index.md[narrow dependencies], like map() and filter() , are pipelined together into one set of tasks in each stage, but operations with shuffle dependencies require multiple stages, i.e. one to write a set of map output files, and another to read those files after a barrier. In the end, every stage will have only shuffle dependencies on other stages, and may compute multiple operations inside it. The actual pipelining of these operations happens in the RDD.compute() functions of various RDDs, e.g. MappedRDD , FilteredRDD , etc. At some point of time in a stage's life, every partition of the stage gets transformed into a task - scheduler:ShuffleMapTask.md[ShuffleMapTask] or scheduler:ResultTask.md[ResultTask] for scheduler:ShuffleMapStage.md[ShuffleMapStage] and scheduler:ResultStage.md[ResultStage], respectively. Partitions are computed in jobs, and result stages may not always need to compute all partitions in their target RDD, e.g. for actions like first() and lookup() . DAGScheduler prints the following INFO message when there are tasks to submit: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[86] at reduceByKey at <console>:24) There is also the following DEBUG message with pending partitions: New pending partitions: Set(0) Tasks are later submitted to scheduler:TaskScheduler.md[Task Scheduler] (via taskScheduler.submitTasks ). When no tasks in a stage can be submitted, the following DEBUG message shows in the logs: FIXME == [[findMissingPartitions]] Finding Missing Partitions","title":"Stage"},{"location":"scheduler/Stage/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Stage/#findmissingpartitions-seqint","text":"findMissingPartitions gives the partition ids that are missing and need to be computed. findMissingPartitions is used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submitMissingTasks] and scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion]. == [[failedOnFetchAndShouldAbort]] failedOnFetchAndShouldAbort Method Stage.failedOnFetchAndShouldAbort(stageAttemptId: Int): Boolean checks whether the number of fetch failed attempts (using fetchFailedAttemptIds ) exceeds the number of consecutive failures allowed for a given stage (that should then be aborted) NOTE: The number of consecutive failures for a stage is not configurable. == [[latestInfo]] Getting StageInfo For Most Recent Attempt","title":"findMissingPartitions(): Seq[Int]"},{"location":"scheduler/Stage/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Stage/#latestinfo-stageinfo","text":"latestInfo simply returns the <<_latestInfo, most recent StageInfo >> (i.e. makes it accessible). == [[makeNewStageAttempt]] Creating New Stage Attempt","title":"latestInfo: StageInfo"},{"location":"scheduler/Stage/#source-scala_2","text":"makeNewStageAttempt( numPartitionsToCompute: Int, taskLocalityPreferences: Seq[Seq[TaskLocation]] = Seq.empty): Unit makeNewStageAttempt executor:TaskMetrics.md[creates a new TaskMetrics ] and executor:TaskMetrics.md#register[registers the internal accumulators (using the RDD's SparkContext )]. NOTE: makeNewStageAttempt uses < > that was defined when < Stage was created>>. makeNewStageAttempt sets <<_latestInfo, _latestInfo>> to be a scheduler:spark-scheduler-StageInfo.md#fromStage[ StageInfo from the current stage] (with < >, numPartitionsToCompute , and taskLocalityPreferences ). makeNewStageAttempt increments < > counter. makeNewStageAttempt is used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit the missing tasks of a stage]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[details]] details | Long description of the stage Used when...FIXME | [[fetchFailedAttemptIds]] fetchFailedAttemptIds | FIXME Used when...FIXME | [[jobIds]] jobIds | Set of spark-scheduler-ActiveJob.md[jobs] the stage belongs to. Used when...FIXME | [[name]] name | Name of the stage Used when...FIXME | [[nextAttemptId]] nextAttemptId | The ID for the next attempt of the stage. Used when...FIXME | [[numPartitions]] numPartitions | Number of partitions Used when...FIXME | [[pendingPartitions]] pendingPartitions | Set of pending spark-rdd-partitions.md[partitions] Used when...FIXME | [[_latestInfo]] _latestInfo | Internal cache with...FIXME Used when...FIXME |===","title":"[source, scala]"},{"location":"scheduler/Task/","text":"= Task Task is the < > of smallest individual < > that < >. [[contract]] .Task Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | runTask a| [[runTask]] [source, scala] \u00b6 runTask(context: TaskContext): T \u00b6 Runs the task Used exclusively when Task is requested to < > |=== Task is < > when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit missing tasks of a stage]. NOTE: Task is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. .Tasks Are Runtime Representation of RDD Partitions image::spark-rdd-partitions-job-stage-tasks.png[align=\"center\"] [[creating-instance]] Task is described by the following: [[stageId]] Stage ID [[stageAttemptId]] Stage (execution) attempt ID [[partitionId]] Partition ID [[localProperties]] Local properties [[serializedTaskMetrics]] Serialized executor:TaskMetrics.md[] ( Array[Byte] ) [[jobId]] Optional ID of the scheduler:spark-scheduler-ActiveJob.md[ActiveJob] (default: None ) [[appId]] Optional ID of the Spark application (default: None ) [[appAttemptId]] Optional ID of the Spark application's (execution) attempt ID (default: None ) [[isBarrier]] isBarrier flag that is to say whether the task belongs to a barrier stage (default: false ) Task can be < > (possibly on < >). Tasks are executor:Executor.md#launchTask[launched on executors] and < TaskRunner starts>>. In other words, a task is a computation on the records in a RDD partition in a stage of a RDD in a Spark job. NOTE: In Scala Task is actually Task[T] in which T is the type of the result of a task (i.e. the type of the value computed). [[implementations]] .Tasks [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Task | Description | scheduler:ResultTask.md[ResultTask] | [[ResultTask]] Computes a scheduler:ResultStage.md[ResultStage] and gives the result back to the driver | scheduler:ShuffleMapTask.md[ShuffleMapTask] | [[ShuffleMapTask]] Computes a scheduler:ShuffleMapStage.md[ShuffleMapStage] |=== In most cases, the last stage of a Spark job consists of one or more scheduler:ResultTask.md[ResultTasks], while earlier stages are scheduler:ShuffleMapTask.md[ShuffleMapTasks]. NOTE: It is possible to have one or more scheduler:ShuffleMapTask.md[ShuffleMapTasks] as part of the last stage. A task can only belong to one stage and operate on a single partition. All tasks in a stage must be completed before the stages that follow can start. Tasks are spawned one by one for each stage and partition. == [[preferredLocations]] preferredLocations Method [source, scala] \u00b6 preferredLocations: Seq[TaskLocation] = Nil \u00b6 scheduler:TaskLocation.md[TaskLocations] that represent preferred locations (executors) to execute the task on. Empty by default and so no task location preferences are defined that says the task could be launched on any executor. NOTE: Defined by the < >, i.e. scheduler:ShuffleMapTask.md#preferredLocations[ShuffleMapTask] and scheduler:ResultTask.md#preferredLocations[ResultTask]. NOTE: preferredLocations is used exclusively when TaskSetManager is requested to scheduler:TaskSetManager.md#addPendingTask[register a task as pending execution] and scheduler:TaskSetManager.md#dequeueSpeculativeTask[dequeueSpeculativeTask]. == [[run]] Running Task Thread -- run Final Method [source, scala] \u00b6 run( taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem): T run storage:BlockManager.md#registerTask[registers the task (identified as taskAttemptId ) with the local BlockManager ]. NOTE: run uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ]. run spark-TaskContextImpl.md#creating-instance[creates a TaskContextImpl ] that in turn becomes the task's spark-TaskContext.md#setTaskContext[TaskContext]. NOTE: run is a final method and so must not be overriden. run checks <<_killed, _killed>> flag and, if enabled, < > (with interruptThread flag disabled). run creates a Hadoop CallerContext and sets it. run < >. NOTE: This is the moment when the custom Task 's < > is executed. In the end, run spark-TaskContextImpl.md#markTaskCompleted[notifies TaskContextImpl that the task has completed] (regardless of the final outcome -- a success or a failure). In case of any exceptions, run spark-TaskContextImpl.md#markTaskFailed[notifies TaskContextImpl that the task has failed]. run storage:MemoryStore.md#releaseUnrollMemoryForThisTask[requests MemoryStore to release unroll memory for this task] (for both ON_HEAP and OFF_HEAP memory modes). NOTE: run uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ] that it uses to access storage:BlockManager.md#memoryStore[MemoryStore]. run memory:MemoryManager.md[requests MemoryManager to notify any tasks waiting for execution memory to be freed to wake up and try to acquire memory again]. run spark-TaskContext.md#unset[unsets the task's TaskContext ]. NOTE: run uses core:SparkEnv.md#memoryManager[ SparkEnv to access the current MemoryManager ]. NOTE: run is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool sometime in the future)]). . The Task instance has just been deserialized from taskBytes that were sent over the wire to an executor. localProperties and memory:TaskMemoryManager.md[TaskMemoryManager] are already assigned. == [[states]][[TaskState]] Task States A task can be in one of the following states (as described by TaskState enumeration): LAUNCHING RUNNING when the task is being started. FINISHED when the task finished with the serialized result. FAILED when the task fails, e.g. when shuffle:FetchFailedException.md[FetchFailedException], CommitDeniedException or any Throwable occurs KILLED when an executor kills a task. LOST States are the values of org.apache.spark.TaskState . NOTE: Task status updates are sent from executors to the driver through executor:ExecutorBackend.md[]. Task is finished when it is in one of FINISHED , FAILED , KILLED , LOST . LOST and FAILED states are considered failures. TIP: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState ]. == [[collectAccumulatorUpdates]] Collect Latest Values of (Internal and External) Accumulators -- collectAccumulatorUpdates Method [source, scala] \u00b6 collectAccumulatorUpdates(taskFailed: Boolean = false): Seq[AccumulableInfo] \u00b6 collectAccumulatorUpdates collects the latest values of internal and external accumulators from a task (and returns the values as a collection of spark-accumulators.md#AccumulableInfo[AccumulableInfo]). Internally, collectAccumulatorUpdates spark-TaskContextImpl.md#taskMetrics[takes TaskMetrics ]. NOTE: collectAccumulatorUpdates uses < > to access the task's TaskMetrics . collectAccumulatorUpdates collects the latest values of: executor:TaskMetrics.md#internalAccums[internal accumulators] whose current value is not the zero value and the RESULT_SIZE accumulator (regardless whether the value is its zero or not). executor:TaskMetrics.md#externalAccums[external accumulators] when taskFailed is disabled ( false ) or which spark-accumulators.md#countFailedValues[should be included on failures]. collectAccumulatorUpdates returns an empty collection when < > is not initialized. NOTE: collectAccumulatorUpdates is used when executor:TaskRunner.md#run[ TaskRunner runs a task] (and sends a task's final results back to the driver). == [[kill]] Killing Task -- kill Method [source, scala] \u00b6 kill(interruptThread: Boolean) \u00b6 kill marks the task to be killed, i.e. it sets the internal _killed flag to true . kill calls spark-TaskContextImpl.md#markInterrupted[TaskContextImpl.markInterrupted] when context is set. If interruptThread is enabled and the internal taskThread is available, kill interrupts it. CAUTION: FIXME When could context and interruptThread not be set? == [[internal-registries]] Internal Properties .Task's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | _executorDeserializeCpuTime | [[_executorDeserializeCpuTime]] | _executorDeserializeTime | [[_executorDeserializeTime]] | _reasonIfKilled | [[_reasonIfKilled]] | _killed | [[_killed]] | context | [[context]] < > Set to be a < > or < > when the < > flag is enabled or not, respectively, when Task is requested to < > | epoch | [[epoch]] Task epoch Starts as -1 Set when TaskSetManager is scheduler:TaskSetManager.md[created] (to be the scheduler:MapOutputTrackerMaster.md#getEpoch[epoch] of the MapOutputTrackerMaster ) | metrics | [[metrics]] executor:TaskMetrics.md[] Created lazily when < > from < >. | taskMemoryManager | [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] that manages the memory allocated by the task. | taskThread | [[taskThread]] |===","title":"Task"},{"location":"scheduler/Task/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/Task/#runtaskcontext-taskcontext-t","text":"Runs the task Used exclusively when Task is requested to < > |=== Task is < > when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit missing tasks of a stage]. NOTE: Task is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. .Tasks Are Runtime Representation of RDD Partitions image::spark-rdd-partitions-job-stage-tasks.png[align=\"center\"] [[creating-instance]] Task is described by the following: [[stageId]] Stage ID [[stageAttemptId]] Stage (execution) attempt ID [[partitionId]] Partition ID [[localProperties]] Local properties [[serializedTaskMetrics]] Serialized executor:TaskMetrics.md[] ( Array[Byte] ) [[jobId]] Optional ID of the scheduler:spark-scheduler-ActiveJob.md[ActiveJob] (default: None ) [[appId]] Optional ID of the Spark application (default: None ) [[appAttemptId]] Optional ID of the Spark application's (execution) attempt ID (default: None ) [[isBarrier]] isBarrier flag that is to say whether the task belongs to a barrier stage (default: false ) Task can be < > (possibly on < >). Tasks are executor:Executor.md#launchTask[launched on executors] and < TaskRunner starts>>. In other words, a task is a computation on the records in a RDD partition in a stage of a RDD in a Spark job. NOTE: In Scala Task is actually Task[T] in which T is the type of the result of a task (i.e. the type of the value computed). [[implementations]] .Tasks [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Task | Description | scheduler:ResultTask.md[ResultTask] | [[ResultTask]] Computes a scheduler:ResultStage.md[ResultStage] and gives the result back to the driver | scheduler:ShuffleMapTask.md[ShuffleMapTask] | [[ShuffleMapTask]] Computes a scheduler:ShuffleMapStage.md[ShuffleMapStage] |=== In most cases, the last stage of a Spark job consists of one or more scheduler:ResultTask.md[ResultTasks], while earlier stages are scheduler:ShuffleMapTask.md[ShuffleMapTasks]. NOTE: It is possible to have one or more scheduler:ShuffleMapTask.md[ShuffleMapTasks] as part of the last stage. A task can only belong to one stage and operate on a single partition. All tasks in a stage must be completed before the stages that follow can start. Tasks are spawned one by one for each stage and partition. == [[preferredLocations]] preferredLocations Method","title":"runTask(context: TaskContext): T"},{"location":"scheduler/Task/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/Task/#preferredlocations-seqtasklocation-nil","text":"scheduler:TaskLocation.md[TaskLocations] that represent preferred locations (executors) to execute the task on. Empty by default and so no task location preferences are defined that says the task could be launched on any executor. NOTE: Defined by the < >, i.e. scheduler:ShuffleMapTask.md#preferredLocations[ShuffleMapTask] and scheduler:ResultTask.md#preferredLocations[ResultTask]. NOTE: preferredLocations is used exclusively when TaskSetManager is requested to scheduler:TaskSetManager.md#addPendingTask[register a task as pending execution] and scheduler:TaskSetManager.md#dequeueSpeculativeTask[dequeueSpeculativeTask]. == [[run]] Running Task Thread -- run Final Method","title":"preferredLocations: Seq[TaskLocation] = Nil"},{"location":"scheduler/Task/#source-scala_2","text":"run( taskAttemptId: Long, attemptNumber: Int, metricsSystem: MetricsSystem): T run storage:BlockManager.md#registerTask[registers the task (identified as taskAttemptId ) with the local BlockManager ]. NOTE: run uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ]. run spark-TaskContextImpl.md#creating-instance[creates a TaskContextImpl ] that in turn becomes the task's spark-TaskContext.md#setTaskContext[TaskContext]. NOTE: run is a final method and so must not be overriden. run checks <<_killed, _killed>> flag and, if enabled, < > (with interruptThread flag disabled). run creates a Hadoop CallerContext and sets it. run < >. NOTE: This is the moment when the custom Task 's < > is executed. In the end, run spark-TaskContextImpl.md#markTaskCompleted[notifies TaskContextImpl that the task has completed] (regardless of the final outcome -- a success or a failure). In case of any exceptions, run spark-TaskContextImpl.md#markTaskFailed[notifies TaskContextImpl that the task has failed]. run storage:MemoryStore.md#releaseUnrollMemoryForThisTask[requests MemoryStore to release unroll memory for this task] (for both ON_HEAP and OFF_HEAP memory modes). NOTE: run uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ] that it uses to access storage:BlockManager.md#memoryStore[MemoryStore]. run memory:MemoryManager.md[requests MemoryManager to notify any tasks waiting for execution memory to be freed to wake up and try to acquire memory again]. run spark-TaskContext.md#unset[unsets the task's TaskContext ]. NOTE: run uses core:SparkEnv.md#memoryManager[ SparkEnv to access the current MemoryManager ]. NOTE: run is used exclusively when TaskRunner is requested to executor:TaskRunner.md#run[run] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool sometime in the future)]). . The Task instance has just been deserialized from taskBytes that were sent over the wire to an executor. localProperties and memory:TaskMemoryManager.md[TaskMemoryManager] are already assigned. == [[states]][[TaskState]] Task States A task can be in one of the following states (as described by TaskState enumeration): LAUNCHING RUNNING when the task is being started. FINISHED when the task finished with the serialized result. FAILED when the task fails, e.g. when shuffle:FetchFailedException.md[FetchFailedException], CommitDeniedException or any Throwable occurs KILLED when an executor kills a task. LOST States are the values of org.apache.spark.TaskState . NOTE: Task status updates are sent from executors to the driver through executor:ExecutorBackend.md[]. Task is finished when it is in one of FINISHED , FAILED , KILLED , LOST . LOST and FAILED states are considered failures. TIP: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState ]. == [[collectAccumulatorUpdates]] Collect Latest Values of (Internal and External) Accumulators -- collectAccumulatorUpdates Method","title":"[source, scala]"},{"location":"scheduler/Task/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/Task/#collectaccumulatorupdatestaskfailed-boolean-false-seqaccumulableinfo","text":"collectAccumulatorUpdates collects the latest values of internal and external accumulators from a task (and returns the values as a collection of spark-accumulators.md#AccumulableInfo[AccumulableInfo]). Internally, collectAccumulatorUpdates spark-TaskContextImpl.md#taskMetrics[takes TaskMetrics ]. NOTE: collectAccumulatorUpdates uses < > to access the task's TaskMetrics . collectAccumulatorUpdates collects the latest values of: executor:TaskMetrics.md#internalAccums[internal accumulators] whose current value is not the zero value and the RESULT_SIZE accumulator (regardless whether the value is its zero or not). executor:TaskMetrics.md#externalAccums[external accumulators] when taskFailed is disabled ( false ) or which spark-accumulators.md#countFailedValues[should be included on failures]. collectAccumulatorUpdates returns an empty collection when < > is not initialized. NOTE: collectAccumulatorUpdates is used when executor:TaskRunner.md#run[ TaskRunner runs a task] (and sends a task's final results back to the driver). == [[kill]] Killing Task -- kill Method","title":"collectAccumulatorUpdates(taskFailed: Boolean = false): Seq[AccumulableInfo]"},{"location":"scheduler/Task/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/Task/#killinterruptthread-boolean","text":"kill marks the task to be killed, i.e. it sets the internal _killed flag to true . kill calls spark-TaskContextImpl.md#markInterrupted[TaskContextImpl.markInterrupted] when context is set. If interruptThread is enabled and the internal taskThread is available, kill interrupts it. CAUTION: FIXME When could context and interruptThread not be set? == [[internal-registries]] Internal Properties .Task's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | _executorDeserializeCpuTime | [[_executorDeserializeCpuTime]] | _executorDeserializeTime | [[_executorDeserializeTime]] | _reasonIfKilled | [[_reasonIfKilled]] | _killed | [[_killed]] | context | [[context]] < > Set to be a < > or < > when the < > flag is enabled or not, respectively, when Task is requested to < > | epoch | [[epoch]] Task epoch Starts as -1 Set when TaskSetManager is scheduler:TaskSetManager.md[created] (to be the scheduler:MapOutputTrackerMaster.md#getEpoch[epoch] of the MapOutputTrackerMaster ) | metrics | [[metrics]] executor:TaskMetrics.md[] Created lazily when < > from < >. | taskMemoryManager | [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] that manages the memory allocated by the task. | taskThread | [[taskThread]] |===","title":"kill(interruptThread: Boolean)"},{"location":"scheduler/TaskContext/","text":"TaskContext \u00b6 TaskContext is the < > for < > that serve the following purpose: Hold contextual information about a scheduler:Task.md[task] at execution, e.g. < >, < >, < >, < > Give access to the lifecycle of a task, e.g. < >, < > A task can access the TaskContext instance using < > object method (that simply returns null unless executed within the execution thread of a task). [source, scala] \u00b6 import org.apache.spark.TaskContext val ctx = TaskContext.get TaskContext allows for < > and < > that were set on the driver. [[contract]] .TaskContext Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | addTaskCompletionListener a| [[addTaskCompletionListener]] [source, scala] \u00b6 addTaskCompletionListener( listener: TaskCompletionListener): TaskContext // a concrete method for a Scala-friendly usage addTaskCompletionListener U : TaskContext Registers a TaskCompletionListener Used when...FIXME | addTaskFailureListener a| [[addTaskFailureListener]] [source, scala] \u00b6 addTaskFailureListener(listener: TaskFailureListener): TaskContext // a concrete method for a Scala-friendly usage addTaskFailureListener(f: (TaskContext, Throwable) => Unit): TaskContext Registers a TaskFailureListener Used when...FIXME | attemptNumber a| [[attemptNumber]] [source, scala] \u00b6 attemptNumber(): Int \u00b6 Specifies how many times the task has been attempted to execute (starting from 0 ) Used when...FIXME | fetchFailed a| [[fetchFailed]] [source, scala] \u00b6 fetchFailed: Option[FetchFailedException] \u00b6 Used when...FIXME | getKillReason a| [[getKillReason]] [source, scala] \u00b6 getKillReason(): Option[String] \u00b6 Used when...FIXME | getLocalProperties a| [[getLocalProperties]] [source, scala] \u00b6 getLocalProperties: Properties \u00b6 Used when...FIXME | getLocalProperty a| [[getLocalProperty]] [source, scala] \u00b6 getLocalProperty(key: String): String \u00b6 Used when...FIXME | getMetricsSources a| [[getMetricsSources]] [source, scala] \u00b6 getMetricsSources(sourceName: String): Seq[Source] \u00b6 < > by sourceName which are associated with the instance that runs the task. Used when...FIXME | isCompleted a| [[isCompleted]] [source, scala] \u00b6 isCompleted(): Boolean \u00b6 Used when...FIXME | isInterrupted a| [[isInterrupted]] [source, scala] \u00b6 isInterrupted(): Boolean \u00b6 Used when...FIXME | isRunningLocally a| [[isRunningLocally]] [source, scala] \u00b6 isRunningLocally(): Boolean \u00b6 Used when...FIXME | killTaskIfInterrupted a| [[killTaskIfInterrupted]] [source, scala] \u00b6 killTaskIfInterrupted(): Unit \u00b6 Used when...FIXME | markInterrupted a| [[markInterrupted]] [source, scala] \u00b6 markInterrupted(reason: String): Unit \u00b6 Used when...FIXME | markTaskCompleted a| [[markTaskCompleted]] [source, scala] \u00b6 markTaskCompleted(error: Option[Throwable]): Unit \u00b6 Used when...FIXME | markTaskFailed a| [[markTaskFailed]] [source, scala] \u00b6 markTaskFailed(error: Throwable): Unit \u00b6 Used when...FIXME | partitionId a| [[partitionId]] [source, scala] \u00b6 partitionId(): Int \u00b6 ID of the spark-rdd-Partition.md[Partition] computed by the task Used when...FIXME | registerAccumulator a| [[registerAccumulator]] [source, scala] \u00b6 registerAccumulator(a: AccumulatorV2[_, _]): Unit \u00b6 Used when...FIXME | setFetchFailed a| [[setFetchFailed]] [source, scala] \u00b6 setFetchFailed(fetchFailed: FetchFailedException): Unit \u00b6 Used when...FIXME | stageAttemptNumber a| [[stageAttemptNumber]] [source, scala] \u00b6 stageAttemptNumber(): Int \u00b6 Used when...FIXME | stageId a| [[stageId]] [source, scala] \u00b6 stageId(): Int \u00b6 ID of the scheduler:Stage.md[Stage] the task belongs to Used when...FIXME | taskAttemptId a| [[taskAttemptId]] [source, scala] \u00b6 taskAttemptId(): Long \u00b6 Task (execution) attempt ID Used when...FIXME | taskMemoryManager a| [[taskMemoryManager]] [source, scala] \u00b6 taskMemoryManager(): TaskMemoryManager \u00b6 memory:TaskMemoryManager.md[TaskMemoryManager] Used when...FIXME | taskMetrics a| [[taskMetrics]] [source, scala] \u00b6 taskMetrics(): TaskMetrics \u00b6 executor:TaskMetrics.md[] Used when...FIXME |=== [[implementations]] .TaskContexts [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | TaskContext | Description | < > | [[BarrierTaskContext]] | < > | [[TaskContextImpl]] |=== == [[setTaskContext]] Setting Thread-Local TaskContext -- setTaskContext Object Method [source, scala] \u00b6 setTaskContext(tc: TaskContext): Unit \u00b6 setTaskContext binds the given TaskContext as a thread-local variable. [NOTE] \u00b6 setTaskContext is used when: Task is requested to scheduler:Task.md#run[run] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool) sometime in the future]) * other cases of less importance \u00b6 == [[get]] Accessing Active TaskContext -- get Object Method [source, scala] \u00b6 get(): TaskContext \u00b6 get returns the thread-local TaskContext instance (by requesting the taskContext thread-local variable to get the instance). NOTE: get is a method of TaskContext object in Scala and so it is just one instance available (per classloader). With the https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal ] variable ( ThreadLocal[TaskContext] ), the TaskContext instance is thread-local and so allows for associating state with the thread of a task. [source, scala] \u00b6 val rdd = sc.range(0, 3, numSlices = 3) assert(rdd.partitions.size == 3) rdd.foreach { n => import org.apache.spark.TaskContext val tc = TaskContext.get val msg = s\"\"\"|------------------- |partitionId: ${tc.partitionId} |stageId: ${tc.stageId} |attemptNum: ${tc.attemptNumber} |taskAttemptId: ${tc.taskAttemptId} |-------------------\"\"\".stripMargin println(msg) } == [[registering-task-listeners]] Registering Task Listeners Using TaskContext object you can register task listeners for < > and < >. === [[addTaskCompletionListener]] addTaskCompletionListener Method [source, scala] \u00b6 addTaskCompletionListener(listener: TaskCompletionListener): TaskContext addTaskCompletionListener(f: (TaskContext) => Unit): TaskContext addTaskCompletionListener methods register a TaskCompletionListener listener to be executed on task completion. NOTE: It will be executed regardless of the final state of a task - success, failure, or cancellation. [source, scala] \u00b6 val rdd = sc.range(0, 5, numSlices = 1) import org.apache.spark.TaskContext val printTaskInfo = (tc: TaskContext) => { val msg = s\"\"\"|------------------- |partitionId: ${tc.partitionId} |stageId: ${tc.stageId} |attemptNum: ${tc.attemptNumber} |taskAttemptId: ${tc.taskAttemptId} |-------------------\"\"\".stripMargin println(msg) } rdd.foreachPartition { _ => val tc = TaskContext.get tc.addTaskCompletionListener(printTaskInfo) } === [[addTaskFailureListener]] addTaskFailureListener Method [source, scala] \u00b6 addTaskFailureListener(listener: TaskFailureListener): TaskContext addTaskFailureListener(f: (TaskContext, Throwable) => Unit): TaskContext addTaskFailureListener methods register a TaskFailureListener listener to be executed on task failure only. It can be executed multiple times since a task can be re-attempted when it fails. [source, scala] \u00b6 val rdd = sc.range(0, 2, numSlices = 2) import org.apache.spark.TaskContext val printTaskErrorInfo = (tc: TaskContext, error: Throwable) => { val msg = s\"\"\"|------------------- |partitionId: ${tc.partitionId} |stageId: ${tc.stageId} |attemptNum: ${tc.attemptNumber} |taskAttemptId: ${tc.taskAttemptId} |error: ${error.toString} |-------------------\"\"\".stripMargin println(msg) } val throwExceptionForOddNumber = (n: Long) => { if (n % 2 == 1) { throw new Exception(s\"No way it will pass for odd number: $n\") } } // FIXME It won't work. rdd.map(throwExceptionForOddNumber).foreachPartition { _ => val tc = TaskContext.get tc.addTaskFailureListener(printTaskErrorInfo) } // Listener registration matters. rdd.mapPartitions { (it: Iterator[Long]) => val tc = TaskContext.get tc.addTaskFailureListener(printTaskErrorInfo) it }.map(throwExceptionForOddNumber).count","title":"TaskContext"},{"location":"scheduler/TaskContext/#taskcontext","text":"TaskContext is the < > for < > that serve the following purpose: Hold contextual information about a scheduler:Task.md[task] at execution, e.g. < >, < >, < >, < > Give access to the lifecycle of a task, e.g. < >, < > A task can access the TaskContext instance using < > object method (that simply returns null unless executed within the execution thread of a task).","title":"TaskContext"},{"location":"scheduler/TaskContext/#source-scala","text":"import org.apache.spark.TaskContext val ctx = TaskContext.get TaskContext allows for < > and < > that were set on the driver. [[contract]] .TaskContext Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | addTaskCompletionListener a| [[addTaskCompletionListener]]","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_1","text":"addTaskCompletionListener( listener: TaskCompletionListener): TaskContext // a concrete method for a Scala-friendly usage addTaskCompletionListener U : TaskContext Registers a TaskCompletionListener Used when...FIXME | addTaskFailureListener a| [[addTaskFailureListener]]","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_2","text":"addTaskFailureListener(listener: TaskFailureListener): TaskContext // a concrete method for a Scala-friendly usage addTaskFailureListener(f: (TaskContext, Throwable) => Unit): TaskContext Registers a TaskFailureListener Used when...FIXME | attemptNumber a| [[attemptNumber]]","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#attemptnumber-int","text":"Specifies how many times the task has been attempted to execute (starting from 0 ) Used when...FIXME | fetchFailed a| [[fetchFailed]]","title":"attemptNumber(): Int"},{"location":"scheduler/TaskContext/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#fetchfailed-optionfetchfailedexception","text":"Used when...FIXME | getKillReason a| [[getKillReason]]","title":"fetchFailed: Option[FetchFailedException]"},{"location":"scheduler/TaskContext/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#getkillreason-optionstring","text":"Used when...FIXME | getLocalProperties a| [[getLocalProperties]]","title":"getKillReason(): Option[String]"},{"location":"scheduler/TaskContext/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#getlocalproperties-properties","text":"Used when...FIXME | getLocalProperty a| [[getLocalProperty]]","title":"getLocalProperties: Properties"},{"location":"scheduler/TaskContext/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#getlocalpropertykey-string-string","text":"Used when...FIXME | getMetricsSources a| [[getMetricsSources]]","title":"getLocalProperty(key: String): String"},{"location":"scheduler/TaskContext/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#getmetricssourcessourcename-string-seqsource","text":"< > by sourceName which are associated with the instance that runs the task. Used when...FIXME | isCompleted a| [[isCompleted]]","title":"getMetricsSources(sourceName: String): Seq[Source]"},{"location":"scheduler/TaskContext/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#iscompleted-boolean","text":"Used when...FIXME | isInterrupted a| [[isInterrupted]]","title":"isCompleted(): Boolean"},{"location":"scheduler/TaskContext/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#isinterrupted-boolean","text":"Used when...FIXME | isRunningLocally a| [[isRunningLocally]]","title":"isInterrupted(): Boolean"},{"location":"scheduler/TaskContext/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#isrunninglocally-boolean","text":"Used when...FIXME | killTaskIfInterrupted a| [[killTaskIfInterrupted]]","title":"isRunningLocally(): Boolean"},{"location":"scheduler/TaskContext/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#killtaskifinterrupted-unit","text":"Used when...FIXME | markInterrupted a| [[markInterrupted]]","title":"killTaskIfInterrupted(): Unit"},{"location":"scheduler/TaskContext/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#markinterruptedreason-string-unit","text":"Used when...FIXME | markTaskCompleted a| [[markTaskCompleted]]","title":"markInterrupted(reason: String): Unit"},{"location":"scheduler/TaskContext/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#marktaskcompletederror-optionthrowable-unit","text":"Used when...FIXME | markTaskFailed a| [[markTaskFailed]]","title":"markTaskCompleted(error: Option[Throwable]): Unit"},{"location":"scheduler/TaskContext/#source-scala_15","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#marktaskfailederror-throwable-unit","text":"Used when...FIXME | partitionId a| [[partitionId]]","title":"markTaskFailed(error: Throwable): Unit"},{"location":"scheduler/TaskContext/#source-scala_16","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#partitionid-int","text":"ID of the spark-rdd-Partition.md[Partition] computed by the task Used when...FIXME | registerAccumulator a| [[registerAccumulator]]","title":"partitionId(): Int"},{"location":"scheduler/TaskContext/#source-scala_17","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#registeraccumulatora-accumulatorv2_-_-unit","text":"Used when...FIXME | setFetchFailed a| [[setFetchFailed]]","title":"registerAccumulator(a: AccumulatorV2[_, _]): Unit"},{"location":"scheduler/TaskContext/#source-scala_18","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#setfetchfailedfetchfailed-fetchfailedexception-unit","text":"Used when...FIXME | stageAttemptNumber a| [[stageAttemptNumber]]","title":"setFetchFailed(fetchFailed: FetchFailedException): Unit"},{"location":"scheduler/TaskContext/#source-scala_19","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#stageattemptnumber-int","text":"Used when...FIXME | stageId a| [[stageId]]","title":"stageAttemptNumber(): Int"},{"location":"scheduler/TaskContext/#source-scala_20","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#stageid-int","text":"ID of the scheduler:Stage.md[Stage] the task belongs to Used when...FIXME | taskAttemptId a| [[taskAttemptId]]","title":"stageId(): Int"},{"location":"scheduler/TaskContext/#source-scala_21","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#taskattemptid-long","text":"Task (execution) attempt ID Used when...FIXME | taskMemoryManager a| [[taskMemoryManager]]","title":"taskAttemptId(): Long"},{"location":"scheduler/TaskContext/#source-scala_22","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#taskmemorymanager-taskmemorymanager","text":"memory:TaskMemoryManager.md[TaskMemoryManager] Used when...FIXME | taskMetrics a| [[taskMetrics]]","title":"taskMemoryManager(): TaskMemoryManager"},{"location":"scheduler/TaskContext/#source-scala_23","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#taskmetrics-taskmetrics","text":"executor:TaskMetrics.md[] Used when...FIXME |=== [[implementations]] .TaskContexts [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | TaskContext | Description | < > | [[BarrierTaskContext]] | < > | [[TaskContextImpl]] |=== == [[setTaskContext]] Setting Thread-Local TaskContext -- setTaskContext Object Method","title":"taskMetrics(): TaskMetrics"},{"location":"scheduler/TaskContext/#source-scala_24","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#settaskcontexttc-taskcontext-unit","text":"setTaskContext binds the given TaskContext as a thread-local variable.","title":"setTaskContext(tc: TaskContext): Unit"},{"location":"scheduler/TaskContext/#note","text":"setTaskContext is used when: Task is requested to scheduler:Task.md#run[run] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool) sometime in the future])","title":"[NOTE]"},{"location":"scheduler/TaskContext/#other-cases-of-less-importance","text":"== [[get]] Accessing Active TaskContext -- get Object Method","title":"* other cases of less importance"},{"location":"scheduler/TaskContext/#source-scala_25","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContext/#get-taskcontext","text":"get returns the thread-local TaskContext instance (by requesting the taskContext thread-local variable to get the instance). NOTE: get is a method of TaskContext object in Scala and so it is just one instance available (per classloader). With the https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[ThreadLocal ] variable ( ThreadLocal[TaskContext] ), the TaskContext instance is thread-local and so allows for associating state with the thread of a task.","title":"get(): TaskContext"},{"location":"scheduler/TaskContext/#source-scala_26","text":"val rdd = sc.range(0, 3, numSlices = 3) assert(rdd.partitions.size == 3) rdd.foreach { n => import org.apache.spark.TaskContext val tc = TaskContext.get val msg = s\"\"\"|------------------- |partitionId: ${tc.partitionId} |stageId: ${tc.stageId} |attemptNum: ${tc.attemptNumber} |taskAttemptId: ${tc.taskAttemptId} |-------------------\"\"\".stripMargin println(msg) } == [[registering-task-listeners]] Registering Task Listeners Using TaskContext object you can register task listeners for < > and < >. === [[addTaskCompletionListener]] addTaskCompletionListener Method","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_27","text":"addTaskCompletionListener(listener: TaskCompletionListener): TaskContext addTaskCompletionListener(f: (TaskContext) => Unit): TaskContext addTaskCompletionListener methods register a TaskCompletionListener listener to be executed on task completion. NOTE: It will be executed regardless of the final state of a task - success, failure, or cancellation.","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_28","text":"val rdd = sc.range(0, 5, numSlices = 1) import org.apache.spark.TaskContext val printTaskInfo = (tc: TaskContext) => { val msg = s\"\"\"|------------------- |partitionId: ${tc.partitionId} |stageId: ${tc.stageId} |attemptNum: ${tc.attemptNumber} |taskAttemptId: ${tc.taskAttemptId} |-------------------\"\"\".stripMargin println(msg) } rdd.foreachPartition { _ => val tc = TaskContext.get tc.addTaskCompletionListener(printTaskInfo) } === [[addTaskFailureListener]] addTaskFailureListener Method","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_29","text":"addTaskFailureListener(listener: TaskFailureListener): TaskContext addTaskFailureListener(f: (TaskContext, Throwable) => Unit): TaskContext addTaskFailureListener methods register a TaskFailureListener listener to be executed on task failure only. It can be executed multiple times since a task can be re-attempted when it fails.","title":"[source, scala]"},{"location":"scheduler/TaskContext/#source-scala_30","text":"val rdd = sc.range(0, 2, numSlices = 2) import org.apache.spark.TaskContext val printTaskErrorInfo = (tc: TaskContext, error: Throwable) => { val msg = s\"\"\"|------------------- |partitionId: ${tc.partitionId} |stageId: ${tc.stageId} |attemptNum: ${tc.attemptNumber} |taskAttemptId: ${tc.taskAttemptId} |error: ${error.toString} |-------------------\"\"\".stripMargin println(msg) } val throwExceptionForOddNumber = (n: Long) => { if (n % 2 == 1) { throw new Exception(s\"No way it will pass for odd number: $n\") } } // FIXME It won't work. rdd.map(throwExceptionForOddNumber).foreachPartition { _ => val tc = TaskContext.get tc.addTaskFailureListener(printTaskErrorInfo) } // Listener registration matters. rdd.mapPartitions { (it: Iterator[Long]) => val tc = TaskContext.get tc.addTaskFailureListener(printTaskErrorInfo) it }.map(throwExceptionForOddNumber).count","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/","text":"== [[TaskContextImpl]] TaskContextImpl -- Default TaskContext TaskContextImpl is a concrete < > that is < > exclusively when Task is requested to scheduler:Task.md#run[run] (when Executor is requested to executor:Executor.md#launchTask[launch a task (on \"Executor task launch worker\" thread pool) sometime in the future]). [[creating-instance]] TaskContextImpl takes the following to be created: [[stageId]] Stage ID [[stageAttemptNumber]] Stage (execution) attempt ID [[partitionId]] Partition ID [[taskAttemptId]] Task (execution) attempt ID [[attemptNumber]] Attempt ID [[taskMemoryManager]] memory:TaskMemoryManager.md[TaskMemoryManager] [[localProperties]] Local properties [[metricsSystem]] metrics:spark-metrics-MetricsSystem.md[MetricsSystem] [[taskMetrics]] executor:TaskMetrics.md[] [[internal-registries]] .TaskContextImpl's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | onCompleteCallbacks a| [[onCompleteCallbacks]] ( ArrayBuffer[TaskCompletionListener] ) Used when...FIXME | onFailureCallbacks a| [[onFailureCallbacks]] ( ArrayBuffer[TaskFailureListener] ) Used when...FIXME | reasonIfKilled a| [[reasonIfKilled]] Reason if the task was killed Used when...FIXME | completed a| [[completed]][[isCompleted]] Flag whether...FIXME Default: false Used when...FIXME | failed a| [[failed]] Flag whether...FIXME Default: false Used when...FIXME | failure a| [[failure]] java.lang.Throwable that caused a failure Used when...FIXME | _fetchFailedException a| [[_fetchFailedException]] shuffle:FetchFailedException.md[FetchFailedException] if there was a fetch failure Used when...FIXME |=== === [[addTaskCompletionListener]] addTaskCompletionListener Method [source, scala] \u00b6 addTaskCompletionListener(listener: TaskCompletionListener): TaskContext \u00b6 NOTE: addTaskCompletionListener is part of the < > to register a TaskCompletionListener . addTaskCompletionListener ...FIXME === [[addTaskFailureListener]] addTaskFailureListener Method [source, scala] \u00b6 addTaskFailureListener(listener: TaskFailureListener): TaskContext \u00b6 NOTE: addTaskFailureListener is part of the < > to register a TaskFailureListener . addTaskFailureListener ...FIXME === [[markTaskFailed]] markTaskFailed Method [source, scala] \u00b6 markTaskFailed(error: Throwable): Unit \u00b6 NOTE: markTaskFailed is part of the < > to mark the task as failed and trigger the TaskFailureListeners . markTaskFailed ...FIXME === [[markTaskCompleted]] markTaskCompleted Method [source, scala] \u00b6 markTaskCompleted(error: Option[Throwable]): Unit \u00b6 NOTE: markTaskCompleted is part of the < > to mark the task as completed and trigger the TaskCompletionListeners . markTaskCompleted ...FIXME === [[invokeListeners]] invokeListeners Internal Method [source, scala] \u00b6 invokeListeners T ( callback: T => Unit): Unit invokeListeners ...FIXME NOTE: invokeListeners is used when...FIXME === [[markInterrupted]] markInterrupted Method [source, scala] \u00b6 markInterrupted(reason: String): Unit \u00b6 NOTE: markInterrupted is part of the < > to mark the task for interruption, i.e. cancellation. markInterrupted ...FIXME === [[killTaskIfInterrupted]] killTaskIfInterrupted Method [source, scala] \u00b6 killTaskIfInterrupted(): Unit \u00b6 NOTE: killTaskIfInterrupted is part of the < > to mark the task for interruption, i.e. cancellation. killTaskIfInterrupted ...FIXME","title":"TaskContextImpl"},{"location":"scheduler/TaskContextImpl/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#addtaskcompletionlistenerlistener-taskcompletionlistener-taskcontext","text":"NOTE: addTaskCompletionListener is part of the < > to register a TaskCompletionListener . addTaskCompletionListener ...FIXME === [[addTaskFailureListener]] addTaskFailureListener Method","title":"addTaskCompletionListener(listener: TaskCompletionListener): TaskContext"},{"location":"scheduler/TaskContextImpl/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#addtaskfailurelistenerlistener-taskfailurelistener-taskcontext","text":"NOTE: addTaskFailureListener is part of the < > to register a TaskFailureListener . addTaskFailureListener ...FIXME === [[markTaskFailed]] markTaskFailed Method","title":"addTaskFailureListener(listener: TaskFailureListener): TaskContext"},{"location":"scheduler/TaskContextImpl/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#marktaskfailederror-throwable-unit","text":"NOTE: markTaskFailed is part of the < > to mark the task as failed and trigger the TaskFailureListeners . markTaskFailed ...FIXME === [[markTaskCompleted]] markTaskCompleted Method","title":"markTaskFailed(error: Throwable): Unit"},{"location":"scheduler/TaskContextImpl/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#marktaskcompletederror-optionthrowable-unit","text":"NOTE: markTaskCompleted is part of the < > to mark the task as completed and trigger the TaskCompletionListeners . markTaskCompleted ...FIXME === [[invokeListeners]] invokeListeners Internal Method","title":"markTaskCompleted(error: Option[Throwable]): Unit"},{"location":"scheduler/TaskContextImpl/#source-scala_4","text":"invokeListeners T ( callback: T => Unit): Unit invokeListeners ...FIXME NOTE: invokeListeners is used when...FIXME === [[markInterrupted]] markInterrupted Method","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#markinterruptedreason-string-unit","text":"NOTE: markInterrupted is part of the < > to mark the task for interruption, i.e. cancellation. markInterrupted ...FIXME === [[killTaskIfInterrupted]] killTaskIfInterrupted Method","title":"markInterrupted(reason: String): Unit"},{"location":"scheduler/TaskContextImpl/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskContextImpl/#killtaskifinterrupted-unit","text":"NOTE: killTaskIfInterrupted is part of the < > to mark the task for interruption, i.e. cancellation. killTaskIfInterrupted ...FIXME","title":"killTaskIfInterrupted(): Unit"},{"location":"scheduler/TaskDescription/","text":"= TaskDescription [[creating-instance]] TaskDescription is a metadata of a scheduler:Task.md[task] with the following properties: [[taskId]] Task ID [[attemptNumber]] Task attempt number [[executorId]] Executor ID [[name]] Task name [[index]] Task index (within the scheduler:TaskSet.md[TaskSet]) [[addedFiles]] Added files (as Map[String, Long] ) [[addedJars]] Added JAR files (as Map[String, Long] ) [[properties]] Properties [[serializedTask]] Serialized task (as ByteBuffer ) The < > of the task is of the format: task [taskID] in stage [taskSetID] TaskDescription is < > exclusively when TaskSetManager is requested to scheduler:TaskSetManager.md#resourceOffer[find a task ready for execution (given a resource offer)]. [[toString]] The textual representation of a TaskDescription is as follows: TaskDescription(TID=[taskId], index=[index]) == [[decode]] Decoding TaskDescription (from Serialized Format) [source, scala] \u00b6 decode(byteBuffer: ByteBuffer): TaskDescription \u00b6 decode simply decodes (< >) a TaskDescription from the serialized format ( ByteBuffer ). Internally, decode ...FIXME [NOTE] \u00b6 decode is used when: CoarseGrainedExecutorBackend is requested to CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message] * Spark on Mesos' MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launch a task] \u00b6 == [[encode]] Encoding TaskDescription (to Serialized Format) [source, scala] \u00b6 encode(taskDescription: TaskDescription): ByteBuffer \u00b6 encode simply encodes the TaskDescription to a serialized format ( ByteBuffer ). Internally, encode ...FIXME [NOTE] \u00b6 encode is used when: DriverEndpoint (of CoarseGrainedSchedulerBackend ) is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launchTasks] * Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to createMesosTask \u00b6","title":"TaskDescription"},{"location":"scheduler/TaskDescription/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskDescription/#decodebytebuffer-bytebuffer-taskdescription","text":"decode simply decodes (< >) a TaskDescription from the serialized format ( ByteBuffer ). Internally, decode ...FIXME","title":"decode(byteBuffer: ByteBuffer): TaskDescription"},{"location":"scheduler/TaskDescription/#note","text":"decode is used when: CoarseGrainedExecutorBackend is requested to CoarseGrainedExecutorBackend.md#LaunchTask[handle a LaunchTask message]","title":"[NOTE]"},{"location":"scheduler/TaskDescription/#spark-on-mesos-mesosexecutorbackend-is-requested-to-spark-on-mesosspark-executor-backends-mesosexecutorbackendmdlaunchtasklaunch-a-task","text":"== [[encode]] Encoding TaskDescription (to Serialized Format)","title":"* Spark on Mesos' MesosExecutorBackend is requested to spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md#launchTask[launch a task]"},{"location":"scheduler/TaskDescription/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/TaskDescription/#encodetaskdescription-taskdescription-bytebuffer","text":"encode simply encodes the TaskDescription to a serialized format ( ByteBuffer ). Internally, encode ...FIXME","title":"encode(taskDescription: TaskDescription): ByteBuffer"},{"location":"scheduler/TaskDescription/#note_1","text":"encode is used when: DriverEndpoint (of CoarseGrainedSchedulerBackend ) is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launchTasks]","title":"[NOTE]"},{"location":"scheduler/TaskDescription/#spark-on-mesos-mesosfinegrainedschedulerbackend-is-requested-to-createmesostask","text":"","title":"* Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to createMesosTask"},{"location":"scheduler/TaskInfo/","text":"== [[TaskInfo]] TaskInfo TaskInfo is information about a running task attempt inside a scheduler:TaskSet.md[TaskSet]. TaskInfo is created when: scheduler:TaskSetManager.md#resourceOffer[ TaskSetManager dequeues a task for execution (given resource offer)] (and records the task as running) TaskUIData does dropInternalAndSQLAccumulables JsonProtocol utility is used to spark-history-server:JsonProtocol.md#taskInfoFromJson[re-create a task details from JSON] NOTE: Back then, at the commit 63051dd2bcc4bf09d413ff7cf89a37967edc33ba, when TaskInfo was first merged to Apache Spark on 07/06/12, TaskInfo was part of spark.scheduler.mesos package -- note \"Mesos\" in the name of the package that shows how much Spark and Mesos influenced each other at that time. [[internal-registries]] .TaskInfo's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[finishTime]] finishTime | Time when TaskInfo was < >. Used when...FIXME |=== === [[creating-instance]] Creating TaskInfo Instance TaskInfo takes the following when created: [[taskId]] Task ID [[index]] Index of the task within its scheduler:TaskSet.md[TaskSet] that may not necessarily be the same as the ID of the RDD partition that the task is computing. [[attemptNumber]] Task attempt ID [[launchTime]] Time when the task was dequeued for execution [[executorId]] Executor that has been offered (as a resource) to run the task [[host]] Host of the < > [[taskLocality]] scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality], i.e. locality preference of the task [[speculative]] Flag whether a task is speculative or not TaskInfo initializes the < >. === [[markFinished]] Marking Task As Finished (Successfully or Not) -- markFinished Method [source, scala] \u00b6 markFinished(state: TaskState, time: Long = System.currentTimeMillis): Unit \u00b6 markFinished records the input time as < >. markFinished marks TaskInfo as < > when the input state is FAILED or < > for state being KILLED . NOTE: markFinished is used when TaskSetManager is notified that a task has finished scheduler:TaskSetManager.md#handleSuccessfulTask[successfully] or scheduler:TaskSetManager.md#handleFailedTask[failed].","title":"TaskInfo"},{"location":"scheduler/TaskInfo/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskInfo/#markfinishedstate-taskstate-time-long-systemcurrenttimemillis-unit","text":"markFinished records the input time as < >. markFinished marks TaskInfo as < > when the input state is FAILED or < > for state being KILLED . NOTE: markFinished is used when TaskSetManager is notified that a task has finished scheduler:TaskSetManager.md#handleSuccessfulTask[successfully] or scheduler:TaskSetManager.md#handleFailedTask[failed].","title":"markFinished(state: TaskState, time: Long = System.currentTimeMillis): Unit"},{"location":"scheduler/TaskLocation/","text":"TaskLocation \u00b6 TaskLocation represents a placement preference of an RDD partition, i.e. a hint of the location to submit scheduler:Task.md[tasks] for execution. TaskLocations are tracked by scheduler:DAGScheduler.md#cacheLocs[DAGScheduler] for scheduler:DAGScheduler.md#submitMissingTasks[submitting missing tasks of a stage]. TaskLocation is available as scheduler:Task.md#preferredLocations[preferredLocations] of a task. [[host]] Every TaskLocation describes the location by host name, but could also use other location-related metadata. TaskLocations of an RDD and a partition is available using ROOT:SparkContext.md#getPreferredLocs[SparkContext.getPreferredLocs] method. Sealed TaskLocation is a Scala private[spark] sealed trait so all the available implementations of TaskLocation trait are in a single Scala file. == [[ExecutorCacheTaskLocation]] ExecutorCacheTaskLocation ExecutorCacheTaskLocation describes a < > and an executor. ExecutorCacheTaskLocation informs the Scheduler to prefer a given executor, but the next level of preference is any executor on the same host if this is not possible. == [[HDFSCacheTaskLocation]] HDFSCacheTaskLocation HDFSCacheTaskLocation describes a < > that is cached by HDFS. Used exclusively when rdd:spark-rdd-HadoopRDD.md#getPreferredLocations[HadoopRDD] and rdd:spark-rdd-NewHadoopRDD.md#getPreferredLocations[NewHadoopRDD] are requested for their placement preferences (aka preferred locations ). == [[HostTaskLocation]] HostTaskLocation HostTaskLocation describes a < > only.","title":"TaskLocation"},{"location":"scheduler/TaskLocation/#tasklocation","text":"TaskLocation represents a placement preference of an RDD partition, i.e. a hint of the location to submit scheduler:Task.md[tasks] for execution. TaskLocations are tracked by scheduler:DAGScheduler.md#cacheLocs[DAGScheduler] for scheduler:DAGScheduler.md#submitMissingTasks[submitting missing tasks of a stage]. TaskLocation is available as scheduler:Task.md#preferredLocations[preferredLocations] of a task. [[host]] Every TaskLocation describes the location by host name, but could also use other location-related metadata. TaskLocations of an RDD and a partition is available using ROOT:SparkContext.md#getPreferredLocs[SparkContext.getPreferredLocs] method. Sealed TaskLocation is a Scala private[spark] sealed trait so all the available implementations of TaskLocation trait are in a single Scala file. == [[ExecutorCacheTaskLocation]] ExecutorCacheTaskLocation ExecutorCacheTaskLocation describes a < > and an executor. ExecutorCacheTaskLocation informs the Scheduler to prefer a given executor, but the next level of preference is any executor on the same host if this is not possible. == [[HDFSCacheTaskLocation]] HDFSCacheTaskLocation HDFSCacheTaskLocation describes a < > that is cached by HDFS. Used exclusively when rdd:spark-rdd-HadoopRDD.md#getPreferredLocations[HadoopRDD] and rdd:spark-rdd-NewHadoopRDD.md#getPreferredLocations[NewHadoopRDD] are requested for their placement preferences (aka preferred locations ). == [[HostTaskLocation]] HostTaskLocation HostTaskLocation describes a < > only.","title":"TaskLocation"},{"location":"scheduler/TaskResult/","text":"== [[TaskResult]] TaskResults -- DirectTaskResult and IndirectTaskResult TaskResult models a task result. It has exactly two concrete implementations: < > is the TaskResult to be serialized and sent over the wire to the driver together with the result bytes and accumulators. < > is the TaskResult that is just a pointer to a task result in a BlockManager . The decision of the concrete TaskResult is made when a executor:TaskRunner.md#run[ TaskRunner finishes running a task and checks the size of the result]. NOTE: The types are private[spark] . === [[DirectTaskResult]] DirectTaskResult Task Result [source, scala] \u00b6 DirectTaskResult T extends TaskResult[T] with Externalizable DirectTaskResult is the < > of scheduler:Task.md#run[running a task] (that is later executor:TaskRunner.md#run[returned serialized to the driver]) when the size of the task's result is smaller than ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize] and executor:Executor.md#spark.task.maxDirectResultSize[spark.task.maxDirectResultSize] (or scheduler:CoarseGrainedSchedulerBackend.md#spark.rpc.message.maxSize[spark.rpc.message.maxSize] whatever is smaller). NOTE: DirectTaskResult is Java's https://docs.oracle.com/javase/8/docs/api/java/io/Externalizable.html[java.io.Externalizable ]. === [[IndirectTaskResult]] IndirectTaskResult Task Result [source, scala] \u00b6 IndirectTaskResult T extends TaskResult[T] with Serializable IndirectTaskResult is a < > that... NOTE: IndirectTaskResult is Java's https://docs.oracle.com/javase/8/docs/api/java/io/Serializable.html[java.io.Serializable ].","title":"TaskResult"},{"location":"scheduler/TaskResult/#source-scala","text":"DirectTaskResult T extends TaskResult[T] with Externalizable DirectTaskResult is the < > of scheduler:Task.md#run[running a task] (that is later executor:TaskRunner.md#run[returned serialized to the driver]) when the size of the task's result is smaller than ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize] and executor:Executor.md#spark.task.maxDirectResultSize[spark.task.maxDirectResultSize] (or scheduler:CoarseGrainedSchedulerBackend.md#spark.rpc.message.maxSize[spark.rpc.message.maxSize] whatever is smaller). NOTE: DirectTaskResult is Java's https://docs.oracle.com/javase/8/docs/api/java/io/Externalizable.html[java.io.Externalizable ]. === [[IndirectTaskResult]] IndirectTaskResult Task Result","title":"[source, scala]"},{"location":"scheduler/TaskResult/#source-scala_1","text":"IndirectTaskResult T extends TaskResult[T] with Serializable IndirectTaskResult is a < > that... NOTE: IndirectTaskResult is Java's https://docs.oracle.com/javase/8/docs/api/java/io/Serializable.html[java.io.Serializable ].","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/","text":"== [[TaskResultGetter]] TaskResultGetter TaskResultGetter is a helper class of scheduler:TaskSchedulerImpl.md#statusUpdate[TaskSchedulerImpl] for asynchronous deserialization of < > (possibly fetching remote blocks) or < >. CAUTION: FIXME Image with the dependencies TIP: Consult scheduler:Task.md#states[Task States] in Tasks to learn about the different task states. NOTE: The only instance of TaskResultGetter is created while scheduler:TaskSchedulerImpl.md#creating-instance[ TaskSchedulerImpl is created]. TaskResultGetter requires a core:SparkEnv.md[SparkEnv] and scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] to be created and is stopped when scheduler:TaskSchedulerImpl.md#stop[ TaskSchedulerImpl stops]. TaskResultGetter uses < task-result-getter asynchronous task executor>> for operation. [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.scheduler.TaskResultGetter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskResultGetter=DEBUG Refer to spark-logging.md[Logging]. \u00b6 === [[getTaskResultExecutor]][[task-result-getter]] task-result-getter Asynchronous Task Executor [source, scala] \u00b6 getTaskResultExecutor: ExecutorService \u00b6 getTaskResultExecutor creates a daemon thread pool with < > threads and task-result-getter prefix. TIP: Read up on https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor ] that getTaskResultExecutor uses under the covers. === [[stop]] stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the internal < task-result-getter asynchronous task executor>>. === [[serializer]] serializer Attribute [source, scala] \u00b6 serializer: ThreadLocal[SerializerInstance] \u00b6 serializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to deserialize byte buffers (with TaskResult s or a TaskEndReason ). When created for a new thread, serializer is initialized with a new instance of Serializer (using core:SparkEnv.md#closureSerializer[SparkEnv.closureSerializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. === [[taskResultSerializer]] taskResultSerializer Attribute [source, scala] \u00b6 taskResultSerializer: ThreadLocal[SerializerInstance] \u00b6 taskResultSerializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to... When created for a new thread, taskResultSerializer is initialized with a new instance of Serializer (using core:SparkEnv.md#serializer[SparkEnv.serializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. === [[enqueueSuccessfulTask]] Enqueuing Successful Task (Deserializing Task Result and Notifying TaskSchedulerImpl) -- enqueueSuccessfulTask Method [source, scala] \u00b6 enqueueSuccessfulTask( taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer): Unit enqueueSuccessfulTask submits an asynchronous task (to < > asynchronous task executor) that first deserializes serializedData to a DirectTaskResult , then updates the internal accumulator (with the size of the DirectTaskResult ) and ultimately notifies the TaskSchedulerImpl that the tid task was completed and scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[the task result was received successfully] or scheduler:TaskSchedulerImpl.md#handleFailedTask[not]. NOTE: enqueueSuccessfulTask is just the asynchronous task enqueued for execution by < > asynchronous task executor at some point in the future. Internally, the enqueued task first deserializes serializedData to a TaskResult (using the internal thread-local < >). The spark-scheduler-TaskResult.md[TaskResult] could be a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult] or a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult]. For a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult], the task scheduler:TaskSetManager.md#canFetchMoreResults[checks the available memory for the task result] and, when the size overflows ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize], it simply returns. NOTE: enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else. That is why the scheduler:TaskSetManager.md#canFetchMoreResults[check for quota does abort] when there is not enough memory. Otherwise, when there is enough memory to hold the task result, it deserializes the DirectTaskResult (using the internal thread-local < >). For a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult], the task checks the available memory for the task result and, when the size could overflow the maximum result size, it storage:BlockManagerMaster.md#removeBlock[removes the block] and simply returns. Otherwise, when there is enough memory to hold the task result, you should see the following DEBUG message in the logs: DEBUG Fetching indirect task result for TID [tid] The task scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[notifies TaskSchedulerImpl that it is about to fetch a remote block for a task result]. It then storage:BlockManager.md#getRemoteBytes[gets the block from remote block managers (as serialized bytes)]. When the block could not be fetched, scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed] (with TaskResultLost task failure reason) and the task simply returns. NOTE: enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else and so the real handling is when scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed]. The task result (as a serialized byte buffer) is then deserialized to a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult] (using the internal thread-local < >) and deserialized again using the internal thread-local < > (just like for the DirectTaskResult case). The storage:BlockManagerMaster.md#removeBlock[block is removed from BlockManagerMaster ] and simply returns. NOTE: A spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] is deserialized twice to become the final deserialized task result (using < > for a DirectTaskResult ). Compare it to a DirectTaskResult task result that is deserialized once only. With no exceptions thrown, enqueueSuccessfulTask scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[informs the TaskSchedulerImpl that the tid task was completed and the task result was received]. A ClassNotFoundException leads to scheduler:TaskSetManager.md#abort[aborting the TaskSet ] (with ClassNotFound with classloader: [loader] error message) while any non-fatal exception shows the following ERROR message in the logs followed by scheduler:TaskSetManager.md#abort[aborting the TaskSet ]. ERROR Exception while getting task result NOTE: enqueueSuccessfulTask is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#statusUpdate[handle a task status update] (and the task has finished successfully). === [[enqueueFailedTask]] Deserializing TaskFailedReason and Notifying TaskSchedulerImpl -- enqueueFailedTask Method [source, scala] \u00b6 enqueueFailedTask( taskSetManager: TaskSetManager, tid: Long, taskState: TaskState.TaskState, serializedData: ByteBuffer): Unit enqueueFailedTask submits an asynchronous task (to < task-result-getter asynchronous task executor>>) that first attempts to deserialize a TaskFailedReason from serializedData (using the internal thread-local < >) and then scheduler:TaskSchedulerImpl.md#handleFailedTask[notifies TaskSchedulerImpl that the task has failed]. Any ClassNotFoundException leads to the following ERROR message in the logs (without breaking the flow of enqueueFailedTask ): ERROR Could not deserialize TaskEndReason: ClassNotFound with classloader [loader] NOTE: enqueueFailedTask is called when scheduler:TaskSchedulerImpl.md#statusUpdate[ TaskSchedulerImpl is notified about a task that has failed (and is in FAILED , KILLED or LOST state)]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_resultGetter_threads]] spark.resultGetter.threads | 4 | The number of threads for TaskResultGetter . |===","title":"TaskResultGetter"},{"location":"scheduler/TaskResultGetter/#tip","text":"Enable DEBUG logging level for org.apache.spark.scheduler.TaskResultGetter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskResultGetter=DEBUG","title":"[TIP]"},{"location":"scheduler/TaskResultGetter/#refer-to-spark-loggingmdlogging","text":"=== [[getTaskResultExecutor]][[task-result-getter]] task-result-getter Asynchronous Task Executor","title":"Refer to spark-logging.md[Logging]."},{"location":"scheduler/TaskResultGetter/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#gettaskresultexecutor-executorservice","text":"getTaskResultExecutor creates a daemon thread pool with < > threads and task-result-getter prefix. TIP: Read up on https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor ] that getTaskResultExecutor uses under the covers. === [[stop]] stop Method","title":"getTaskResultExecutor: ExecutorService"},{"location":"scheduler/TaskResultGetter/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#stop-unit","text":"stop stops the internal < task-result-getter asynchronous task executor>>. === [[serializer]] serializer Attribute","title":"stop(): Unit"},{"location":"scheduler/TaskResultGetter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#serializer-threadlocalserializerinstance","text":"serializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to deserialize byte buffers (with TaskResult s or a TaskEndReason ). When created for a new thread, serializer is initialized with a new instance of Serializer (using core:SparkEnv.md#closureSerializer[SparkEnv.closureSerializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. === [[taskResultSerializer]] taskResultSerializer Attribute","title":"serializer: ThreadLocal[SerializerInstance]"},{"location":"scheduler/TaskResultGetter/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#taskresultserializer-threadlocalserializerinstance","text":"taskResultSerializer is a thread-local serializer:SerializerInstance.md[SerializerInstance] that TaskResultGetter uses to... When created for a new thread, taskResultSerializer is initialized with a new instance of Serializer (using core:SparkEnv.md#serializer[SparkEnv.serializer]). NOTE: TaskResultGetter uses https://docs.oracle.com/javase/8/docs/api/java/lang/ThreadLocal.html[java.lang.ThreadLocal ] for the thread-local SerializerInstance variable. === [[enqueueSuccessfulTask]] Enqueuing Successful Task (Deserializing Task Result and Notifying TaskSchedulerImpl) -- enqueueSuccessfulTask Method","title":"taskResultSerializer: ThreadLocal[SerializerInstance]"},{"location":"scheduler/TaskResultGetter/#source-scala_4","text":"enqueueSuccessfulTask( taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer): Unit enqueueSuccessfulTask submits an asynchronous task (to < > asynchronous task executor) that first deserializes serializedData to a DirectTaskResult , then updates the internal accumulator (with the size of the DirectTaskResult ) and ultimately notifies the TaskSchedulerImpl that the tid task was completed and scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[the task result was received successfully] or scheduler:TaskSchedulerImpl.md#handleFailedTask[not]. NOTE: enqueueSuccessfulTask is just the asynchronous task enqueued for execution by < > asynchronous task executor at some point in the future. Internally, the enqueued task first deserializes serializedData to a TaskResult (using the internal thread-local < >). The spark-scheduler-TaskResult.md[TaskResult] could be a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult] or a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult]. For a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult], the task scheduler:TaskSetManager.md#canFetchMoreResults[checks the available memory for the task result] and, when the size overflows ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize], it simply returns. NOTE: enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else. That is why the scheduler:TaskSetManager.md#canFetchMoreResults[check for quota does abort] when there is not enough memory. Otherwise, when there is enough memory to hold the task result, it deserializes the DirectTaskResult (using the internal thread-local < >). For a spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult], the task checks the available memory for the task result and, when the size could overflow the maximum result size, it storage:BlockManagerMaster.md#removeBlock[removes the block] and simply returns. Otherwise, when there is enough memory to hold the task result, you should see the following DEBUG message in the logs: DEBUG Fetching indirect task result for TID [tid] The task scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[notifies TaskSchedulerImpl that it is about to fetch a remote block for a task result]. It then storage:BlockManager.md#getRemoteBytes[gets the block from remote block managers (as serialized bytes)]. When the block could not be fetched, scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed] (with TaskResultLost task failure reason) and the task simply returns. NOTE: enqueueSuccessfulTask is a mere thread so returning from a thread is to do nothing else and so the real handling is when scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl is informed]. The task result (as a serialized byte buffer) is then deserialized to a spark-scheduler-TaskResult.md#DirectTaskResult[DirectTaskResult] (using the internal thread-local < >) and deserialized again using the internal thread-local < > (just like for the DirectTaskResult case). The storage:BlockManagerMaster.md#removeBlock[block is removed from BlockManagerMaster ] and simply returns. NOTE: A spark-scheduler-TaskResult.md#IndirectTaskResult[IndirectTaskResult] is deserialized twice to become the final deserialized task result (using < > for a DirectTaskResult ). Compare it to a DirectTaskResult task result that is deserialized once only. With no exceptions thrown, enqueueSuccessfulTask scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[informs the TaskSchedulerImpl that the tid task was completed and the task result was received]. A ClassNotFoundException leads to scheduler:TaskSetManager.md#abort[aborting the TaskSet ] (with ClassNotFound with classloader: [loader] error message) while any non-fatal exception shows the following ERROR message in the logs followed by scheduler:TaskSetManager.md#abort[aborting the TaskSet ]. ERROR Exception while getting task result NOTE: enqueueSuccessfulTask is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#statusUpdate[handle a task status update] (and the task has finished successfully). === [[enqueueFailedTask]] Deserializing TaskFailedReason and Notifying TaskSchedulerImpl -- enqueueFailedTask Method","title":"[source, scala]"},{"location":"scheduler/TaskResultGetter/#source-scala_5","text":"enqueueFailedTask( taskSetManager: TaskSetManager, tid: Long, taskState: TaskState.TaskState, serializedData: ByteBuffer): Unit enqueueFailedTask submits an asynchronous task (to < task-result-getter asynchronous task executor>>) that first attempts to deserialize a TaskFailedReason from serializedData (using the internal thread-local < >) and then scheduler:TaskSchedulerImpl.md#handleFailedTask[notifies TaskSchedulerImpl that the task has failed]. Any ClassNotFoundException leads to the following ERROR message in the logs (without breaking the flow of enqueueFailedTask ): ERROR Could not deserialize TaskEndReason: ClassNotFound with classloader [loader] NOTE: enqueueFailedTask is called when scheduler:TaskSchedulerImpl.md#statusUpdate[ TaskSchedulerImpl is notified about a task that has failed (and is in FAILED , KILLED or LOST state)]. === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_resultGetter_threads]] spark.resultGetter.threads | 4 | The number of threads for TaskResultGetter . |===","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/","text":"TaskScheduler \u00b6 TaskScheduler is an abstraction of < > that can < > in a Spark application (per < >). NOTE: TaskScheduler works closely with scheduler:DAGScheduler.md[DAGScheduler] that < > (for every stage in a Spark job). TaskScheduler can track the executors available in a Spark application using < > and < > interceptors (that inform about active and lost executors, respectively). == [[submitTasks]] Submitting Tasks for Execution [source, scala] \u00b6 submitTasks( taskSet: TaskSet): Unit Submits the tasks (of the given scheduler:TaskSet.md[TaskSet]) for execution. Used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit missing tasks (of a stage)]. == [[executorHeartbeatReceived]] Handling Executor Heartbeat [source, scala] \u00b6 executorHeartbeatReceived( execId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId): Boolean Handles a heartbeat from an executor Returns true when the execId executor is managed by the TaskScheduler. false indicates that the executor:Executor.md#reportHeartBeat[block manager (on the executor) should re-register]. Used when HeartbeatReceiver RPC endpoint is requested to ROOT:spark-HeartbeatReceiver.md#Heartbeat[handle a Heartbeat (with task metrics) from an executor] == [[killTaskAttempt]] Killing Task [source, scala] \u00b6 killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean Kills a task (attempt) Used when DAGScheduler is requested to scheduler:DAGScheduler.md#killTaskAttempt[kill a task] == [[workerRemoved]] workerRemoved Notification [source, scala] \u00b6 workerRemoved( workerId: String, host: String, message: String): Unit Used when DriverEndpoint is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#removeWorker[handle a RemoveWorker event] == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | applicationAttemptId a| [[applicationAttemptId]] [source, scala] \u00b6 applicationAttemptId(): Option[String] \u00b6 Unique identifier of an (execution) attempt of the Spark application Used when SparkContext is created | cancelTasks a| [[cancelTasks]] [source, scala] \u00b6 cancelTasks( stageId: Int, interruptThread: Boolean): Unit Cancels all the tasks of a given scheduler:Stage.md[stage] Used when DAGScheduler is requested to scheduler:DAGScheduler.md#failJobAndIndependentStages[failJobAndIndependentStages] | defaultParallelism a| [[defaultParallelism]] [source, scala] \u00b6 defaultParallelism(): Int \u00b6 Default level of parallelism Used when SparkContext is requested for the ROOT:SparkContext.md#defaultParallelism[default level of parallelism] | executorLost a| [[executorLost]] [source, scala] \u00b6 executorLost( executorId: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: HeartbeatReceiver RPC endpoint is requested to ROOT:spark-HeartbeatReceiver.md#expireDeadHosts[expireDeadHosts] DriverEndpoint RPC endpoint is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#removeExecutor[removes] ( forgets ) and scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#disableExecutor[disables] a malfunctioning executor (i.e. either lost or blacklisted for some reason) Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to recordSlaveLost | killAllTaskAttempts a| [[killAllTaskAttempts]] [source, scala] \u00b6 killAllTaskAttempts( stageId: Int, interruptThread: Boolean, reason: String): Unit Used when: DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#cancelTasks[cancel all the tasks of a stage] | rootPool a| [[rootPool]] [source, scala] \u00b6 rootPool: Pool \u00b6 Top-level (root) scheduler:spark-scheduler-Pool.md[schedulable pool] Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] SparkContext is requested to ROOT:SparkContext.md#getAllPools[getAllPools] and ROOT:SparkContext.md#getPoolForName[getPoolForName] TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers], scheduler:TaskSchedulerImpl.md#checkSpeculatableTasks[checkSpeculatableTasks], and scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | schedulingMode a| [[schedulingMode]] [source, scala] \u00b6 schedulingMode: SchedulingMode \u00b6 scheduler:spark-scheduler-SchedulingMode.md[Scheduling mode] Used when: TaskSchedulerImpl is scheduler:TaskSchedulerImpl.md#rootPool[created] and scheduler:TaskSchedulerImpl.md#initialize[initialized] SparkContext is requested to ROOT:SparkContext.md#getSchedulingMode[getSchedulingMode] | setDAGScheduler a| [[setDAGScheduler]] [source, scala] \u00b6 setDAGScheduler(dagScheduler: DAGScheduler): Unit \u00b6 Associates a scheduler:DAGScheduler.md[DAGScheduler] Used when DAGScheduler is scheduler:DAGScheduler.md#creating-instance[created] | start a| [[start]] [source, scala] \u00b6 start(): Unit \u00b6 Starts the TaskScheduler Used when SparkContext is created | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Stops the TaskScheduler Used when DAGScheduler is requested to scheduler:DAGScheduler.md#stop[stop] |=== == [[implementations]] TaskSchedulers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | TaskScheduler | Description | scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] | [[TaskSchedulerImpl]] Default Spark scheduler | spark-on-yarn:spark-yarn-yarnscheduler.md[YarnScheduler] | [[YarnScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[client] deploy mode in spark-on-yarn:index.md[Spark on YARN] | spark-on-yarn:spark-yarn-yarnclusterscheduler.md[YarnClusterScheduler] | [[YarnClusterScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[cluster] deploy mode in spark-on-yarn:index.md[Spark on YARN] |=== == [[lifecycle]] Lifecycle A TaskScheduler is created while ROOT:SparkContext.md#creating-instance[SparkContext is being created] (by calling ROOT:SparkContext.md#createTaskScheduler[SparkContext.createTaskScheduler] for a given ROOT:spark-deployment-environments.md[master URL] and tools:spark-submit.md#deploy-mode[deploy mode]). .TaskScheduler uses SchedulerBackend to support different clusters image::taskscheduler-uses-schedulerbackend.png[align=\"center\"] At this point in SparkContext's lifecycle, the internal _taskScheduler points at the TaskScheduler (and it is \"announced\" by sending a blocking ROOT:spark-HeartbeatReceiver.md#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint]). The < > right after the blocking TaskSchedulerIsSet message receives a response. The < > and the < > are set at this point (and SparkContext uses the application id to set ROOT:SparkConf.md#spark.app.id[spark.app.id] Spark property, and configure webui:spark-webui-SparkUI.md[SparkUI], and storage:BlockManager.md[BlockManager]). CAUTION: FIXME The application id is described as \"associated with the job.\" in TaskScheduler, but I think it is \"associated with the application\" and you can have many jobs per application. Right before SparkContext is fully initialized, < > is called. The internal _taskScheduler is cleared (i.e. set to null ) while ROOT:SparkContext.md#stop[SparkContext is being stopped]. < > while scheduler:DAGScheduler.md#stop[DAGScheduler is being stopped]. WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now? == [[postStartHook]] Post-Start Initialization [source, scala] \u00b6 postStartHook(): Unit \u00b6 postStartHook does nothing by default, but allows < > for some additional post-start initialization. postStartHook is used when: SparkContext is created Spark on YARN's YarnClusterScheduler is requested to spark-on-yarn:spark-yarn-yarnclusterscheduler.md#postStartHook[postStartHook] == [[applicationId]][[appId]] Unique Identifier of Spark Application [source, scala] \u00b6 applicationId(): String \u00b6 applicationId is the unique identifier of the Spark application and defaults to spark-application-[currentTimeMillis] . applicationId is used when SparkContext is created.","title":"TaskScheduler"},{"location":"scheduler/TaskScheduler/#taskscheduler","text":"TaskScheduler is an abstraction of < > that can < > in a Spark application (per < >). NOTE: TaskScheduler works closely with scheduler:DAGScheduler.md[DAGScheduler] that < > (for every stage in a Spark job). TaskScheduler can track the executors available in a Spark application using < > and < > interceptors (that inform about active and lost executors, respectively). == [[submitTasks]] Submitting Tasks for Execution","title":"TaskScheduler"},{"location":"scheduler/TaskScheduler/#source-scala","text":"submitTasks( taskSet: TaskSet): Unit Submits the tasks (of the given scheduler:TaskSet.md[TaskSet]) for execution. Used when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit missing tasks (of a stage)]. == [[executorHeartbeatReceived]] Handling Executor Heartbeat","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_1","text":"executorHeartbeatReceived( execId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId): Boolean Handles a heartbeat from an executor Returns true when the execId executor is managed by the TaskScheduler. false indicates that the executor:Executor.md#reportHeartBeat[block manager (on the executor) should re-register]. Used when HeartbeatReceiver RPC endpoint is requested to ROOT:spark-HeartbeatReceiver.md#Heartbeat[handle a Heartbeat (with task metrics) from an executor] == [[killTaskAttempt]] Killing Task","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_2","text":"killTaskAttempt( taskId: Long, interruptThread: Boolean, reason: String): Boolean Kills a task (attempt) Used when DAGScheduler is requested to scheduler:DAGScheduler.md#killTaskAttempt[kill a task] == [[workerRemoved]] workerRemoved Notification","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_3","text":"workerRemoved( workerId: String, host: String, message: String): Unit Used when DriverEndpoint is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#removeWorker[handle a RemoveWorker event] == [[contract]] Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | applicationAttemptId a| [[applicationAttemptId]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#applicationattemptid-optionstring","text":"Unique identifier of an (execution) attempt of the Spark application Used when SparkContext is created | cancelTasks a| [[cancelTasks]]","title":"applicationAttemptId(): Option[String]"},{"location":"scheduler/TaskScheduler/#source-scala_5","text":"cancelTasks( stageId: Int, interruptThread: Boolean): Unit Cancels all the tasks of a given scheduler:Stage.md[stage] Used when DAGScheduler is requested to scheduler:DAGScheduler.md#failJobAndIndependentStages[failJobAndIndependentStages] | defaultParallelism a| [[defaultParallelism]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#defaultparallelism-int","text":"Default level of parallelism Used when SparkContext is requested for the ROOT:SparkContext.md#defaultParallelism[default level of parallelism] | executorLost a| [[executorLost]]","title":"defaultParallelism(): Int"},{"location":"scheduler/TaskScheduler/#source-scala_7","text":"executorLost( executorId: String, reason: ExecutorLossReason): Unit Handles an executor lost event Used when: HeartbeatReceiver RPC endpoint is requested to ROOT:spark-HeartbeatReceiver.md#expireDeadHosts[expireDeadHosts] DriverEndpoint RPC endpoint is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#removeExecutor[removes] ( forgets ) and scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#disableExecutor[disables] a malfunctioning executor (i.e. either lost or blacklisted for some reason) Spark on Mesos' MesosFineGrainedSchedulerBackend is requested to recordSlaveLost | killAllTaskAttempts a| [[killAllTaskAttempts]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_8","text":"killAllTaskAttempts( stageId: Int, interruptThread: Boolean, reason: String): Unit Used when: DAGScheduler is requested to scheduler:DAGScheduler.md#handleTaskCompletion[handleTaskCompletion] TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#cancelTasks[cancel all the tasks of a stage] | rootPool a| [[rootPool]]","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#rootpool-pool","text":"Top-level (root) scheduler:spark-scheduler-Pool.md[schedulable pool] Used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#initialize[initialize] SparkContext is requested to ROOT:SparkContext.md#getAllPools[getAllPools] and ROOT:SparkContext.md#getPoolForName[getPoolForName] TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers], scheduler:TaskSchedulerImpl.md#checkSpeculatableTasks[checkSpeculatableTasks], and scheduler:TaskSchedulerImpl.md#removeExecutor[removeExecutor] | schedulingMode a| [[schedulingMode]]","title":"rootPool: Pool"},{"location":"scheduler/TaskScheduler/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#schedulingmode-schedulingmode","text":"scheduler:spark-scheduler-SchedulingMode.md[Scheduling mode] Used when: TaskSchedulerImpl is scheduler:TaskSchedulerImpl.md#rootPool[created] and scheduler:TaskSchedulerImpl.md#initialize[initialized] SparkContext is requested to ROOT:SparkContext.md#getSchedulingMode[getSchedulingMode] | setDAGScheduler a| [[setDAGScheduler]]","title":"schedulingMode: SchedulingMode"},{"location":"scheduler/TaskScheduler/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#setdagschedulerdagscheduler-dagscheduler-unit","text":"Associates a scheduler:DAGScheduler.md[DAGScheduler] Used when DAGScheduler is scheduler:DAGScheduler.md#creating-instance[created] | start a| [[start]]","title":"setDAGScheduler(dagScheduler: DAGScheduler): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#start-unit","text":"Starts the TaskScheduler Used when SparkContext is created | stop a| [[stop]]","title":"start(): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#stop-unit","text":"Stops the TaskScheduler Used when DAGScheduler is requested to scheduler:DAGScheduler.md#stop[stop] |=== == [[implementations]] TaskSchedulers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | TaskScheduler | Description | scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] | [[TaskSchedulerImpl]] Default Spark scheduler | spark-on-yarn:spark-yarn-yarnscheduler.md[YarnScheduler] | [[YarnScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[client] deploy mode in spark-on-yarn:index.md[Spark on YARN] | spark-on-yarn:spark-yarn-yarnclusterscheduler.md[YarnClusterScheduler] | [[YarnClusterScheduler]] TaskScheduler for tools:spark-submit.md#deploy-mode[cluster] deploy mode in spark-on-yarn:index.md[Spark on YARN] |=== == [[lifecycle]] Lifecycle A TaskScheduler is created while ROOT:SparkContext.md#creating-instance[SparkContext is being created] (by calling ROOT:SparkContext.md#createTaskScheduler[SparkContext.createTaskScheduler] for a given ROOT:spark-deployment-environments.md[master URL] and tools:spark-submit.md#deploy-mode[deploy mode]). .TaskScheduler uses SchedulerBackend to support different clusters image::taskscheduler-uses-schedulerbackend.png[align=\"center\"] At this point in SparkContext's lifecycle, the internal _taskScheduler points at the TaskScheduler (and it is \"announced\" by sending a blocking ROOT:spark-HeartbeatReceiver.md#TaskSchedulerIsSet[ TaskSchedulerIsSet message to HeartbeatReceiver RPC endpoint]). The < > right after the blocking TaskSchedulerIsSet message receives a response. The < > and the < > are set at this point (and SparkContext uses the application id to set ROOT:SparkConf.md#spark.app.id[spark.app.id] Spark property, and configure webui:spark-webui-SparkUI.md[SparkUI], and storage:BlockManager.md[BlockManager]). CAUTION: FIXME The application id is described as \"associated with the job.\" in TaskScheduler, but I think it is \"associated with the application\" and you can have many jobs per application. Right before SparkContext is fully initialized, < > is called. The internal _taskScheduler is cleared (i.e. set to null ) while ROOT:SparkContext.md#stop[SparkContext is being stopped]. < > while scheduler:DAGScheduler.md#stop[DAGScheduler is being stopped]. WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now? == [[postStartHook]] Post-Start Initialization","title":"stop(): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#poststarthook-unit","text":"postStartHook does nothing by default, but allows < > for some additional post-start initialization. postStartHook is used when: SparkContext is created Spark on YARN's YarnClusterScheduler is requested to spark-on-yarn:spark-yarn-yarnclusterscheduler.md#postStartHook[postStartHook] == [[applicationId]][[appId]] Unique Identifier of Spark Application","title":"postStartHook(): Unit"},{"location":"scheduler/TaskScheduler/#source-scala_15","text":"","title":"[source, scala]"},{"location":"scheduler/TaskScheduler/#applicationid-string","text":"applicationId is the unique identifier of the Spark application and defaults to spark-application-[currentTimeMillis] . applicationId is used when SparkContext is created.","title":"applicationId(): String"},{"location":"scheduler/TaskSchedulerImpl/","text":"TaskSchedulerImpl \u00b6 TaskSchedulerImpl is the default TaskScheduler that uses a SchedulerBackend to schedule tasks (for execution on a cluster manager). When a Spark application starts (and so an instance of ROOT:SparkContext.md#creating-instance[SparkContext is created]) TaskSchedulerImpl with a scheduler:SchedulerBackend.md[SchedulerBackend] and scheduler:DAGScheduler.md[DAGScheduler] are created and soon started. TaskSchedulerImpl < >. TaskSchedulerImpl can < > (that however is spark-on-yarn:spark-yarn-yarnscheduler.md[only used with Hadoop YARN cluster manager]). Using ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property you can select the scheduler:spark-scheduler-SchedulingMode.md[scheduling policy]. TaskSchedulerImpl < > using scheduler:spark-scheduler-SchedulableBuilder.md[SchedulableBuilders]. [[CPUS_PER_TASK]] TaskSchedulerImpl uses ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] configuration property for...FIXME Creating Instance \u00b6 TaskSchedulerImpl takes the following to be created: [[sc]] ROOT:SparkContext.md[] < > [[isLocal]] isLocal flag for local and cluster run modes (default: false ) TaskSchedulerImpl initializes the < >. TaskSchedulerImpl sets scheduler:TaskScheduler.md#schedulingMode[schedulingMode] to the value of ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property. NOTE: schedulingMode is part of scheduler:TaskScheduler.md#schedulingMode[TaskScheduler] contract. Failure to set schedulingMode results in a SparkException : Unrecognized spark.scheduler.mode: [schedulingModeConf] Ultimately, TaskSchedulerImpl creates a scheduler:TaskResultGetter.md[TaskResultGetter]. == [[backend]] SchedulerBackend TaskSchedulerImpl is assigned a scheduler:SchedulerBackend.md[SchedulerBackend] when requested to < >. The lifecycle of the SchedulerBackend is tightly coupled to the lifecycle of the owning TaskSchedulerImpl: When < > so is the scheduler:SchedulerBackend.md#start[SchedulerBackend] When < >, so is the scheduler:SchedulerBackend.md#stop[SchedulerBackend] TaskSchedulerImpl < > before requesting it for the following: scheduler:SchedulerBackend.md#reviveOffers[Reviving resource offers] when requested to < >, < >, < >, < >, and < > scheduler:SchedulerBackend.md#killTask[Killing tasks] when requested to < > and < > scheduler:SchedulerBackend.md#defaultParallelism[Default parallelism], < > and < > when requested for the < >, scheduler:SchedulerBackend.md#applicationId[applicationId] and scheduler:SchedulerBackend.md#applicationAttemptId[applicationAttemptId], respectively == [[applicationId]] Unique Identifier of Spark Application [source, scala] \u00b6 applicationId(): String \u00b6 NOTE: applicationId is part of scheduler:TaskScheduler.md#applicationId[TaskScheduler] contract. applicationId simply request the < > for the scheduler:SchedulerBackend.md#applicationId[applicationId]. == [[nodeBlacklist]] nodeBlacklist Method CAUTION: FIXME == [[cleanupTaskState]] cleanupTaskState Method CAUTION: FIXME == [[newTaskId]] newTaskId Method CAUTION: FIXME == [[getExecutorsAliveOnHost]] getExecutorsAliveOnHost Method CAUTION: FIXME == [[isExecutorAlive]] isExecutorAlive Method CAUTION: FIXME == [[hasExecutorsAliveOnHost]] hasExecutorsAliveOnHost Method CAUTION: FIXME == [[hasHostAliveOnRack]] hasHostAliveOnRack Method CAUTION: FIXME == [[executorLost]] executorLost Method CAUTION: FIXME == [[mapOutputTracker]] mapOutputTracker CAUTION: FIXME == [[starvationTimer]] starvationTimer CAUTION: FIXME == [[executorHeartbeatReceived]] executorHeartbeatReceived Method [source, scala] \u00b6 executorHeartbeatReceived( execId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId): Boolean executorHeartbeatReceived is...FIXME executorHeartbeatReceived is part of the scheduler:TaskScheduler.md#executorHeartbeatReceived[TaskScheduler] contract. == [[cancelTasks]] Cancelling All Tasks of Stage -- cancelTasks Method [source, scala] \u00b6 cancelTasks(stageId: Int, interruptThread: Boolean): Unit \u00b6 NOTE: cancelTasks is part of scheduler:TaskScheduler.md#contract[TaskScheduler contract]. cancelTasks cancels all tasks submitted for execution in a stage stageId . NOTE: cancelTasks is used exclusively when DAGScheduler scheduler:DAGScheduler.md#failJobAndIndependentStages[cancels a stage]. == [[handleSuccessfulTask]] handleSuccessfulTask Method [source, scala] \u00b6 handleSuccessfulTask( taskSetManager: TaskSetManager, tid: Long, taskResult: DirectTaskResult[_]): Unit handleSuccessfulTask simply scheduler:TaskSetManager.md#handleSuccessfulTask[forwards the call to the input taskSetManager ] (passing tid and taskResult ). NOTE: handleSuccessfulTask is called when scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskSchedulerGetter has managed to deserialize the task result of a task that finished successfully]. == [[handleTaskGettingResult]] handleTaskGettingResult Method [source, scala] \u00b6 handleTaskGettingResult(taskSetManager: TaskSetManager, tid: Long): Unit \u00b6 handleTaskGettingResult simply scheduler:TaskSetManager.md#handleTaskGettingResult[forwards the call to the taskSetManager ]. NOTE: handleTaskGettingResult is used to inform that scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskResultGetter enqueues a successful task with IndirectTaskResult task result (and so is about to fetch a remote block from a BlockManager )]. == [[applicationAttemptId]] applicationAttemptId Method [source, scala] \u00b6 applicationAttemptId(): Option[String] \u00b6 CAUTION: FIXME == [[getRackForHost]] Tracking Racks per Hosts and Ports -- getRackForHost Method [source, scala] \u00b6 getRackForHost(value: String): Option[String] \u00b6 getRackForHost is a method to know about the racks per hosts and ports. By default, it assumes that racks are unknown (i.e. the method returns None ). NOTE: It is overriden by the YARN-specific TaskScheduler spark-on-yarn:spark-yarn-yarnscheduler.md[YarnScheduler]. getRackForHost is currently used in two places: < > to track hosts per rack (using the < hostsByRack registry>>) while processing resource offers. < > to...FIXME scheduler:TaskSetManager.md#addPendingTask[TaskSetManager.addPendingTask], scheduler:TaskSetManager.md#[TaskSetManager.dequeueTask], and scheduler:TaskSetManager.md#dequeueSpeculativeTask[TaskSetManager.dequeueSpeculativeTask] == [[initialize]] Initializing -- initialize Method [source, scala] \u00b6 initialize( backend: SchedulerBackend): Unit initialize initializes TaskSchedulerImpl. .TaskSchedulerImpl initialization image::TaskSchedulerImpl-initialize.png[align=\"center\"] initialize saves the input < >. initialize then sets < Pool >> as an empty-named spark-scheduler-Pool.md[Pool] (passing in < >, initMinShare and initWeight as 0 ). NOTE: < > is defined when < >. NOTE: < > and < > are a part of scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. initialize sets < > (based on < >): spark-scheduler-FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] for FIFO scheduling mode spark-scheduler-FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling mode initialize spark-scheduler-SchedulableBuilder.md#buildPools[requests SchedulableBuilder to build pools]. CAUTION: FIXME Why are rootPool and schedulableBuilder created only now? What do they need that it is not available when TaskSchedulerImpl is created? NOTE: initialize is called while ROOT:SparkContext.md#createTaskScheduler[SparkContext is created and creates SchedulerBackend and TaskScheduler ]. == [[start]] Starting TaskSchedulerImpl [source, scala] \u00b6 start(): Unit \u00b6 start starts the scheduler:SchedulerBackend.md[scheduler backend]. .Starting TaskSchedulerImpl in Spark Standalone image::taskschedulerimpl-start-standalone.png[align=\"center\"] start also starts < task-scheduler-speculation executor service>>. == [[statusUpdate]] Handling Task Status Update -- statusUpdate Method [source, scala] \u00b6 statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer): Unit \u00b6 statusUpdate finds scheduler:TaskSetManager.md[TaskSetManager] for the input tid task (in < >). When state is LOST , statusUpdate ...FIXME NOTE: TaskState.LOST is only used by the deprecated Mesos fine-grained scheduling mode. When state is one of the scheduler:Task.md#states[finished states], i.e. FINISHED , FAILED , KILLED or LOST , statusUpdate < > for the input tid . statusUpdate scheduler:TaskSetManager.md#removeRunningTask[requests TaskSetManager to unregister tid from running tasks]. statusUpdate requests < > to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[schedule an asynchrounous task to deserialize the task result (and notify TaskSchedulerImpl back)] for tid in FINISHED state and scheduler:TaskResultGetter.md#enqueueFailedTask[schedule an asynchrounous task to deserialize TaskFailedReason (and notify TaskSchedulerImpl back)] for tid in the other finished states (i.e. FAILED , KILLED , LOST ). If a task is in LOST state, statusUpdate scheduler:DAGScheduler.md#executorLost[notifies DAGScheduler that the executor was lost] (with SlaveLost and the reason Task [tid] was lost, so marking the executor as lost as well. ) and scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. In case the TaskSetManager for tid could not be found (in < > registry), you should see the following ERROR message in the logs: ERROR Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates) Any exception is caught and reported as ERROR message in the logs: ERROR Exception in statusUpdate CAUTION: FIXME image with scheduler backends calling TaskSchedulerImpl.statusUpdate . [NOTE] \u00b6 statusUpdate is used when: DriverEndpoint (of scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend]) is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StatusUpdate[handle a StatusUpdate message] LocalEndpoint is requested to spark-local:spark-LocalEndpoint.md#StatusUpdate[handle a StatusUpdate message] * MesosFineGrainedSchedulerBackend is requested to handle a task status update \u00b6 == [[speculationScheduler]][[task-scheduler-speculation]] task-scheduler-speculation Scheduled Executor Service -- speculationScheduler Internal Attribute speculationScheduler is a http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService ] with the name task-scheduler-speculation for ROOT:speculative-execution-of-tasks.md[]. When < > (in non-local run mode) with ROOT:configuration-properties.md#spark.speculation[spark.speculation] enabled, speculationScheduler is used to schedule < > to execute periodically every ROOT:configuration-properties.md#spark.speculation.interval[spark.speculation.interval] after the initial spark.speculation.interval passes. speculationScheduler is shut down when < >. == [[checkSpeculatableTasks]] Checking for Speculatable Tasks [source, scala] \u00b6 checkSpeculatableTasks(): Unit \u00b6 checkSpeculatableTasks requests rootPool to check for speculatable tasks (if they ran for more than 100 ms) and, if there any, requests scheduler:SchedulerBackend.md#reviveOffers[SchedulerBackend to revive offers]. NOTE: checkSpeculatableTasks is executed periodically as part of ROOT:speculative-execution-of-tasks.md[]. == [[maxTaskFailures]] Acceptable Number of Task Failures TaskSchedulerImpl can be given the acceptable number of task failures when created or defaults to ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property. The number of task failures is used when < > through scheduler:TaskSetManager.md[TaskSetManager]. == [[removeExecutor]] Cleaning up After Removing Executor -- removeExecutor Internal Method [source, scala] \u00b6 removeExecutor(executorId: String, reason: ExecutorLossReason): Unit \u00b6 removeExecutor removes the executorId executor from the following < >: < >, executorIdToHost , executorsByHost , and hostsByRack . If the affected hosts and racks are the last entries in executorsByHost and hostsByRack , appropriately, they are removed from the registries. Unless reason is LossReasonPending , the executor is removed from executorIdToHost registry and spark-scheduler-Schedulable.md#executorLost[TaskSetManagers get notified]. NOTE: The internal removeExecutor is called as part of < > and scheduler:TaskScheduler.md#executorLost[executorLost]. == [[postStartHook]] Handling Nearly-Completed SparkContext Initialization -- postStartHook Callback [source, scala] \u00b6 postStartHook(): Unit \u00b6 NOTE: postStartHook is part of the scheduler:TaskScheduler.md#postStartHook[TaskScheduler Contract] to notify a scheduler:TaskScheduler.md[task scheduler] that the SparkContext (and hence the Spark application itself) is about to finish initialization. postStartHook simply < >. == [[stop]] Stopping TaskSchedulerImpl -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop() stops all the internal services, i.e. < task-scheduler-speculation executor service>>, scheduler:SchedulerBackend.md[SchedulerBackend], scheduler:TaskResultGetter.md[TaskResultGetter], and < > timer. == [[defaultParallelism]] Finding Default Level of Parallelism -- defaultParallelism Method [source, scala] \u00b6 defaultParallelism(): Int \u00b6 NOTE: defaultParallelism is part of scheduler:TaskScheduler.md#defaultParallelism[TaskScheduler contract] as a hint for sizing jobs. defaultParallelism simply requests < > for the scheduler:SchedulerBackend.md#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism is a hint for sizing jobs that SparkContext ROOT:SparkContext.md#defaultParallelism[uses to create RDDs with the right number of partitions when not specified explicitly]. == [[submitTasks]] Submitting Tasks (of TaskSet) for Execution -- submitTasks Method [source, scala] \u00b6 submitTasks(taskSet: TaskSet): Unit \u00b6 NOTE: submitTasks is part of the scheduler:TaskScheduler.md#submitTasks[TaskScheduler Contract] to submit the tasks (of the given scheduler:TaskSet.md[TaskSet]) for execution. In essence, submitTasks registers a new scheduler:TaskSetManager.md[TaskSetManager] (for the given scheduler:TaskSet.md[TaskSet]) and requests the < > to scheduler:SchedulerBackend.md#reviveOffers[handle resource allocation offers (from the scheduling system)]. .TaskSchedulerImpl.submitTasks image::taskschedulerImpl-submitTasks.png[align=\"center\"] Internally, submitTasks first prints out the following INFO message to the logs: Adding task set [id] with [length] tasks submitTasks then < > (for the given scheduler:TaskSet.md[TaskSet] and the < >). submitTasks registers ( adds ) the TaskSetManager per scheduler:TaskSet.md#stageId[stage] and scheduler:TaskSet.md#stageAttemptId[stage attempt] IDs (of the scheduler:TaskSet.md[TaskSet]) in the < > internal registry. NOTE: < > internal registry tracks the scheduler:TaskSetManager.md[TaskSetManagers] (that represent scheduler:TaskSet.md[TaskSets]) per stage and stage attempts. In other words, there could be many TaskSetManagers for a single stage, each representing a unique stage attempt. NOTE: Not only could a task be retried (cf. < >), but also a single stage. submitTasks makes sure that there is exactly one active TaskSetManager (with different TaskSet ) across all the managers (for the stage). Otherwise, submitTasks throws an IllegalStateException : more than one active taskSet for stage [stage]: [TaskSet ids] NOTE: TaskSetManager is considered active when it is not a zombie . submitTasks requests the < > to spark-scheduler-SchedulableBuilder.md#addTaskSetManager[add the TaskSetManager to the schedulable pool]. NOTE: The scheduler:TaskScheduler.md#rootPool[schedulable pool] can be a single flat linked queue (in spark-scheduler-FIFOSchedulableBuilder.md[FIFO scheduling mode]) or a hierarchy of pools of Schedulables (in spark-scheduler-FairSchedulableBuilder.md[FAIR scheduling mode]). submitTasks < > to make sure that the requested resources (i.e. CPU and memory) are assigned to the Spark application for a < > (the very first time the Spark application is started per < > flag). NOTE: The very first time (< > flag is false ) in cluster mode only (i.e. isLocal of the TaskSchedulerImpl is false ), starvationTimer is scheduled to execute after ROOT:configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] to ensure that the requested resources, i.e. CPUs and memory, were assigned by a cluster manager. NOTE: After the first ROOT:configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] passes, the < > internal flag is true . In the end, submitTasks requests the < > to scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. TIP: Use dag-scheduler-event-loop thread to step through the code in a debugger. === [[submitTasks-starvationTimer]] Scheduling Starvation Task Every time the starvation timer thread is executed and hasLaunchedTask flag is false , the following WARN message is printed out to the logs: WARN Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Otherwise, when the hasLaunchedTask flag is true the timer thread cancels itself. == [[createTaskSetManager]] Creating TaskSetManager -- createTaskSetManager Method [source, scala] \u00b6 createTaskSetManager(taskSet: TaskSet, maxTaskFailures: Int): TaskSetManager \u00b6 createTaskSetManager scheduler:TaskSetManager.md#creating-instance[creates a TaskSetManager ] (passing on the reference to TaskSchedulerImpl, the input taskSet and maxTaskFailures , and optional BlacklistTracker ). NOTE: createTaskSetManager uses the optional < > that is specified when < >. NOTE: createTaskSetManager is used exclusively when < TaskSet )>>. == [[handleFailedTask]] Notifying TaskSetManager that Task Failed -- handleFailedTask Method [source, scala] \u00b6 handleFailedTask( taskSetManager: TaskSetManager, tid: Long, taskState: TaskState, reason: TaskFailedReason): Unit handleFailedTask scheduler:TaskSetManager.md#handleFailedTask[notifies taskSetManager that tid task has failed] and, only when scheduler:TaskSetManager.md#zombie-state[ taskSetManager is not in zombie state] and tid is not in KILLED state, scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. NOTE: handleFailedTask is called when scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskResultGetter deserializes a TaskFailedReason ] for a failed task. == [[taskSetFinished]] taskSetFinished Method [source, scala] \u00b6 taskSetFinished(manager: TaskSetManager): Unit \u00b6 taskSetFinished looks all scheduler:TaskSet.md[TaskSet]s up by the stage id (in < > registry) and removes the stage attempt from them, possibly with removing the entire stage record from taskSetsByStageIdAndAttempt registry completely (if there are no other attempts registered). .TaskSchedulerImpl.taskSetFinished is called when all tasks are finished image::taskschedulerimpl-tasksetmanager-tasksetfinished.png[align=\"center\"] NOTE: A TaskSetManager manages a TaskSet for a stage. taskSetFinished then spark-scheduler-Pool.md#removeSchedulable[removes manager from the parent's schedulable pool]. You should see the following INFO message in the logs: Removed TaskSet [id], whose tasks have all completed, from pool [name] NOTE: taskSetFinished method is called when scheduler:TaskSetManager.md#maybeFinishTaskSet[ TaskSetManager has received the results of all the tasks in a TaskSet ]. == [[executorAdded]] Notifying DAGScheduler About New Executor -- executorAdded Method [source, scala] \u00b6 executorAdded(execId: String, host: String) \u00b6 executorAdded just scheduler:DAGScheduler.md#executorAdded[notifies DAGScheduler that an executor was added]. CAUTION: FIXME Image with a call from TaskSchedulerImpl to DAGScheduler, please. NOTE: executorAdded uses < > that was given when < >. == [[waitBackendReady]] Waiting Until SchedulerBackend is Ready -- waitBackendReady Internal Method [source, scala] \u00b6 waitBackendReady(): Unit \u00b6 waitBackendReady waits until the < > is scheduler:SchedulerBackend.md#isReady[ready]. If it is, waitBackendReady returns immediately. Otherwise, waitBackendReady keeps checking every 100 milliseconds (hardcoded) or the < > is ROOT:SparkContext.md#stopped[stopped]. NOTE: A SchedulerBackend is scheduler:SchedulerBackend.md#isReady[ready] by default. If the SparkContext happens to be stopped while waiting, waitBackendReady throws an IllegalStateException : Spark context stopped while waiting for backend NOTE: waitBackendReady is used exclusively when TaskSchedulerImpl is requested to < >. == [[resourceOffers]] Creating TaskDescriptions For Available Executor Resource Offers [source, scala] \u00b6 resourceOffers( offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] resourceOffers takes the resources offers (as < >) and generates a collection of tasks (as spark-scheduler-TaskDescription.md[TaskDescription]) to launch (given the resources available). NOTE: < > represents a resource offer with CPU cores free to use on an executor. .Processing Executor Resource Offers image::taskscheduler-resourceOffers.png[align=\"center\"] Internally, resourceOffers first updates < > and < > lookup tables to record new hosts and executors (given the input offers ). For new executors (not in < >) resourceOffers < DAGScheduler that an executor was added>>. NOTE: TaskSchedulerImpl uses resourceOffers to track active executors. CAUTION: FIXME a picture with executorAdded call from TaskSchedulerImpl to DAGScheduler. resourceOffers requests BlacklistTracker to applyBlacklistTimeout and filters out offers on blacklisted nodes and executors. NOTE: resourceOffers uses the optional < > that was given when < >. CAUTION: FIXME Expand on blacklisting resourceOffers then randomly shuffles offers (to evenly distribute tasks across executors and avoid over-utilizing some executors) and initializes the local data structures tasks and availableCpus (as shown in the figure below). .Internal Structures of resourceOffers with 5 WorkerOffers (with 4, 2, 0, 3, 2 free cores) image::TaskSchedulerImpl-resourceOffers-internal-structures.png[align=\"center\"] resourceOffers spark-scheduler-Pool.md#getSortedTaskSetQueue[takes TaskSets in scheduling order] from scheduler:TaskScheduler.md#rootPool[top-level Schedulable Pool]. .TaskSchedulerImpl Requesting TaskSets (as TaskSetManagers) from Root Pool image::TaskSchedulerImpl-resourceOffers-rootPool-getSortedTaskSetQueue.png[align=\"center\"] [NOTE] \u00b6 rootPool is configured when < >. rootPool is part of the scheduler:TaskScheduler.md#rootPool[TaskScheduler Contract] and exclusively managed by scheduler:spark-scheduler-SchedulableBuilder.md[SchedulableBuilders], i.e. scheduler:spark-scheduler-FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] and scheduler:spark-scheduler-FairSchedulableBuilder.md[FairSchedulableBuilder] (that scheduler:spark-scheduler-SchedulableBuilder.md#addTaskSetManager[manage registering TaskSetManagers with the root pool]). scheduler:TaskSetManager.md[TaskSetManager] manages execution of the tasks in a single scheduler:TaskSet.md[TaskSet] that represents a single scheduler:Stage.md[Stage]. \u00b6 For every TaskSetManager (in scheduling order), you should see the following DEBUG message in the logs: parentName: [name], name: [name], runningTasks: [count] Only if a new executor was added, resourceOffers scheduler:TaskSetManager.md#executorAdded[notifies every TaskSetManager about the change] (to recompute locality preferences). resourceOffers then takes every TaskSetManager (in scheduling order) and offers them each node in increasing order of locality levels (per scheduler:TaskSetManager.md#computeValidLocalityLevels[TaskSetManager's valid locality levels]). NOTE: A TaskSetManager scheduler:TaskSetManager.md##computeValidLocalityLevels[computes locality levels of the tasks] it manages. For every TaskSetManager and the TaskSetManager 's valid locality level, resourceOffers tries to < > as long as the TaskSetManager manages to launch a task (given the locality level). If resourceOffers did not manage to offer resources to a TaskSetManager so it could launch any task, resourceOffers scheduler:TaskSetManager.md#abortIfCompletelyBlacklisted[requests the TaskSetManager to abort the TaskSet if completely blacklisted]. When resourceOffers managed to launch a task, the internal < > flag gets enabled (that effectively means what the name says \"there were executors and I managed to launch a task\" ). [NOTE] \u00b6 resourceOffers is used when: scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#makeOffers[ CoarseGrainedSchedulerBackend (via RPC endpoint) makes executor resource offers] spark-local:spark-LocalEndpoint.md#reviveOffers[ LocalEndpoint revives resource offers] * Spark on Mesos' MesosFineGrainedSchedulerBackend does resourceOffers \u00b6 == [[resourceOfferSingleTaskSet]] Finding Tasks from TaskSetManager to Schedule on Executors -- resourceOfferSingleTaskSet Internal Method [source, scala] \u00b6 resourceOfferSingleTaskSet( taskSet: TaskSetManager, maxLocality: TaskLocality, shuffledOffers: Seq[WorkerOffer], availableCpus: Array[Int], tasks: Seq[ArrayBuffer[TaskDescription]]): Boolean resourceOfferSingleTaskSet takes every WorkerOffer (from the input shuffledOffers ) and (only if the number of available CPU cores (using the input availableCpus ) is at least ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus]) scheduler:TaskSetManager.md#resourceOffer[requests TaskSetManager (as the input taskSet ) to find a Task to execute (given the resource offer)] (as an executor, a host, and the input maxLocality ). resourceOfferSingleTaskSet adds the task to the input tasks collection. resourceOfferSingleTaskSet records the task id and TaskSetManager in the following registries: < > < > < > resourceOfferSingleTaskSet decreases ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] from the input availableCpus (for the WorkerOffer ). NOTE: resourceOfferSingleTaskSet makes sure that the number of available CPU cores (in the input availableCpus per WorkerOffer ) is at least 0 . If there is a TaskNotSerializableException , you should see the following ERROR in the logs: ERROR Resource offer failed, task set [name] was not serializable resourceOfferSingleTaskSet returns whether a task was launched or not. NOTE: resourceOfferSingleTaskSet is used when TaskSchedulerImpl < TaskDescriptions for available executor resource offers (with CPU cores)>>. == [[TaskLocality]] TaskLocality -- Task Locality Preference TaskLocality represents a task locality preference and can be one of the following (from most localized to the widest): . PROCESS_LOCAL . NODE_LOCAL . NO_PREF . RACK_LOCAL . ANY == [[WorkerOffer]] WorkerOffer -- Free CPU Cores on Executor [source, scala] \u00b6 WorkerOffer(executorId: String, host: String, cores: Int) \u00b6 WorkerOffer represents a resource offer with free CPU cores available on an executorId executor on a host . == [[workerRemoved]] workerRemoved Method [source, scala] \u00b6 workerRemoved( workerId: String, host: String, message: String): Unit workerRemoved prints out the following INFO message to the logs: Handle removed worker [workerId]: [message] workerRemoved then requests the < > to scheduler:DAGScheduler.md#workerRemoved[handle it]. workerRemoved is part of the scheduler:TaskScheduler.md#workerRemoved[TaskScheduler] abstraction. == [[maybeInitBarrierCoordinator]] maybeInitBarrierCoordinator Method [source,scala] \u00b6 maybeInitBarrierCoordinator(): Unit \u00b6 maybeInitBarrierCoordinator...FIXME maybeInitBarrierCoordinator is used when TaskSchedulerImpl is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.TaskSchedulerImpl logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.scheduler.TaskSchedulerImpl=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | dagScheduler a| [[dagScheduler]] scheduler:DAGScheduler.md[DAGScheduler] Used when...FIXME | executorIdToHost a| [[executorIdToHost]] Lookup table of hosts per executor. Used when...FIXME | executorIdToRunningTaskIds a| [[executorIdToRunningTaskIds]] Lookup table of running tasks per executor. Used when...FIXME | executorIdToTaskCount a| [[executorIdToTaskCount]] Lookup table of the number of running tasks by executor:Executor.md[]. | executorsByHost a| [[executorsByHost]] Collection of executor:Executor.md[executors] per host | hasLaunchedTask a| [[hasLaunchedTask]] Flag...FIXME Used when...FIXME | hostToExecutors a| [[hostToExecutors]] Lookup table of executors per hosts in a cluster. Used when...FIXME | hostsByRack a| [[hostsByRack]] Lookup table of hosts per rack. Used when...FIXME | nextTaskId a| [[nextTaskId]] The next scheduler:Task.md[task] id counting from 0 . Used when TaskSchedulerImpl... | rootPool a| [[rootPool]] spark-scheduler-Pool.md[Schedulable pool] Used when TaskSchedulerImpl... | schedulableBuilder a| [[schedulableBuilder]] < > Created when TaskSchedulerImpl is requested to < > and can be one of two available builders: spark-scheduler-FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] when scheduling policy is FIFO (which is the default scheduling policy). spark-scheduler-FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling policy. NOTE: Use ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property to select the scheduling policy. | schedulingMode a| [[schedulingMode]] spark-scheduler-SchedulingMode.md[SchedulingMode] Used when TaskSchedulerImpl... | taskSetsByStageIdAndAttempt a| [[taskSetsByStageIdAndAttempt]] Lookup table of scheduler:TaskSet.md[TaskSet] by stage and attempt ids. | taskIdToExecutorId a| [[taskIdToExecutorId]] Lookup table of executor:Executor.md[] by task id. | taskIdToTaskSetManager a| [[taskIdToTaskSetManager]] Registry of active scheduler:TaskSetManager.md[TaskSetManagers] per task id. |===","title":"TaskSchedulerImpl"},{"location":"scheduler/TaskSchedulerImpl/#taskschedulerimpl","text":"TaskSchedulerImpl is the default TaskScheduler that uses a SchedulerBackend to schedule tasks (for execution on a cluster manager). When a Spark application starts (and so an instance of ROOT:SparkContext.md#creating-instance[SparkContext is created]) TaskSchedulerImpl with a scheduler:SchedulerBackend.md[SchedulerBackend] and scheduler:DAGScheduler.md[DAGScheduler] are created and soon started. TaskSchedulerImpl < >. TaskSchedulerImpl can < > (that however is spark-on-yarn:spark-yarn-yarnscheduler.md[only used with Hadoop YARN cluster manager]). Using ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property you can select the scheduler:spark-scheduler-SchedulingMode.md[scheduling policy]. TaskSchedulerImpl < > using scheduler:spark-scheduler-SchedulableBuilder.md[SchedulableBuilders]. [[CPUS_PER_TASK]] TaskSchedulerImpl uses ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] configuration property for...FIXME","title":"TaskSchedulerImpl"},{"location":"scheduler/TaskSchedulerImpl/#creating-instance","text":"TaskSchedulerImpl takes the following to be created: [[sc]] ROOT:SparkContext.md[] < > [[isLocal]] isLocal flag for local and cluster run modes (default: false ) TaskSchedulerImpl initializes the < >. TaskSchedulerImpl sets scheduler:TaskScheduler.md#schedulingMode[schedulingMode] to the value of ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property. NOTE: schedulingMode is part of scheduler:TaskScheduler.md#schedulingMode[TaskScheduler] contract. Failure to set schedulingMode results in a SparkException : Unrecognized spark.scheduler.mode: [schedulingModeConf] Ultimately, TaskSchedulerImpl creates a scheduler:TaskResultGetter.md[TaskResultGetter]. == [[backend]] SchedulerBackend TaskSchedulerImpl is assigned a scheduler:SchedulerBackend.md[SchedulerBackend] when requested to < >. The lifecycle of the SchedulerBackend is tightly coupled to the lifecycle of the owning TaskSchedulerImpl: When < > so is the scheduler:SchedulerBackend.md#start[SchedulerBackend] When < >, so is the scheduler:SchedulerBackend.md#stop[SchedulerBackend] TaskSchedulerImpl < > before requesting it for the following: scheduler:SchedulerBackend.md#reviveOffers[Reviving resource offers] when requested to < >, < >, < >, < >, and < > scheduler:SchedulerBackend.md#killTask[Killing tasks] when requested to < > and < > scheduler:SchedulerBackend.md#defaultParallelism[Default parallelism], < > and < > when requested for the < >, scheduler:SchedulerBackend.md#applicationId[applicationId] and scheduler:SchedulerBackend.md#applicationAttemptId[applicationAttemptId], respectively == [[applicationId]] Unique Identifier of Spark Application","title":"Creating Instance"},{"location":"scheduler/TaskSchedulerImpl/#source-scala","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#applicationid-string","text":"NOTE: applicationId is part of scheduler:TaskScheduler.md#applicationId[TaskScheduler] contract. applicationId simply request the < > for the scheduler:SchedulerBackend.md#applicationId[applicationId]. == [[nodeBlacklist]] nodeBlacklist Method CAUTION: FIXME == [[cleanupTaskState]] cleanupTaskState Method CAUTION: FIXME == [[newTaskId]] newTaskId Method CAUTION: FIXME == [[getExecutorsAliveOnHost]] getExecutorsAliveOnHost Method CAUTION: FIXME == [[isExecutorAlive]] isExecutorAlive Method CAUTION: FIXME == [[hasExecutorsAliveOnHost]] hasExecutorsAliveOnHost Method CAUTION: FIXME == [[hasHostAliveOnRack]] hasHostAliveOnRack Method CAUTION: FIXME == [[executorLost]] executorLost Method CAUTION: FIXME == [[mapOutputTracker]] mapOutputTracker CAUTION: FIXME == [[starvationTimer]] starvationTimer CAUTION: FIXME == [[executorHeartbeatReceived]] executorHeartbeatReceived Method","title":"applicationId(): String"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_1","text":"executorHeartbeatReceived( execId: String, accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])], blockManagerId: BlockManagerId): Boolean executorHeartbeatReceived is...FIXME executorHeartbeatReceived is part of the scheduler:TaskScheduler.md#executorHeartbeatReceived[TaskScheduler] contract. == [[cancelTasks]] Cancelling All Tasks of Stage -- cancelTasks Method","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_2","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#canceltasksstageid-int-interruptthread-boolean-unit","text":"NOTE: cancelTasks is part of scheduler:TaskScheduler.md#contract[TaskScheduler contract]. cancelTasks cancels all tasks submitted for execution in a stage stageId . NOTE: cancelTasks is used exclusively when DAGScheduler scheduler:DAGScheduler.md#failJobAndIndependentStages[cancels a stage]. == [[handleSuccessfulTask]] handleSuccessfulTask Method","title":"cancelTasks(stageId: Int, interruptThread: Boolean): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_3","text":"handleSuccessfulTask( taskSetManager: TaskSetManager, tid: Long, taskResult: DirectTaskResult[_]): Unit handleSuccessfulTask simply scheduler:TaskSetManager.md#handleSuccessfulTask[forwards the call to the input taskSetManager ] (passing tid and taskResult ). NOTE: handleSuccessfulTask is called when scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskSchedulerGetter has managed to deserialize the task result of a task that finished successfully]. == [[handleTaskGettingResult]] handleTaskGettingResult Method","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_4","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#handletaskgettingresulttasksetmanager-tasksetmanager-tid-long-unit","text":"handleTaskGettingResult simply scheduler:TaskSetManager.md#handleTaskGettingResult[forwards the call to the taskSetManager ]. NOTE: handleTaskGettingResult is used to inform that scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskResultGetter enqueues a successful task with IndirectTaskResult task result (and so is about to fetch a remote block from a BlockManager )]. == [[applicationAttemptId]] applicationAttemptId Method","title":"handleTaskGettingResult(taskSetManager: TaskSetManager, tid: Long): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#applicationattemptid-optionstring","text":"CAUTION: FIXME == [[getRackForHost]] Tracking Racks per Hosts and Ports -- getRackForHost Method","title":"applicationAttemptId(): Option[String]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#getrackforhostvalue-string-optionstring","text":"getRackForHost is a method to know about the racks per hosts and ports. By default, it assumes that racks are unknown (i.e. the method returns None ). NOTE: It is overriden by the YARN-specific TaskScheduler spark-on-yarn:spark-yarn-yarnscheduler.md[YarnScheduler]. getRackForHost is currently used in two places: < > to track hosts per rack (using the < hostsByRack registry>>) while processing resource offers. < > to...FIXME scheduler:TaskSetManager.md#addPendingTask[TaskSetManager.addPendingTask], scheduler:TaskSetManager.md#[TaskSetManager.dequeueTask], and scheduler:TaskSetManager.md#dequeueSpeculativeTask[TaskSetManager.dequeueSpeculativeTask] == [[initialize]] Initializing -- initialize Method","title":"getRackForHost(value: String): Option[String]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_7","text":"initialize( backend: SchedulerBackend): Unit initialize initializes TaskSchedulerImpl. .TaskSchedulerImpl initialization image::TaskSchedulerImpl-initialize.png[align=\"center\"] initialize saves the input < >. initialize then sets < Pool >> as an empty-named spark-scheduler-Pool.md[Pool] (passing in < >, initMinShare and initWeight as 0 ). NOTE: < > is defined when < >. NOTE: < > and < > are a part of scheduler:TaskScheduler.md#contract[TaskScheduler Contract]. initialize sets < > (based on < >): spark-scheduler-FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] for FIFO scheduling mode spark-scheduler-FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling mode initialize spark-scheduler-SchedulableBuilder.md#buildPools[requests SchedulableBuilder to build pools]. CAUTION: FIXME Why are rootPool and schedulableBuilder created only now? What do they need that it is not available when TaskSchedulerImpl is created? NOTE: initialize is called while ROOT:SparkContext.md#createTaskScheduler[SparkContext is created and creates SchedulerBackend and TaskScheduler ]. == [[start]] Starting TaskSchedulerImpl","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#start-unit","text":"start starts the scheduler:SchedulerBackend.md[scheduler backend]. .Starting TaskSchedulerImpl in Spark Standalone image::taskschedulerimpl-start-standalone.png[align=\"center\"] start also starts < task-scheduler-speculation executor service>>. == [[statusUpdate]] Handling Task Status Update -- statusUpdate Method","title":"start(): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#statusupdatetid-long-state-taskstate-serializeddata-bytebuffer-unit","text":"statusUpdate finds scheduler:TaskSetManager.md[TaskSetManager] for the input tid task (in < >). When state is LOST , statusUpdate ...FIXME NOTE: TaskState.LOST is only used by the deprecated Mesos fine-grained scheduling mode. When state is one of the scheduler:Task.md#states[finished states], i.e. FINISHED , FAILED , KILLED or LOST , statusUpdate < > for the input tid . statusUpdate scheduler:TaskSetManager.md#removeRunningTask[requests TaskSetManager to unregister tid from running tasks]. statusUpdate requests < > to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[schedule an asynchrounous task to deserialize the task result (and notify TaskSchedulerImpl back)] for tid in FINISHED state and scheduler:TaskResultGetter.md#enqueueFailedTask[schedule an asynchrounous task to deserialize TaskFailedReason (and notify TaskSchedulerImpl back)] for tid in the other finished states (i.e. FAILED , KILLED , LOST ). If a task is in LOST state, statusUpdate scheduler:DAGScheduler.md#executorLost[notifies DAGScheduler that the executor was lost] (with SlaveLost and the reason Task [tid] was lost, so marking the executor as lost as well. ) and scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. In case the TaskSetManager for tid could not be found (in < > registry), you should see the following ERROR message in the logs: ERROR Ignoring update with state [state] for TID [tid] because its task set is gone (this is likely the result of receiving duplicate task finished status updates) Any exception is caught and reported as ERROR message in the logs: ERROR Exception in statusUpdate CAUTION: FIXME image with scheduler backends calling TaskSchedulerImpl.statusUpdate .","title":"statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer): Unit"},{"location":"scheduler/TaskSchedulerImpl/#note","text":"statusUpdate is used when: DriverEndpoint (of scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend]) is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#StatusUpdate[handle a StatusUpdate message] LocalEndpoint is requested to spark-local:spark-LocalEndpoint.md#StatusUpdate[handle a StatusUpdate message]","title":"[NOTE]"},{"location":"scheduler/TaskSchedulerImpl/#mesosfinegrainedschedulerbackend-is-requested-to-handle-a-task-status-update","text":"== [[speculationScheduler]][[task-scheduler-speculation]] task-scheduler-speculation Scheduled Executor Service -- speculationScheduler Internal Attribute speculationScheduler is a http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService ] with the name task-scheduler-speculation for ROOT:speculative-execution-of-tasks.md[]. When < > (in non-local run mode) with ROOT:configuration-properties.md#spark.speculation[spark.speculation] enabled, speculationScheduler is used to schedule < > to execute periodically every ROOT:configuration-properties.md#spark.speculation.interval[spark.speculation.interval] after the initial spark.speculation.interval passes. speculationScheduler is shut down when < >. == [[checkSpeculatableTasks]] Checking for Speculatable Tasks","title":"* MesosFineGrainedSchedulerBackend is requested to handle a task status update"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_10","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#checkspeculatabletasks-unit","text":"checkSpeculatableTasks requests rootPool to check for speculatable tasks (if they ran for more than 100 ms) and, if there any, requests scheduler:SchedulerBackend.md#reviveOffers[SchedulerBackend to revive offers]. NOTE: checkSpeculatableTasks is executed periodically as part of ROOT:speculative-execution-of-tasks.md[]. == [[maxTaskFailures]] Acceptable Number of Task Failures TaskSchedulerImpl can be given the acceptable number of task failures when created or defaults to ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property. The number of task failures is used when < > through scheduler:TaskSetManager.md[TaskSetManager]. == [[removeExecutor]] Cleaning up After Removing Executor -- removeExecutor Internal Method","title":"checkSpeculatableTasks(): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#removeexecutorexecutorid-string-reason-executorlossreason-unit","text":"removeExecutor removes the executorId executor from the following < >: < >, executorIdToHost , executorsByHost , and hostsByRack . If the affected hosts and racks are the last entries in executorsByHost and hostsByRack , appropriately, they are removed from the registries. Unless reason is LossReasonPending , the executor is removed from executorIdToHost registry and spark-scheduler-Schedulable.md#executorLost[TaskSetManagers get notified]. NOTE: The internal removeExecutor is called as part of < > and scheduler:TaskScheduler.md#executorLost[executorLost]. == [[postStartHook]] Handling Nearly-Completed SparkContext Initialization -- postStartHook Callback","title":"removeExecutor(executorId: String, reason: ExecutorLossReason): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_12","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#poststarthook-unit","text":"NOTE: postStartHook is part of the scheduler:TaskScheduler.md#postStartHook[TaskScheduler Contract] to notify a scheduler:TaskScheduler.md[task scheduler] that the SparkContext (and hence the Spark application itself) is about to finish initialization. postStartHook simply < >. == [[stop]] Stopping TaskSchedulerImpl -- stop Method","title":"postStartHook(): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#stop-unit","text":"stop() stops all the internal services, i.e. < task-scheduler-speculation executor service>>, scheduler:SchedulerBackend.md[SchedulerBackend], scheduler:TaskResultGetter.md[TaskResultGetter], and < > timer. == [[defaultParallelism]] Finding Default Level of Parallelism -- defaultParallelism Method","title":"stop(): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#defaultparallelism-int","text":"NOTE: defaultParallelism is part of scheduler:TaskScheduler.md#defaultParallelism[TaskScheduler contract] as a hint for sizing jobs. defaultParallelism simply requests < > for the scheduler:SchedulerBackend.md#defaultParallelism[default level of parallelism]. NOTE: Default level of parallelism is a hint for sizing jobs that SparkContext ROOT:SparkContext.md#defaultParallelism[uses to create RDDs with the right number of partitions when not specified explicitly]. == [[submitTasks]] Submitting Tasks (of TaskSet) for Execution -- submitTasks Method","title":"defaultParallelism(): Int"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_15","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#submittaskstaskset-taskset-unit","text":"NOTE: submitTasks is part of the scheduler:TaskScheduler.md#submitTasks[TaskScheduler Contract] to submit the tasks (of the given scheduler:TaskSet.md[TaskSet]) for execution. In essence, submitTasks registers a new scheduler:TaskSetManager.md[TaskSetManager] (for the given scheduler:TaskSet.md[TaskSet]) and requests the < > to scheduler:SchedulerBackend.md#reviveOffers[handle resource allocation offers (from the scheduling system)]. .TaskSchedulerImpl.submitTasks image::taskschedulerImpl-submitTasks.png[align=\"center\"] Internally, submitTasks first prints out the following INFO message to the logs: Adding task set [id] with [length] tasks submitTasks then < > (for the given scheduler:TaskSet.md[TaskSet] and the < >). submitTasks registers ( adds ) the TaskSetManager per scheduler:TaskSet.md#stageId[stage] and scheduler:TaskSet.md#stageAttemptId[stage attempt] IDs (of the scheduler:TaskSet.md[TaskSet]) in the < > internal registry. NOTE: < > internal registry tracks the scheduler:TaskSetManager.md[TaskSetManagers] (that represent scheduler:TaskSet.md[TaskSets]) per stage and stage attempts. In other words, there could be many TaskSetManagers for a single stage, each representing a unique stage attempt. NOTE: Not only could a task be retried (cf. < >), but also a single stage. submitTasks makes sure that there is exactly one active TaskSetManager (with different TaskSet ) across all the managers (for the stage). Otherwise, submitTasks throws an IllegalStateException : more than one active taskSet for stage [stage]: [TaskSet ids] NOTE: TaskSetManager is considered active when it is not a zombie . submitTasks requests the < > to spark-scheduler-SchedulableBuilder.md#addTaskSetManager[add the TaskSetManager to the schedulable pool]. NOTE: The scheduler:TaskScheduler.md#rootPool[schedulable pool] can be a single flat linked queue (in spark-scheduler-FIFOSchedulableBuilder.md[FIFO scheduling mode]) or a hierarchy of pools of Schedulables (in spark-scheduler-FairSchedulableBuilder.md[FAIR scheduling mode]). submitTasks < > to make sure that the requested resources (i.e. CPU and memory) are assigned to the Spark application for a < > (the very first time the Spark application is started per < > flag). NOTE: The very first time (< > flag is false ) in cluster mode only (i.e. isLocal of the TaskSchedulerImpl is false ), starvationTimer is scheduled to execute after ROOT:configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] to ensure that the requested resources, i.e. CPUs and memory, were assigned by a cluster manager. NOTE: After the first ROOT:configuration-properties.md#spark.starvation.timeout[spark.starvation.timeout] passes, the < > internal flag is true . In the end, submitTasks requests the < > to scheduler:SchedulerBackend.md#reviveOffers[reviveOffers]. TIP: Use dag-scheduler-event-loop thread to step through the code in a debugger. === [[submitTasks-starvationTimer]] Scheduling Starvation Task Every time the starvation timer thread is executed and hasLaunchedTask flag is false , the following WARN message is printed out to the logs: WARN Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources Otherwise, when the hasLaunchedTask flag is true the timer thread cancels itself. == [[createTaskSetManager]] Creating TaskSetManager -- createTaskSetManager Method","title":"submitTasks(taskSet: TaskSet): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_16","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#createtasksetmanagertaskset-taskset-maxtaskfailures-int-tasksetmanager","text":"createTaskSetManager scheduler:TaskSetManager.md#creating-instance[creates a TaskSetManager ] (passing on the reference to TaskSchedulerImpl, the input taskSet and maxTaskFailures , and optional BlacklistTracker ). NOTE: createTaskSetManager uses the optional < > that is specified when < >. NOTE: createTaskSetManager is used exclusively when < TaskSet )>>. == [[handleFailedTask]] Notifying TaskSetManager that Task Failed -- handleFailedTask Method","title":"createTaskSetManager(taskSet: TaskSet, maxTaskFailures: Int): TaskSetManager"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_17","text":"handleFailedTask( taskSetManager: TaskSetManager, tid: Long, taskState: TaskState, reason: TaskFailedReason): Unit handleFailedTask scheduler:TaskSetManager.md#handleFailedTask[notifies taskSetManager that tid task has failed] and, only when scheduler:TaskSetManager.md#zombie-state[ taskSetManager is not in zombie state] and tid is not in KILLED state, scheduler:SchedulerBackend.md#reviveOffers[requests SchedulerBackend to revive offers]. NOTE: handleFailedTask is called when scheduler:TaskResultGetter.md#enqueueSuccessfulTask[ TaskResultGetter deserializes a TaskFailedReason ] for a failed task. == [[taskSetFinished]] taskSetFinished Method","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_18","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#tasksetfinishedmanager-tasksetmanager-unit","text":"taskSetFinished looks all scheduler:TaskSet.md[TaskSet]s up by the stage id (in < > registry) and removes the stage attempt from them, possibly with removing the entire stage record from taskSetsByStageIdAndAttempt registry completely (if there are no other attempts registered). .TaskSchedulerImpl.taskSetFinished is called when all tasks are finished image::taskschedulerimpl-tasksetmanager-tasksetfinished.png[align=\"center\"] NOTE: A TaskSetManager manages a TaskSet for a stage. taskSetFinished then spark-scheduler-Pool.md#removeSchedulable[removes manager from the parent's schedulable pool]. You should see the following INFO message in the logs: Removed TaskSet [id], whose tasks have all completed, from pool [name] NOTE: taskSetFinished method is called when scheduler:TaskSetManager.md#maybeFinishTaskSet[ TaskSetManager has received the results of all the tasks in a TaskSet ]. == [[executorAdded]] Notifying DAGScheduler About New Executor -- executorAdded Method","title":"taskSetFinished(manager: TaskSetManager): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_19","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#executoraddedexecid-string-host-string","text":"executorAdded just scheduler:DAGScheduler.md#executorAdded[notifies DAGScheduler that an executor was added]. CAUTION: FIXME Image with a call from TaskSchedulerImpl to DAGScheduler, please. NOTE: executorAdded uses < > that was given when < >. == [[waitBackendReady]] Waiting Until SchedulerBackend is Ready -- waitBackendReady Internal Method","title":"executorAdded(execId: String, host: String)"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_20","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#waitbackendready-unit","text":"waitBackendReady waits until the < > is scheduler:SchedulerBackend.md#isReady[ready]. If it is, waitBackendReady returns immediately. Otherwise, waitBackendReady keeps checking every 100 milliseconds (hardcoded) or the < > is ROOT:SparkContext.md#stopped[stopped]. NOTE: A SchedulerBackend is scheduler:SchedulerBackend.md#isReady[ready] by default. If the SparkContext happens to be stopped while waiting, waitBackendReady throws an IllegalStateException : Spark context stopped while waiting for backend NOTE: waitBackendReady is used exclusively when TaskSchedulerImpl is requested to < >. == [[resourceOffers]] Creating TaskDescriptions For Available Executor Resource Offers","title":"waitBackendReady(): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_21","text":"resourceOffers( offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] resourceOffers takes the resources offers (as < >) and generates a collection of tasks (as spark-scheduler-TaskDescription.md[TaskDescription]) to launch (given the resources available). NOTE: < > represents a resource offer with CPU cores free to use on an executor. .Processing Executor Resource Offers image::taskscheduler-resourceOffers.png[align=\"center\"] Internally, resourceOffers first updates < > and < > lookup tables to record new hosts and executors (given the input offers ). For new executors (not in < >) resourceOffers < DAGScheduler that an executor was added>>. NOTE: TaskSchedulerImpl uses resourceOffers to track active executors. CAUTION: FIXME a picture with executorAdded call from TaskSchedulerImpl to DAGScheduler. resourceOffers requests BlacklistTracker to applyBlacklistTimeout and filters out offers on blacklisted nodes and executors. NOTE: resourceOffers uses the optional < > that was given when < >. CAUTION: FIXME Expand on blacklisting resourceOffers then randomly shuffles offers (to evenly distribute tasks across executors and avoid over-utilizing some executors) and initializes the local data structures tasks and availableCpus (as shown in the figure below). .Internal Structures of resourceOffers with 5 WorkerOffers (with 4, 2, 0, 3, 2 free cores) image::TaskSchedulerImpl-resourceOffers-internal-structures.png[align=\"center\"] resourceOffers spark-scheduler-Pool.md#getSortedTaskSetQueue[takes TaskSets in scheduling order] from scheduler:TaskScheduler.md#rootPool[top-level Schedulable Pool]. .TaskSchedulerImpl Requesting TaskSets (as TaskSetManagers) from Root Pool image::TaskSchedulerImpl-resourceOffers-rootPool-getSortedTaskSetQueue.png[align=\"center\"]","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#note_1","text":"rootPool is configured when < >. rootPool is part of the scheduler:TaskScheduler.md#rootPool[TaskScheduler Contract] and exclusively managed by scheduler:spark-scheduler-SchedulableBuilder.md[SchedulableBuilders], i.e. scheduler:spark-scheduler-FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] and scheduler:spark-scheduler-FairSchedulableBuilder.md[FairSchedulableBuilder] (that scheduler:spark-scheduler-SchedulableBuilder.md#addTaskSetManager[manage registering TaskSetManagers with the root pool]).","title":"[NOTE]"},{"location":"scheduler/TaskSchedulerImpl/#schedulertasksetmanagermdtasksetmanager-manages-execution-of-the-tasks-in-a-single-schedulertasksetmdtaskset-that-represents-a-single-schedulerstagemdstage","text":"For every TaskSetManager (in scheduling order), you should see the following DEBUG message in the logs: parentName: [name], name: [name], runningTasks: [count] Only if a new executor was added, resourceOffers scheduler:TaskSetManager.md#executorAdded[notifies every TaskSetManager about the change] (to recompute locality preferences). resourceOffers then takes every TaskSetManager (in scheduling order) and offers them each node in increasing order of locality levels (per scheduler:TaskSetManager.md#computeValidLocalityLevels[TaskSetManager's valid locality levels]). NOTE: A TaskSetManager scheduler:TaskSetManager.md##computeValidLocalityLevels[computes locality levels of the tasks] it manages. For every TaskSetManager and the TaskSetManager 's valid locality level, resourceOffers tries to < > as long as the TaskSetManager manages to launch a task (given the locality level). If resourceOffers did not manage to offer resources to a TaskSetManager so it could launch any task, resourceOffers scheduler:TaskSetManager.md#abortIfCompletelyBlacklisted[requests the TaskSetManager to abort the TaskSet if completely blacklisted]. When resourceOffers managed to launch a task, the internal < > flag gets enabled (that effectively means what the name says \"there were executors and I managed to launch a task\" ).","title":"scheduler:TaskSetManager.md[TaskSetManager] manages execution of the tasks in a single scheduler:TaskSet.md[TaskSet] that represents a single scheduler:Stage.md[Stage]."},{"location":"scheduler/TaskSchedulerImpl/#note_2","text":"resourceOffers is used when: scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#makeOffers[ CoarseGrainedSchedulerBackend (via RPC endpoint) makes executor resource offers] spark-local:spark-LocalEndpoint.md#reviveOffers[ LocalEndpoint revives resource offers]","title":"[NOTE]"},{"location":"scheduler/TaskSchedulerImpl/#spark-on-mesos-mesosfinegrainedschedulerbackend-does-resourceoffers","text":"== [[resourceOfferSingleTaskSet]] Finding Tasks from TaskSetManager to Schedule on Executors -- resourceOfferSingleTaskSet Internal Method","title":"* Spark on Mesos' MesosFineGrainedSchedulerBackend does resourceOffers"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_22","text":"resourceOfferSingleTaskSet( taskSet: TaskSetManager, maxLocality: TaskLocality, shuffledOffers: Seq[WorkerOffer], availableCpus: Array[Int], tasks: Seq[ArrayBuffer[TaskDescription]]): Boolean resourceOfferSingleTaskSet takes every WorkerOffer (from the input shuffledOffers ) and (only if the number of available CPU cores (using the input availableCpus ) is at least ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus]) scheduler:TaskSetManager.md#resourceOffer[requests TaskSetManager (as the input taskSet ) to find a Task to execute (given the resource offer)] (as an executor, a host, and the input maxLocality ). resourceOfferSingleTaskSet adds the task to the input tasks collection. resourceOfferSingleTaskSet records the task id and TaskSetManager in the following registries: < > < > < > resourceOfferSingleTaskSet decreases ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] from the input availableCpus (for the WorkerOffer ). NOTE: resourceOfferSingleTaskSet makes sure that the number of available CPU cores (in the input availableCpus per WorkerOffer ) is at least 0 . If there is a TaskNotSerializableException , you should see the following ERROR in the logs: ERROR Resource offer failed, task set [name] was not serializable resourceOfferSingleTaskSet returns whether a task was launched or not. NOTE: resourceOfferSingleTaskSet is used when TaskSchedulerImpl < TaskDescriptions for available executor resource offers (with CPU cores)>>. == [[TaskLocality]] TaskLocality -- Task Locality Preference TaskLocality represents a task locality preference and can be one of the following (from most localized to the widest): . PROCESS_LOCAL . NODE_LOCAL . NO_PREF . RACK_LOCAL . ANY == [[WorkerOffer]] WorkerOffer -- Free CPU Cores on Executor","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_23","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#workerofferexecutorid-string-host-string-cores-int","text":"WorkerOffer represents a resource offer with free CPU cores available on an executorId executor on a host . == [[workerRemoved]] workerRemoved Method","title":"WorkerOffer(executorId: String, host: String, cores: Int)"},{"location":"scheduler/TaskSchedulerImpl/#source-scala_24","text":"workerRemoved( workerId: String, host: String, message: String): Unit workerRemoved prints out the following INFO message to the logs: Handle removed worker [workerId]: [message] workerRemoved then requests the < > to scheduler:DAGScheduler.md#workerRemoved[handle it]. workerRemoved is part of the scheduler:TaskScheduler.md#workerRemoved[TaskScheduler] abstraction. == [[maybeInitBarrierCoordinator]] maybeInitBarrierCoordinator Method","title":"[source, scala]"},{"location":"scheduler/TaskSchedulerImpl/#sourcescala","text":"","title":"[source,scala]"},{"location":"scheduler/TaskSchedulerImpl/#maybeinitbarriercoordinator-unit","text":"maybeInitBarrierCoordinator...FIXME maybeInitBarrierCoordinator is used when TaskSchedulerImpl is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.scheduler.TaskSchedulerImpl logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"maybeInitBarrierCoordinator(): Unit"},{"location":"scheduler/TaskSchedulerImpl/#source","text":"","title":"[source]"},{"location":"scheduler/TaskSchedulerImpl/#log4jloggerorgapachesparkschedulertaskschedulerimplall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | dagScheduler a| [[dagScheduler]] scheduler:DAGScheduler.md[DAGScheduler] Used when...FIXME | executorIdToHost a| [[executorIdToHost]] Lookup table of hosts per executor. Used when...FIXME | executorIdToRunningTaskIds a| [[executorIdToRunningTaskIds]] Lookup table of running tasks per executor. Used when...FIXME | executorIdToTaskCount a| [[executorIdToTaskCount]] Lookup table of the number of running tasks by executor:Executor.md[]. | executorsByHost a| [[executorsByHost]] Collection of executor:Executor.md[executors] per host | hasLaunchedTask a| [[hasLaunchedTask]] Flag...FIXME Used when...FIXME | hostToExecutors a| [[hostToExecutors]] Lookup table of executors per hosts in a cluster. Used when...FIXME | hostsByRack a| [[hostsByRack]] Lookup table of hosts per rack. Used when...FIXME | nextTaskId a| [[nextTaskId]] The next scheduler:Task.md[task] id counting from 0 . Used when TaskSchedulerImpl... | rootPool a| [[rootPool]] spark-scheduler-Pool.md[Schedulable pool] Used when TaskSchedulerImpl... | schedulableBuilder a| [[schedulableBuilder]] < > Created when TaskSchedulerImpl is requested to < > and can be one of two available builders: spark-scheduler-FIFOSchedulableBuilder.md[FIFOSchedulableBuilder] when scheduling policy is FIFO (which is the default scheduling policy). spark-scheduler-FairSchedulableBuilder.md[FairSchedulableBuilder] for FAIR scheduling policy. NOTE: Use ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property to select the scheduling policy. | schedulingMode a| [[schedulingMode]] spark-scheduler-SchedulingMode.md[SchedulingMode] Used when TaskSchedulerImpl... | taskSetsByStageIdAndAttempt a| [[taskSetsByStageIdAndAttempt]] Lookup table of scheduler:TaskSet.md[TaskSet] by stage and attempt ids. | taskIdToExecutorId a| [[taskIdToExecutorId]] Lookup table of executor:Executor.md[] by task id. | taskIdToTaskSetManager a| [[taskIdToTaskSetManager]] Registry of active scheduler:TaskSetManager.md[TaskSetManagers] per task id. |===","title":"log4j.logger.org.apache.spark.scheduler.TaskSchedulerImpl=ALL"},{"location":"scheduler/TaskSet/","text":"= [[TaskSet]] TaskSet TaskSet is a < > of a single < > (and a < >) that are missing ( uncomputed ), i.e. for which computation results are unavailable (as RDD blocks on storage:BlockManager.md[BlockManagers] on executors). In other words, a TaskSet represents the missing partitions of a stage that (as tasks) can be run right away based on the data that is already on the cluster, e.g. map output files from previous stages, though they may fail if this data becomes unavailable. NOTE: Since the < > of a TaskSet are only the missing tasks, their number does not necessarily have to be the number of all the tasks of a < >. For a brand new stage (that has never been attempted to compute) their numbers are exactly the same. TaskSet is < > exclusively when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit the missing tasks of a stage]. NOTE: Once scheduler:DAGScheduler.md#submitMissingTasks[submitted] for execution (to a scheduler:TaskScheduler.md[TaskScheduler]), the execution of the TaskSet is managed by a scheduler:TaskSetManager.md[TaskSetManager] that allows for ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] (default: 1 for < > and 4 for < >). [[creating-instance]] TaskSet takes the following to be created: [[tasks]] Collection of scheduler:Task.md[tasks] ( Array[Task[_]] ) [[stageId]] Stage ID [[stageAttemptId]] Stage execution attempt ID [[priority]] Priority (for < >) [[properties]] Key-value properties [[id]] TaskSet is uniquely identified by an id that is the < > followed by the < > with the comma ( . ) in-between. [stageId].[stageAttemptId] [[toString]] A textual representation ( toString ) of TaskSet is TaskSet [id] . TaskSet [stageId].[stageAttemptId] == [[fifo-scheduling]] Task Scheduling Prioritization in FIFO Scheduling The < > of a TaskSet is exactly the ID of the earliest-created active job that needs the stage (that is given when DAGScheduler is requested to scheduler:DAGScheduler.md#submitMissingTasks[submit the missing tasks of a stage]). Once scheduler:DAGScheduler.md#submitMissingTasks[submitted] for execution (to a scheduler:TaskScheduler.md[TaskScheduler]), the < > of a TaskSet is the scheduler:TaskSetManager.md#priority[priority] of the TaskSetManager (which is a < >) that is used for task prioritization ( prioritizing scheduling of tasks ) in the < > scheduling mode.","title":"TaskSet"},{"location":"scheduler/TaskSetBlacklist/","text":"== [[TaskSetBlacklist]] TaskSetBlacklist -- Blacklisting Executors and Nodes For TaskSet CAUTION: FIXME === [[updateBlacklistForFailedTask]] updateBlacklistForFailedTask Method CAUTION: FIXME === [[isExecutorBlacklistedForTaskSet]] isExecutorBlacklistedForTaskSet Method CAUTION: FIXME === [[isNodeBlacklistedForTaskSet]] isNodeBlacklistedForTaskSet Method CAUTION: FIXME","title":"TaskSetBlacklist"},{"location":"scheduler/TaskSetManager/","text":"== [[TaskSetManager]] TaskSetManager TaskSetManager is a < > that manages scheduling of tasks of a < >. NOTE: A scheduler:TaskSet.md[TaskSet] represents a set of scheduler:Task.md[tasks] that correspond to missing spark-rdd-partitions.md[partitions] of a scheduler:Stage.md[stage]. TaskSetManager is < > exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#createTaskSetManager[create one] (when submitting tasks for a given TaskSet ). .TaskSetManager and its Dependencies image::TaskSetManager-TaskSchedulerImpl-TaskSet.png[align=\"center\"] When < > with a given < >, TaskSetManager < >. TaskSetManager is notified when a task (from the TaskSet it manages) finishes -- < > or due to a < > (in task execution or < >). TaskSetManager uses < > to control how many times a < > before an < TaskSet gets aborted>> that can take the following values: 1 for local/spark-local.md[ local run mode] maxFailures in local/spark-local.md#local-with-retries[Spark local-with-retries] (i.e. local[N, maxFailures] ) ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for local/spark-local.md[Spark local-cluster] and spark-cluster.md[Spark clustered] (using Spark Standalone, Mesos and YARN) [[maxResultSize]] TaskSetManager uses ROOT:configuration-properties.md#MAX_RESULT_SIZE[spark.driver.maxResultSize] configuration property (default: 1g ) to < >. The responsibilities of a TaskSetManager include: < > < > < > [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.scheduler.TaskSchedulerImpl (or org.apache.spark.scheduler.cluster.YarnScheduler for YARN) and org.apache.spark.scheduler.TaskSetManager and execute the following two-stage job to see their low-level innerworkings. A cluster manager is recommended since it gives more task localization choices (with YARN additionally supporting rack localization). $ ./bin/spark-shell --master yarn --conf spark.ui.showConsoleProgress=false // Keep # partitions low to keep # messages low scala> sc.parallelize(0 to 9, 3).groupBy(_ % 3).count INFO YarnScheduler: Adding task set 0.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.2.87, executor 1, partition 0, PROCESS_LOCAL, 7541 bytes) INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.2.87, executor 2, partition 1, PROCESS_LOCAL, 7541 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.0.2.87, executor 1, partition 2, PROCESS_LOCAL, 7598 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 518 ms on 10.0.2.87 (executor 1) (1/3) INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 512 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 51 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool INFO YarnScheduler: Adding task set 1.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 1.0: 1 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NODE_LOCAL, RACK_LOCAL, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, 10.0.2.87, executor 2, partition 0, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, 10.0.2.87, executor 1, partition 1, NODE_LOCAL, 7348 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, 10.0.2.87, executor 1, partition 2, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 130 ms on 10.0.2.87 (executor 1) (1/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level RACK_LOCAL DEBUG TaskSetManager: No tasks for locality level RACK_LOCAL, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 133 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 21 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool res0: Long = 3 ==== [[internal-registries]] .TaskSetManager's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[allPendingTasks]] allPendingTasks | Indices of all the pending tasks to execute (regardless of their localization preferences). Updated with an task index when TaskSetManager < >. | [[calculatedTasks]] calculatedTasks | The number of the tasks that have already completed execution. Starts from 0 when a < TaskSetManager is created>> and is only incremented when the < TaskSetManager checks that there is enough memory to fetch a task result>>. | [[copiesRunning]] copiesRunning | The number of task copies currently running per task (index in its task set). The number of task copies of a task is increased when < > or < > and decreased when < > or < > (for a shuffle map stage and no external shuffle service). [[currentLocalityIndex]] currentLocalityIndex | [[epoch]] epoch | Current scheduler:MapOutputTracker.md#getEpoch[map output tracker epoch]. | [[failedExecutors]] failedExecutors | Lookup table of spark-scheduler-TaskInfo.md[TaskInfo] indices that failed to executor ids and the time of the failure. Used in < >. | [[isZombie]] isZombie | Disabled, i.e. false , by default. Read < > in this document. [[lastLaunchTime]] lastLaunchTime [[localityWaits]] localityWaits | [[myLocalityLevels]] myLocalityLevels | scheduler:TaskSchedulerImpl.md#TaskLocality[ TaskLocality locality preferences] of the pending tasks in the < > ranging from PROCESS_LOCAL through NODE_LOCAL , NO_PREF , and RACK_LOCAL to ANY . NOTE: myLocalityLevels may contain only a few of all the available TaskLocality preferences with ANY as a mandatory task locality preference. < > immediately when < TaskSetManager is created>>. < > every change in the status of executors. [[name]] name | [[numFailures]] numFailures | Array of the number of task failures per < >. Incremented when TaskSetManager < > and immediatelly checked if above < >. | [[numTasks]] numTasks | Number of < > to compute. | [[pendingTasksForExecutor]] pendingTasksForExecutor | Lookup table of the indices of tasks pending execution per executor. Updated with an task index and executor when TaskSetManager < > (and the location is a ExecutorCacheTaskLocation or HDFSCacheTaskLocation ). | [[pendingTasksForHost]] pendingTasksForHost | Lookup table of the indices of tasks pending execution per host. Updated with an task index and host when TaskSetManager < >. | [[pendingTasksForRack]] pendingTasksForRack | Lookup table of the indices of tasks pending execution per rack. Updated with an task index and rack when TaskSetManager < >. | [[pendingTasksWithNoPrefs]] pendingTasksWithNoPrefs | Lookup table of the indices of tasks pending execution with no location preferences. Updated with an task index when TaskSetManager < >. [[priority]] priority [[recentExceptions]] recentExceptions | [[runningTasksSet]] runningTasksSet | Collection of running tasks that a TaskSetManager manages. Used to implement < > (that is simply the size of runningTasksSet but a required part of any spark-scheduler-Schedulable.md#contract[Schedulable]). runningTasksSet is expanded when < > and shrinked when < >. Used in scheduler:TaskSchedulerImpl.md#cancelTasks[ TaskSchedulerImpl to cancel tasks]. [[speculatableTasks]] speculatableTasks | [[stageId]] stageId | The stage's id a TaskSetManager runs for. Set when < TaskSetManager is created>>. NOTE: stageId is part of spark-scheduler-Schedulable.md#contract[Schedulable contract]. | [[successful]] successful | Status of < > (with a boolean flag, i.e. true or false , per task). All tasks start with their flags disabled, i.e. false , when < TaskSetManager is created>>. The flag for a task is turned on, i.e. true , when a task finishes < > but also < >. A flag is explicitly turned off only for < ShuffleMapTask tasks when their executor is lost>>. | [[taskAttempts]] taskAttempts | Registry of spark-scheduler-TaskInfo.md[TaskInfos] per every task attempt per task. | [[taskInfos]] taskInfos | Registry of spark-scheduler-TaskInfo.md[TaskInfos] per task id. Updated with the task (id) and the corresponding TaskInfo when TaskSetManager < >. NOTE: It appears that the entires stay forever, i.e. are never removed (perhaps because the maintenance overhead is not needed given a TaskSetManager is a short-lived entity). | [[tasks]] tasks | Lookup table of scheduler:Task.md[Tasks] (per partition id) to schedule execution of. NOTE: The tasks all belong to a single < > that was given when < TaskSetManager was created>> (which actually represent a single scheduler:Stage.md[Stage]). [[tasksSuccessful]] tasksSuccessful | [[totalResultSize]] totalResultSize | The current total size of the result of all the tasks that have finished. Starts from 0 when < TaskSetManager is created>>. Only increased with the size of a task result whenever a TaskSetManager < >. |=== [[logging]] [TIP] ==== Enable DEBUG logging level for org.apache.spark.scheduler.TaskSetManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG Refer to spark-logging.md[Logging]. \u00b6 === [[isTaskBlacklistedOnExecOrNode]] isTaskBlacklistedOnExecOrNode Internal Method [source, scala] \u00b6 isTaskBlacklistedOnExecOrNode( index: Int, execId: String, host: String): Boolean isTaskBlacklistedOnExecOrNode ...FIXME NOTE: isTaskBlacklistedOnExecOrNode is used when TaskSetManager is requested to < > and < >. === [[getLocalityIndex]] getLocalityIndex Method [source, scala] \u00b6 getLocalityIndex(locality: TaskLocality.TaskLocality): Int \u00b6 getLocalityIndex ...FIXME NOTE: getLocalityIndex is used when TaskSetManager is requested to < > and < >. === [[dequeueSpeculativeTask]] dequeueSpeculativeTask Internal Method [source, scala] \u00b6 dequeueSpeculativeTask( execId: String, host: String, locality: TaskLocality.Value): Option[(Int, TaskLocality.Value)] dequeueSpeculativeTask ...FIXME NOTE: dequeueSpeculativeTask is used exclusively when TaskSetManager is requested to < >. === [[executorAdded]] executorAdded Method [source, scala] \u00b6 executorAdded(): Unit \u00b6 executorAdded simply < >. NOTE: executorAdded is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[abortIfCompletelyBlacklisted]] abortIfCompletelyBlacklisted Internal Method [source, scala] \u00b6 abortIfCompletelyBlacklisted( hostToExecutors: HashMap[String, HashSet[String]]): Unit abortIfCompletelyBlacklisted ...FIXME NOTE: abortIfCompletelyBlacklisted is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[schedulable]] TaskSetManager is Schedulable TaskSetManager is a spark-scheduler-Schedulable.md[Schedulable] with the following implementation: name is TaskSet_[taskSet.stageId.toString] no parent is ever assigned, i.e. it is always null . + It means that it can only be a leaf in the tree of Schedulables (with spark-scheduler-Pool.md[Pools] being the nodes). schedulingMode always returns SchedulingMode.NONE (since there is nothing to schedule). weight is always 1 . minShare is always 0 . runningTasks is the number of running tasks in the internal runningTasksSet . priority is the priority of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.priority ). stageId is the stage id of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.stageId ). schedulableQueue returns no queue, i.e. null . addSchedulable and removeSchedulable do nothing. getSchedulableByName always returns null . getSortedTaskSetQueue returns a one-element collection with the sole element being itself. < > < > === [[handleTaskGettingResult]] Marking Task As Fetching Indirect Result -- handleTaskGettingResult Method [source, scala] \u00b6 handleTaskGettingResult(tid: Long): Unit \u00b6 handleTaskGettingResult finds spark-scheduler-TaskInfo.md[TaskInfo] for tid task in < > internal registry and marks it as fetching indirect task result. It then scheduler:DAGScheduler.md#taskGettingResult[notifies DAGScheduler ]. NOTE: handleTaskGettingResult is executed when scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[ TaskSchedulerImpl is notified about fetching indirect task result]. === [[addRunningTask]] Registering Running Task -- addRunningTask Method [source, scala] \u00b6 addRunningTask(tid: Long): Unit \u00b6 addRunningTask adds tid to < > internal registry and spark-scheduler-Pool.md#increaseRunningTasks[requests the parent pool to increase the number of running tasks] (if defined). === [[removeRunningTask]] Unregistering Running Task -- removeRunningTask Method [source, scala] \u00b6 removeRunningTask(tid: Long): Unit \u00b6 removeRunningTask removes tid from < > internal registry and spark-scheduler-Pool.md#decreaseRunningTasks[requests the parent pool to decrease the number of running task] (if defined). === [[checkSpeculatableTasks]] Checking Speculatable Tasks -- checkSpeculatableTasks Method [source, scala] \u00b6 checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean \u00b6 NOTE: checkSpeculatableTasks is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. checkSpeculatableTasks checks whether there are speculatable tasks in a TaskSet . NOTE: checkSpeculatableTasks is called when for ROOT:speculative-execution-of-tasks.md[]. If the TaskSetManager is < > or has a single task in TaskSet, it assumes no speculatable tasks. The method goes on with the assumption of no speculatable tasks by default. It computes the minimum number of finished tasks for speculation (as ROOT:configuration-properties.md#spark.speculation.quantile[spark.speculation.quantile] of all the finished tasks). You should see the DEBUG message in the logs: DEBUG Checking for speculative tasks: minFinished = [minFinishedForSpeculation] It then checks whether the number is equal or greater than the number of tasks completed successfully (using tasksSuccessful ). Having done that, it computes the median duration of all the successfully completed tasks (using < taskInfos internal registry>>) and task length threshold using the median duration multiplied by ROOT:configuration-properties.md#spark.speculation.multiplier[spark.speculation.multiplier] that has to be equal or less than 100 . You should see the DEBUG message in the logs: DEBUG Task length threshold for speculation: [threshold] For each task (using < taskInfos internal registry>>) that is not marked as successful yet (using successful ) for which there is only one copy running (using copiesRunning ) and the task takes more time than the calculated threshold, but it was not in speculatableTasks it is assumed speculatable . You should see the following INFO message in the logs: INFO Marking task [index] in stage [taskSet.id] (on [info.host]) as speculatable because it ran more than [threshold] ms The task gets added to the internal speculatableTasks collection. The method responds positively. === [[getAllowedLocalityLevel]] getAllowedLocalityLevel Internal Method [source, scala] \u00b6 getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality \u00b6 getAllowedLocalityLevel ...FIXME NOTE: getAllowedLocalityLevel is used exclusively when TaskSetManager is requested to < >. === [[resourceOffer]] Finding Task For Execution (Given Resource Offer) -- resourceOffer Method [source, scala] \u00b6 resourceOffer( execId: String, host: String, maxLocality: TaskLocality): Option[TaskDescription] (only if < > is defined) resourceOffer requests TaskSetBlacklist to check if the input spark-scheduler-TaskSetBlacklist.md#isExecutorBlacklistedForTaskSet[ execId executor] or spark-scheduler-TaskSetBlacklist.md#isNodeBlacklistedForTaskSet[ host node] are blacklisted. When TaskSetManager is a < > or the resource offer (as executor and host) is blacklisted, resourceOffer finds no tasks to execute (and returns no spark-scheduler-TaskDescription.md[TaskDescription]). NOTE: resourceOffer finds a task to schedule for a resource offer when neither TaskSetManager is a < > nor the resource offer is blacklisted. resourceOffer calculates the allowed task locality for task selection. When the input maxLocality is not NO_PREF task locality, resourceOffer < > (for the current time) and sets it as the current task locality if more localized (specific). NOTE: scheduler:TaskSchedulerImpl.md[TaskLocality] can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . resourceOffer < >. If a task (index) is found, resourceOffer takes the scheduler:Task.md[Task] (from < > registry). resourceOffer scheduler:TaskSchedulerImpl.md#newTaskId[requests TaskSchedulerImpl for the id for the new task]. resourceOffer increments the < > and finds the task attempt number (as the size of < > entries for the task index). resourceOffer spark-scheduler-TaskInfo.md#creating-instance[creates a TaskInfo ] that is then registered in < > and < >. If the maximum acceptable task locality is not NO_PREF , resourceOffer < > (using the task's locality) and records it as < > with the current time as < >. resourceOffer serializes the task. NOTE: resourceOffer uses core:SparkEnv.md#closureSerializer[ SparkEnv to access the closure Serializer ] and serializer:Serializer.md#newInstance[create an instance thereof]. If the task serialization fails, you should see the following ERROR message in the logs: Failed to serialize task [taskId], not attempting to retry it. resourceOffer < TaskSet >> with the following message and reports a TaskNotSerializableException . [options=\"wrap\"] \u00b6 Failed to serialize task [taskId], not attempting to retry it. Exception during serialization: [exception] \u00b6 resourceOffer checks the size of the serialized task. If it is greater than 100 kB, you should see the following WARN message in the logs: [options=\"wrap\"] \u00b6 WARN Stage [id] contains a task of very large size ([size] KB). The maximum recommended task size is 100 KB. \u00b6 NOTE: The size of the serializable task, i.e. 100 kB, is not configurable. If however the serialization went well and the size is fine too, resourceOffer < >. You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO TaskSetManager: Starting [name] (TID [id], [host], executor [id], partition [id], [taskLocality], [size] bytes) \u00b6 For example: [options=\"wrap\"] \u00b6 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 2054 bytes) \u00b6 resourceOffer scheduler:DAGScheduler.md#taskStarted[notifies DAGScheduler that the task has been started]. IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has started. NOTE: resourceOffer is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOfferSingleTaskSet[resourceOfferSingleTaskSet]. === [[dequeueTask]] Dequeueing Task For Execution (Given Locality Information) -- dequeueTask Internal Method [source, scala] \u00b6 dequeueTask(execId: String, host: String, maxLocality: TaskLocality): Option[(Int, TaskLocality, Boolean)] \u00b6 dequeueTask tries to < > (meeting localization requirements) using < execId executor>>. If a task is found, dequeueTask returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. dequeueTask then goes over all the possible scheduler:TaskSchedulerImpl.md#TaskLocality[task localities] and checks what locality is allowed given the input maxLocality . dequeueTask checks out NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY in that order. For NODE_LOCAL dequeueTask tries to < > (meeting localization requirements) using < host host>> and if found returns its index, NODE_LOCAL task locality and the speculative marker disabled. For NO_PREF dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. NOTE: For NO_PREF the task locality is PROCESS_LOCAL . For RACK_LOCAL dequeueTask scheduler:TaskSchedulerImpl.md#getRackForHost[finds the rack for the input host ] and if available tries to < > (meeting localization requirements) using < >. If a task is found, dequeueTask returns its index, RACK_LOCAL task locality and the speculative marker disabled. For ANY dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, ANY task locality and the speculative marker disabled. In the end, when no task could be found, dequeueTask < > and if found returns its index, locality and the speculative marker enabled. NOTE: The speculative marker is enabled for a task only when dequeueTask did not manage to find a task for the available task localities and did find a speculative task. NOTE: dequeueTask is used exclusively when TaskSetManager is requested to < >. === [[dequeueTaskFromList]] Finding Higest Task Index (Not Blacklisted, With No Copies Running and Not Completed Already) -- dequeueTaskFromList Internal Method [source, scala] \u00b6 dequeueTaskFromList( execId: String, host: String, list: ArrayBuffer[Int]): Option[Int] dequeueTaskFromList takes task indices from the input list backwards (from the last to the first entry). For every index dequeueTaskFromList checks if it is not < execId executor and host >> and if not, checks that: < > is 0 the task has not been marked as < > If so, dequeueTaskFromList returns the task index. If dequeueTaskFromList has checked all the indices and no index has passed the checks, dequeueTaskFromList returns None (to indicate that no index has met the requirements). NOTE: dequeueTaskFromList is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForExecutor]] Finding Tasks (Indices) Registered For Execution on Executor -- getPendingTasksForExecutor Internal Method [source, scala] \u00b6 getPendingTasksForExecutor(executorId: String): ArrayBuffer[Int] \u00b6 getPendingTasksForExecutor finds pending tasks (indices) registered for execution on the input executorId executor (in < > internal registry). NOTE: getPendingTasksForExecutor may find no matching tasks and return an empty collection. NOTE: getPendingTasksForExecutor is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForHost]] Finding Tasks (Indices) Registered For Execution on Host -- getPendingTasksForHost Internal Method [source, scala] \u00b6 getPendingTasksForHost(host: String): ArrayBuffer[Int] \u00b6 getPendingTasksForHost finds pending tasks (indices) registered for execution on the input host host (in < > internal registry). NOTE: getPendingTasksForHost may find no matching tasks and return an empty collection. NOTE: getPendingTasksForHost is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForRack]] Finding Tasks (Indices) Registered For Execution on Rack -- getPendingTasksForRack Internal Method [source, scala] \u00b6 getPendingTasksForRack(rack: String): ArrayBuffer[Int] \u00b6 getPendingTasksForRack finds pending tasks (indices) registered for execution on the input rack rack (in < > internal registry). NOTE: getPendingTasksForRack may find no matching tasks and return an empty collection. NOTE: getPendingTasksForRack is used exclusively when TaskSetManager is requested to < >. === [[scheduling-tasks]] Scheduling Tasks in TaskSet CAUTION: FIXME For each submitted < >, a new TaskSetManager is created. The TaskSetManager completely and exclusively owns a TaskSet submitted for execution. CAUTION: FIXME A picture with TaskSetManager owning TaskSet CAUTION: FIXME What component knows about TaskSet and TaskSetManager. Isn't it that TaskSets are created by DAGScheduler while TaskSetManager is used by TaskSchedulerImpl only? TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences. === [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs: DEBUG Valid locality levels for [taskSet]: [levels] CAUTION: FIXME What's delay scheduling? === [[events]] Events Once a task has finished, TaskSetManager informs scheduler:DAGScheduler.md#taskEnded[DAGScheduler]. CAUTION: FIXME === [[handleSuccessfulTask]] Recording Successful Task And Notifying DAGScheduler -- handleSuccessfulTask Method [source, scala] \u00b6 handleSuccessfulTask( tid: Long, result: DirectTaskResult[_]): Unit handleSuccessfulTask records the tid task as finished, scheduler:DAGScheduler.md#taskEnded[notifies the DAGScheduler that the task has ended] and < TaskSet finished>>. NOTE: handleSuccessfulTask is executed after scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[ TaskSchedulerImpl has been informed that tid task finished successfully (and the task result was deserialized)]. Internally, handleSuccessfulTask finds spark-scheduler-TaskInfo.md[TaskInfo] (in < > internal registry) and marks it as FINISHED . It then removes tid task from < > internal registry. handleSuccessfulTask scheduler:DAGScheduler.md#taskEnded[notifies DAGScheduler that tid task ended successfully] (with the Task object from < > internal registry and the result as Success ). At this point, handleSuccessfulTask finds the other < > of tid task and scheduler:SchedulerBackend.md#killTask[requests SchedulerBackend to kill them] (since they are no longer necessary now when at least one task attempt has completed successfully). You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Killing attempt [attemptNumber] for task [id] in stage [id] (TID [id]) on [host] as the attempt [attemptNumber] succeeded on [host] \u00b6 CAUTION: FIXME Review taskAttempts If tid has not yet been recorded as < >, handleSuccessfulTask increases < > counter. You should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Finished task [id] in stage [id] (TID [taskId]) in [duration] ms on [host] (executor [executorId]) ([tasksSuccessful]/[numTasks]) \u00b6 tid task is marked as < >. If the number of task that have finished successfully is exactly the number of the tasks to execute (in the TaskSet ), the TaskSetManager becomes a < >. If tid task was already recorded as < >, you should merely see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Ignoring task-finished event for [id] in stage [id] because task [index] has already completed successfully \u00b6 Ultimately, handleSuccessfulTask < TaskSet finished>>. NOTE: handleSuccessfulTask is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[handleSuccessfulTask]. === [[maybeFinishTaskSet]] Attempting to Mark TaskSet Finished -- maybeFinishTaskSet Internal Method [source, scala] \u00b6 maybeFinishTaskSet(): Unit \u00b6 maybeFinishTaskSet scheduler:TaskSchedulerImpl.md#taskSetFinished[notifies TaskSchedulerImpl that a TaskSet has finished] when there are no other < > and the < >. === [[task-retries]] Retrying Tasks on Failure CAUTION: FIXME Up to ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] attempts === Task retries and spark.task.maxFailures When you start Spark program you set up ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed. TIP: In Spark shell with local master, ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] is fixed to 1 and you need to use local/spark-local.md[local-with-retries master] to change it to some other value. In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely 0.0 . $ ./bin/spark-shell --master \"local[*, 5]\" ... scala> sc.textFile(\"README.md\", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception(\"Partition 2 marked failed\") else it).count ... 15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25) 15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1) 15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) ... 15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2) ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) 15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4) 15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4] 15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job 15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1 15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s 15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0 15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed === [[zombie-state]] Zombie state A TaskSetManager is in zombie state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the taskset has been < >. While in zombie state, a TaskSetManager can launch no new tasks and < TaskDescription to resourceOffers>>. A TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks. === [[abort]] Aborting TaskSet -- abort Method [source, scala] \u00b6 abort( message: String, exception: Option[Throwable] = None): Unit abort informs scheduler:DAGScheduler.md#taskSetFailed[ DAGScheduler that the TaskSet has been aborted]. CAUTION: FIXME image with DAGScheduler call The TaskSetManager enters < >. In the end, abort < TaskSet finished>>. [NOTE] \u00b6 abort is used when: TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueueSuccessfulTask] (that has failed) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#cancelTasks[cancelTasks] and scheduler:TaskSchedulerImpl.md#error[error] TaskSetManager is requested to < >, < >, < >, and < > * DriverEndpoint is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks on executors] \u00b6 === [[creating-instance]] Creating TaskSetManager Instance TaskSetManager takes the following to be created: [[sched]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[taskSet]] scheduler:TaskSet.md[TaskSet] [[maxTaskFailures]] Number of task failures, i.e. how many times a < > before an entire TaskSet is < > [[blacklistTracker]] (optional) BlacklistTracker (default: None ) [[clock]] Clock (default: SystemClock ) TaskSetManager initializes the < >. NOTE: maxTaskFailures is 1 for local run mode, maxFailures for Spark local-with-retries, and ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for Spark local-cluster and Spark with cluster managers (Spark Standalone, Mesos and YARN). TaskSetManager scheduler:MapOutputTracker.md#getEpoch[requests the current epoch from MapOutputTracker ] and sets it on all tasks in the taskset. NOTE: TaskSetManager uses < > (that was given when < >) to scheduler:TaskSchedulerImpl.md#mapOutputTracker[access the current MapOutputTracker ]. You should see the following DEBUG in the logs: DEBUG Epoch for [taskSet]: [epoch] CAUTION: FIXME Why is the epoch important? NOTE: TaskSetManager requests scheduler:TaskSchedulerImpl.md#mapOutputTracker[ MapOutputTracker from TaskSchedulerImpl ] which is likely for unit testing only since core:SparkEnv.md#mapOutputTracker[ MapOutputTracker is available using SparkEnv ]. TaskSetManager < > (in reverse order from the highest partition to the lowest). CAUTION: FIXME Why is reverse order important? The code says it's to execute tasks with low indices first. === [[handleFailedTask]] Getting Notified that Task Failed -- handleFailedTask Method [source, scala] \u00b6 handleFailedTask( tid: Long, state: TaskState, reason: TaskFailedReason): Unit handleFailedTask finds spark-scheduler-TaskInfo.md[TaskInfo] of tid task in < > internal registry and simply quits if the task is already marked as failed or killed. .TaskSetManager Gets Notified that Task Has Failed image::TaskSetManager-handleFailedTask.png[align=\"center\"] NOTE: handleFailedTask is executed after scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl has been informed that tid task failed] or < >. In either case, tasks could not finish successfully or could not report their status back. handleFailedTask < tid task from the internal registry of running tasks>> and then spark-scheduler-TaskInfo.md#markFinished[marks the corresponding TaskInfo as finished] (passing in the input state ). handleFailedTask decrements the number of the running copies of tid task (in < > internal registry). NOTE: With ROOT:speculative-execution-of-tasks.md[] enabled, there can be many copies of a task running simultaneuosly. handleFailedTask uses the following pattern as the reason of the failure: Lost task [id] in stage [taskSetId] (TID [tid], [host], executor [executorId]): [reason] handleFailedTask then calculates the failure exception per the input reason (follow the links for more details): < > < > < > < > NOTE: Description of how the final failure exception is \"computed\" was moved to respective sections below to make the reading slightly more pleasant and comprehensible. handleFailedTask scheduler:DAGScheduler.md#taskEnded[informs DAGScheduler that tid task has ended] (passing on the Task instance from < > internal registry, the input reason , null result, calculated accumUpdates per failure, and the spark-scheduler-TaskInfo.md[TaskInfo]). IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has ended. If tid task has already been marked as completed (in < > internal registry) you should see the following INFO message in the logs: [options=\"wrap\"] \u00b6 INFO Task [id] in stage [id] (TID [tid]) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded). \u00b6 TIP: Read up on ROOT:speculative-execution-of-tasks.md[] to find out why a single task could be executed multiple times. If however tid task was not recorded as < >, handleFailedTask < >. If the TaskSetManager is not a < > and the task failed reason should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted (i.e. TaskFailedReason.countTowardsTaskFailures attribute is enabled), the optional spark-scheduler-TaskSetBlacklist.md#updateBlacklistForFailedTask[ TaskSetBlacklist is notified] (passing on the host, executor and the task's index). handleFailedTask then increments the < > for tid task and checks if the number of failures is equal or greater than the < TaskSet >> (as defined when the < TaskSetManager was created>>). If so, i.e. the number of task failures of tid task reached the maximum value, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] failed [maxTaskFailures] times; aborting job And handleFailedTask < TaskSet >> with the following message and then quits: Task [index] in stage [id] failed [maxTaskFailures] times, most recent failure: [failureReason] In the end (except when the number of failures of tid task grew beyond the acceptable number), handleFailedTask < TaskSet as finished>>. [NOTE] \u00b6 handleFailedTask is used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#handleFailedTask[handle a failed task] * TaskSetManager is requested to < > and < > \u00b6 ==== [[handleFailedTask-FetchFailed]] FetchFailed TaskFailedReason For FetchFailed you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Unless tid has already been marked as successful (in < > internal registry), it becomes so and the < TaskSet >> gets increased. The TaskSetManager enters < >. The failure exception is empty. ==== [[handleFailedTask-ExceptionFailure]] ExceptionFailure TaskFailedReason For ExceptionFailure , handleFailedTask checks if the exception is of type NotSerializableException . If so, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] (TID [tid]) had a not serializable result: [description]; not retrying And handleFailedTask < TaskSet >> and then quits. Otherwise, if the exception is not of type NotSerializableException , handleFailedTask accesses accumulators and calculates whether to print the WARN message (with the failure reason) or the INFO message. If the failure has already been reported (and is therefore a duplication), ROOT:configuration-properties.md#spark.logging.exceptionPrintInterval[spark.logging.exceptionPrintInterval] is checked before reprinting the duplicate exception in its entirety. For full printout of the ExceptionFailure , the following WARN appears in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Otherwise, the following INFO appears in the logs: INFO Lost task [id] in stage [id] (TID [tid]) on [host], executor [id]: [className] ([description]) [duplicate [dupCount]] The exception in ExceptionFailure becomes the failure exception. ==== [[handleFailedTask-ExecutorLostFailure]] ExecutorLostFailure TaskFailedReason For ExecutorLostFailure if not exitCausedByApp , you should see the following INFO in the logs: INFO Task [tid] failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task. The failure exception is empty. ==== [[handleFailedTask-TaskFailedReason]] Other TaskFailedReasons For the other TaskFailedReasons, you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] The failure exception is empty. === [[addPendingTask]] Registering Task As Pending Execution (Per Preferred Locations) -- addPendingTask Internal Method [source, scala] \u00b6 addPendingTask(index: Int): Unit \u00b6 addPendingTask registers a index task in the pending-task lists that the task should be eventually scheduled to (per its preferred locations). Internally, addPendingTask takes the scheduler:Task.md#preferredLocations[preferred locations of the task] (given index ) and registers the task in the internal pending-task registries for every preferred location: < > when the scheduler:TaskLocation.md[TaskLocation] is ExecutorCacheTaskLocation . < > for the hosts of a scheduler:TaskLocation.md[TaskLocation]. < > for the scheduler:TaskSchedulerImpl.md#getRackForHost[racks from TaskSchedulerImpl per the host] (of a scheduler:TaskLocation.md[TaskLocation]). For a scheduler:TaskLocation.md[TaskLocation] being HDFSCacheTaskLocation , addPendingTask scheduler:TaskSchedulerImpl.md#getExecutorsAliveOnHost[requests TaskSchedulerImpl for the executors on the host] (of a preferred location) and registers the task in < > for every executor (if available). You should see the following INFO message in the logs: INFO Pending task [index] has a cached location at [host] , where there are executors [executors] When addPendingTask could not find executors for a HDFSCacheTaskLocation preferred location, you should see the following DEBUG message in the logs: DEBUG Pending task [index] has a cached location at [host] , but there are no executors alive there. If the task has no location preferences, addPendingTask registers it in < >. addPendingTask always registers the task in < >. NOTE: addPendingTask is used immediatelly when TaskSetManager < > and later when handling a < > or < >. === [[executorLost]] Re-enqueuing ShuffleMapTasks (with no ExternalShuffleService) and Reporting All Running Tasks on Lost Executor as Failed -- executorLost Method [source, scala] \u00b6 executorLost(execId: String, host: String, reason: ExecutorLossReason): Unit \u00b6 executorLost re-enqueues all the scheduler:ShuffleMapTask.md[ShuffleMapTasks] that have completed already on the lost executor (when deploy:ExternalShuffleService.md[external shuffle service] is not in use) and < >. NOTE: executorLost is part of the spark-scheduler-Schedulable.md#contract[Schedulable contract] that scheduler:TaskSchedulerImpl.md#removeExecutor[ TaskSchedulerImpl uses to inform TaskSetManagers about lost executors]. NOTE: Since TaskSetManager manages execution of the tasks in a single scheduler:TaskSet.md[TaskSet], when an executor gets lost, the affected tasks that have been running on the failed executor need to be re-enqueued. executorLost is the mechanism to \"announce\" the event to all TaskSetManagers . Internally, executorLost first checks whether the < > are scheduler:ShuffleMapTask.md[ShuffleMapTasks] and whether an deploy:ExternalShuffleService.md[external shuffle service] is enabled (that could serve the map shuffle outputs in case of failure). NOTE: executorLost checks out the first task in < > as it is assumed the other belong to the same stage. If the task is a scheduler:ShuffleMapTask.md[ShuffleMapTask], the entire < > is for a scheduler:ShuffleMapStage.md[ShuffleMapStage]. NOTE: executorLost uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ] and finds out whether an storage:BlockManager.md#externalShuffleServiceEnabled[external shuffle service is enabled] or not (based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). If executorLost is indeed due to an executor lost that executed tasks for a scheduler:ShuffleMapStage.md[ShuffleMapStage] (that this TaskSetManager manages) and no external shuffle server is enabled, executorLost finds < > that were scheduled on this lost executor and marks the < > as not executed yet. NOTE: executorLost uses records every tasks on the lost executor in < > (as false ) and decrements < >, and < > for every task. executorLost < > and scheduler:DAGScheduler.md#taskEnded[informs DAGScheduler that the tasks (on the lost executor) have ended] (with scheduler:DAGScheduler.md#handleTaskCompletion-Resubmitted[Resubmitted] reason). NOTE: executorLost uses scheduler:TaskSchedulerImpl.md#dagScheduler[ TaskSchedulerImpl to access the DAGScheduler ]. TaskSchedulerImpl is given when the < TaskSetManager was created>>. Regardless of whether this TaskSetManager manages ShuffleMapTasks or not (it could also manage scheduler:ResultTask.md[ResultTasks]) and whether the external shuffle service is used or not, executorLost finds all < > on this lost executor and < > (with the task state FAILED ). NOTE: executorLost finds out if the reason for the executor lost is due to application fault, i.e. assumes ExecutorExited 's exit status as the indicator, ExecutorKilled for non-application's fault and any other reason is an application fault. executorLost < >. === [[recomputeLocality]] Recomputing Task Locality Preferences -- recomputeLocality Method [source, scala] \u00b6 recomputeLocality(): Unit \u00b6 recomputeLocality recomputes the internal caches: < >, < > and < >. CAUTION: FIXME But why are the caches important (and have to be recomputed)? recomputeLocality records the current scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] level of this TaskSetManager (that is < > in < >). NOTE: TaskLocality is one of PROCESS_LOCAL , NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY values. recomputeLocality < > and saves the result in < > internal cache. recomputeLocality computes < > (by < > for every locality level in < > internal cache). In the end, recomputeLocality < > of the previous locality level and records it in < >. NOTE: recomputeLocality is used when TaskSetManager gets notified about status change in executors, i.e. when an executor is < > or < >. === [[computeValidLocalityLevels]] Computing Locality Levels (for Scheduled Tasks) -- computeValidLocalityLevels Internal Method [source, scala] \u00b6 computeValidLocalityLevels(): Array[TaskLocality] \u00b6 computeValidLocalityLevels computes valid locality levels for tasks that were registered in corresponding registries per locality level. NOTE: scheduler:TaskSchedulerImpl.md[TaskLocality] is a task locality preference and can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . .TaskLocalities and Corresponding Internal Registries [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Internal Registry | PROCESS_LOCAL | < > | NODE_LOCAL | < > | NO_PREF | < > | RACK_LOCAL | < > |=== computeValidLocalityLevels walks over every internal registry and if it is not empty < > for the corresponding TaskLocality and proceeds with it only when the locality wait is not 0 . For TaskLocality with pending tasks, computeValidLocalityLevels asks TaskSchedulerImpl whether there is at least one executor alive (for scheduler:TaskSchedulerImpl.md#isExecutorAlive[PROCESS_LOCAL], scheduler:TaskSchedulerImpl.md#hasExecutorsAliveOnHost[NODE_LOCAL] and scheduler:TaskSchedulerImpl.md#hasHostAliveOnRack[RACK_LOCAL]) and if so registers the TaskLocality . NOTE: computeValidLocalityLevels uses < > that was given when < TaskSetManager was created>>. computeValidLocalityLevels always registers ANY task locality level. In the end, you should see the following DEBUG message in the logs: DEBUG TaskSetManager: Valid locality levels for [taskSet]: [comma-separated levels] NOTE: computeValidLocalityLevels is used when TaskSetManager < > and later to < >. === [[getLocalityWait]] Finding Locality Wait -- getLocalityWait Internal Method [source, scala] \u00b6 getLocalityWait(level: TaskLocality): Long \u00b6 getLocalityWait finds locality wait (in milliseconds) for a given scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality]. getLocalityWait uses ROOT:configuration-properties.md#spark.locality.wait[spark.locality.wait] (default: 3s ) when the TaskLocality -specific property is not defined or 0 for NO_PREF and ANY . NOTE: NO_PREF and ANY task localities have no locality wait. .TaskLocalities and Corresponding Spark Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Spark Property | PROCESS_LOCAL | ROOT:configuration-properties.md#spark.locality.wait.process[spark.locality.wait.process] | NODE_LOCAL | ROOT:configuration-properties.md#spark.locality.wait.node[spark.locality.wait.node] | RACK_LOCAL | ROOT:configuration-properties.md#spark.locality.wait.rack[spark.locality.wait.rack] |=== NOTE: getLocalityWait is used when TaskSetManager calculates < >, < > and < >. === [[canFetchMoreResults]] Checking Available Memory For Task Results -- canFetchMoreResults Method [source, scala] \u00b6 canFetchMoreResults(size: Long): Boolean \u00b6 canFetchMoreResults checks whether there is enough memory to fetch the result of a task. Internally, canFetchMoreResults increments the internal < > with the input size (which is the size of the result of a task) and increments the internal < >. If the current internal < > is bigger than the ROOT:configuration-properties.md#maxResultSize[maximum result size], canFetchMoreResults prints out the following ERROR message to the logs: Total size of serialized results of [calculatedTasks] tasks ([totalResultSize]) is bigger than spark.driver.maxResultSize ([maxResultSize]) NOTE: canFetchMoreResults uses ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize] configuration property to control the maximum result size. The default value is 1g . In the end, canFetchMoreResults < > the < > and returns false . Otherwise, canFetchMoreResults returns true . NOTE: canFetchMoreResults is used exclusively when TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueue a successful task].","title":"TaskSetManager"},{"location":"scheduler/TaskSetManager/#tip","text":"Enable DEBUG logging level for org.apache.spark.scheduler.TaskSchedulerImpl (or org.apache.spark.scheduler.cluster.YarnScheduler for YARN) and org.apache.spark.scheduler.TaskSetManager and execute the following two-stage job to see their low-level innerworkings. A cluster manager is recommended since it gives more task localization choices (with YARN additionally supporting rack localization). $ ./bin/spark-shell --master yarn --conf spark.ui.showConsoleProgress=false // Keep # partitions low to keep # messages low scala> sc.parallelize(0 to 9, 3).groupBy(_ % 3).count INFO YarnScheduler: Adding task set 0.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.2.87, executor 1, partition 0, PROCESS_LOCAL, 7541 bytes) INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.2.87, executor 2, partition 1, PROCESS_LOCAL, 7541 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.0.2.87, executor 1, partition 2, PROCESS_LOCAL, 7598 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 518 ms on 10.0.2.87 (executor 1) (1/3) INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 512 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_0.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 51 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool INFO YarnScheduler: Adding task set 1.0 with 3 tasks DEBUG TaskSetManager: Epoch for TaskSet 1.0: 1 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NODE_LOCAL, RACK_LOCAL, ANY DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, 10.0.2.87, executor 2, partition 0, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, 10.0.2.87, executor 1, partition 1, NODE_LOCAL, 7348 bytes) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, 10.0.2.87, executor 1, partition 2, NODE_LOCAL, 7348 bytes) INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 4) in 130 ms on 10.0.2.87 (executor 1) (1/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 1 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level RACK_LOCAL DEBUG TaskSetManager: No tasks for locality level RACK_LOCAL, so moving to locality level ANY INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 133 ms on 10.0.2.87 (executor 2) (2/3) DEBUG YarnScheduler: parentName: , name: TaskSet_1.0, runningTasks: 0 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 5) in 21 ms on 10.0.2.87 (executor 1) (3/3) INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool res0: Long = 3 ==== [[internal-registries]] .TaskSetManager's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[allPendingTasks]] allPendingTasks | Indices of all the pending tasks to execute (regardless of their localization preferences). Updated with an task index when TaskSetManager < >. | [[calculatedTasks]] calculatedTasks | The number of the tasks that have already completed execution. Starts from 0 when a < TaskSetManager is created>> and is only incremented when the < TaskSetManager checks that there is enough memory to fetch a task result>>. | [[copiesRunning]] copiesRunning | The number of task copies currently running per task (index in its task set). The number of task copies of a task is increased when < > or < > and decreased when < > or < > (for a shuffle map stage and no external shuffle service). [[currentLocalityIndex]] currentLocalityIndex | [[epoch]] epoch | Current scheduler:MapOutputTracker.md#getEpoch[map output tracker epoch]. | [[failedExecutors]] failedExecutors | Lookup table of spark-scheduler-TaskInfo.md[TaskInfo] indices that failed to executor ids and the time of the failure. Used in < >. | [[isZombie]] isZombie | Disabled, i.e. false , by default. Read < > in this document. [[lastLaunchTime]] lastLaunchTime [[localityWaits]] localityWaits | [[myLocalityLevels]] myLocalityLevels | scheduler:TaskSchedulerImpl.md#TaskLocality[ TaskLocality locality preferences] of the pending tasks in the < > ranging from PROCESS_LOCAL through NODE_LOCAL , NO_PREF , and RACK_LOCAL to ANY . NOTE: myLocalityLevels may contain only a few of all the available TaskLocality preferences with ANY as a mandatory task locality preference. < > immediately when < TaskSetManager is created>>. < > every change in the status of executors. [[name]] name | [[numFailures]] numFailures | Array of the number of task failures per < >. Incremented when TaskSetManager < > and immediatelly checked if above < >. | [[numTasks]] numTasks | Number of < > to compute. | [[pendingTasksForExecutor]] pendingTasksForExecutor | Lookup table of the indices of tasks pending execution per executor. Updated with an task index and executor when TaskSetManager < > (and the location is a ExecutorCacheTaskLocation or HDFSCacheTaskLocation ). | [[pendingTasksForHost]] pendingTasksForHost | Lookup table of the indices of tasks pending execution per host. Updated with an task index and host when TaskSetManager < >. | [[pendingTasksForRack]] pendingTasksForRack | Lookup table of the indices of tasks pending execution per rack. Updated with an task index and rack when TaskSetManager < >. | [[pendingTasksWithNoPrefs]] pendingTasksWithNoPrefs | Lookup table of the indices of tasks pending execution with no location preferences. Updated with an task index when TaskSetManager < >. [[priority]] priority [[recentExceptions]] recentExceptions | [[runningTasksSet]] runningTasksSet | Collection of running tasks that a TaskSetManager manages. Used to implement < > (that is simply the size of runningTasksSet but a required part of any spark-scheduler-Schedulable.md#contract[Schedulable]). runningTasksSet is expanded when < > and shrinked when < >. Used in scheduler:TaskSchedulerImpl.md#cancelTasks[ TaskSchedulerImpl to cancel tasks]. [[speculatableTasks]] speculatableTasks | [[stageId]] stageId | The stage's id a TaskSetManager runs for. Set when < TaskSetManager is created>>. NOTE: stageId is part of spark-scheduler-Schedulable.md#contract[Schedulable contract]. | [[successful]] successful | Status of < > (with a boolean flag, i.e. true or false , per task). All tasks start with their flags disabled, i.e. false , when < TaskSetManager is created>>. The flag for a task is turned on, i.e. true , when a task finishes < > but also < >. A flag is explicitly turned off only for < ShuffleMapTask tasks when their executor is lost>>. | [[taskAttempts]] taskAttempts | Registry of spark-scheduler-TaskInfo.md[TaskInfos] per every task attempt per task. | [[taskInfos]] taskInfos | Registry of spark-scheduler-TaskInfo.md[TaskInfos] per task id. Updated with the task (id) and the corresponding TaskInfo when TaskSetManager < >. NOTE: It appears that the entires stay forever, i.e. are never removed (perhaps because the maintenance overhead is not needed given a TaskSetManager is a short-lived entity). | [[tasks]] tasks | Lookup table of scheduler:Task.md[Tasks] (per partition id) to schedule execution of. NOTE: The tasks all belong to a single < > that was given when < TaskSetManager was created>> (which actually represent a single scheduler:Stage.md[Stage]). [[tasksSuccessful]] tasksSuccessful | [[totalResultSize]] totalResultSize | The current total size of the result of all the tasks that have finished. Starts from 0 when < TaskSetManager is created>>. Only increased with the size of a task result whenever a TaskSetManager < >. |=== [[logging]] [TIP] ==== Enable DEBUG logging level for org.apache.spark.scheduler.TaskSetManager logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.scheduler.TaskSetManager=DEBUG","title":"[TIP]"},{"location":"scheduler/TaskSetManager/#refer-to-spark-loggingmdlogging","text":"=== [[isTaskBlacklistedOnExecOrNode]] isTaskBlacklistedOnExecOrNode Internal Method","title":"Refer to spark-logging.md[Logging]."},{"location":"scheduler/TaskSetManager/#source-scala","text":"isTaskBlacklistedOnExecOrNode( index: Int, execId: String, host: String): Boolean isTaskBlacklistedOnExecOrNode ...FIXME NOTE: isTaskBlacklistedOnExecOrNode is used when TaskSetManager is requested to < > and < >. === [[getLocalityIndex]] getLocalityIndex Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getlocalityindexlocality-tasklocalitytasklocality-int","text":"getLocalityIndex ...FIXME NOTE: getLocalityIndex is used when TaskSetManager is requested to < > and < >. === [[dequeueSpeculativeTask]] dequeueSpeculativeTask Internal Method","title":"getLocalityIndex(locality: TaskLocality.TaskLocality): Int"},{"location":"scheduler/TaskSetManager/#source-scala_2","text":"dequeueSpeculativeTask( execId: String, host: String, locality: TaskLocality.Value): Option[(Int, TaskLocality.Value)] dequeueSpeculativeTask ...FIXME NOTE: dequeueSpeculativeTask is used exclusively when TaskSetManager is requested to < >. === [[executorAdded]] executorAdded Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#executoradded-unit","text":"executorAdded simply < >. NOTE: executorAdded is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[abortIfCompletelyBlacklisted]] abortIfCompletelyBlacklisted Internal Method","title":"executorAdded(): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_4","text":"abortIfCompletelyBlacklisted( hostToExecutors: HashMap[String, HashSet[String]]): Unit abortIfCompletelyBlacklisted ...FIXME NOTE: abortIfCompletelyBlacklisted is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOffers[resourceOffers]. === [[schedulable]] TaskSetManager is Schedulable TaskSetManager is a spark-scheduler-Schedulable.md[Schedulable] with the following implementation: name is TaskSet_[taskSet.stageId.toString] no parent is ever assigned, i.e. it is always null . + It means that it can only be a leaf in the tree of Schedulables (with spark-scheduler-Pool.md[Pools] being the nodes). schedulingMode always returns SchedulingMode.NONE (since there is nothing to schedule). weight is always 1 . minShare is always 0 . runningTasks is the number of running tasks in the internal runningTasksSet . priority is the priority of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.priority ). stageId is the stage id of the owned scheduler:TaskSet.md[TaskSet] (using taskSet.stageId ). schedulableQueue returns no queue, i.e. null . addSchedulable and removeSchedulable do nothing. getSchedulableByName always returns null . getSortedTaskSetQueue returns a one-element collection with the sole element being itself. < > < > === [[handleTaskGettingResult]] Marking Task As Fetching Indirect Result -- handleTaskGettingResult Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#handletaskgettingresulttid-long-unit","text":"handleTaskGettingResult finds spark-scheduler-TaskInfo.md[TaskInfo] for tid task in < > internal registry and marks it as fetching indirect task result. It then scheduler:DAGScheduler.md#taskGettingResult[notifies DAGScheduler ]. NOTE: handleTaskGettingResult is executed when scheduler:TaskSchedulerImpl.md#handleTaskGettingResult[ TaskSchedulerImpl is notified about fetching indirect task result]. === [[addRunningTask]] Registering Running Task -- addRunningTask Method","title":"handleTaskGettingResult(tid: Long): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#addrunningtasktid-long-unit","text":"addRunningTask adds tid to < > internal registry and spark-scheduler-Pool.md#increaseRunningTasks[requests the parent pool to increase the number of running tasks] (if defined). === [[removeRunningTask]] Unregistering Running Task -- removeRunningTask Method","title":"addRunningTask(tid: Long): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#removerunningtasktid-long-unit","text":"removeRunningTask removes tid from < > internal registry and spark-scheduler-Pool.md#decreaseRunningTasks[requests the parent pool to decrease the number of running task] (if defined). === [[checkSpeculatableTasks]] Checking Speculatable Tasks -- checkSpeculatableTasks Method","title":"removeRunningTask(tid: Long): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#checkspeculatabletasksmintimetospeculation-int-boolean","text":"NOTE: checkSpeculatableTasks is part of the spark-scheduler-Schedulable.md#contract[Schedulable Contract]. checkSpeculatableTasks checks whether there are speculatable tasks in a TaskSet . NOTE: checkSpeculatableTasks is called when for ROOT:speculative-execution-of-tasks.md[]. If the TaskSetManager is < > or has a single task in TaskSet, it assumes no speculatable tasks. The method goes on with the assumption of no speculatable tasks by default. It computes the minimum number of finished tasks for speculation (as ROOT:configuration-properties.md#spark.speculation.quantile[spark.speculation.quantile] of all the finished tasks). You should see the DEBUG message in the logs: DEBUG Checking for speculative tasks: minFinished = [minFinishedForSpeculation] It then checks whether the number is equal or greater than the number of tasks completed successfully (using tasksSuccessful ). Having done that, it computes the median duration of all the successfully completed tasks (using < taskInfos internal registry>>) and task length threshold using the median duration multiplied by ROOT:configuration-properties.md#spark.speculation.multiplier[spark.speculation.multiplier] that has to be equal or less than 100 . You should see the DEBUG message in the logs: DEBUG Task length threshold for speculation: [threshold] For each task (using < taskInfos internal registry>>) that is not marked as successful yet (using successful ) for which there is only one copy running (using copiesRunning ) and the task takes more time than the calculated threshold, but it was not in speculatableTasks it is assumed speculatable . You should see the following INFO message in the logs: INFO Marking task [index] in stage [taskSet.id] (on [info.host]) as speculatable because it ran more than [threshold] ms The task gets added to the internal speculatableTasks collection. The method responds positively. === [[getAllowedLocalityLevel]] getAllowedLocalityLevel Internal Method","title":"checkSpeculatableTasks(minTimeToSpeculation: Int): Boolean"},{"location":"scheduler/TaskSetManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getallowedlocalitylevelcurtime-long-tasklocalitytasklocality","text":"getAllowedLocalityLevel ...FIXME NOTE: getAllowedLocalityLevel is used exclusively when TaskSetManager is requested to < >. === [[resourceOffer]] Finding Task For Execution (Given Resource Offer) -- resourceOffer Method","title":"getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality"},{"location":"scheduler/TaskSetManager/#source-scala_10","text":"resourceOffer( execId: String, host: String, maxLocality: TaskLocality): Option[TaskDescription] (only if < > is defined) resourceOffer requests TaskSetBlacklist to check if the input spark-scheduler-TaskSetBlacklist.md#isExecutorBlacklistedForTaskSet[ execId executor] or spark-scheduler-TaskSetBlacklist.md#isNodeBlacklistedForTaskSet[ host node] are blacklisted. When TaskSetManager is a < > or the resource offer (as executor and host) is blacklisted, resourceOffer finds no tasks to execute (and returns no spark-scheduler-TaskDescription.md[TaskDescription]). NOTE: resourceOffer finds a task to schedule for a resource offer when neither TaskSetManager is a < > nor the resource offer is blacklisted. resourceOffer calculates the allowed task locality for task selection. When the input maxLocality is not NO_PREF task locality, resourceOffer < > (for the current time) and sets it as the current task locality if more localized (specific). NOTE: scheduler:TaskSchedulerImpl.md[TaskLocality] can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . resourceOffer < >. If a task (index) is found, resourceOffer takes the scheduler:Task.md[Task] (from < > registry). resourceOffer scheduler:TaskSchedulerImpl.md#newTaskId[requests TaskSchedulerImpl for the id for the new task]. resourceOffer increments the < > and finds the task attempt number (as the size of < > entries for the task index). resourceOffer spark-scheduler-TaskInfo.md#creating-instance[creates a TaskInfo ] that is then registered in < > and < >. If the maximum acceptable task locality is not NO_PREF , resourceOffer < > (using the task's locality) and records it as < > with the current time as < >. resourceOffer serializes the task. NOTE: resourceOffer uses core:SparkEnv.md#closureSerializer[ SparkEnv to access the closure Serializer ] and serializer:Serializer.md#newInstance[create an instance thereof]. If the task serialization fails, you should see the following ERROR message in the logs: Failed to serialize task [taskId], not attempting to retry it. resourceOffer < TaskSet >> with the following message and reports a TaskNotSerializableException .","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#failed-to-serialize-task-taskid-not-attempting-to-retry-it-exception-during-serialization-exception","text":"resourceOffer checks the size of the serialized task. If it is greater than 100 kB, you should see the following WARN message in the logs:","title":"Failed to serialize task [taskId], not attempting to retry it. Exception during serialization: [exception]"},{"location":"scheduler/TaskSetManager/#optionswrap_1","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#warn-stage-id-contains-a-task-of-very-large-size-size-kb-the-maximum-recommended-task-size-is-100-kb","text":"NOTE: The size of the serializable task, i.e. 100 kB, is not configurable. If however the serialization went well and the size is fine too, resourceOffer < >. You should see the following INFO message in the logs:","title":"WARN Stage [id] contains a task of very large size ([size] KB). The maximum recommended task size is 100 KB."},{"location":"scheduler/TaskSetManager/#optionswrap_2","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-tasksetmanager-starting-name-tid-id-host-executor-id-partition-id-tasklocality-size-bytes","text":"For example:","title":"INFO TaskSetManager: Starting [name] (TID [id], [host], executor [id], partition [id], [taskLocality], [size] bytes)"},{"location":"scheduler/TaskSetManager/#optionswrap_3","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-tasksetmanager-starting-task-10-in-stage-00-tid-1-localhost-partition-1-process_local-2054-bytes","text":"resourceOffer scheduler:DAGScheduler.md#taskStarted[notifies DAGScheduler that the task has been started]. IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has started. NOTE: resourceOffer is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#resourceOfferSingleTaskSet[resourceOfferSingleTaskSet]. === [[dequeueTask]] Dequeueing Task For Execution (Given Locality Information) -- dequeueTask Internal Method","title":"INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 2054 bytes)"},{"location":"scheduler/TaskSetManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#dequeuetaskexecid-string-host-string-maxlocality-tasklocality-optionint-tasklocality-boolean","text":"dequeueTask tries to < > (meeting localization requirements) using < execId executor>>. If a task is found, dequeueTask returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. dequeueTask then goes over all the possible scheduler:TaskSchedulerImpl.md#TaskLocality[task localities] and checks what locality is allowed given the input maxLocality . dequeueTask checks out NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY in that order. For NODE_LOCAL dequeueTask tries to < > (meeting localization requirements) using < host host>> and if found returns its index, NODE_LOCAL task locality and the speculative marker disabled. For NO_PREF dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, PROCESS_LOCAL task locality and the speculative marker disabled. NOTE: For NO_PREF the task locality is PROCESS_LOCAL . For RACK_LOCAL dequeueTask scheduler:TaskSchedulerImpl.md#getRackForHost[finds the rack for the input host ] and if available tries to < > (meeting localization requirements) using < >. If a task is found, dequeueTask returns its index, RACK_LOCAL task locality and the speculative marker disabled. For ANY dequeueTask tries to < > (meeting localization requirements) using < > internal registry and if found returns its index, ANY task locality and the speculative marker disabled. In the end, when no task could be found, dequeueTask < > and if found returns its index, locality and the speculative marker enabled. NOTE: The speculative marker is enabled for a task only when dequeueTask did not manage to find a task for the available task localities and did find a speculative task. NOTE: dequeueTask is used exclusively when TaskSetManager is requested to < >. === [[dequeueTaskFromList]] Finding Higest Task Index (Not Blacklisted, With No Copies Running and Not Completed Already) -- dequeueTaskFromList Internal Method","title":"dequeueTask(execId: String, host: String, maxLocality: TaskLocality): Option[(Int, TaskLocality, Boolean)]"},{"location":"scheduler/TaskSetManager/#source-scala_12","text":"dequeueTaskFromList( execId: String, host: String, list: ArrayBuffer[Int]): Option[Int] dequeueTaskFromList takes task indices from the input list backwards (from the last to the first entry). For every index dequeueTaskFromList checks if it is not < execId executor and host >> and if not, checks that: < > is 0 the task has not been marked as < > If so, dequeueTaskFromList returns the task index. If dequeueTaskFromList has checked all the indices and no index has passed the checks, dequeueTaskFromList returns None (to indicate that no index has met the requirements). NOTE: dequeueTaskFromList is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForExecutor]] Finding Tasks (Indices) Registered For Execution on Executor -- getPendingTasksForExecutor Internal Method","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#source-scala_13","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getpendingtasksforexecutorexecutorid-string-arraybufferint","text":"getPendingTasksForExecutor finds pending tasks (indices) registered for execution on the input executorId executor (in < > internal registry). NOTE: getPendingTasksForExecutor may find no matching tasks and return an empty collection. NOTE: getPendingTasksForExecutor is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForHost]] Finding Tasks (Indices) Registered For Execution on Host -- getPendingTasksForHost Internal Method","title":"getPendingTasksForExecutor(executorId: String): ArrayBuffer[Int]"},{"location":"scheduler/TaskSetManager/#source-scala_14","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getpendingtasksforhosthost-string-arraybufferint","text":"getPendingTasksForHost finds pending tasks (indices) registered for execution on the input host host (in < > internal registry). NOTE: getPendingTasksForHost may find no matching tasks and return an empty collection. NOTE: getPendingTasksForHost is used exclusively when TaskSetManager is requested to < >. === [[getPendingTasksForRack]] Finding Tasks (Indices) Registered For Execution on Rack -- getPendingTasksForRack Internal Method","title":"getPendingTasksForHost(host: String): ArrayBuffer[Int]"},{"location":"scheduler/TaskSetManager/#source-scala_15","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getpendingtasksforrackrack-string-arraybufferint","text":"getPendingTasksForRack finds pending tasks (indices) registered for execution on the input rack rack (in < > internal registry). NOTE: getPendingTasksForRack may find no matching tasks and return an empty collection. NOTE: getPendingTasksForRack is used exclusively when TaskSetManager is requested to < >. === [[scheduling-tasks]] Scheduling Tasks in TaskSet CAUTION: FIXME For each submitted < >, a new TaskSetManager is created. The TaskSetManager completely and exclusively owns a TaskSet submitted for execution. CAUTION: FIXME A picture with TaskSetManager owning TaskSet CAUTION: FIXME What component knows about TaskSet and TaskSetManager. Isn't it that TaskSets are created by DAGScheduler while TaskSetManager is used by TaskSchedulerImpl only? TaskSetManager keeps track of the tasks pending execution per executor, host, rack or with no locality preferences. === [[locality-aware-scheduling]] Locality-Aware Scheduling aka Delay Scheduling TaskSetManager computes locality levels for the TaskSet for delay scheduling. While computing you should see the following DEBUG in the logs: DEBUG Valid locality levels for [taskSet]: [levels] CAUTION: FIXME What's delay scheduling? === [[events]] Events Once a task has finished, TaskSetManager informs scheduler:DAGScheduler.md#taskEnded[DAGScheduler]. CAUTION: FIXME === [[handleSuccessfulTask]] Recording Successful Task And Notifying DAGScheduler -- handleSuccessfulTask Method","title":"getPendingTasksForRack(rack: String): ArrayBuffer[Int]"},{"location":"scheduler/TaskSetManager/#source-scala_16","text":"handleSuccessfulTask( tid: Long, result: DirectTaskResult[_]): Unit handleSuccessfulTask records the tid task as finished, scheduler:DAGScheduler.md#taskEnded[notifies the DAGScheduler that the task has ended] and < TaskSet finished>>. NOTE: handleSuccessfulTask is executed after scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[ TaskSchedulerImpl has been informed that tid task finished successfully (and the task result was deserialized)]. Internally, handleSuccessfulTask finds spark-scheduler-TaskInfo.md[TaskInfo] (in < > internal registry) and marks it as FINISHED . It then removes tid task from < > internal registry. handleSuccessfulTask scheduler:DAGScheduler.md#taskEnded[notifies DAGScheduler that tid task ended successfully] (with the Task object from < > internal registry and the result as Success ). At this point, handleSuccessfulTask finds the other < > of tid task and scheduler:SchedulerBackend.md#killTask[requests SchedulerBackend to kill them] (since they are no longer necessary now when at least one task attempt has completed successfully). You should see the following INFO message in the logs:","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#optionswrap_4","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-killing-attempt-attemptnumber-for-task-id-in-stage-id-tid-id-on-host-as-the-attempt-attemptnumber-succeeded-on-host","text":"CAUTION: FIXME Review taskAttempts If tid has not yet been recorded as < >, handleSuccessfulTask increases < > counter. You should see the following INFO message in the logs:","title":"INFO Killing attempt [attemptNumber] for task [id] in stage [id] (TID [id]) on [host] as the attempt [attemptNumber] succeeded on [host]"},{"location":"scheduler/TaskSetManager/#optionswrap_5","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-finished-task-id-in-stage-id-tid-taskid-in-duration-ms-on-host-executor-executorid-taskssuccessfulnumtasks","text":"tid task is marked as < >. If the number of task that have finished successfully is exactly the number of the tasks to execute (in the TaskSet ), the TaskSetManager becomes a < >. If tid task was already recorded as < >, you should merely see the following INFO message in the logs:","title":"INFO Finished task [id] in stage [id] (TID [taskId]) in [duration] ms on [host] (executor [executorId]) ([tasksSuccessful]/[numTasks])"},{"location":"scheduler/TaskSetManager/#optionswrap_6","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-ignoring-task-finished-event-for-id-in-stage-id-because-task-index-has-already-completed-successfully","text":"Ultimately, handleSuccessfulTask < TaskSet finished>>. NOTE: handleSuccessfulTask is used exclusively when TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#handleSuccessfulTask[handleSuccessfulTask]. === [[maybeFinishTaskSet]] Attempting to Mark TaskSet Finished -- maybeFinishTaskSet Internal Method","title":"INFO Ignoring task-finished event for [id] in stage [id] because task [index] has already completed successfully"},{"location":"scheduler/TaskSetManager/#source-scala_17","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#maybefinishtaskset-unit","text":"maybeFinishTaskSet scheduler:TaskSchedulerImpl.md#taskSetFinished[notifies TaskSchedulerImpl that a TaskSet has finished] when there are no other < > and the < >. === [[task-retries]] Retrying Tasks on Failure CAUTION: FIXME Up to ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] attempts === Task retries and spark.task.maxFailures When you start Spark program you set up ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] for the number of failures that are acceptable until TaskSetManager gives up and marks a job failed. TIP: In Spark shell with local master, ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] is fixed to 1 and you need to use local/spark-local.md[local-with-retries master] to change it to some other value. In the following example, you are going to execute a job with two partitions and keep one failing at all times (by throwing an exception). The aim is to learn the behavior of retrying task execution in a stage in TaskSet. You will only look at a single task execution, namely 0.0 . $ ./bin/spark-shell --master \"local[*, 5]\" ... scala> sc.textFile(\"README.md\", 2).mapPartitionsWithIndex((idx, it) => if (idx == 0) throw new Exception(\"Partition 2 marked failed\") else it).count ... 15/10/27 17:24:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at mapPartitionsWithIndex at <console>:25) 15/10/27 17:24:56 DEBUG DAGScheduler: New pending partitions: Set(0, 1) 15/10/27 17:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) ... 15/10/27 17:24:56 INFO Executor: Running task 0.0 in stage 1.0 (TID 2) ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, localhost, partition 0,PROCESS_LOCAL, 2062 bytes) 15/10/27 17:24:56 INFO Executor: Running task 0.1 in stage 1.0 (TID 4) 15/10/27 17:24:56 INFO HadoopRDD: Input split: file:/Users/jacek/dev/oss/spark/README.md:0+1784 15/10/27 17:24:56 ERROR Executor: Exception in task 0.1 in stage 1.0 (TID 4) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 ERROR Executor: Exception in task 0.4 in stage 1.0 (TID 7) java.lang.Exception: Partition 2 marked failed ... 15/10/27 17:24:56 INFO TaskSetManager: Lost task 0.4 in stage 1.0 (TID 7) on executor localhost: java.lang.Exception (Partition 2 marked failed) [duplicate 4] 15/10/27 17:24:56 ERROR TaskSetManager: Task 0 in stage 1.0 failed 5 times; aborting job 15/10/27 17:24:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 15/10/27 17:24:56 INFO TaskSchedulerImpl: Cancelling stage 1 15/10/27 17:24:56 INFO DAGScheduler: ResultStage 1 (count at <console>:25) failed in 0.058 s 15/10/27 17:24:56 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0 15/10/27 17:24:56 INFO DAGScheduler: Job 1 failed: count at <console>:25, took 0.085810 s org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 5 times, most recent failure: Lost task 0.4 in stage 1.0 (TID 7, localhost): java.lang.Exception: Partition 2 marked failed === [[zombie-state]] Zombie state A TaskSetManager is in zombie state when all tasks in a taskset have completed successfully (regardless of the number of task attempts), or if the taskset has been < >. While in zombie state, a TaskSetManager can launch no new tasks and < TaskDescription to resourceOffers>>. A TaskSetManager remains in the zombie state until all tasks have finished running, i.e. to continue to track and account for the running tasks. === [[abort]] Aborting TaskSet -- abort Method","title":"maybeFinishTaskSet(): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_18","text":"abort( message: String, exception: Option[Throwable] = None): Unit abort informs scheduler:DAGScheduler.md#taskSetFailed[ DAGScheduler that the TaskSet has been aborted]. CAUTION: FIXME image with DAGScheduler call The TaskSetManager enters < >. In the end, abort < TaskSet finished>>.","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#note","text":"abort is used when: TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueueSuccessfulTask] (that has failed) TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#cancelTasks[cancelTasks] and scheduler:TaskSchedulerImpl.md#error[error] TaskSetManager is requested to < >, < >, < >, and < >","title":"[NOTE]"},{"location":"scheduler/TaskSetManager/#driverendpoint-is-requested-to-schedulercoarsegrainedschedulerbackend-driverendpointmdlaunchtaskslaunch-tasks-on-executors","text":"=== [[creating-instance]] Creating TaskSetManager Instance TaskSetManager takes the following to be created: [[sched]] scheduler:TaskSchedulerImpl.md[TaskSchedulerImpl] [[taskSet]] scheduler:TaskSet.md[TaskSet] [[maxTaskFailures]] Number of task failures, i.e. how many times a < > before an entire TaskSet is < > [[blacklistTracker]] (optional) BlacklistTracker (default: None ) [[clock]] Clock (default: SystemClock ) TaskSetManager initializes the < >. NOTE: maxTaskFailures is 1 for local run mode, maxFailures for Spark local-with-retries, and ROOT:configuration-properties.md#spark.task.maxFailures[spark.task.maxFailures] configuration property for Spark local-cluster and Spark with cluster managers (Spark Standalone, Mesos and YARN). TaskSetManager scheduler:MapOutputTracker.md#getEpoch[requests the current epoch from MapOutputTracker ] and sets it on all tasks in the taskset. NOTE: TaskSetManager uses < > (that was given when < >) to scheduler:TaskSchedulerImpl.md#mapOutputTracker[access the current MapOutputTracker ]. You should see the following DEBUG in the logs: DEBUG Epoch for [taskSet]: [epoch] CAUTION: FIXME Why is the epoch important? NOTE: TaskSetManager requests scheduler:TaskSchedulerImpl.md#mapOutputTracker[ MapOutputTracker from TaskSchedulerImpl ] which is likely for unit testing only since core:SparkEnv.md#mapOutputTracker[ MapOutputTracker is available using SparkEnv ]. TaskSetManager < > (in reverse order from the highest partition to the lowest). CAUTION: FIXME Why is reverse order important? The code says it's to execute tasks with low indices first. === [[handleFailedTask]] Getting Notified that Task Failed -- handleFailedTask Method","title":"* DriverEndpoint is requested to scheduler:CoarseGrainedSchedulerBackend-DriverEndpoint.md#launchTasks[launch tasks on executors]"},{"location":"scheduler/TaskSetManager/#source-scala_19","text":"handleFailedTask( tid: Long, state: TaskState, reason: TaskFailedReason): Unit handleFailedTask finds spark-scheduler-TaskInfo.md[TaskInfo] of tid task in < > internal registry and simply quits if the task is already marked as failed or killed. .TaskSetManager Gets Notified that Task Has Failed image::TaskSetManager-handleFailedTask.png[align=\"center\"] NOTE: handleFailedTask is executed after scheduler:TaskSchedulerImpl.md#handleFailedTask[ TaskSchedulerImpl has been informed that tid task failed] or < >. In either case, tasks could not finish successfully or could not report their status back. handleFailedTask < tid task from the internal registry of running tasks>> and then spark-scheduler-TaskInfo.md#markFinished[marks the corresponding TaskInfo as finished] (passing in the input state ). handleFailedTask decrements the number of the running copies of tid task (in < > internal registry). NOTE: With ROOT:speculative-execution-of-tasks.md[] enabled, there can be many copies of a task running simultaneuosly. handleFailedTask uses the following pattern as the reason of the failure: Lost task [id] in stage [taskSetId] (TID [tid], [host], executor [executorId]): [reason] handleFailedTask then calculates the failure exception per the input reason (follow the links for more details): < > < > < > < > NOTE: Description of how the final failure exception is \"computed\" was moved to respective sections below to make the reading slightly more pleasant and comprehensible. handleFailedTask scheduler:DAGScheduler.md#taskEnded[informs DAGScheduler that tid task has ended] (passing on the Task instance from < > internal registry, the input reason , null result, calculated accumUpdates per failure, and the spark-scheduler-TaskInfo.md[TaskInfo]). IMPORTANT: This is the moment when TaskSetManager informs DAGScheduler that a task has ended. If tid task has already been marked as completed (in < > internal registry) you should see the following INFO message in the logs:","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#optionswrap_7","text":"","title":"[options=\"wrap\"]"},{"location":"scheduler/TaskSetManager/#info-task-id-in-stage-id-tid-tid-failed-but-the-task-will-not-be-re-executed-either-because-the-task-failed-with-a-shuffle-data-fetch-failure-so-the-previous-stage-needs-to-be-re-run-or-because-a-different-copy-of-the-task-has-already-succeeded","text":"TIP: Read up on ROOT:speculative-execution-of-tasks.md[] to find out why a single task could be executed multiple times. If however tid task was not recorded as < >, handleFailedTask < >. If the TaskSetManager is not a < > and the task failed reason should be counted towards the maximum number of times the task is allowed to fail before the stage is aborted (i.e. TaskFailedReason.countTowardsTaskFailures attribute is enabled), the optional spark-scheduler-TaskSetBlacklist.md#updateBlacklistForFailedTask[ TaskSetBlacklist is notified] (passing on the host, executor and the task's index). handleFailedTask then increments the < > for tid task and checks if the number of failures is equal or greater than the < TaskSet >> (as defined when the < TaskSetManager was created>>). If so, i.e. the number of task failures of tid task reached the maximum value, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] failed [maxTaskFailures] times; aborting job And handleFailedTask < TaskSet >> with the following message and then quits: Task [index] in stage [id] failed [maxTaskFailures] times, most recent failure: [failureReason] In the end (except when the number of failures of tid task grew beyond the acceptable number), handleFailedTask < TaskSet as finished>>.","title":"INFO Task [id] in stage [id] (TID [tid]) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded)."},{"location":"scheduler/TaskSetManager/#note_1","text":"handleFailedTask is used when: TaskSchedulerImpl is requested to scheduler:TaskSchedulerImpl.md#handleFailedTask[handle a failed task]","title":"[NOTE]"},{"location":"scheduler/TaskSetManager/#tasksetmanager-is-requested-to-and","text":"==== [[handleFailedTask-FetchFailed]] FetchFailed TaskFailedReason For FetchFailed you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Unless tid has already been marked as successful (in < > internal registry), it becomes so and the < TaskSet >> gets increased. The TaskSetManager enters < >. The failure exception is empty. ==== [[handleFailedTask-ExceptionFailure]] ExceptionFailure TaskFailedReason For ExceptionFailure , handleFailedTask checks if the exception is of type NotSerializableException . If so, you should see the following ERROR message in the logs: ERROR Task [id] in stage [id] (TID [tid]) had a not serializable result: [description]; not retrying And handleFailedTask < TaskSet >> and then quits. Otherwise, if the exception is not of type NotSerializableException , handleFailedTask accesses accumulators and calculates whether to print the WARN message (with the failure reason) or the INFO message. If the failure has already been reported (and is therefore a duplication), ROOT:configuration-properties.md#spark.logging.exceptionPrintInterval[spark.logging.exceptionPrintInterval] is checked before reprinting the duplicate exception in its entirety. For full printout of the ExceptionFailure , the following WARN appears in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] Otherwise, the following INFO appears in the logs: INFO Lost task [id] in stage [id] (TID [tid]) on [host], executor [id]: [className] ([description]) [duplicate [dupCount]] The exception in ExceptionFailure becomes the failure exception. ==== [[handleFailedTask-ExecutorLostFailure]] ExecutorLostFailure TaskFailedReason For ExecutorLostFailure if not exitCausedByApp , you should see the following INFO in the logs: INFO Task [tid] failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task. The failure exception is empty. ==== [[handleFailedTask-TaskFailedReason]] Other TaskFailedReasons For the other TaskFailedReasons, you should see the following WARN message in the logs: WARN Lost task [id] in stage [id] (TID [tid], [host], executor [id]): [reason] The failure exception is empty. === [[addPendingTask]] Registering Task As Pending Execution (Per Preferred Locations) -- addPendingTask Internal Method","title":"* TaskSetManager is requested to &lt;&gt; and &lt;&gt;"},{"location":"scheduler/TaskSetManager/#source-scala_20","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#addpendingtaskindex-int-unit","text":"addPendingTask registers a index task in the pending-task lists that the task should be eventually scheduled to (per its preferred locations). Internally, addPendingTask takes the scheduler:Task.md#preferredLocations[preferred locations of the task] (given index ) and registers the task in the internal pending-task registries for every preferred location: < > when the scheduler:TaskLocation.md[TaskLocation] is ExecutorCacheTaskLocation . < > for the hosts of a scheduler:TaskLocation.md[TaskLocation]. < > for the scheduler:TaskSchedulerImpl.md#getRackForHost[racks from TaskSchedulerImpl per the host] (of a scheduler:TaskLocation.md[TaskLocation]). For a scheduler:TaskLocation.md[TaskLocation] being HDFSCacheTaskLocation , addPendingTask scheduler:TaskSchedulerImpl.md#getExecutorsAliveOnHost[requests TaskSchedulerImpl for the executors on the host] (of a preferred location) and registers the task in < > for every executor (if available). You should see the following INFO message in the logs: INFO Pending task [index] has a cached location at [host] , where there are executors [executors] When addPendingTask could not find executors for a HDFSCacheTaskLocation preferred location, you should see the following DEBUG message in the logs: DEBUG Pending task [index] has a cached location at [host] , but there are no executors alive there. If the task has no location preferences, addPendingTask registers it in < >. addPendingTask always registers the task in < >. NOTE: addPendingTask is used immediatelly when TaskSetManager < > and later when handling a < > or < >. === [[executorLost]] Re-enqueuing ShuffleMapTasks (with no ExternalShuffleService) and Reporting All Running Tasks on Lost Executor as Failed -- executorLost Method","title":"addPendingTask(index: Int): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_21","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#executorlostexecid-string-host-string-reason-executorlossreason-unit","text":"executorLost re-enqueues all the scheduler:ShuffleMapTask.md[ShuffleMapTasks] that have completed already on the lost executor (when deploy:ExternalShuffleService.md[external shuffle service] is not in use) and < >. NOTE: executorLost is part of the spark-scheduler-Schedulable.md#contract[Schedulable contract] that scheduler:TaskSchedulerImpl.md#removeExecutor[ TaskSchedulerImpl uses to inform TaskSetManagers about lost executors]. NOTE: Since TaskSetManager manages execution of the tasks in a single scheduler:TaskSet.md[TaskSet], when an executor gets lost, the affected tasks that have been running on the failed executor need to be re-enqueued. executorLost is the mechanism to \"announce\" the event to all TaskSetManagers . Internally, executorLost first checks whether the < > are scheduler:ShuffleMapTask.md[ShuffleMapTasks] and whether an deploy:ExternalShuffleService.md[external shuffle service] is enabled (that could serve the map shuffle outputs in case of failure). NOTE: executorLost checks out the first task in < > as it is assumed the other belong to the same stage. If the task is a scheduler:ShuffleMapTask.md[ShuffleMapTask], the entire < > is for a scheduler:ShuffleMapStage.md[ShuffleMapStage]. NOTE: executorLost uses core:SparkEnv.md#blockManager[ SparkEnv to access the current BlockManager ] and finds out whether an storage:BlockManager.md#externalShuffleServiceEnabled[external shuffle service is enabled] or not (based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). If executorLost is indeed due to an executor lost that executed tasks for a scheduler:ShuffleMapStage.md[ShuffleMapStage] (that this TaskSetManager manages) and no external shuffle server is enabled, executorLost finds < > that were scheduled on this lost executor and marks the < > as not executed yet. NOTE: executorLost uses records every tasks on the lost executor in < > (as false ) and decrements < >, and < > for every task. executorLost < > and scheduler:DAGScheduler.md#taskEnded[informs DAGScheduler that the tasks (on the lost executor) have ended] (with scheduler:DAGScheduler.md#handleTaskCompletion-Resubmitted[Resubmitted] reason). NOTE: executorLost uses scheduler:TaskSchedulerImpl.md#dagScheduler[ TaskSchedulerImpl to access the DAGScheduler ]. TaskSchedulerImpl is given when the < TaskSetManager was created>>. Regardless of whether this TaskSetManager manages ShuffleMapTasks or not (it could also manage scheduler:ResultTask.md[ResultTasks]) and whether the external shuffle service is used or not, executorLost finds all < > on this lost executor and < > (with the task state FAILED ). NOTE: executorLost finds out if the reason for the executor lost is due to application fault, i.e. assumes ExecutorExited 's exit status as the indicator, ExecutorKilled for non-application's fault and any other reason is an application fault. executorLost < >. === [[recomputeLocality]] Recomputing Task Locality Preferences -- recomputeLocality Method","title":"executorLost(execId: String, host: String, reason: ExecutorLossReason): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_22","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#recomputelocality-unit","text":"recomputeLocality recomputes the internal caches: < >, < > and < >. CAUTION: FIXME But why are the caches important (and have to be recomputed)? recomputeLocality records the current scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality] level of this TaskSetManager (that is < > in < >). NOTE: TaskLocality is one of PROCESS_LOCAL , NODE_LOCAL , NO_PREF , RACK_LOCAL and ANY values. recomputeLocality < > and saves the result in < > internal cache. recomputeLocality computes < > (by < > for every locality level in < > internal cache). In the end, recomputeLocality < > of the previous locality level and records it in < >. NOTE: recomputeLocality is used when TaskSetManager gets notified about status change in executors, i.e. when an executor is < > or < >. === [[computeValidLocalityLevels]] Computing Locality Levels (for Scheduled Tasks) -- computeValidLocalityLevels Internal Method","title":"recomputeLocality(): Unit"},{"location":"scheduler/TaskSetManager/#source-scala_23","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#computevalidlocalitylevels-arraytasklocality","text":"computeValidLocalityLevels computes valid locality levels for tasks that were registered in corresponding registries per locality level. NOTE: scheduler:TaskSchedulerImpl.md[TaskLocality] is a task locality preference and can be the most localized PROCESS_LOCAL , NODE_LOCAL through NO_PREF and RACK_LOCAL to ANY . .TaskLocalities and Corresponding Internal Registries [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Internal Registry | PROCESS_LOCAL | < > | NODE_LOCAL | < > | NO_PREF | < > | RACK_LOCAL | < > |=== computeValidLocalityLevels walks over every internal registry and if it is not empty < > for the corresponding TaskLocality and proceeds with it only when the locality wait is not 0 . For TaskLocality with pending tasks, computeValidLocalityLevels asks TaskSchedulerImpl whether there is at least one executor alive (for scheduler:TaskSchedulerImpl.md#isExecutorAlive[PROCESS_LOCAL], scheduler:TaskSchedulerImpl.md#hasExecutorsAliveOnHost[NODE_LOCAL] and scheduler:TaskSchedulerImpl.md#hasHostAliveOnRack[RACK_LOCAL]) and if so registers the TaskLocality . NOTE: computeValidLocalityLevels uses < > that was given when < TaskSetManager was created>>. computeValidLocalityLevels always registers ANY task locality level. In the end, you should see the following DEBUG message in the logs: DEBUG TaskSetManager: Valid locality levels for [taskSet]: [comma-separated levels] NOTE: computeValidLocalityLevels is used when TaskSetManager < > and later to < >. === [[getLocalityWait]] Finding Locality Wait -- getLocalityWait Internal Method","title":"computeValidLocalityLevels(): Array[TaskLocality]"},{"location":"scheduler/TaskSetManager/#source-scala_24","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#getlocalitywaitlevel-tasklocality-long","text":"getLocalityWait finds locality wait (in milliseconds) for a given scheduler:TaskSchedulerImpl.md#TaskLocality[TaskLocality]. getLocalityWait uses ROOT:configuration-properties.md#spark.locality.wait[spark.locality.wait] (default: 3s ) when the TaskLocality -specific property is not defined or 0 for NO_PREF and ANY . NOTE: NO_PREF and ANY task localities have no locality wait. .TaskLocalities and Corresponding Spark Properties [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TaskLocality | Spark Property | PROCESS_LOCAL | ROOT:configuration-properties.md#spark.locality.wait.process[spark.locality.wait.process] | NODE_LOCAL | ROOT:configuration-properties.md#spark.locality.wait.node[spark.locality.wait.node] | RACK_LOCAL | ROOT:configuration-properties.md#spark.locality.wait.rack[spark.locality.wait.rack] |=== NOTE: getLocalityWait is used when TaskSetManager calculates < >, < > and < >. === [[canFetchMoreResults]] Checking Available Memory For Task Results -- canFetchMoreResults Method","title":"getLocalityWait(level: TaskLocality): Long"},{"location":"scheduler/TaskSetManager/#source-scala_25","text":"","title":"[source, scala]"},{"location":"scheduler/TaskSetManager/#canfetchmoreresultssize-long-boolean","text":"canFetchMoreResults checks whether there is enough memory to fetch the result of a task. Internally, canFetchMoreResults increments the internal < > with the input size (which is the size of the result of a task) and increments the internal < >. If the current internal < > is bigger than the ROOT:configuration-properties.md#maxResultSize[maximum result size], canFetchMoreResults prints out the following ERROR message to the logs: Total size of serialized results of [calculatedTasks] tasks ([totalResultSize]) is bigger than spark.driver.maxResultSize ([maxResultSize]) NOTE: canFetchMoreResults uses ROOT:configuration-properties.md#spark.driver.maxResultSize[spark.driver.maxResultSize] configuration property to control the maximum result size. The default value is 1g . In the end, canFetchMoreResults < > the < > and returns false . Otherwise, canFetchMoreResults returns true . NOTE: canFetchMoreResults is used exclusively when TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueue a successful task].","title":"canFetchMoreResults(size: Long): Boolean"},{"location":"scheduler/spark-scheduler-StageInfo/","text":"= StageInfo == [[fromStage]] fromStage Method [source, scala] \u00b6 fromStage( stage: Stage, attemptId: Int, numTasks: Option[Int] = None, taskMetrics: TaskMetrics = null, taskLocalityPreferences: Seq[Seq[TaskLocation]] fromStage...FIXME fromStage is used when Stage is created (and initializes the scheduler:Stage.md#_latestInfo[StageInfo]) and scheduler:Stage.md#makeNewStageAttempt[makeNewStageAttempt].","title":"StageInfo"},{"location":"scheduler/spark-scheduler-StageInfo/#source-scala","text":"fromStage( stage: Stage, attemptId: Int, numTasks: Option[Int] = None, taskMetrics: TaskMetrics = null, taskLocalityPreferences: Seq[Seq[TaskLocation]] fromStage...FIXME fromStage is used when Stage is created (and initializes the scheduler:Stage.md#_latestInfo[StageInfo]) and scheduler:Stage.md#makeNewStageAttempt[makeNewStageAttempt].","title":"[source, scala]"},{"location":"serializer/","text":"Serialization System \u00b6 Serialization System is a core component of Apache Spark with pluggable serializers for RDD and shuffle data. Serialization System uses SerializerManager to select the Serializer to use (based on spark.serializer configuration property).","title":"Serialization System"},{"location":"serializer/#serialization-system","text":"Serialization System is a core component of Apache Spark with pluggable serializers for RDD and shuffle data. Serialization System uses SerializerManager to select the Serializer to use (based on spark.serializer configuration property).","title":"Serialization System"},{"location":"serializer/DeserializationStream/","text":"= DeserializationStream DeserializationStream is an abstraction of streams for reading serialized objects. == [[readObject]] readObject Method [source, scala] \u00b6 readObject T: ClassTag : T \u00b6 readObject...FIXME readObject is used when...FIXME == [[readKey]] readKey Method [source, scala] \u00b6 readKey T: ClassTag : T \u00b6 readKey < > representing the key of a key-value record. readKey is used when...FIXME == [[readValue]] readValue Method [source, scala] \u00b6 readValue T: ClassTag : T \u00b6 readValue < > representing the value of a key-value record. readValue is used when...FIXME == [[asIterator]] asIterator Method [source, scala] \u00b6 asIterator: Iterator[Any] \u00b6 asIterator...FIXME asIterator is used when...FIXME == [[asKeyValueIterator]] asKeyValueIterator Method [source, scala] \u00b6 asKeyValueIterator: Iterator[Any] \u00b6 asKeyValueIterator...FIXME asKeyValueIterator is used when...FIXME","title":"DeserializationStream"},{"location":"serializer/DeserializationStream/#source-scala","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#readobjectt-classtag-t","text":"readObject...FIXME readObject is used when...FIXME == [[readKey]] readKey Method","title":"readObjectT: ClassTag: T"},{"location":"serializer/DeserializationStream/#source-scala_1","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#readkeyt-classtag-t","text":"readKey < > representing the key of a key-value record. readKey is used when...FIXME == [[readValue]] readValue Method","title":"readKeyT: ClassTag: T"},{"location":"serializer/DeserializationStream/#source-scala_2","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#readvaluet-classtag-t","text":"readValue < > representing the value of a key-value record. readValue is used when...FIXME == [[asIterator]] asIterator Method","title":"readValueT: ClassTag: T"},{"location":"serializer/DeserializationStream/#source-scala_3","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#asiterator-iteratorany","text":"asIterator...FIXME asIterator is used when...FIXME == [[asKeyValueIterator]] asKeyValueIterator Method","title":"asIterator: Iterator[Any]"},{"location":"serializer/DeserializationStream/#source-scala_4","text":"","title":"[source, scala]"},{"location":"serializer/DeserializationStream/#askeyvalueiterator-iteratorany","text":"asKeyValueIterator...FIXME asKeyValueIterator is used when...FIXME","title":"asKeyValueIterator: Iterator[Any]"},{"location":"serializer/SerializationStream/","text":"= SerializationStream SerializationStream is an abstraction of < > for writing serialized key-value records. == [[implementations]] Available SerializationStreams [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | SerializationStream | Description | JavaSerializationStream | [[JavaSerializationStream]] | KryoSerializationStream | [[KryoSerializationStream]] |=== == [[writeAll]] Writing All Records [source, scala] \u00b6 writeAll T: ClassTag : SerializationStream writeAll < > of the given iterator. writeAll is used when: ReliableCheckpointRDD utility is requested to writePartitionToCheckpointFile SerializerManager is requested to serializer:SerializerManager.md#dataSerializeStream[dataSerializeStream] and serializer:SerializerManager.md#dataSerializeWithExplicitClassTag[dataSerializeWithExplicitClassTag] == [[writeObject]] Writing Object [source, scala] \u00b6 writeObject T: ClassTag : SerializationStream writeObject is the most general-purpose method to write an object. writeObject is used when...FIXME == [[writeKey]] Writing Key (of Key-Value Record) [source, scala] \u00b6 writeKey T: ClassTag : SerializationStream writeKey < > representing the key of a key-value record. writeKey is used when: UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#insertRecordIntoSorter[insert a record into a ShuffleExternalSorter] DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#write[write the key and value of a record] == [[writeValue]] Writing Value (of Key-Value Record) [source, scala] \u00b6 writeValue T: ClassTag : SerializationStream writeValue < > representing the value of a key-value record. writeValue is used when: UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#insertRecordIntoSorter[insert a record into a ShuffleExternalSorter] DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#write[write the key and value of a record] == [[flush]] Flushing Stream [source, scala] \u00b6 flush(): Unit \u00b6 flush is used when: UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#insertRecordIntoSorter[insert a record into a ShuffleExternalSorter] DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#commitAndGet[commitAndGet]","title":"SerializationStream"},{"location":"serializer/SerializationStream/#source-scala","text":"writeAll T: ClassTag : SerializationStream writeAll < > of the given iterator. writeAll is used when: ReliableCheckpointRDD utility is requested to writePartitionToCheckpointFile SerializerManager is requested to serializer:SerializerManager.md#dataSerializeStream[dataSerializeStream] and serializer:SerializerManager.md#dataSerializeWithExplicitClassTag[dataSerializeWithExplicitClassTag] == [[writeObject]] Writing Object","title":"[source, scala]"},{"location":"serializer/SerializationStream/#source-scala_1","text":"writeObject T: ClassTag : SerializationStream writeObject is the most general-purpose method to write an object. writeObject is used when...FIXME == [[writeKey]] Writing Key (of Key-Value Record)","title":"[source, scala]"},{"location":"serializer/SerializationStream/#source-scala_2","text":"writeKey T: ClassTag : SerializationStream writeKey < > representing the key of a key-value record. writeKey is used when: UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#insertRecordIntoSorter[insert a record into a ShuffleExternalSorter] DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#write[write the key and value of a record] == [[writeValue]] Writing Value (of Key-Value Record)","title":"[source, scala]"},{"location":"serializer/SerializationStream/#source-scala_3","text":"writeValue T: ClassTag : SerializationStream writeValue < > representing the value of a key-value record. writeValue is used when: UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#insertRecordIntoSorter[insert a record into a ShuffleExternalSorter] DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#write[write the key and value of a record] == [[flush]] Flushing Stream","title":"[source, scala]"},{"location":"serializer/SerializationStream/#source-scala_4","text":"","title":"[source, scala]"},{"location":"serializer/SerializationStream/#flush-unit","text":"flush is used when: UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#insertRecordIntoSorter[insert a record into a ShuffleExternalSorter] DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#commitAndGet[commitAndGet]","title":"flush(): Unit"},{"location":"serializer/Serializer/","text":"Serializer \u00b6 Serializer is an abstraction of < > that are intended to be used to serialize/de-serialize data in a single Spark application. == [[implementations]] Available Serializers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Serializer | Description | JavaSerializer | [[JavaSerializer]] | KryoSerializer | [[KryoSerializer]] |=== == [[newInstance]] newInstance Method [source, scala] \u00b6 newInstance(): SerializerInstance \u00b6 newInstance...FIXME newInstance is used when...FIXME == [[setDefaultClassLoader]] setDefaultClassLoader Method [source, scala] \u00b6 setDefaultClassLoader( classLoader: ClassLoader): Serializer setDefaultClassLoader...FIXME setDefaultClassLoader is used when...FIXME == [[supportsRelocationOfSerializedObjects]] supportsRelocationOfSerializedObjects Property [source, scala] \u00b6 supportsRelocationOfSerializedObjects: Boolean \u00b6 supportsRelocationOfSerializedObjects should be enabled (i.e. true) only when reordering the bytes of serialized objects in serialization stream output is equivalent to having re-ordered those elements prior to serializing them. supportsRelocationOfSerializedObjects is disabled ( false ) by default. NOTE: KryoSerializer uses autoReset for supportsRelocationOfSerializedObjects. NOTE: supportsRelocationOfSerializedObjects is enabled in UnsafeRowSerializer .","title":"Serializer"},{"location":"serializer/Serializer/#serializer","text":"Serializer is an abstraction of < > that are intended to be used to serialize/de-serialize data in a single Spark application. == [[implementations]] Available Serializers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Serializer | Description | JavaSerializer | [[JavaSerializer]] | KryoSerializer | [[KryoSerializer]] |=== == [[newInstance]] newInstance Method","title":"Serializer"},{"location":"serializer/Serializer/#source-scala","text":"","title":"[source, scala]"},{"location":"serializer/Serializer/#newinstance-serializerinstance","text":"newInstance...FIXME newInstance is used when...FIXME == [[setDefaultClassLoader]] setDefaultClassLoader Method","title":"newInstance(): SerializerInstance"},{"location":"serializer/Serializer/#source-scala_1","text":"setDefaultClassLoader( classLoader: ClassLoader): Serializer setDefaultClassLoader...FIXME setDefaultClassLoader is used when...FIXME == [[supportsRelocationOfSerializedObjects]] supportsRelocationOfSerializedObjects Property","title":"[source, scala]"},{"location":"serializer/Serializer/#source-scala_2","text":"","title":"[source, scala]"},{"location":"serializer/Serializer/#supportsrelocationofserializedobjects-boolean","text":"supportsRelocationOfSerializedObjects should be enabled (i.e. true) only when reordering the bytes of serialized objects in serialization stream output is equivalent to having re-ordered those elements prior to serializing them. supportsRelocationOfSerializedObjects is disabled ( false ) by default. NOTE: KryoSerializer uses autoReset for supportsRelocationOfSerializedObjects. NOTE: supportsRelocationOfSerializedObjects is enabled in UnsafeRowSerializer .","title":"supportsRelocationOfSerializedObjects: Boolean"},{"location":"serializer/SerializerInstance/","text":"= SerializerInstance SerializerInstance is an abstraction of instances of a serializer, for use by one thread at a time. == [[serialize]] serialize Method [source, scala] \u00b6 serialize T: ClassTag : ByteBuffer serialize...FIXME serialize is used when...FIXME == [[deserialize]] deserialize Method [source, scala] \u00b6 deserialize T: ClassTag : T deserialize T: ClassTag : T deserialize...FIXME deserialize is used when...FIXME == [[serializeStream]] Serializing Output Stream [source, scala] \u00b6 serializeStream( s: OutputStream): SerializationStream serializeStream...FIXME serializeStream is used when...FIXME == [[deserializeStream]] Deserializing Input Stream [source, scala] \u00b6 deserializeStream( s: InputStream): DeserializationStream deserializeStream...FIXME deserializeStream is used when...FIXME","title":"SerializerInstance"},{"location":"serializer/SerializerInstance/#source-scala","text":"serialize T: ClassTag : ByteBuffer serialize...FIXME serialize is used when...FIXME == [[deserialize]] deserialize Method","title":"[source, scala]"},{"location":"serializer/SerializerInstance/#source-scala_1","text":"deserialize T: ClassTag : T deserialize T: ClassTag : T deserialize...FIXME deserialize is used when...FIXME == [[serializeStream]] Serializing Output Stream","title":"[source, scala]"},{"location":"serializer/SerializerInstance/#source-scala_2","text":"serializeStream( s: OutputStream): SerializationStream serializeStream...FIXME serializeStream is used when...FIXME == [[deserializeStream]] Deserializing Input Stream","title":"[source, scala]"},{"location":"serializer/SerializerInstance/#source-scala_3","text":"deserializeStream( s: InputStream): DeserializationStream deserializeStream...FIXME deserializeStream is used when...FIXME","title":"[source, scala]"},{"location":"serializer/SerializerManager/","text":"SerializerManager \u00b6 SerializerManager is used to < > for shuffle blocks (either the default < > or < > based on the key and value of a record). == [[creating-instance]] Creating Instance SerializerManager takes the following to be created: [[defaultSerializer]] serializer:Serializer.md[Serializer] [[conf]] ROOT:SparkConf.md[SparkConf] [[encryptionKey]] Optional encryption key ( [Array[Byte]] ) SerializerManager is created when SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv for the driver and executors]. == [[SparkEnv]] Accessing SerializerManager Using SparkEnv SerializerManager is available using core:SparkEnv.md#serializerManager[SparkEnv] on the driver and executors. [source, scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.serializerManager == [[kryoSerializer]] KryoSerializer SerializerManager creates a KryoSerializer when created. KryoSerializer is used as a < > when the type of a given key and value is < >. == [[wrapForCompression]] Wrapping Input or Output Stream of Block for Compression [source, scala] \u00b6 wrapForCompression( blockId: BlockId, s: OutputStream): OutputStream wrapForCompression( blockId: BlockId, s: InputStream): InputStream wrapForCompression...FIXME wrapForCompression is used when: SerializerManager is requested to < >, < >, < > and < > SerializedValuesHolder (of storage:MemoryStore.md[MemoryStore]) is requested for a SerializationStream == [[wrapStream]] Wrapping Input or Output Stream for Block [source, scala] \u00b6 wrapStream( blockId: BlockId, s: InputStream): InputStream wrapStream( blockId: BlockId, s: OutputStream): OutputStream wrapStream...FIXME wrapStream is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] DiskMapIterator (of shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap]) is requested for nextBatchStream SpillReader (of shuffle:ExternalSorter.md[ExternalSorter]) is requested for nextBatchStream memory:UnsafeSorterSpillReader.md[UnsafeSorterSpillReader] is created DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#open[open] == [[dataSerializeStream]] dataSerializeStream Method [source, scala] \u00b6 dataSerializeStream T: ClassTag : Unit dataSerializeStream...FIXME dataSerializeStream is used when BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator] and storage:BlockManager.md#dropFromMemory[dropFromMemory]. == [[dataSerializeWithExplicitClassTag]] dataSerializeWithExplicitClassTag Method [source, scala] \u00b6 dataSerializeWithExplicitClassTag( blockId: BlockId, values: Iterator[ ], classTag: ClassTag[ ]): ChunkedByteBuffer dataSerializeWithExplicitClassTag...FIXME dataSerializeWithExplicitClassTag is used when BlockManager is requested to storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes]. == [[dataDeserializeStream]] dataDeserializeStream Method [source, scala] \u00b6 dataDeserializeStream T (classTag: ClassTag[T]): Iterator[T] dataDeserializeStream...FIXME dataDeserializeStream is used when: BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#getRemoteValues[getRemoteValues] and storage:BlockManager.md#doPutBytes[doPutBytes] MemoryStore is requested to storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes] (when PartiallySerializedBlock is requested for a PartiallyUnrolledIterator) == [[getSerializer]] Selecting Serializer [source, scala] \u00b6 getSerializer( ct: ClassTag[ ], autoPick: Boolean): Serializer getSerializer( keyClassTag: ClassTag[ ], valueClassTag: ClassTag[_]): Serializer getSerializer returns the < > when the given arguments are < >. Otherwise, getSerializer returns the < >. getSerializer is used when: ShuffledRDD is requested for rdd:ShuffledRDD.md#getDependencies[dependencies]. SerializerManager is requested to < >, < > and < > SerializedValuesHolder (of storage:MemoryStore.md[MemoryStore]) is requested for a SerializationStream == [[canUseKryo]] Checking Whether Kryo Serializer Could Be Used [source, scala] \u00b6 canUseKryo( ct: ClassTag[_]): Boolean canUseKryo is true when the given ClassTag is a primitive, an array of primitives or a String. Otherwise, canUseKryo is false . canUseKryo is used when SerializerManager is requested for a < >. == [[shouldCompress]] shouldCompress Method [source, scala] \u00b6 shouldCompress( blockId: BlockId): Boolean shouldCompress...FIXME shouldCompress is used when SerializerManager is requested to < >.","title":"SerializerManager"},{"location":"serializer/SerializerManager/#serializermanager","text":"SerializerManager is used to < > for shuffle blocks (either the default < > or < > based on the key and value of a record). == [[creating-instance]] Creating Instance SerializerManager takes the following to be created: [[defaultSerializer]] serializer:Serializer.md[Serializer] [[conf]] ROOT:SparkConf.md[SparkConf] [[encryptionKey]] Optional encryption key ( [Array[Byte]] ) SerializerManager is created when SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv for the driver and executors]. == [[SparkEnv]] Accessing SerializerManager Using SparkEnv SerializerManager is available using core:SparkEnv.md#serializerManager[SparkEnv] on the driver and executors.","title":"SerializerManager"},{"location":"serializer/SerializerManager/#source-scala","text":"import org.apache.spark.SparkEnv SparkEnv.get.serializerManager == [[kryoSerializer]] KryoSerializer SerializerManager creates a KryoSerializer when created. KryoSerializer is used as a < > when the type of a given key and value is < >. == [[wrapForCompression]] Wrapping Input or Output Stream of Block for Compression","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_1","text":"wrapForCompression( blockId: BlockId, s: OutputStream): OutputStream wrapForCompression( blockId: BlockId, s: InputStream): InputStream wrapForCompression...FIXME wrapForCompression is used when: SerializerManager is requested to < >, < >, < > and < > SerializedValuesHolder (of storage:MemoryStore.md[MemoryStore]) is requested for a SerializationStream == [[wrapStream]] Wrapping Input or Output Stream for Block","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_2","text":"wrapStream( blockId: BlockId, s: InputStream): InputStream wrapStream( blockId: BlockId, s: OutputStream): OutputStream wrapStream...FIXME wrapStream is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] DiskMapIterator (of shuffle:ExternalAppendOnlyMap.md[ExternalAppendOnlyMap]) is requested for nextBatchStream SpillReader (of shuffle:ExternalSorter.md[ExternalSorter]) is requested for nextBatchStream memory:UnsafeSorterSpillReader.md[UnsafeSorterSpillReader] is created DiskBlockObjectWriter is requested to storage:DiskBlockObjectWriter.md#open[open] == [[dataSerializeStream]] dataSerializeStream Method","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_3","text":"dataSerializeStream T: ClassTag : Unit dataSerializeStream...FIXME dataSerializeStream is used when BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator] and storage:BlockManager.md#dropFromMemory[dropFromMemory]. == [[dataSerializeWithExplicitClassTag]] dataSerializeWithExplicitClassTag Method","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_4","text":"dataSerializeWithExplicitClassTag( blockId: BlockId, values: Iterator[ ], classTag: ClassTag[ ]): ChunkedByteBuffer dataSerializeWithExplicitClassTag...FIXME dataSerializeWithExplicitClassTag is used when BlockManager is requested to storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes]. == [[dataDeserializeStream]] dataDeserializeStream Method","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_5","text":"dataDeserializeStream T (classTag: ClassTag[T]): Iterator[T] dataDeserializeStream...FIXME dataDeserializeStream is used when: BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#getRemoteValues[getRemoteValues] and storage:BlockManager.md#doPutBytes[doPutBytes] MemoryStore is requested to storage:MemoryStore.md#putIteratorAsBytes[putIteratorAsBytes] (when PartiallySerializedBlock is requested for a PartiallyUnrolledIterator) == [[getSerializer]] Selecting Serializer","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_6","text":"getSerializer( ct: ClassTag[ ], autoPick: Boolean): Serializer getSerializer( keyClassTag: ClassTag[ ], valueClassTag: ClassTag[_]): Serializer getSerializer returns the < > when the given arguments are < >. Otherwise, getSerializer returns the < >. getSerializer is used when: ShuffledRDD is requested for rdd:ShuffledRDD.md#getDependencies[dependencies]. SerializerManager is requested to < >, < > and < > SerializedValuesHolder (of storage:MemoryStore.md[MemoryStore]) is requested for a SerializationStream == [[canUseKryo]] Checking Whether Kryo Serializer Could Be Used","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_7","text":"canUseKryo( ct: ClassTag[_]): Boolean canUseKryo is true when the given ClassTag is a primitive, an array of primitives or a String. Otherwise, canUseKryo is false . canUseKryo is used when SerializerManager is requested for a < >. == [[shouldCompress]] shouldCompress Method","title":"[source, scala]"},{"location":"serializer/SerializerManager/#source-scala_8","text":"shouldCompress( blockId: BlockId): Boolean shouldCompress...FIXME shouldCompress is used when SerializerManager is requested to < >.","title":"[source, scala]"},{"location":"shuffle/","text":"Shuffle System \u00b6 Shuffle System is a core service of Apache Spark that is responsible for shuffle block management. The core abstraction is ShuffleManager with the default and only known implementation being SortShuffleManager . spark.shuffle.manager configuration property allows for a custom ShuffleManager . Resources \u00b6 Improving Apache Spark Downscaling by Christopher Crosbie (Google) Ben Sidhom (Google) Spark shuffle introduction by Raymond Liu (aka colorant )","title":"Shuffle System"},{"location":"shuffle/#shuffle-system","text":"Shuffle System is a core service of Apache Spark that is responsible for shuffle block management. The core abstraction is ShuffleManager with the default and only known implementation being SortShuffleManager . spark.shuffle.manager configuration property allows for a custom ShuffleManager .","title":"Shuffle System"},{"location":"shuffle/#resources","text":"Improving Apache Spark Downscaling by Christopher Crosbie (Google) Ben Sidhom (Google) Spark shuffle introduction by Raymond Liu (aka colorant )","title":"Resources"},{"location":"shuffle/BaseShuffleHandle/","text":"BaseShuffleHandle \u2014 Fallback Shuffle Handle \u00b6 BaseShuffleHandle is a ShuffleHandle that is created solely to capture the parameters when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ): [[shuffleId]] shuffleId [[numMaps]] numMaps [[dependency]] ShuffleDependency NOTE: BaseShuffleHandle is the last possible choice when SortShuffleManager.md#registerShuffle[ SortShuffleManager is requested for a ShuffleHandle ] (after shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle] and shuffle:SerializedShuffleHandle.md[SerializedShuffleHandle] have already been considered and failed the check). Demo \u00b6 // Start a Spark application, e.g. spark-shell, with the Spark properties to trigger selection of BaseShuffleHandle: // 1. spark.shuffle.spill.numElementsForceSpillThreshold=1 // 2. spark.shuffle.sort.bypassMergeThreshold=1 // numSlices > spark.shuffle.sort.bypassMergeThreshold scala> val rdd = sc.parallelize(0 to 4, numSlices = 2).groupBy(_ % 2) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle 0 because an aggregator is defined res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@1160c54b) scala> rdd.getNumPartitions res1: Int = 2 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@1160c54b // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 2 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.BaseShuffleHandle@22b0fe7e","title":"BaseShuffleHandle"},{"location":"shuffle/BaseShuffleHandle/#baseshufflehandle-fallback-shuffle-handle","text":"BaseShuffleHandle is a ShuffleHandle that is created solely to capture the parameters when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ): [[shuffleId]] shuffleId [[numMaps]] numMaps [[dependency]] ShuffleDependency NOTE: BaseShuffleHandle is the last possible choice when SortShuffleManager.md#registerShuffle[ SortShuffleManager is requested for a ShuffleHandle ] (after shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle] and shuffle:SerializedShuffleHandle.md[SerializedShuffleHandle] have already been considered and failed the check).","title":"BaseShuffleHandle &mdash; Fallback Shuffle Handle"},{"location":"shuffle/BaseShuffleHandle/#demo","text":"// Start a Spark application, e.g. spark-shell, with the Spark properties to trigger selection of BaseShuffleHandle: // 1. spark.shuffle.spill.numElementsForceSpillThreshold=1 // 2. spark.shuffle.sort.bypassMergeThreshold=1 // numSlices > spark.shuffle.sort.bypassMergeThreshold scala> val rdd = sc.parallelize(0 to 4, numSlices = 2).groupBy(_ % 2) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle 0 because an aggregator is defined res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@1160c54b) scala> rdd.getNumPartitions res1: Int = 2 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@1160c54b // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 2 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.BaseShuffleHandle@22b0fe7e","title":"Demo"},{"location":"shuffle/BlockStoreShuffleReader/","text":"== [[BlockStoreShuffleReader]] BlockStoreShuffleReader BlockStoreShuffleReader is the one and only known spark-shuffle-ShuffleReader.md[ShuffleReader] that < > (for a range of < > and < > reduce partitions) from a shuffle by requesting them from block managers. BlockStoreShuffleReader is < > exclusively when SortShuffleManager is requested for the SortShuffleManager.md#getReader[ShuffleReader] for a range of reduce partitions. === [[read]] Reading Combined Records For Reduce Task [source, scala] \u00b6 read(): Iterator[Product2[K, C]] \u00b6 NOTE: read is part of spark-shuffle-ShuffleReader.md#read[ShuffleReader Contract]. Internally, read first storage:ShuffleBlockFetcherIterator.md#creating-instance[creates a ShuffleBlockFetcherIterator ] (passing in the values of < >, < > and < > Spark properties). NOTE: read uses storage:BlockManager.md#shuffleClient[ BlockManager to access ShuffleClient ] to create ShuffleBlockFetcherIterator . NOTE: read uses scheduler:MapOutputTracker.md#getMapSizesByExecutorId[ MapOutputTracker to find the BlockManagers with the shuffle blocks and sizes] to create ShuffleBlockFetcherIterator . read creates a new serializer:SerializerInstance.md[SerializerInstance] (using Serializer from ShuffleDependency ). read creates a key/value iterator by deserializeStream every shuffle block stream. read updates the spark-TaskContext.md#taskMetrics[context task metrics] for each record read. NOTE: read uses CompletionIterator (to count the records read) and spark-InterruptibleIterator.md[InterruptibleIterator] (to support task cancellation). If the ShuffleDependency has an Aggregator defined , read wraps the current iterator inside an iterator defined by Aggregator.combineCombinersByKey (for mapSideCombine enabled ) or Aggregator.combineValuesByKey otherwise. NOTE: run reports an exception when ShuffleDependency has no Aggregator defined with mapSideCombine flag enabled . For keyOrdering defined in the ShuffleDependency , run does the following: shuffle:ExternalSorter.md#creating-instance[Creates an ExternalSorter ] shuffle:ExternalSorter.md#insertAll[Inserts all the records] into the ExternalSorter Updates context TaskMetrics Returns a CompletionIterator for the ExternalSorter === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_reducer_maxSizeInFlight]] spark.reducer.maxSizeInFlight | 48m | Maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task. Since each output requires a new buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory. Used when < BlockStoreShuffleReader creates a ShuffleBlockFetcherIterator to read records>>. | [[spark_reducer_maxReqsInFlight]] spark.reducer.maxReqsInFlight | (unlimited) | The maximum number of remote requests to fetch blocks at any given point. When the number of hosts in the cluster increases, it might lead to very large number of in-bound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated. Used when < BlockStoreShuffleReader creates a ShuffleBlockFetcherIterator to read records>>. | [[spark_shuffle_detectCorrupt]] spark.shuffle.detectCorrupt | true | Controls whether to detect any corruption in fetched blocks. Used when < BlockStoreShuffleReader creates a ShuffleBlockFetcherIterator to read records>>. |=== === [[creating-instance]] Creating BlockStoreShuffleReader Instance BlockStoreShuffleReader takes the following when created: [[handle]] spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle] [[startPartition]] Reduce start partition index [[endPartition]] Reduce end partition index [[context]] spark-TaskContext.md[TaskContext] [[serializerManager]] serializer:SerializerManager.md[SerializerManager] [[blockManager]] storage:BlockManager.md[BlockManager] [[mapOutputTracker]] scheduler:MapOutputTracker.md[MapOutputTracker] BlockStoreShuffleReader initializes the < >. NOTE: BlockStoreShuffleReader uses SparkEnv to access the < >, < > and < >.","title":"BlockStoreShuffleReader"},{"location":"shuffle/BlockStoreShuffleReader/#source-scala","text":"","title":"[source, scala]"},{"location":"shuffle/BlockStoreShuffleReader/#read-iteratorproduct2k-c","text":"NOTE: read is part of spark-shuffle-ShuffleReader.md#read[ShuffleReader Contract]. Internally, read first storage:ShuffleBlockFetcherIterator.md#creating-instance[creates a ShuffleBlockFetcherIterator ] (passing in the values of < >, < > and < > Spark properties). NOTE: read uses storage:BlockManager.md#shuffleClient[ BlockManager to access ShuffleClient ] to create ShuffleBlockFetcherIterator . NOTE: read uses scheduler:MapOutputTracker.md#getMapSizesByExecutorId[ MapOutputTracker to find the BlockManagers with the shuffle blocks and sizes] to create ShuffleBlockFetcherIterator . read creates a new serializer:SerializerInstance.md[SerializerInstance] (using Serializer from ShuffleDependency ). read creates a key/value iterator by deserializeStream every shuffle block stream. read updates the spark-TaskContext.md#taskMetrics[context task metrics] for each record read. NOTE: read uses CompletionIterator (to count the records read) and spark-InterruptibleIterator.md[InterruptibleIterator] (to support task cancellation). If the ShuffleDependency has an Aggregator defined , read wraps the current iterator inside an iterator defined by Aggregator.combineCombinersByKey (for mapSideCombine enabled ) or Aggregator.combineValuesByKey otherwise. NOTE: run reports an exception when ShuffleDependency has no Aggregator defined with mapSideCombine flag enabled . For keyOrdering defined in the ShuffleDependency , run does the following: shuffle:ExternalSorter.md#creating-instance[Creates an ExternalSorter ] shuffle:ExternalSorter.md#insertAll[Inserts all the records] into the ExternalSorter Updates context TaskMetrics Returns a CompletionIterator for the ExternalSorter === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_reducer_maxSizeInFlight]] spark.reducer.maxSizeInFlight | 48m | Maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task. Since each output requires a new buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory. Used when < BlockStoreShuffleReader creates a ShuffleBlockFetcherIterator to read records>>. | [[spark_reducer_maxReqsInFlight]] spark.reducer.maxReqsInFlight | (unlimited) | The maximum number of remote requests to fetch blocks at any given point. When the number of hosts in the cluster increases, it might lead to very large number of in-bound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated. Used when < BlockStoreShuffleReader creates a ShuffleBlockFetcherIterator to read records>>. | [[spark_shuffle_detectCorrupt]] spark.shuffle.detectCorrupt | true | Controls whether to detect any corruption in fetched blocks. Used when < BlockStoreShuffleReader creates a ShuffleBlockFetcherIterator to read records>>. |=== === [[creating-instance]] Creating BlockStoreShuffleReader Instance BlockStoreShuffleReader takes the following when created: [[handle]] spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle] [[startPartition]] Reduce start partition index [[endPartition]] Reduce end partition index [[context]] spark-TaskContext.md[TaskContext] [[serializerManager]] serializer:SerializerManager.md[SerializerManager] [[blockManager]] storage:BlockManager.md[BlockManager] [[mapOutputTracker]] scheduler:MapOutputTracker.md[MapOutputTracker] BlockStoreShuffleReader initializes the < >. NOTE: BlockStoreShuffleReader uses SparkEnv to access the < >, < > and < >.","title":"read(): Iterator[Product2[K, C]]"},{"location":"shuffle/BypassMergeSortShuffleHandle/","text":"BypassMergeSortShuffleHandle \u2014 Marker Interface for Bypass Merge Sort Shuffle Handles \u00b6 BypassMergeSortShuffleHandles is a BaseShuffleHandle with no additional methods or fields and serves only to identify the choice of bypass merge sort shuffle . Like BaseShuffleHandle , BypassMergeSortShuffleHandles takes shuffleId , numMaps , and a ShuffleDependency . BypassMergeSortShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ). Demo \u00b6 scala> val rdd = sc.parallelize(0 to 8).groupBy(_ % 3) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@655875bb) scala> rdd.getNumPartitions res1: Int = 8 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@655875bb // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // spark.shuffle.sort.bypassMergeThreshold == 200 // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 8 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.sort.BypassMergeSortShuffleHandle@68893394","title":"BypassMergeSortShuffleHandle"},{"location":"shuffle/BypassMergeSortShuffleHandle/#bypassmergesortshufflehandle-marker-interface-for-bypass-merge-sort-shuffle-handles","text":"BypassMergeSortShuffleHandles is a BaseShuffleHandle with no additional methods or fields and serves only to identify the choice of bypass merge sort shuffle . Like BaseShuffleHandle , BypassMergeSortShuffleHandles takes shuffleId , numMaps , and a ShuffleDependency . BypassMergeSortShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ).","title":"BypassMergeSortShuffleHandle &mdash; Marker Interface for Bypass Merge Sort Shuffle Handles"},{"location":"shuffle/BypassMergeSortShuffleHandle/#demo","text":"scala> val rdd = sc.parallelize(0 to 8).groupBy(_ % 3) rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24 scala> rdd.dependencies res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@655875bb) scala> rdd.getNumPartitions res1: Int = 8 scala> import org.apache.spark.ShuffleDependency import org.apache.spark.ShuffleDependency scala> val shuffleDep = rdd.dependencies(0).asInstanceOf[ShuffleDependency[Int, Int, Int]] shuffleDep: org.apache.spark.ShuffleDependency[Int,Int,Int] = org.apache.spark.ShuffleDependency@655875bb // mapSideCombine is disabled scala> shuffleDep.mapSideCombine res2: Boolean = false // aggregator defined scala> shuffleDep.aggregator res3: Option[org.apache.spark.Aggregator[Int,Int,Int]] = Some(Aggregator(<function1>,<function2>,<function2>)) // spark.shuffle.sort.bypassMergeThreshold == 200 // the number of reduce partitions < spark.shuffle.sort.bypassMergeThreshold scala> shuffleDep.partitioner.numPartitions res4: Int = 8 scala> shuffleDep.shuffleHandle res5: org.apache.spark.shuffle.ShuffleHandle = org.apache.spark.shuffle.sort.BypassMergeSortShuffleHandle@68893394","title":"Demo"},{"location":"shuffle/BypassMergeSortShuffleWriter/","text":"= BypassMergeSortShuffleWriter BypassMergeSortShuffleWriter is a shuffle:ShuffleWriter.md[ShuffleWriter] for scheduler:ShuffleMapTask.md[ShuffleMapTasks] to < >. .BypassMergeSortShuffleWriter and DiskBlockObjectWriters image::BypassMergeSortShuffleWriter-write.png[align=\"center\"] == [[creating-instance]] Creating Instance BypassMergeSortShuffleWriter takes the following to be created: [[blockManager]] storage:BlockManager.md[BlockManager] < > [[handle]] shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle ] [[mapId]] Map ID [[taskContext]] scheduler:spark-TaskContext.md[TaskContext] [[conf]] ROOT:SparkConf.md[SparkConf] BypassMergeSortShuffleWriter is created when SortShuffleManager is requested for a SortShuffleManager.md#getWriter[ShuffleWriter] (for a < >). == [[partitionWriters]] Per-Partition DiskBlockObjectWriters BypassMergeSortShuffleWriter uses storage:DiskBlockObjectWriter.md[DiskBlockObjectWriter] when...FIXME == [[shuffleBlockResolver]] IndexShuffleBlockResolver BypassMergeSortShuffleWriter is given a shuffle:IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] to be created. BypassMergeSortShuffleWriter uses the IndexShuffleBlockResolver for < > (to shuffle:IndexShuffleBlockResolver.md#writeIndexFileAndCommit[writeIndexFileAndCommit] and shuffle:IndexShuffleBlockResolver.md#getDataFile[getDataFile]). == [[serializer]] Serializer When created, BypassMergeSortShuffleWriter requests the shuffle:spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] (of the given < >) for the Serializer . BypassMergeSortShuffleWriter creates a new instance of the Serializer for < >. == [[configuration-properties]] Configuration Properties === [[fileBufferSize]][[spark.shuffle.file.buffer]] spark.shuffle.file.buffer BypassMergeSortShuffleWriter uses ROOT:configuration-properties.md#spark.shuffle.file.buffer[spark.shuffle.file.buffer] configuration property for...FIXME === [[transferToEnabled]][[spark.file.transferTo]] spark.file.transferTo BypassMergeSortShuffleWriter uses ROOT:configuration-properties.md#spark.file.transferTo[spark.file.transferTo] configuration property to control whether to use Java New I/O while < >. == [[write]] Writing Records to Shuffle File [source, java] \u00b6 void write( Iterator > records) write creates a new instance of the < >. write initializes the < > and < > internal registries (for storage:DiskBlockObjectWriter.md[DiskBlockObjectWriters] and FileSegments for < >, respectively). write requests the < > for the storage:BlockManager.md#diskBlockManager[DiskBlockManager] and for < > write requests it for a storage:DiskBlockManager.md#createTempShuffleBlock[shuffle block ID and the file]. write creates a storage:BlockManager.md#getDiskWriter[DiskBlockObjectWriter] for the shuffle block (using the < >). write stores the reference to DiskBlockObjectWriters in the < > internal registry. After all DiskBlockObjectWriters are created, write requests the < > to executor:ShuffleWriteMetrics.md#incWriteTime[increment shuffle write time metric]. For every record (a key-value pair), write requests the < > for the rdd:Partitioner.md#getPartition[partition ID] for the key. The partition ID is then used as an index of the partition writer (among the < >) to storage:DiskBlockObjectWriter.md#write[write the current record out to a block file]. Once all records have been writted out to their respective block files, write does the following for every < >: . Requests the DiskBlockObjectWriter to storage:DiskBlockObjectWriter.md#commitAndGet[commit and return a corresponding FileSegment of the shuffle block] . Saves the (reference to) FileSegments in the < > internal registry . Requests the DiskBlockObjectWriter to storage:DiskBlockObjectWriter.md#close[close] NOTE: At this point, all the records are in shuffle block files on a local disk. The records are split across block files by key. write requests the < > for the shuffle:IndexShuffleBlockResolver.md#getDataFile[shuffle file] for the < > and the < >. write creates a temporary file (based on the name of the shuffle file) and < >. The size of every per-partition shuffle files is saved as the < > internal registry. NOTE: At this point, all the per-partition shuffle block files are one single map shuffle data file. write requests the < > to shuffle:IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files] for the < > and the < > (with the < > and the temporary shuffle output file). write returns a scheduler:MapStatus.md[shuffle map output status] (with the storage:BlockManager.md#shuffleServerId[shuffle server ID] and the < >). write is part of shuffle:ShuffleWriter.md#write[ShuffleWriter] abstraction. === [[write-no-records]] No Records When there is no records to write out, write initializes the < > internal array (of < > size) with all elements being 0. write requests the < > to shuffle:IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files], but the difference (compared to when there are records to write) is that the dataTmp argument is simply null . write sets the internal mapStatus (with the address of storage:BlockManager.md[BlockManager] in use and < >). === [[write-requirements]] Requirements write requires that there are no < >. == [[writePartitionedFile]] Concatenating Per-Partition Files Into Single File [source, java] \u00b6 long[] writePartitionedFile( File outputFile) writePartitionedFile creates a file output stream for the input outputFile in append mode. writePartitionedFile starts tracking write time (as writeStartTime ). For every < > partition, writePartitionedFile takes the file from the FileSegment (from < >) and creates a file input stream to read raw bytes. writePartitionedFile then < outputFile >> (possibly using Java New I/O per < > flag set when < >) and records the length of the shuffle data file (in lengths internal array). In the end, writePartitionedFile executor:ShuffleWriteMetrics.md#incWriteTime[increments shuffle write time], clears < > array and returns the lengths of the shuffle data files per partition. writePartitionedFile is used when BypassMergeSortShuffleWriter is requested to < >. == [[copyStream]] Copying Raw Bytes Between Input Streams [source, scala] \u00b6 copyStream( in: InputStream, out: OutputStream, closeStreams: Boolean = false, transferToEnabled: Boolean = false): Long copyStream branches off depending on the type of in and out streams, i.e. whether they are both FileInputStream with transferToEnabled input flag is enabled. If they are both FileInputStream with transferToEnabled enabled, copyStream gets their FileChannels and transfers bytes from the input file to the output file and counts the number of bytes, possibly zero, that were actually transferred. NOTE: copyStream uses Java's {java-javadoc-url}/java/nio/channels/FileChannel.html[java.nio.channels.FileChannel] to manage file channels. If either in and out input streams are not FileInputStream or transferToEnabled flag is disabled (default), copyStream reads data from in to write to out and counts the number of bytes written. copyStream can optionally close in and out streams (depending on the input closeStreams -- disabled by default). NOTE: Utils.copyStream is used when < > (among other places). TIP: Visit the official web site of https://jcp.org/jsr/detail/51.jsp[JSR 51: New I/O APIs for the Java Platform] and read up on {java-javadoc-url}/java/nio/package-summary.html[java.nio package]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | numPartitions | [[numPartitions]] | partitionWriterSegments | [[partitionWriterSegments]] | mapStatus | [[mapStatus]] scheduler:MapStatus.md[MapStatus] that < > Initialized every time BypassMergeSortShuffleWriter < >. Used when < > (with success enabled) as a marker if < > and < >. | partitionLengths | [[partitionLengths]] Temporary array of partition lengths after records are < >. Initialized every time BypassMergeSortShuffleWriter < > before passing it in to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[IndexShuffleBlockResolver]). After IndexShuffleBlockResolver finishes, it is used to initialize < > internal property. |===","title":"BypassMergeSortShuffleWriter"},{"location":"shuffle/BypassMergeSortShuffleWriter/#source-java","text":"void write( Iterator > records) write creates a new instance of the < >. write initializes the < > and < > internal registries (for storage:DiskBlockObjectWriter.md[DiskBlockObjectWriters] and FileSegments for < >, respectively). write requests the < > for the storage:BlockManager.md#diskBlockManager[DiskBlockManager] and for < > write requests it for a storage:DiskBlockManager.md#createTempShuffleBlock[shuffle block ID and the file]. write creates a storage:BlockManager.md#getDiskWriter[DiskBlockObjectWriter] for the shuffle block (using the < >). write stores the reference to DiskBlockObjectWriters in the < > internal registry. After all DiskBlockObjectWriters are created, write requests the < > to executor:ShuffleWriteMetrics.md#incWriteTime[increment shuffle write time metric]. For every record (a key-value pair), write requests the < > for the rdd:Partitioner.md#getPartition[partition ID] for the key. The partition ID is then used as an index of the partition writer (among the < >) to storage:DiskBlockObjectWriter.md#write[write the current record out to a block file]. Once all records have been writted out to their respective block files, write does the following for every < >: . Requests the DiskBlockObjectWriter to storage:DiskBlockObjectWriter.md#commitAndGet[commit and return a corresponding FileSegment of the shuffle block] . Saves the (reference to) FileSegments in the < > internal registry . Requests the DiskBlockObjectWriter to storage:DiskBlockObjectWriter.md#close[close] NOTE: At this point, all the records are in shuffle block files on a local disk. The records are split across block files by key. write requests the < > for the shuffle:IndexShuffleBlockResolver.md#getDataFile[shuffle file] for the < > and the < >. write creates a temporary file (based on the name of the shuffle file) and < >. The size of every per-partition shuffle files is saved as the < > internal registry. NOTE: At this point, all the per-partition shuffle block files are one single map shuffle data file. write requests the < > to shuffle:IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files] for the < > and the < > (with the < > and the temporary shuffle output file). write returns a scheduler:MapStatus.md[shuffle map output status] (with the storage:BlockManager.md#shuffleServerId[shuffle server ID] and the < >). write is part of shuffle:ShuffleWriter.md#write[ShuffleWriter] abstraction. === [[write-no-records]] No Records When there is no records to write out, write initializes the < > internal array (of < > size) with all elements being 0. write requests the < > to shuffle:IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files], but the difference (compared to when there are records to write) is that the dataTmp argument is simply null . write sets the internal mapStatus (with the address of storage:BlockManager.md[BlockManager] in use and < >). === [[write-requirements]] Requirements write requires that there are no < >. == [[writePartitionedFile]] Concatenating Per-Partition Files Into Single File","title":"[source, java]"},{"location":"shuffle/BypassMergeSortShuffleWriter/#source-java_1","text":"long[] writePartitionedFile( File outputFile) writePartitionedFile creates a file output stream for the input outputFile in append mode. writePartitionedFile starts tracking write time (as writeStartTime ). For every < > partition, writePartitionedFile takes the file from the FileSegment (from < >) and creates a file input stream to read raw bytes. writePartitionedFile then < outputFile >> (possibly using Java New I/O per < > flag set when < >) and records the length of the shuffle data file (in lengths internal array). In the end, writePartitionedFile executor:ShuffleWriteMetrics.md#incWriteTime[increments shuffle write time], clears < > array and returns the lengths of the shuffle data files per partition. writePartitionedFile is used when BypassMergeSortShuffleWriter is requested to < >. == [[copyStream]] Copying Raw Bytes Between Input Streams","title":"[source, java]"},{"location":"shuffle/BypassMergeSortShuffleWriter/#source-scala","text":"copyStream( in: InputStream, out: OutputStream, closeStreams: Boolean = false, transferToEnabled: Boolean = false): Long copyStream branches off depending on the type of in and out streams, i.e. whether they are both FileInputStream with transferToEnabled input flag is enabled. If they are both FileInputStream with transferToEnabled enabled, copyStream gets their FileChannels and transfers bytes from the input file to the output file and counts the number of bytes, possibly zero, that were actually transferred. NOTE: copyStream uses Java's {java-javadoc-url}/java/nio/channels/FileChannel.html[java.nio.channels.FileChannel] to manage file channels. If either in and out input streams are not FileInputStream or transferToEnabled flag is disabled (default), copyStream reads data from in to write to out and counts the number of bytes written. copyStream can optionally close in and out streams (depending on the input closeStreams -- disabled by default). NOTE: Utils.copyStream is used when < > (among other places). TIP: Visit the official web site of https://jcp.org/jsr/detail/51.jsp[JSR 51: New I/O APIs for the Java Platform] and read up on {java-javadoc-url}/java/nio/package-summary.html[java.nio package]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"shuffle/BypassMergeSortShuffleWriter/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/BypassMergeSortShuffleWriter/#log4jloggerorgapachesparkshufflesortbypassmergesortshufflewriterall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | numPartitions | [[numPartitions]] | partitionWriterSegments | [[partitionWriterSegments]] | mapStatus | [[mapStatus]] scheduler:MapStatus.md[MapStatus] that < > Initialized every time BypassMergeSortShuffleWriter < >. Used when < > (with success enabled) as a marker if < > and < >. | partitionLengths | [[partitionLengths]] Temporary array of partition lengths after records are < >. Initialized every time BypassMergeSortShuffleWriter < > before passing it in to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[IndexShuffleBlockResolver]). After IndexShuffleBlockResolver finishes, it is used to initialize < > internal property. |===","title":"log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ALL"},{"location":"shuffle/ExternalAppendOnlyMap/","text":"ExternalAppendOnlyMap \u00b6 ExternalAppendOnlyMap is a Spillable of SizeTrackers. ExternalAppendOnlyMap[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. Creating Instance \u00b6 ExternalAppendOnlyMap takes the following to be created: [[createCombiner]] createCombiner function ( V => C ) [[mergeValue]] mergeValue function ( (C, V) => C ) [[mergeCombiners]] mergeCombiners function ( (C, C) => C ) [[serializer]] Optional serializer:Serializer.md[Serializer] (default: core:SparkEnv.md#serializer[system Serializer]) [[blockManager]] Optional storage:BlockManager.md[BlockManager] (default: core:SparkEnv.md#blockManager[system BlockManager]) [[context]] Optional scheduler:spark-TaskContext.md[TaskContext] (default: scheduler:spark-TaskContext.md#get[current TaskContext]) [[serializerManager]] Optional serializer:SerializerManager.md[SerializerManager] (default: core:SparkEnv.md#serializerManager[system SerializerManager]) ExternalAppendOnlyMap is created when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition == [[currentMap]] SizeTrackingAppendOnlyMap ExternalAppendOnlyMap manages a SizeTrackingAppendOnlyMap. A SizeTrackingAppendOnlyMap is created immediately when ExternalAppendOnlyMap is and every time when < > and < > spilled to disk. SizeTrackingAppendOnlyMap are dereferenced ( null ed) for the memory to be garbage-collected when < > and < >. SizeTrackingAppendOnlyMap is used when < >, < >, < > and < >. == [[insertAll]] Inserting All Key-Value Pairs (from Iterator) [source, scala] \u00b6 insertAll( entries: Iterator[Product2[K, V]]): Unit [[insertAll-update-function]] insertAll creates an update function that uses the < > function for an existing value or the < > function for a new value. For every key-value pair (from the input iterator), insertAll does the following: Requests the < > for the estimated size and, if greater than the <<_peakMemoryUsedBytes, _peakMemoryUsedBytes>> metric, updates it. shuffle:Spillable.md#maybeSpill[Spills to a disk if necessary] and, if spilled, creates a new < > Requests the < > to change value for the current value (with the < > function) shuffle:Spillable.md#addElementsRead[Increments the elements read counter] === [[insertAll-usage]] Usage insertAll is used when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition ExternalAppendOnlyMap is requested to < > === [[insertAll-requirements]] Requirements insertAll throws an IllegalStateException when the < > internal registry is null : [source,plaintext] \u00b6 Cannot insert new elements into a map after calling iterator \u00b6 == [[iterator]] Iterator of \"Combined\" Pairs [source, scala] \u00b6 iterator: Iterator[(K, C)] \u00b6 iterator...FIXME iterator is used when...FIXME == [[spill]] Spilling to Disk if Necessary [source, scala] \u00b6 spill( collection: SizeTracker): Unit spill...FIXME spill is used when...FIXME == [[forceSpill]] Forcing Disk Spilling [source, scala] \u00b6 forceSpill(): Boolean \u00b6 forceSpill returns a flag to indicate whether spilling to disk has really happened ( true ) or not ( false ). forceSpill branches off per the current state it is in (and should rather use a state-aware implementation). When a < > is in use, forceSpill requests it to spill and, if it did, dereferences ( null ify) the < >. forceSpill returns whatever the spilling of the < > returned. When there is at least one element in the < >, forceSpill < > it. forceSpill then creates a new < > and always returns true . In other cases, forceSpill simply returns false . forceSpill is part of the shuffle:Spillable.md[Spillable] abstraction. == [[freeCurrentMap]] Freeing Up SizeTrackingAppendOnlyMap and Releasing Memory [source, scala] \u00b6 freeCurrentMap(): Unit \u00b6 freeCurrentMap dereferences ( null ify) the < > (if there still was one) followed by shuffle:Spillable.md#releaseMemory[releasing all memory]. freeCurrentMap is used when SpillableIterator is requested to destroy itself. == [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Method [source, scala] \u00b6 spillMemoryIteratorToDisk( inMemoryIterator: Iterator[(K, C)]): DiskMapIterator spillMemoryIteratorToDisk...FIXME spillMemoryIteratorToDisk is used when...FIXME","title":"ExternalAppendOnlyMap"},{"location":"shuffle/ExternalAppendOnlyMap/#externalappendonlymap","text":"ExternalAppendOnlyMap is a Spillable of SizeTrackers. ExternalAppendOnlyMap[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values.","title":"ExternalAppendOnlyMap"},{"location":"shuffle/ExternalAppendOnlyMap/#creating-instance","text":"ExternalAppendOnlyMap takes the following to be created: [[createCombiner]] createCombiner function ( V => C ) [[mergeValue]] mergeValue function ( (C, V) => C ) [[mergeCombiners]] mergeCombiners function ( (C, C) => C ) [[serializer]] Optional serializer:Serializer.md[Serializer] (default: core:SparkEnv.md#serializer[system Serializer]) [[blockManager]] Optional storage:BlockManager.md[BlockManager] (default: core:SparkEnv.md#blockManager[system BlockManager]) [[context]] Optional scheduler:spark-TaskContext.md[TaskContext] (default: scheduler:spark-TaskContext.md#get[current TaskContext]) [[serializerManager]] Optional serializer:SerializerManager.md[SerializerManager] (default: core:SparkEnv.md#serializerManager[system SerializerManager]) ExternalAppendOnlyMap is created when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition == [[currentMap]] SizeTrackingAppendOnlyMap ExternalAppendOnlyMap manages a SizeTrackingAppendOnlyMap. A SizeTrackingAppendOnlyMap is created immediately when ExternalAppendOnlyMap is and every time when < > and < > spilled to disk. SizeTrackingAppendOnlyMap are dereferenced ( null ed) for the memory to be garbage-collected when < > and < >. SizeTrackingAppendOnlyMap is used when < >, < >, < > and < >. == [[insertAll]] Inserting All Key-Value Pairs (from Iterator)","title":"Creating Instance"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala","text":"insertAll( entries: Iterator[Product2[K, V]]): Unit [[insertAll-update-function]] insertAll creates an update function that uses the < > function for an existing value or the < > function for a new value. For every key-value pair (from the input iterator), insertAll does the following: Requests the < > for the estimated size and, if greater than the <<_peakMemoryUsedBytes, _peakMemoryUsedBytes>> metric, updates it. shuffle:Spillable.md#maybeSpill[Spills to a disk if necessary] and, if spilled, creates a new < > Requests the < > to change value for the current value (with the < > function) shuffle:Spillable.md#addElementsRead[Increments the elements read counter] === [[insertAll-usage]] Usage insertAll is used when: Aggregator is requested to rdd:Aggregator.md#combineValuesByKey[combineValuesByKey] and rdd:Aggregator.md#combineCombinersByKey[combineCombinersByKey] CoGroupedRDD is requested to compute a partition ExternalAppendOnlyMap is requested to < > === [[insertAll-requirements]] Requirements insertAll throws an IllegalStateException when the < > internal registry is null :","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/ExternalAppendOnlyMap/#cannot-insert-new-elements-into-a-map-after-calling-iterator","text":"== [[iterator]] Iterator of \"Combined\" Pairs","title":"Cannot insert new elements into a map after calling iterator"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_1","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#iterator-iteratork-c","text":"iterator...FIXME iterator is used when...FIXME == [[spill]] Spilling to Disk if Necessary","title":"iterator: Iterator[(K, C)]"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_2","text":"spill( collection: SizeTracker): Unit spill...FIXME spill is used when...FIXME == [[forceSpill]] Forcing Disk Spilling","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_3","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#forcespill-boolean","text":"forceSpill returns a flag to indicate whether spilling to disk has really happened ( true ) or not ( false ). forceSpill branches off per the current state it is in (and should rather use a state-aware implementation). When a < > is in use, forceSpill requests it to spill and, if it did, dereferences ( null ify) the < >. forceSpill returns whatever the spilling of the < > returned. When there is at least one element in the < >, forceSpill < > it. forceSpill then creates a new < > and always returns true . In other cases, forceSpill simply returns false . forceSpill is part of the shuffle:Spillable.md[Spillable] abstraction. == [[freeCurrentMap]] Freeing Up SizeTrackingAppendOnlyMap and Releasing Memory","title":"forceSpill(): Boolean"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_4","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalAppendOnlyMap/#freecurrentmap-unit","text":"freeCurrentMap dereferences ( null ify) the < > (if there still was one) followed by shuffle:Spillable.md#releaseMemory[releasing all memory]. freeCurrentMap is used when SpillableIterator is requested to destroy itself. == [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Method","title":"freeCurrentMap(): Unit"},{"location":"shuffle/ExternalAppendOnlyMap/#source-scala_5","text":"spillMemoryIteratorToDisk( inMemoryIterator: Iterator[(K, C)]): DiskMapIterator spillMemoryIteratorToDisk...FIXME spillMemoryIteratorToDisk is used when...FIXME","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/","text":"= [[ExternalSorter]] ExternalSorter ExternalSorter is a shuffle:Spillable.md[Spillable] of WritablePartitionedPairCollection of pairs (of K keys and C values). ExternalSorter[K, V, C] is a parameterized type of K keys, V values, and C combiner (partial) values. == [[creating-instance]] Creating Instance ExternalSorter takes the following to be created: [[context]] scheduler:spark-TaskContext.md[TaskContext] [[aggregator]] Optional rdd:Aggregator.md[Aggregator] (default: undefined) [[partitioner]] Optional rdd:Partitioner[Partitioner] (default: undefined) [[ordering]] Optional Scala's http://www.scala-lang.org/api/current/scala/math/Ordering.html[Ordering ] for keys (default: undefined) [[serializer]] Optional serializer:Serializer.md[Serializer] (default: core:SparkEnv.md#serializer[system Serializer]) ExternalSorter is created when: SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records] (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag ) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read records] (with sort ordering defined) == [[in-memory-collection]][[buffer]][[map]] In-Memory Collections of Records ExternalSorter uses PartitionedPairBuffers or PartitionedAppendOnlyMaps to store records in memory before spilling to disk. ExternalSorter uses PartitionedPairBuffers when created with no < > specified. Otherwise, ExternalSorter uses PartitionedAppendOnlyMaps. ExternalSorter creates a PartitionedPairBuffer and a PartitionedAppendOnlyMap when created. ExternalSorter inserts records to the collections when < >. ExternalSorter < > and, shuffle:Spillable.md#maybeSpill[if so], creates a new collection. ExternalSorter releases the collections ( null s them) when requested to < > and < >. That is when the JVM garbage collector takes care of evicting them from memory completely. == [[peakMemoryUsedBytes]][[_peakMemoryUsedBytes]] Peak Size of In-Memory Collection ExternalSorter tracks the peak size (in bytes) of the < > whenever requested to < >. The peak size is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task] (with a sort ordering defined) ExternalSorter is requested to < > == [[spills]] Spills Internal Registry ExternalSorter manages spilled files. == [[insertAll]] Inserting Records [source, scala] \u00b6 insertAll( records: Iterator[Product2[K, V]]): Unit insertAll branches off per whether the optional < > was < > or < > (to create the < >). insertAll takes all records eagerly and materializes the given records iterator. === [[insertAll-shouldCombine]] Map-Side Aggregator Specified If there is an Aggregator specified, insertAll creates an update function based on the rdd:Aggregator.md#mergeValue[mergeValue] and rdd:Aggregator.md#createCombiner[createCombiner] functions of the Aggregator. For every record, insertAll shuffle:Spillable.md#addElementsRead[increment internal read counter]. insertAll requests the < > to changeValue for the key (made up of the < > of the key of the current record and the key itself, i.e. (partition, key) ) with the update function. In the end, insertAll < > (with the usingMap flag on since the < > was updated). === [[insertAll-no-aggregator]] No Map-Side Aggregator Specified With no Aggregator specified, insertAll iterates over all the records and uses the < > instead. For every record, insertAll shuffle:Spillable.md#addElementsRead[increment internal read counter]. insertAll requests the < > to insert with the < > of the key of the current record, the key itself and the value of the current record. In the end, insertAll < > (with the usingMap flag off since this time the < > was updated, not the < >). === [[insertAll-usage]] Usage insertAll is used when: SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records] (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag ) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read records] (with sort ordering defined) == [[writePartitionedFile]] Writing All Records Into Partitioned File [source, scala] \u00b6 writePartitionedFile( blockId: BlockId, outputFile: File): Array[Long] writePartitionedFile...FIXME writePartitionedFile is used when SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records]. == [[stop]] Stopping ExternalSorter [source, scala] \u00b6 stop(): Unit \u00b6 stop...FIXME stop is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read records] (with sort ordering defined) SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#stop[stop] == [[spill]] Spilling Data to Disk [source, scala] \u00b6 spill( collection: WritablePartitionedPairCollection[K, C]): Unit spill requests the given WritablePartitionedPairCollection for a destructive WritablePartitionedIterator. spill < > (with the destructive WritablePartitionedIterator) that creates a SpilledFile. spill adds the SpilledFile to the < > internal registry. spill is part of the Spillable.md#spill[Spillable] abstraction. == [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Method [source, scala] \u00b6 spillMemoryIteratorToDisk( inMemoryIterator: WritablePartitionedIterator): SpilledFile spillMemoryIteratorToDisk...FIXME spillMemoryIteratorToDisk is used when: ExternalSorter is requested to < > SpillableIterator is requested to spill == [[maybeSpillCollection]] Spilling In-Memory Collection to Disk [source, scala] \u00b6 maybeSpillCollection( usingMap: Boolean): Unit maybeSpillCollection branches per the input usingMap flag (that is to determine which in-memory collection to use, the < > or the < >). maybeSpillCollection requests the collection to estimate size (in bytes) that is tracked as the < > metric (for every size bigger than what is currently recorded). maybeSpillCollection shuffle:Spillable.md#maybeSpill[spills the collection to disk if needed]. If spilled, maybeSpillCollection creates a new collection (a new PartitionedAppendOnlyMap or a new PartitionedPairBuffer). maybeSpillCollection is used when ExternalSorter is requested to < >. == [[iterator]] iterator Method [source, scala] \u00b6 iterator: Iterator[Product2[K, C]] \u00b6 iterator...FIXME iterator is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task]. == [[partitionedIterator]] partitionedIterator Method [source, scala] \u00b6 partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])] \u00b6 partitionedIterator...FIXME partitionedIterator is used when ExternalSorter is requested for an < > and to < > == [[logging]] Logging Enable ALL logging level for org.apache.spark.util.collection.ExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.util.collection.ExternalSorter=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"ExternalSorter"},{"location":"shuffle/ExternalSorter/#source-scala","text":"insertAll( records: Iterator[Product2[K, V]]): Unit insertAll branches off per whether the optional < > was < > or < > (to create the < >). insertAll takes all records eagerly and materializes the given records iterator. === [[insertAll-shouldCombine]] Map-Side Aggregator Specified If there is an Aggregator specified, insertAll creates an update function based on the rdd:Aggregator.md#mergeValue[mergeValue] and rdd:Aggregator.md#createCombiner[createCombiner] functions of the Aggregator. For every record, insertAll shuffle:Spillable.md#addElementsRead[increment internal read counter]. insertAll requests the < > to changeValue for the key (made up of the < > of the key of the current record and the key itself, i.e. (partition, key) ) with the update function. In the end, insertAll < > (with the usingMap flag on since the < > was updated). === [[insertAll-no-aggregator]] No Map-Side Aggregator Specified With no Aggregator specified, insertAll iterates over all the records and uses the < > instead. For every record, insertAll shuffle:Spillable.md#addElementsRead[increment internal read counter]. insertAll requests the < > to insert with the < > of the key of the current record, the key itself and the value of the current record. In the end, insertAll < > (with the usingMap flag off since this time the < > was updated, not the < >). === [[insertAll-usage]] Usage insertAll is used when: SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records] (as a ExternalSorter[K, V, C] or ExternalSorter[K, V, V] based on Map-Size Partial Aggregation Flag ) BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read records] (with sort ordering defined) == [[writePartitionedFile]] Writing All Records Into Partitioned File","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#source-scala_1","text":"writePartitionedFile( blockId: BlockId, outputFile: File): Array[Long] writePartitionedFile...FIXME writePartitionedFile is used when SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write records]. == [[stop]] Stopping ExternalSorter","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#stop-unit","text":"stop...FIXME stop is used when: BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read records] (with sort ordering defined) SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#stop[stop] == [[spill]] Spilling Data to Disk","title":"stop(): Unit"},{"location":"shuffle/ExternalSorter/#source-scala_3","text":"spill( collection: WritablePartitionedPairCollection[K, C]): Unit spill requests the given WritablePartitionedPairCollection for a destructive WritablePartitionedIterator. spill < > (with the destructive WritablePartitionedIterator) that creates a SpilledFile. spill adds the SpilledFile to the < > internal registry. spill is part of the Spillable.md#spill[Spillable] abstraction. == [[spillMemoryIteratorToDisk]] spillMemoryIteratorToDisk Method","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#source-scala_4","text":"spillMemoryIteratorToDisk( inMemoryIterator: WritablePartitionedIterator): SpilledFile spillMemoryIteratorToDisk...FIXME spillMemoryIteratorToDisk is used when: ExternalSorter is requested to < > SpillableIterator is requested to spill == [[maybeSpillCollection]] Spilling In-Memory Collection to Disk","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#source-scala_5","text":"maybeSpillCollection( usingMap: Boolean): Unit maybeSpillCollection branches per the input usingMap flag (that is to determine which in-memory collection to use, the < > or the < >). maybeSpillCollection requests the collection to estimate size (in bytes) that is tracked as the < > metric (for every size bigger than what is currently recorded). maybeSpillCollection shuffle:Spillable.md#maybeSpill[spills the collection to disk if needed]. If spilled, maybeSpillCollection creates a new collection (a new PartitionedAppendOnlyMap or a new PartitionedPairBuffer). maybeSpillCollection is used when ExternalSorter is requested to < >. == [[iterator]] iterator Method","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#source-scala_6","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#iterator-iteratorproduct2k-c","text":"iterator...FIXME iterator is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined records for a reduce task]. == [[partitionedIterator]] partitionedIterator Method","title":"iterator: Iterator[Product2[K, C]]"},{"location":"shuffle/ExternalSorter/#source-scala_7","text":"","title":"[source, scala]"},{"location":"shuffle/ExternalSorter/#partitionediterator-iteratorint-iteratorproduct2k-c","text":"partitionedIterator...FIXME partitionedIterator is used when ExternalSorter is requested for an < > and to < > == [[logging]] Logging Enable ALL logging level for org.apache.spark.util.collection.ExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"partitionedIterator: Iterator[(Int, Iterator[Product2[K, C]])]"},{"location":"shuffle/ExternalSorter/#source","text":"","title":"[source]"},{"location":"shuffle/ExternalSorter/#log4jloggerorgapachesparkutilcollectionexternalsorterall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.util.collection.ExternalSorter=ALL"},{"location":"shuffle/FetchFailedException/","text":"== [[FetchFailedException]] FetchFailedException FetchFailedException exception executor:TaskRunner.md#run-FetchFailedException[may be thrown when a task runs] (and storage:ShuffleBlockFetcherIterator.md#throwFetchFailedException[ ShuffleBlockFetcherIterator did not manage to fetch shuffle blocks]). FetchFailedException contains the following: the unique identifier for a storage:BlockManager.md[BlockManager] (as storage:BlockManagerId.md[]) shuffleId mapId reduceId A short exception message cause - the root Throwable object When FetchFailedException is reported, executor:TaskRunner.md#run-FetchFailedException[ TaskRunner catches it and notifies ExecutorBackend ] (with TaskState.FAILED task state). The root cause of the FetchFailedException is usually because the executor:Executor.md[] (with the storage:BlockManager.md[BlockManager] for the shuffle blocks) is lost (i.e. no longer available) due to: OutOfMemoryError could be thrown (aka OOMed ) or some other unhandled exception. The cluster manager that manages the workers with the executors of your Spark application, e.g. YARN, enforces the container memory limits and eventually decided to kill the executor due to excessive memory usage. You should review the logs of the Spark application using webui:index.md[web UI], spark-history-server:index.md[Spark History Server] or cluster-specific tools like https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YarnCommands.html#logs[yarn logs -applicationId] for Hadoop YARN. A solution is usually to tune the memory of your Spark application. CAUTION: FIXME Image with the call to ExecutorBackend. === [[toTaskFailedReason]] toTaskFailedReason Method CAUTION: FIXME","title":"FetchFailedException"},{"location":"shuffle/IndexShuffleBlockResolver/","text":"= [[IndexShuffleBlockResolver]] IndexShuffleBlockResolver IndexShuffleBlockResolver is a ShuffleBlockResolver.md[ShuffleBlockResolver] that manages shuffle block data and uses shuffle index files for faster shuffle data access. IndexShuffleBlockResolver is < > for SortShuffleManager.md#shuffleBlockResolver[SortShuffleManager] (for ShuffleManager.md#shuffleBlockResolver[retrieving shuffle block data]). .IndexShuffleBlockResolver and SortShuffleManager image::IndexShuffleBlockResolver-SortShuffleManager.png[align=\"center\"] IndexShuffleBlockResolver can < >, < > and < > shuffle block index and data files (given shuffle and map IDs). IndexShuffleBlockResolver is later used to create the SortShuffleManager.md#getWriter[ShuffleWriter] given a spark-shuffle-ShuffleHandle.md[ShuffleHandle]. == [[creating-instance]] Creating Instance IndexShuffleBlockResolver takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] [[_blockManager]][[blockManager]] storage:BlockManager.md[BlockManager] IndexShuffleBlockResolver initializes the < >. == [[writeIndexFileAndCommit]] Writing Shuffle Index and Data Files [source, scala] \u00b6 writeIndexFileAndCommit( shuffleId: Int, mapId: Int, lengths: Array[Long], dataTmp: File): Unit Internally, writeIndexFileAndCommit first < > for the input shuffleId and mapId . writeIndexFileAndCommit creates a temporary file for the index file (in the same directory) and writes offsets (as the moving sum of the input lengths starting from 0 to the final offset at the end for the end of the output file). NOTE: The offsets are the sizes in the input lengths exactly. .writeIndexFileAndCommit and offsets in a shuffle index file image::IndexShuffleBlockResolver-writeIndexFileAndCommit.png[align=\"center\"] writeIndexFileAndCommit < > for the input shuffleId and mapId . writeIndexFileAndCommit < > (aka consistency check ). If the consistency check fails, it means that another attempt for the same task has already written the map outputs successfully and so the input dataTmp and temporary index files are deleted (as no longer correct). If the consistency check succeeds, the existing index and data files are deleted (if they exist) and the temporary index and data files become \"official\", i.e. renamed to their final names. In case of any IO-related exception, writeIndexFileAndCommit throws a IOException with the messages: fail to rename file [indexTmp] to [indexFile] or fail to rename file [dataTmp] to [dataFile] NOTE: writeIndexFileAndCommit is used when ShuffleWriter.md[ShuffleWriters] are requested to write records to a shuffle system, i.e. shuffle:SortShuffleWriter.md#write[SortShuffleWriter], shuffle:BypassMergeSortShuffleWriter.md#write[BypassMergeSortShuffleWriter], and shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[UnsafeShuffleWriter]. == [[getBlockData]] Creating ManagedBuffer to Read Shuffle Block Data File -- getBlockData Method [source, scala] \u00b6 getBlockData( blockId: ShuffleBlockId): ManagedBuffer NOTE: getBlockData is part of ShuffleBlockResolver.md#getBlockData[ShuffleBlockResolver] contract. Internally, getBlockData < > for the input shuffle blockId . NOTE: storage:BlockId.md#ShuffleBlockId[ShuffleBlockId] knows shuffleId and mapId . getBlockData discards blockId.reduceId bytes of data from the index file. NOTE: getBlockData uses Guava's ++ https://google.github.io/guava/releases/snapshot/api/docs/com/google/common/io/ByteStreams.html#skipFully-java.io.InputStream-long-++[com.google.common.io.ByteStreams ] to skip the bytes. getBlockData reads the start and end offsets from the index file and then creates a FileSegmentManagedBuffer to read the < > for the offsets (using < > internal property). NOTE: The start and end offsets are the offset and the length of the file segment for the block data. In the end, getBlockData closes the index file. == [[checkIndexAndDataFile]] Checking Consistency of Shuffle Index and Data Files and Returning Block Lengths -- checkIndexAndDataFile Internal Method [source, scala] \u00b6 checkIndexAndDataFile( index: File, data: File, blocks: Int): Array[Long] checkIndexAndDataFile first checks if the size of the input index file is exactly the input blocks multiplied by 8 . checkIndexAndDataFile returns null when the numbers, and hence the shuffle index and data files, don't match. checkIndexAndDataFile reads the shuffle index file and converts the offsets into lengths of each block. checkIndexAndDataFile makes sure that the size of the input shuffle data file is exactly the sum of the block lengths. checkIndexAndDataFile returns the block lengths if the numbers match, and null otherwise. NOTE: checkIndexAndDataFile is used exclusively when IndexShuffleBlockResolver is requested to < > (for shuffle and map IDs). == [[removeDataByMap]] Removing Shuffle Index and Data Files (For Shuffle and Map IDs) -- removeDataByMap Method [source, scala] \u00b6 removeDataByMap(shuffleId: Int, mapId: Int): Unit \u00b6 removeDataByMap < > and deletes the shuffle data for the input shuffleId and mapId first followed by < > and deleting the shuffle data index file. When removeDataByMap fails deleting the files, removeDataByMap prints out the following WARN message to the logs. Error deleting data [path] or Error deleting index [path] NOTE: removeDataByMap is used exclusively when SortShuffleManager is requested to SortShuffleManager.md#unregisterShuffle[unregister a shuffle] (remove a shuffle from a shuffle system). == [[stop]] Stopping IndexShuffleBlockResolver -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of ShuffleBlockResolver.md#stop[ShuffleBlockResolver contract]. stop is a noop operation, i.e. does nothing when called. == [[getIndexFile]] Requesting Shuffle Block Index File (from DiskBlockManager) [source, scala] \u00b6 getIndexFile( shuffleId: Int, mapId: Int): File getIndexFile requests the < > for the storage:BlockManager.md#diskBlockManager[DiskBlockManager] that is in turn requested for the storage:DiskBlockManager.md#getFile[shuffle index file] (with a new ShuffleIndexBlockId with the given shuffleId and mapId). NOTE: getIndexFile is used when IndexShuffleBlockResolver < >, < ManagedBuffer to read a shuffle block data file>>, and < >. == [[getDataFile]] Requesting Shuffle Block Data File [source, scala] \u00b6 getDataFile( shuffleId: Int, mapId: Int): File getDataFile requests the < > for the storage:BlockManager.md#diskBlockManager[DiskBlockManager] that is in turn requested for the storage:DiskBlockManager.md#getFile[shuffle block data file] (for a storage:BlockId.md#ShuffleDataBlockId[ShuffleDataBlockId]) [NOTE] \u00b6 getDataFile is used when: IndexShuffleBlockResolver is requested to < >, < >, and < > * shuffle:BypassMergeSortShuffleWriter.md#write[BypassMergeSortShuffleWriter], shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[UnsafeShuffleWriter], and SortShuffleWriter.md#write[SortShuffleWriter] are requested to write records to a shuffle system \u00b6 == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.IndexShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.shuffle.IndexShuffleBlockResolver=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | transportConf a| [[transportConf]] network:TransportConf.md[] for shuffle module Created immediately when IndexShuffleBlockResolver is < > by requesting SparkTransportConf object to network:TransportConf.md#SparkTransportConf-fromSparkConf[create one from SparkConf] |===","title":"IndexShuffleBlockResolver"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala","text":"writeIndexFileAndCommit( shuffleId: Int, mapId: Int, lengths: Array[Long], dataTmp: File): Unit Internally, writeIndexFileAndCommit first < > for the input shuffleId and mapId . writeIndexFileAndCommit creates a temporary file for the index file (in the same directory) and writes offsets (as the moving sum of the input lengths starting from 0 to the final offset at the end for the end of the output file). NOTE: The offsets are the sizes in the input lengths exactly. .writeIndexFileAndCommit and offsets in a shuffle index file image::IndexShuffleBlockResolver-writeIndexFileAndCommit.png[align=\"center\"] writeIndexFileAndCommit < > for the input shuffleId and mapId . writeIndexFileAndCommit < > (aka consistency check ). If the consistency check fails, it means that another attempt for the same task has already written the map outputs successfully and so the input dataTmp and temporary index files are deleted (as no longer correct). If the consistency check succeeds, the existing index and data files are deleted (if they exist) and the temporary index and data files become \"official\", i.e. renamed to their final names. In case of any IO-related exception, writeIndexFileAndCommit throws a IOException with the messages: fail to rename file [indexTmp] to [indexFile] or fail to rename file [dataTmp] to [dataFile] NOTE: writeIndexFileAndCommit is used when ShuffleWriter.md[ShuffleWriters] are requested to write records to a shuffle system, i.e. shuffle:SortShuffleWriter.md#write[SortShuffleWriter], shuffle:BypassMergeSortShuffleWriter.md#write[BypassMergeSortShuffleWriter], and shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[UnsafeShuffleWriter]. == [[getBlockData]] Creating ManagedBuffer to Read Shuffle Block Data File -- getBlockData Method","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala_1","text":"getBlockData( blockId: ShuffleBlockId): ManagedBuffer NOTE: getBlockData is part of ShuffleBlockResolver.md#getBlockData[ShuffleBlockResolver] contract. Internally, getBlockData < > for the input shuffle blockId . NOTE: storage:BlockId.md#ShuffleBlockId[ShuffleBlockId] knows shuffleId and mapId . getBlockData discards blockId.reduceId bytes of data from the index file. NOTE: getBlockData uses Guava's ++ https://google.github.io/guava/releases/snapshot/api/docs/com/google/common/io/ByteStreams.html#skipFully-java.io.InputStream-long-++[com.google.common.io.ByteStreams ] to skip the bytes. getBlockData reads the start and end offsets from the index file and then creates a FileSegmentManagedBuffer to read the < > for the offsets (using < > internal property). NOTE: The start and end offsets are the offset and the length of the file segment for the block data. In the end, getBlockData closes the index file. == [[checkIndexAndDataFile]] Checking Consistency of Shuffle Index and Data Files and Returning Block Lengths -- checkIndexAndDataFile Internal Method","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala_2","text":"checkIndexAndDataFile( index: File, data: File, blocks: Int): Array[Long] checkIndexAndDataFile first checks if the size of the input index file is exactly the input blocks multiplied by 8 . checkIndexAndDataFile returns null when the numbers, and hence the shuffle index and data files, don't match. checkIndexAndDataFile reads the shuffle index file and converts the offsets into lengths of each block. checkIndexAndDataFile makes sure that the size of the input shuffle data file is exactly the sum of the block lengths. checkIndexAndDataFile returns the block lengths if the numbers match, and null otherwise. NOTE: checkIndexAndDataFile is used exclusively when IndexShuffleBlockResolver is requested to < > (for shuffle and map IDs). == [[removeDataByMap]] Removing Shuffle Index and Data Files (For Shuffle and Map IDs) -- removeDataByMap Method","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala_3","text":"","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#removedatabymapshuffleid-int-mapid-int-unit","text":"removeDataByMap < > and deletes the shuffle data for the input shuffleId and mapId first followed by < > and deleting the shuffle data index file. When removeDataByMap fails deleting the files, removeDataByMap prints out the following WARN message to the logs. Error deleting data [path] or Error deleting index [path] NOTE: removeDataByMap is used exclusively when SortShuffleManager is requested to SortShuffleManager.md#unregisterShuffle[unregister a shuffle] (remove a shuffle from a shuffle system). == [[stop]] Stopping IndexShuffleBlockResolver -- stop Method","title":"removeDataByMap(shuffleId: Int, mapId: Int): Unit"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala_4","text":"","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#stop-unit","text":"NOTE: stop is part of ShuffleBlockResolver.md#stop[ShuffleBlockResolver contract]. stop is a noop operation, i.e. does nothing when called. == [[getIndexFile]] Requesting Shuffle Block Index File (from DiskBlockManager)","title":"stop(): Unit"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala_5","text":"getIndexFile( shuffleId: Int, mapId: Int): File getIndexFile requests the < > for the storage:BlockManager.md#diskBlockManager[DiskBlockManager] that is in turn requested for the storage:DiskBlockManager.md#getFile[shuffle index file] (with a new ShuffleIndexBlockId with the given shuffleId and mapId). NOTE: getIndexFile is used when IndexShuffleBlockResolver < >, < ManagedBuffer to read a shuffle block data file>>, and < >. == [[getDataFile]] Requesting Shuffle Block Data File","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#source-scala_6","text":"getDataFile( shuffleId: Int, mapId: Int): File getDataFile requests the < > for the storage:BlockManager.md#diskBlockManager[DiskBlockManager] that is in turn requested for the storage:DiskBlockManager.md#getFile[shuffle block data file] (for a storage:BlockId.md#ShuffleDataBlockId[ShuffleDataBlockId])","title":"[source, scala]"},{"location":"shuffle/IndexShuffleBlockResolver/#note","text":"getDataFile is used when: IndexShuffleBlockResolver is requested to < >, < >, and < >","title":"[NOTE]"},{"location":"shuffle/IndexShuffleBlockResolver/#shufflebypassmergesortshufflewritermdwritebypassmergesortshufflewriter-shuffleunsafeshufflewritermdcloseandwriteoutputunsafeshufflewriter-and-sortshufflewritermdwritesortshufflewriter-are-requested-to-write-records-to-a-shuffle-system","text":"== [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.IndexShuffleBlockResolver logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"* shuffle:BypassMergeSortShuffleWriter.md#write[BypassMergeSortShuffleWriter], shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[UnsafeShuffleWriter], and SortShuffleWriter.md#write[SortShuffleWriter] are requested to write records to a shuffle system"},{"location":"shuffle/IndexShuffleBlockResolver/#source","text":"","title":"[source]"},{"location":"shuffle/IndexShuffleBlockResolver/#log4jloggerorgapachesparkshuffleindexshuffleblockresolverall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | transportConf a| [[transportConf]] network:TransportConf.md[] for shuffle module Created immediately when IndexShuffleBlockResolver is < > by requesting SparkTransportConf object to network:TransportConf.md#SparkTransportConf-fromSparkConf[create one from SparkConf] |===","title":"log4j.logger.org.apache.spark.shuffle.IndexShuffleBlockResolver=ALL"},{"location":"shuffle/SerializedShuffleHandle/","text":"SerializedShuffleHandle \u00b6 SerializedShuffleHandle is a ShuffleHandle to identify the choice of a serialized shuffle . SerializedShuffleHandle is used to create an UnsafeShuffleWriter . Creating Instance \u00b6 SerializedShuffleHandle takes the following to be created: [[shuffleId]] Shuffle ID [[numMaps]] Number of mappers [[dependency]] ShuffleDependency[K, V, V] SerializedShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ). SortShuffleManager determines what shuffle handle to use by first checking out the requirements of BypassMergeSortShuffleHandle before SerializedShuffleHandle 's.","title":"SerializedShuffleHandle"},{"location":"shuffle/SerializedShuffleHandle/#serializedshufflehandle","text":"SerializedShuffleHandle is a ShuffleHandle to identify the choice of a serialized shuffle . SerializedShuffleHandle is used to create an UnsafeShuffleWriter .","title":"SerializedShuffleHandle"},{"location":"shuffle/SerializedShuffleHandle/#creating-instance","text":"SerializedShuffleHandle takes the following to be created: [[shuffleId]] Shuffle ID [[numMaps]] Number of mappers [[dependency]] ShuffleDependency[K, V, V] SerializedShuffleHandle is created when SortShuffleManager is requested for a ShuffleHandle (for a ShuffleDependency ). SortShuffleManager determines what shuffle handle to use by first checking out the requirements of BypassMergeSortShuffleHandle before SerializedShuffleHandle 's.","title":"Creating Instance"},{"location":"shuffle/ShuffleBlockResolver/","text":"= [[ShuffleBlockResolver]] ShuffleBlockResolver ShuffleBlockResolver is an < > of < > that storage:BlockManager.md[BlockManager] uses to < > for a logical shuffle block identifier (i.e. map, reduce, and shuffle). NOTE: Shuffle block data files are often referred to as map outputs files . [[implementations]] NOTE: shuffle:IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] is the default and only known ShuffleBlockResolver in Apache Spark. [[contract]] .ShuffleBlockResolver Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | getBlockData a| [[getBlockData]] [source, scala] \u00b6 getBlockData( blockId: ShuffleBlockId): ManagedBuffer Retrieves the data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md#ShuffleBlockId[block] (a tuple of shuffleId , mapId and reduceId ). Used when BlockManager is requested to retrieve a storage:BlockManager.md#getLocalBytes[block data from a local block manager] and storage:BlockManager.md#getBlockData[block data] | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Stops the ShuffleBlockResolver Used when SortShuffleManager is requested to SortShuffleManager.md#stop[stop] |===","title":"ShuffleBlockResolver"},{"location":"shuffle/ShuffleBlockResolver/#source-scala","text":"getBlockData( blockId: ShuffleBlockId): ManagedBuffer Retrieves the data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md#ShuffleBlockId[block] (a tuple of shuffleId , mapId and reduceId ). Used when BlockManager is requested to retrieve a storage:BlockManager.md#getLocalBytes[block data from a local block manager] and storage:BlockManager.md#getBlockData[block data] | stop a| [[stop]]","title":"[source, scala]"},{"location":"shuffle/ShuffleBlockResolver/#source-scala_1","text":"","title":"[source, scala]"},{"location":"shuffle/ShuffleBlockResolver/#stop-unit","text":"Stops the ShuffleBlockResolver Used when SortShuffleManager is requested to SortShuffleManager.md#stop[stop] |===","title":"stop(): Unit"},{"location":"shuffle/ShuffleExternalSorter/","text":"ShuffleExternalSorter \u00b6 ShuffleExternalSorter is a specialized cache-efficient sorter that sorts arrays of compressed record pointers and partition ids. ShuffleExternalSorter uses only 8 bytes of space per record in the sorting array to fit more of the array into cache. ShuffleExternalSorter is created and used by UnsafeShuffleWriter only. MemoryConsumer \u00b6 ShuffleExternalSorter is a MemoryConsumer with page size of 128 MB (unless TaskMemoryManager uses smaller). ShuffleExternalSorter can spill to disk to free up execution memory . Configuration Properties \u00b6 spark.shuffle.file.buffer \u00b6 ShuffleExternalSorter uses spark.shuffle.file.buffer configuration property for...FIXME spark.shuffle.spill.numElementsForceSpillThreshold \u00b6 ShuffleExternalSorter uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property for...FIXME Creating Instance \u00b6 ShuffleExternalSorter takes the following to be created: TaskMemoryManager BlockManager TaskContext Initial Size Number of Partitions SparkConf ShuffleWriteMetricsReporter ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open a ShuffleExternalSorter . ShuffleInMemorySorter \u00b6 ShuffleExternalSorter manages a ShuffleInMemorySorter : ShuffleInMemorySorter is created immediately when ShuffleExternalSorter is ShuffleInMemorySorter is requested to free up memory and dereferenced ( null ed) when ShuffleExternalSorter is requested to cleanupResources and closeAndGetSpills ShuffleExternalSorter uses the ShuffleInMemorySorter for the following: writeSortedFile spill getMemoryUsage growPointerArrayIfNecessary insertRecord Spilling To Disk \u00b6 long spill ( long size , MemoryConsumer trigger ) spill returns the memory bytes spilled ( spill size ). spill prints out the following INFO message to the logs: Thread [threadId] spilling sort data of [memoryUsage] to disk ([spillsSize] [time|times] so far) spill writeSortedFile (with the isLastFile flag disabled). spill frees up execution memory (and records the memory bytes spilled as spillSize ). spill requests the ShuffleInMemorySorter to reset . In the end, spill requests the TaskContext for TaskMetrics to increase the memory bytes spilled . spill is part of the MemoryConsumer abstraction. closeAndGetSpills \u00b6 SpillInfo [] closeAndGetSpills () closeAndGetSpills ...FIXME closeAndGetSpills is used when UnsafeShuffleWriter is requested to closeAndWriteOutput . getMemoryUsage \u00b6 long getMemoryUsage () getMemoryUsage ...FIXME getMemoryUsage is used when ShuffleExternalSorter is created and requested to spill and updatePeakMemoryUsed . updatePeakMemoryUsed \u00b6 void updatePeakMemoryUsed () updatePeakMemoryUsed ...FIXME updatePeakMemoryUsed is used when ShuffleExternalSorter is requested to getPeakMemoryUsedBytes and freeMemory . writeSortedFile \u00b6 void writeSortedFile ( boolean isLastFile ) writeSortedFile ...FIXME writeSortedFile is used when ShuffleExternalSorter is requested to spill and closeAndGetSpills . cleanupResources \u00b6 void cleanupResources () cleanupResources ...FIXME cleanupResources is used when UnsafeShuffleWriter is requested to write records and stop . Inserting Serialized Record Into ShuffleInMemorySorter \u00b6 void insertRecord ( Object recordBase , long recordOffset , int length , int partitionId ) insertRecord ...FIXME insertRecord growPointerArrayIfNecessary . insertRecord ...FIXME insertRecord acquireNewPageIfNecessary . insertRecord ...FIXME insertRecord is used when UnsafeShuffleWriter is requested to insertRecordIntoSorter growPointerArrayIfNecessary \u00b6 void growPointerArrayIfNecessary () growPointerArrayIfNecessary ...FIXME acquireNewPageIfNecessary \u00b6 void acquireNewPageIfNecessary ( int required ) acquireNewPageIfNecessary ...FIXME freeMemory \u00b6 long freeMemory () freeMemory ...FIXME freeMemory is used when ShuffleExternalSorter is requested to spill , cleanupResources , and closeAndGetSpills . Peak Memory Used \u00b6 long getPeakMemoryUsedBytes () getPeakMemoryUsedBytes ...FIXME getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to updatePeakMemoryUsed . Logging \u00b6 Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL Refer to Logging .","title":"ShuffleExternalSorter"},{"location":"shuffle/ShuffleExternalSorter/#shuffleexternalsorter","text":"ShuffleExternalSorter is a specialized cache-efficient sorter that sorts arrays of compressed record pointers and partition ids. ShuffleExternalSorter uses only 8 bytes of space per record in the sorting array to fit more of the array into cache. ShuffleExternalSorter is created and used by UnsafeShuffleWriter only.","title":"ShuffleExternalSorter"},{"location":"shuffle/ShuffleExternalSorter/#memoryconsumer","text":"ShuffleExternalSorter is a MemoryConsumer with page size of 128 MB (unless TaskMemoryManager uses smaller). ShuffleExternalSorter can spill to disk to free up execution memory .","title":" MemoryConsumer"},{"location":"shuffle/ShuffleExternalSorter/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"shuffle/ShuffleExternalSorter/#sparkshufflefilebuffer","text":"ShuffleExternalSorter uses spark.shuffle.file.buffer configuration property for...FIXME","title":" spark.shuffle.file.buffer"},{"location":"shuffle/ShuffleExternalSorter/#sparkshufflespillnumelementsforcespillthreshold","text":"ShuffleExternalSorter uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property for...FIXME","title":" spark.shuffle.spill.numElementsForceSpillThreshold"},{"location":"shuffle/ShuffleExternalSorter/#creating-instance","text":"ShuffleExternalSorter takes the following to be created: TaskMemoryManager BlockManager TaskContext Initial Size Number of Partitions SparkConf ShuffleWriteMetricsReporter ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open a ShuffleExternalSorter .","title":"Creating Instance"},{"location":"shuffle/ShuffleExternalSorter/#shuffleinmemorysorter","text":"ShuffleExternalSorter manages a ShuffleInMemorySorter : ShuffleInMemorySorter is created immediately when ShuffleExternalSorter is ShuffleInMemorySorter is requested to free up memory and dereferenced ( null ed) when ShuffleExternalSorter is requested to cleanupResources and closeAndGetSpills ShuffleExternalSorter uses the ShuffleInMemorySorter for the following: writeSortedFile spill getMemoryUsage growPointerArrayIfNecessary insertRecord","title":" ShuffleInMemorySorter"},{"location":"shuffle/ShuffleExternalSorter/#spilling-to-disk","text":"long spill ( long size , MemoryConsumer trigger ) spill returns the memory bytes spilled ( spill size ). spill prints out the following INFO message to the logs: Thread [threadId] spilling sort data of [memoryUsage] to disk ([spillsSize] [time|times] so far) spill writeSortedFile (with the isLastFile flag disabled). spill frees up execution memory (and records the memory bytes spilled as spillSize ). spill requests the ShuffleInMemorySorter to reset . In the end, spill requests the TaskContext for TaskMetrics to increase the memory bytes spilled . spill is part of the MemoryConsumer abstraction.","title":" Spilling To Disk"},{"location":"shuffle/ShuffleExternalSorter/#closeandgetspills","text":"SpillInfo [] closeAndGetSpills () closeAndGetSpills ...FIXME closeAndGetSpills is used when UnsafeShuffleWriter is requested to closeAndWriteOutput .","title":" closeAndGetSpills"},{"location":"shuffle/ShuffleExternalSorter/#getmemoryusage","text":"long getMemoryUsage () getMemoryUsage ...FIXME getMemoryUsage is used when ShuffleExternalSorter is created and requested to spill and updatePeakMemoryUsed .","title":" getMemoryUsage"},{"location":"shuffle/ShuffleExternalSorter/#updatepeakmemoryused","text":"void updatePeakMemoryUsed () updatePeakMemoryUsed ...FIXME updatePeakMemoryUsed is used when ShuffleExternalSorter is requested to getPeakMemoryUsedBytes and freeMemory .","title":" updatePeakMemoryUsed"},{"location":"shuffle/ShuffleExternalSorter/#writesortedfile","text":"void writeSortedFile ( boolean isLastFile ) writeSortedFile ...FIXME writeSortedFile is used when ShuffleExternalSorter is requested to spill and closeAndGetSpills .","title":" writeSortedFile"},{"location":"shuffle/ShuffleExternalSorter/#cleanupresources","text":"void cleanupResources () cleanupResources ...FIXME cleanupResources is used when UnsafeShuffleWriter is requested to write records and stop .","title":" cleanupResources"},{"location":"shuffle/ShuffleExternalSorter/#inserting-serialized-record-into-shuffleinmemorysorter","text":"void insertRecord ( Object recordBase , long recordOffset , int length , int partitionId ) insertRecord ...FIXME insertRecord growPointerArrayIfNecessary . insertRecord ...FIXME insertRecord acquireNewPageIfNecessary . insertRecord ...FIXME insertRecord is used when UnsafeShuffleWriter is requested to insertRecordIntoSorter","title":" Inserting Serialized Record Into ShuffleInMemorySorter"},{"location":"shuffle/ShuffleExternalSorter/#growpointerarrayifnecessary","text":"void growPointerArrayIfNecessary () growPointerArrayIfNecessary ...FIXME","title":" growPointerArrayIfNecessary"},{"location":"shuffle/ShuffleExternalSorter/#acquirenewpageifnecessary","text":"void acquireNewPageIfNecessary ( int required ) acquireNewPageIfNecessary ...FIXME","title":" acquireNewPageIfNecessary"},{"location":"shuffle/ShuffleExternalSorter/#freememory","text":"long freeMemory () freeMemory ...FIXME freeMemory is used when ShuffleExternalSorter is requested to spill , cleanupResources , and closeAndGetSpills .","title":" freeMemory"},{"location":"shuffle/ShuffleExternalSorter/#peak-memory-used","text":"long getPeakMemoryUsedBytes () getPeakMemoryUsedBytes ...FIXME getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to updatePeakMemoryUsed .","title":" Peak Memory Used"},{"location":"shuffle/ShuffleExternalSorter/#logging","text":"Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL Refer to Logging .","title":"Logging"},{"location":"shuffle/ShuffleHandle/","text":"ShuffleHandle \u00b6 ShuffleHandle is...FIXME","title":"ShuffleHandle"},{"location":"shuffle/ShuffleHandle/#shufflehandle","text":"ShuffleHandle is...FIXME","title":"ShuffleHandle"},{"location":"shuffle/ShuffleInMemorySorter/","text":"ShuffleInMemorySorter \u00b6 ShuffleInMemorySorter is used by ShuffleExternalSorter to < > using < > sort algorithms. == [[creating-instance]] Creating Instance ShuffleInMemorySorter takes the following to be created: [[consumer]] memory:MemoryConsumer.md[MemoryConsumer] [[initialSize]] Initial size [[useRadixSort]] useRadixSort flag (to indicate whether to use < >) ShuffleInMemorySorter requests the given < > to memory:MemoryConsumer.md#allocateArray[allocate an array] of the given < > for the < >. ShuffleInMemorySorter is created for a shuffle:ShuffleExternalSorter.md#inMemSorter[ShuffleExternalSorter]. == [[getSortedIterator]] Iterator of Records Sorted [source, java] \u00b6 ShuffleSorterIterator getSortedIterator() \u00b6 getSortedIterator...FIXME getSortedIterator is used when ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile]. == [[reset]] Resetting [source, java] \u00b6 void reset() \u00b6 reset...FIXME reset is used when...FIXME == [[numRecords]] numRecords Method [source, java] \u00b6 int numRecords() \u00b6 numRecords...FIXME numRecords is used when...FIXME == [[getUsableCapacity]] Calculating Usable Capacity [source, java] \u00b6 int getUsableCapacity() \u00b6 getUsableCapacity calculates the capacity that is a half or two-third of the memory used for the < >. getUsableCapacity is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[array]] Unsafe LongArray of Record Pointers and Partition IDs ShuffleInMemorySorter uses a LongArray. === [[usableCapacity]] Usable Capacity ShuffleInMemorySorter...FIXME","title":"ShuffleInMemorySorter"},{"location":"shuffle/ShuffleInMemorySorter/#shuffleinmemorysorter","text":"ShuffleInMemorySorter is used by ShuffleExternalSorter to < > using < > sort algorithms. == [[creating-instance]] Creating Instance ShuffleInMemorySorter takes the following to be created: [[consumer]] memory:MemoryConsumer.md[MemoryConsumer] [[initialSize]] Initial size [[useRadixSort]] useRadixSort flag (to indicate whether to use < >) ShuffleInMemorySorter requests the given < > to memory:MemoryConsumer.md#allocateArray[allocate an array] of the given < > for the < >. ShuffleInMemorySorter is created for a shuffle:ShuffleExternalSorter.md#inMemSorter[ShuffleExternalSorter]. == [[getSortedIterator]] Iterator of Records Sorted","title":"ShuffleInMemorySorter"},{"location":"shuffle/ShuffleInMemorySorter/#source-java","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#shufflesorteriterator-getsortediterator","text":"getSortedIterator...FIXME getSortedIterator is used when ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile]. == [[reset]] Resetting","title":"ShuffleSorterIterator getSortedIterator()"},{"location":"shuffle/ShuffleInMemorySorter/#source-java_1","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#void-reset","text":"reset...FIXME reset is used when...FIXME == [[numRecords]] numRecords Method","title":"void reset()"},{"location":"shuffle/ShuffleInMemorySorter/#source-java_2","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#int-numrecords","text":"numRecords...FIXME numRecords is used when...FIXME == [[getUsableCapacity]] Calculating Usable Capacity","title":"int numRecords()"},{"location":"shuffle/ShuffleInMemorySorter/#source-java_3","text":"","title":"[source, java]"},{"location":"shuffle/ShuffleInMemorySorter/#int-getusablecapacity","text":"getUsableCapacity calculates the capacity that is a half or two-third of the memory used for the < >. getUsableCapacity is used when...FIXME == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.ShuffleExternalSorter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"int getUsableCapacity()"},{"location":"shuffle/ShuffleInMemorySorter/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/ShuffleInMemorySorter/#log4jloggerorgapachesparkshufflesortshuffleexternalsorterall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[array]] Unsafe LongArray of Record Pointers and Partition IDs ShuffleInMemorySorter uses a LongArray. === [[usableCapacity]] Usable Capacity ShuffleInMemorySorter...FIXME","title":"log4j.logger.org.apache.spark.shuffle.sort.ShuffleExternalSorter=ALL"},{"location":"shuffle/ShuffleManager/","text":"ShuffleManager \u00b6 ShuffleManager is an abstraction of shuffle managers that manage shuffle data. ShuffleManager is specified using spark.shuffle.manager configuration property. ShuffleManager is used to create a BlockManager . Contract \u00b6 Getting ShuffleReader for ShuffleHandle \u00b6 getReader [ K , C ]( handle : ShuffleHandle , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ) : ShuffleReader [ K , C ] ShuffleReader to read shuffle data in the ShuffleHandle Used when the following RDDs are requested to compute a partition: CoGroupedRDD is requested to compute a partition ShuffledRDD is requested to compute a partition SubtractedRDD is requested to compute a partition ShuffledRowRDD (Spark SQL) is requested to compute a partition getReaderForRange \u00b6 getReaderForRange [ K , C ]( handle : ShuffleHandle , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ) : ShuffleReader [ K , C ] ShuffleReader for a range of reduce partitions to read from map output in the ShuffleHandle Used when ShuffledRowRDD (Spark SQL) is requested to compute a partition Getting ShuffleWriter for ShuffleHandle \u00b6 getWriter [ K , V ]( handle : ShuffleHandle , mapId : Long , context : TaskContext , metrics : ShuffleWriteMetricsReporter ) : ShuffleWriter [ K , V ] ShuffleWriter to write shuffle data in the ShuffleHandle Used when ShuffleWriteProcessor is requested to write a partition Registering Shuffle of ShuffleDependency (and Getting ShuffleHandle) \u00b6 registerShuffle [ K , V , C ]( shuffleId : Int , dependency : ShuffleDependency [ K , V , C ]) : ShuffleHandle Registers a shuffle (by the given shuffleId and ShuffleDependency ) and gives a ShuffleHandle Used when ShuffleDependency is created (and registers with the shuffle system) ShuffleBlockResolver \u00b6 shuffleBlockResolver : ShuffleBlockResolver ShuffleBlockResolver of the shuffle system Used when: SortShuffleManager is requested for a ShuffleWriter for a ShuffleHandle , to unregister a shuffle and stop BlockManager is requested to getLocalBlockData and getHostLocalShuffleData Stopping ShuffleManager \u00b6 stop () : Unit Stops the shuffle system Used when SparkEnv is requested to stop Unregistering Shuffle \u00b6 unregisterShuffle ( shuffleId : Int ) : Boolean Unregisters a given shuffle Used when BlockManagerSlaveEndpoint is requested to handle a RemoveShuffle message Implementations \u00b6 SortShuffleManager Accessing ShuffleManager using SparkEnv \u00b6 ShuffleManager is available on the driver and executors using SparkEnv.shuffleManager . val shuffleManager = SparkEnv . get . shuffleManager","title":"ShuffleManager"},{"location":"shuffle/ShuffleManager/#shufflemanager","text":"ShuffleManager is an abstraction of shuffle managers that manage shuffle data. ShuffleManager is specified using spark.shuffle.manager configuration property. ShuffleManager is used to create a BlockManager .","title":"ShuffleManager"},{"location":"shuffle/ShuffleManager/#contract","text":"","title":"Contract"},{"location":"shuffle/ShuffleManager/#getting-shufflereader-for-shufflehandle","text":"getReader [ K , C ]( handle : ShuffleHandle , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ) : ShuffleReader [ K , C ] ShuffleReader to read shuffle data in the ShuffleHandle Used when the following RDDs are requested to compute a partition: CoGroupedRDD is requested to compute a partition ShuffledRDD is requested to compute a partition SubtractedRDD is requested to compute a partition ShuffledRowRDD (Spark SQL) is requested to compute a partition","title":" Getting ShuffleReader for ShuffleHandle"},{"location":"shuffle/ShuffleManager/#getreaderforrange","text":"getReaderForRange [ K , C ]( handle : ShuffleHandle , startMapIndex : Int , endMapIndex : Int , startPartition : Int , endPartition : Int , context : TaskContext , metrics : ShuffleReadMetricsReporter ) : ShuffleReader [ K , C ] ShuffleReader for a range of reduce partitions to read from map output in the ShuffleHandle Used when ShuffledRowRDD (Spark SQL) is requested to compute a partition","title":" getReaderForRange"},{"location":"shuffle/ShuffleManager/#getting-shufflewriter-for-shufflehandle","text":"getWriter [ K , V ]( handle : ShuffleHandle , mapId : Long , context : TaskContext , metrics : ShuffleWriteMetricsReporter ) : ShuffleWriter [ K , V ] ShuffleWriter to write shuffle data in the ShuffleHandle Used when ShuffleWriteProcessor is requested to write a partition","title":" Getting ShuffleWriter for ShuffleHandle"},{"location":"shuffle/ShuffleManager/#registering-shuffle-of-shuffledependency-and-getting-shufflehandle","text":"registerShuffle [ K , V , C ]( shuffleId : Int , dependency : ShuffleDependency [ K , V , C ]) : ShuffleHandle Registers a shuffle (by the given shuffleId and ShuffleDependency ) and gives a ShuffleHandle Used when ShuffleDependency is created (and registers with the shuffle system)","title":" Registering Shuffle of ShuffleDependency (and Getting ShuffleHandle)"},{"location":"shuffle/ShuffleManager/#shuffleblockresolver","text":"shuffleBlockResolver : ShuffleBlockResolver ShuffleBlockResolver of the shuffle system Used when: SortShuffleManager is requested for a ShuffleWriter for a ShuffleHandle , to unregister a shuffle and stop BlockManager is requested to getLocalBlockData and getHostLocalShuffleData","title":" ShuffleBlockResolver"},{"location":"shuffle/ShuffleManager/#stopping-shufflemanager","text":"stop () : Unit Stops the shuffle system Used when SparkEnv is requested to stop","title":" Stopping ShuffleManager"},{"location":"shuffle/ShuffleManager/#unregistering-shuffle","text":"unregisterShuffle ( shuffleId : Int ) : Boolean Unregisters a given shuffle Used when BlockManagerSlaveEndpoint is requested to handle a RemoveShuffle message","title":" Unregistering Shuffle"},{"location":"shuffle/ShuffleManager/#implementations","text":"SortShuffleManager","title":"Implementations"},{"location":"shuffle/ShuffleManager/#accessing-shufflemanager-using-sparkenv","text":"ShuffleManager is available on the driver and executors using SparkEnv.shuffleManager . val shuffleManager = SparkEnv . get . shuffleManager","title":" Accessing ShuffleManager using SparkEnv"},{"location":"shuffle/ShuffleReader/","text":"ShuffleReader \u00b6 ShuffleReader is a < > of < > to < >. [[contract]] [source, scala] package org.apache.spark.shuffle trait ShuffleReader[K, C] { def read(): Iterator[Product2[K, C]] } NOTE: ShuffleReader is a private[spark] contract. .ShuffleReader Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | read a| [[read]] Reading combined key-value records for a reduce task Used when: CoGroupedRDD , ShuffledRDD , and SubtractedRDD are requested to compute a partition (for a ShuffleDependency dependency) Spark SQL's ShuffledRowRDD is requested to compute a partition |=== [[implementations]] NOTE: shuffle:BlockStoreShuffleReader.md[BlockStoreShuffleReader] is the one and only known < > in Apache Spark.","title":"ShuffleReader"},{"location":"shuffle/ShuffleReader/#shufflereader","text":"ShuffleReader is a < > of < > to < >. [[contract]] [source, scala] package org.apache.spark.shuffle trait ShuffleReader[K, C] { def read(): Iterator[Product2[K, C]] } NOTE: ShuffleReader is a private[spark] contract. .ShuffleReader Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | read a| [[read]] Reading combined key-value records for a reduce task Used when: CoGroupedRDD , ShuffledRDD , and SubtractedRDD are requested to compute a partition (for a ShuffleDependency dependency) Spark SQL's ShuffledRowRDD is requested to compute a partition |=== [[implementations]] NOTE: shuffle:BlockStoreShuffleReader.md[BlockStoreShuffleReader] is the one and only known < > in Apache Spark.","title":"ShuffleReader"},{"location":"shuffle/ShuffleWriteProcessor/","text":"ShuffleWriteProcessor \u00b6 ShuffleWriteProcessor is...FIXME","title":"ShuffleWriteProcessor"},{"location":"shuffle/ShuffleWriteProcessor/#shufflewriteprocessor","text":"ShuffleWriteProcessor is...FIXME","title":"ShuffleWriteProcessor"},{"location":"shuffle/ShuffleWriter/","text":"ShuffleWriter \u00b6 ShuffleWriter of K keys and V values ( ShuffleWriter[K, V] ) is an abstraction of < > that can < > (of a RDD partition) to a shuffle system. ShuffleWriter is used when scheduler:ShuffleMapTask.md[ShuffleMapTask] is requested to scheduler:ShuffleMapTask.md#runTask[run]. == [[implementations]] ShuffleWriters .ShuffleWriters [cols=\"40m,60\",options=\"header\",width=\"100%\"] |=== | ShuffleWriter | Description | shuffle:BypassMergeSortShuffleWriter.md[BypassMergeSortShuffleWriter] | [[BypassMergeSortShuffleWriter]] ShuffleWriter for a shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle] | shuffle:SortShuffleWriter.md[SortShuffleWriter] | [[SortShuffleWriter]] Fallback ShuffleWriter (when neither < > nor < > could be used) | shuffle:UnsafeShuffleWriter.md[UnsafeShuffleWriter] | [[UnsafeShuffleWriter]] ShuffleWriter for shuffle:SerializedShuffleHandle.md[SerializedShuffleHandles] |=== == [[stop]] Stopping ShuffleWriter [source, scala] \u00b6 stop( success: Boolean): Option[MapStatus] Stops ( closes ) the ShuffleWriter and returns a scheduler:MapStatus.md[MapStatus] if the writing completed successfully. The success flag is the status of the task execution. stop is used when ShuffleMapTask is requested to scheduler:ShuffleMapTask.md#runTask[run]. == [[write]] Writing Partition Records Out to Shuffle System [source, scala] \u00b6 write( records: Iterator[Product2[K, V]]): Unit Writes key-value records out to a shuffle system. write is used when ShuffleMapTask is requested to scheduler:ShuffleMapTask.md#runTask[run].","title":"ShuffleWriter"},{"location":"shuffle/ShuffleWriter/#shufflewriter","text":"ShuffleWriter of K keys and V values ( ShuffleWriter[K, V] ) is an abstraction of < > that can < > (of a RDD partition) to a shuffle system. ShuffleWriter is used when scheduler:ShuffleMapTask.md[ShuffleMapTask] is requested to scheduler:ShuffleMapTask.md#runTask[run]. == [[implementations]] ShuffleWriters .ShuffleWriters [cols=\"40m,60\",options=\"header\",width=\"100%\"] |=== | ShuffleWriter | Description | shuffle:BypassMergeSortShuffleWriter.md[BypassMergeSortShuffleWriter] | [[BypassMergeSortShuffleWriter]] ShuffleWriter for a shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle] | shuffle:SortShuffleWriter.md[SortShuffleWriter] | [[SortShuffleWriter]] Fallback ShuffleWriter (when neither < > nor < > could be used) | shuffle:UnsafeShuffleWriter.md[UnsafeShuffleWriter] | [[UnsafeShuffleWriter]] ShuffleWriter for shuffle:SerializedShuffleHandle.md[SerializedShuffleHandles] |=== == [[stop]] Stopping ShuffleWriter","title":"ShuffleWriter"},{"location":"shuffle/ShuffleWriter/#source-scala","text":"stop( success: Boolean): Option[MapStatus] Stops ( closes ) the ShuffleWriter and returns a scheduler:MapStatus.md[MapStatus] if the writing completed successfully. The success flag is the status of the task execution. stop is used when ShuffleMapTask is requested to scheduler:ShuffleMapTask.md#runTask[run]. == [[write]] Writing Partition Records Out to Shuffle System","title":"[source, scala]"},{"location":"shuffle/ShuffleWriter/#source-scala_1","text":"write( records: Iterator[Product2[K, V]]): Unit Writes key-value records out to a shuffle system. write is used when ShuffleMapTask is requested to scheduler:ShuffleMapTask.md#runTask[run].","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/","text":"SortShuffleManager \u00b6 SortShuffleManager is the default and only ShuffleManager in Apache Spark (with the short name sort or tungsten-sort ). Creating Instance \u00b6 SortShuffleManager takes the following to be created: SparkConf SortShuffleManager is created when SparkEnv is created (on the driver and executors at the very beginning of a Spark application's lifecycle). Getting ShuffleWriter For Partition and ShuffleHandle \u00b6 getWriter [ K , V ]( handle : ShuffleHandle , mapId : Int , context : TaskContext ) : ShuffleWriter [ K , V ] getWriter registers the given ShuffleHandle (by the shuffleId and numMaps ) in the numMapsForShuffle internal registry unless already done. Note getWriter expects that the input ShuffleHandle is a BaseShuffleHandle . Moreover, getWriter expects that in two (out of three cases) it is a more specialized IndexShuffleBlockResolver . getWriter then creates a new ShuffleWriter based on the type of the given ShuffleHandle . ShuffleHandle ShuffleWriter SerializedShuffleHandle UnsafeShuffleWriter BypassMergeSortShuffleHandle BypassMergeSortShuffleWriter BaseShuffleHandle SortShuffleWriter getWriter is part of the ShuffleManager abstraction. == [[MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE]] Maximum Number of Partition Identifiers SortShuffleManager allows for (1 << 24) partition identifiers that can be encoded (i.e. 16777216 ). == [[numMapsForShuffle]] numMapsForShuffle Lookup table with the number of mappers producing the output for a shuffle (as {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap]) == [[shuffleBlockResolver]] IndexShuffleBlockResolver [source, scala] \u00b6 shuffleBlockResolver: ShuffleBlockResolver \u00b6 shuffleBlockResolver is an shuffle:IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] that is created immediately when SortShuffleManager is. shuffleBlockResolver is used when SortShuffleManager is requested for a < >, to < > and < >. shuffleBlockResolver is part of the shuffle:ShuffleManager.md#shuffleBlockResolver[ShuffleManager] abstraction. == [[unregisterShuffle]] Unregistering Shuffle [source, scala] \u00b6 unregisterShuffle( shuffleId: Int): Boolean unregisterShuffle tries to remove the given shuffleId from the < > internal registry. If the given shuffleId was registered, unregisterShuffle requests the < > to < > one by one (up to the number of mappers producing the output for the shuffle). unregisterShuffle is part of the shuffle:ShuffleManager.md#unregisterShuffle[ShuffleManager] abstraction. == [[registerShuffle]] Creating ShuffleHandle (For ShuffleDependency) [source, scala] \u00b6 registerShuffle K, V, C : ShuffleHandle CAUTION: FIXME Copy the conditions registerShuffle returns a new ShuffleHandle that can be one of the following: shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle] (with ShuffleDependency[K, V, V] ) when shuffle:SortShuffleWriter.md#shouldBypassMergeSort[shouldBypassMergeSort] condition holds. shuffle:SerializedShuffleHandle.md[SerializedShuffleHandle] (with ShuffleDependency[K, V, V] ) when < >. shuffle:spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle] registerShuffle is part of the shuffle:ShuffleManager.md#registerShuffle[ShuffleManager] abstraction. == [[getReader]] Creating BlockStoreShuffleReader For ShuffleHandle And Reduce Partitions [source, scala] \u00b6 getReader K, C : ShuffleReader[K, C] getReader returns a new shuffle:BlockStoreShuffleReader.md[BlockStoreShuffleReader] passing all the input parameters on to it. getReader assumes that the input ShuffleHandle is of type shuffle:spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle]. getReader is part of the shuffle:ShuffleManager.md#getReader[ShuffleManager] abstraction. == [[stop]] Stopping SortShuffleManager [source, scala] \u00b6 stop(): Unit \u00b6 stop simply requests the < > to shuffle:IndexShuffleBlockResolver.md#stop[stop] (which actually does nothing). stop is part of the shuffle:ShuffleManager.md#stop[ShuffleManager] abstraction. == [[canUseSerializedShuffle]] Requirements of SerializedShuffleHandle (as ShuffleHandle) [source, scala] \u00b6 canUseSerializedShuffle( dependency: ShuffleDependency[_, _, _]): Boolean canUseSerializedShuffle returns true when all of the following hold: . Serializer (of the given ShuffleDependency ) serializer:Serializer.md#supportsRelocationOfSerializedObjects[supports relocation of serialized objects] . No map-side aggregation (the mapSideCombine flag of the given ShuffleDependency is off) . Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than the < > (i.e. (1 << 24) - 1 , i.e. 16777215 ) canUseSerializedShuffle prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Can use serialized shuffle for shuffle [shuffleId] \u00b6 Otherwise, canUseSerializedShuffle does not hold and prints out one of the following DEBUG messages: [source,plaintext] \u00b6 Can't use serialized shuffle for shuffle [id] because the serializer, [name], does not support object relocation Can't use serialized shuffle for shuffle [id] because an aggregator is defined Can't use serialized shuffle for shuffle [id] because it has more than [number] partitions \u00b6 shouldBypassMergeSort is used when SortShuffleManager is requested to shuffle:SortShuffleManager.md#registerShuffle[register a shuffle (and creates a ShuffleHandle)]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.SortShuffleManager=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"SortShuffleManager"},{"location":"shuffle/SortShuffleManager/#sortshufflemanager","text":"SortShuffleManager is the default and only ShuffleManager in Apache Spark (with the short name sort or tungsten-sort ).","title":"SortShuffleManager"},{"location":"shuffle/SortShuffleManager/#creating-instance","text":"SortShuffleManager takes the following to be created: SparkConf SortShuffleManager is created when SparkEnv is created (on the driver and executors at the very beginning of a Spark application's lifecycle).","title":"Creating Instance"},{"location":"shuffle/SortShuffleManager/#getting-shufflewriter-for-partition-and-shufflehandle","text":"getWriter [ K , V ]( handle : ShuffleHandle , mapId : Int , context : TaskContext ) : ShuffleWriter [ K , V ] getWriter registers the given ShuffleHandle (by the shuffleId and numMaps ) in the numMapsForShuffle internal registry unless already done. Note getWriter expects that the input ShuffleHandle is a BaseShuffleHandle . Moreover, getWriter expects that in two (out of three cases) it is a more specialized IndexShuffleBlockResolver . getWriter then creates a new ShuffleWriter based on the type of the given ShuffleHandle . ShuffleHandle ShuffleWriter SerializedShuffleHandle UnsafeShuffleWriter BypassMergeSortShuffleHandle BypassMergeSortShuffleWriter BaseShuffleHandle SortShuffleWriter getWriter is part of the ShuffleManager abstraction. == [[MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE]] Maximum Number of Partition Identifiers SortShuffleManager allows for (1 << 24) partition identifiers that can be encoded (i.e. 16777216 ). == [[numMapsForShuffle]] numMapsForShuffle Lookup table with the number of mappers producing the output for a shuffle (as {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap]) == [[shuffleBlockResolver]] IndexShuffleBlockResolver","title":" Getting ShuffleWriter For Partition and ShuffleHandle"},{"location":"shuffle/SortShuffleManager/#source-scala","text":"","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/#shuffleblockresolver-shuffleblockresolver","text":"shuffleBlockResolver is an shuffle:IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] that is created immediately when SortShuffleManager is. shuffleBlockResolver is used when SortShuffleManager is requested for a < >, to < > and < >. shuffleBlockResolver is part of the shuffle:ShuffleManager.md#shuffleBlockResolver[ShuffleManager] abstraction. == [[unregisterShuffle]] Unregistering Shuffle","title":"shuffleBlockResolver: ShuffleBlockResolver"},{"location":"shuffle/SortShuffleManager/#source-scala_1","text":"unregisterShuffle( shuffleId: Int): Boolean unregisterShuffle tries to remove the given shuffleId from the < > internal registry. If the given shuffleId was registered, unregisterShuffle requests the < > to < > one by one (up to the number of mappers producing the output for the shuffle). unregisterShuffle is part of the shuffle:ShuffleManager.md#unregisterShuffle[ShuffleManager] abstraction. == [[registerShuffle]] Creating ShuffleHandle (For ShuffleDependency)","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/#source-scala_2","text":"registerShuffle K, V, C : ShuffleHandle CAUTION: FIXME Copy the conditions registerShuffle returns a new ShuffleHandle that can be one of the following: shuffle:BypassMergeSortShuffleHandle.md[BypassMergeSortShuffleHandle] (with ShuffleDependency[K, V, V] ) when shuffle:SortShuffleWriter.md#shouldBypassMergeSort[shouldBypassMergeSort] condition holds. shuffle:SerializedShuffleHandle.md[SerializedShuffleHandle] (with ShuffleDependency[K, V, V] ) when < >. shuffle:spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle] registerShuffle is part of the shuffle:ShuffleManager.md#registerShuffle[ShuffleManager] abstraction. == [[getReader]] Creating BlockStoreShuffleReader For ShuffleHandle And Reduce Partitions","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/#source-scala_3","text":"getReader K, C : ShuffleReader[K, C] getReader returns a new shuffle:BlockStoreShuffleReader.md[BlockStoreShuffleReader] passing all the input parameters on to it. getReader assumes that the input ShuffleHandle is of type shuffle:spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle]. getReader is part of the shuffle:ShuffleManager.md#getReader[ShuffleManager] abstraction. == [[stop]] Stopping SortShuffleManager","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/#stop-unit","text":"stop simply requests the < > to shuffle:IndexShuffleBlockResolver.md#stop[stop] (which actually does nothing). stop is part of the shuffle:ShuffleManager.md#stop[ShuffleManager] abstraction. == [[canUseSerializedShuffle]] Requirements of SerializedShuffleHandle (as ShuffleHandle)","title":"stop(): Unit"},{"location":"shuffle/SortShuffleManager/#source-scala_5","text":"canUseSerializedShuffle( dependency: ShuffleDependency[_, _, _]): Boolean canUseSerializedShuffle returns true when all of the following hold: . Serializer (of the given ShuffleDependency ) serializer:Serializer.md#supportsRelocationOfSerializedObjects[supports relocation of serialized objects] . No map-side aggregation (the mapSideCombine flag of the given ShuffleDependency is off) . Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than the < > (i.e. (1 << 24) - 1 , i.e. 16777215 ) canUseSerializedShuffle prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"shuffle/SortShuffleManager/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/SortShuffleManager/#can-use-serialized-shuffle-for-shuffle-shuffleid","text":"Otherwise, canUseSerializedShuffle does not hold and prints out one of the following DEBUG messages:","title":"Can use serialized shuffle for shuffle [shuffleId]"},{"location":"shuffle/SortShuffleManager/#sourceplaintext_1","text":"Can't use serialized shuffle for shuffle [id] because the serializer, [name], does not support object relocation Can't use serialized shuffle for shuffle [id] because an aggregator is defined","title":"[source,plaintext]"},{"location":"shuffle/SortShuffleManager/#cant-use-serialized-shuffle-for-shuffle-id-because-it-has-more-than-number-partitions","text":"shouldBypassMergeSort is used when SortShuffleManager is requested to shuffle:SortShuffleManager.md#registerShuffle[register a shuffle (and creates a ShuffleHandle)]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Can't use serialized shuffle for shuffle [id] because it has more than [number] partitions"},{"location":"shuffle/SortShuffleManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"shuffle/SortShuffleManager/#log4jloggerorgapachesparkshufflesortsortshufflemanagerall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.shuffle.sort.SortShuffleManager=ALL"},{"location":"shuffle/SortShuffleWriter/","text":"SortShuffleWriter \u00b6 SortShuffleWriter is a concrete ShuffleWriter that is used when SortShuffleManager.md#getWriter[ SortShuffleManager returns a ShuffleWriter for ShuffleHandle ] (and the more specialized BypassMergeSortShuffleWriter.md[BypassMergeSortShuffleWriter] and UnsafeShuffleWriter.md[UnsafeShuffleWriter] could not be used). SortShuffleWriter is created when SortShuffleManager.md#getWriter[ SortShuffleManager returns a ShuffleWriter for the fallback BaseShuffleHandle ]. SortShuffleWriter[K, V, C] is a parameterized type with K keys, V values, and C combiner values. == [[creating-instance]] Creating Instance SortShuffleWriter takes the following to be created: [[shuffleBlockResolver]] IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] [[handle]] spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle] [[mapId]] Map ID [[context]] scheduler:spark-TaskContext.md[TaskContext] == [[mapStatus]] MapStatus SortShuffleWriter uses an internal variable for a scheduler:MapStatus.md[MapStatus] after < >. < > itself does not return a value, SortShuffleWriter uses the variable when requested to < > (which allows returning a MapStatus). == [[write]] Writing Records (Into Shuffle Partitioned File In Disk Store) [source, scala] \u00b6 write( records: Iterator[Product2[K, V]]): Unit write creates an ExternalSorter.md[ExternalSorter] based on the spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] (of the < >), namely the Map-Size Partial Aggregation flag. The ExternalSorter uses the aggregator and keyOrdering when the flag is enabled. write requests the ExternalSorter to inserts all the records . write requests the < > for a IndexShuffleBlockResolver.md#getDataFile[shuffle output file] (for the shuffle and the < > IDs) and creates a temporary file alongside the shuffle output file (in the same directory). write creates a storage:BlockId.md#ShuffleBlockId[ShuffleBlockId] (for the shuffle and the < > IDs). write requests ExternalSorter to ExternalSorter.md#writePartitionedFile[write all the records (previously inserted in) into the temporary partitioned file in the disk store]. ExternalSorter returns the length of every partition. write requests < > to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write an index file and commit] (with the shuffle and the < > IDs, the temporary shuffle output file). write creates a scheduler:MapStatus.md[MapStatus] (with the storage:BlockManager.md#shuffleServerId[location of the shuffle server] that serves the shuffle files and the sizes of the shuffle partitions). The MapStatus is later available as the < > internal attribute. write does not handle exceptions so when they occur, they will break the processing. In the end, write deletes the temporary shuffle output file. write prints out the following ERROR message to the logs if the file count not be deleted: Error while deleting temp file [path] write is part of the ShuffleWriter.md#write[ShuffleWriter] abstraction. == [[stop]] Closing SortShuffleWriter (and Calculating MapStatus) [source, scala] \u00b6 stop( success: Boolean): Option[MapStatus] stop turns < > flag on and returns the internal < > if the input success is enabled. Otherwise, when < > flag is already enabled or the input success is disabled, stop returns no MapStatus (i.e. None ). In the end, stop requests the ExternalSorter to ExternalSorter.md#stop[stop] and increments the shuffle write time task metrics. stop is part of the ShuffleWriter.md#contract[ShuffleWriter] abstraction. == [[shouldBypassMergeSort]] Requirements of BypassMergeSortShuffleHandle (as ShuffleHandle) [source, scala] \u00b6 shouldBypassMergeSort( conf: SparkConf, dep: ShuffleDependency[_, _, _]): Boolean shouldBypassMergeSort returns true when all of the following hold: . No map-side aggregation (the mapSideCombine flag of the given ShuffleDependency is off) . Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than spark.shuffle.sort.bypassMergeThreshold configuration property Otherwise, shouldBypassMergeSort does not hold (i.e. false ). shouldBypassMergeSort is used when SortShuffleManager is requested to register a shuffle (and creates a ShuffleHandle) . == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.SortShuffleWriter=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[stopping]] stopping | Internal flag to mark that < >. |===","title":"SortShuffleWriter"},{"location":"shuffle/SortShuffleWriter/#sortshufflewriter","text":"SortShuffleWriter is a concrete ShuffleWriter that is used when SortShuffleManager.md#getWriter[ SortShuffleManager returns a ShuffleWriter for ShuffleHandle ] (and the more specialized BypassMergeSortShuffleWriter.md[BypassMergeSortShuffleWriter] and UnsafeShuffleWriter.md[UnsafeShuffleWriter] could not be used). SortShuffleWriter is created when SortShuffleManager.md#getWriter[ SortShuffleManager returns a ShuffleWriter for the fallback BaseShuffleHandle ]. SortShuffleWriter[K, V, C] is a parameterized type with K keys, V values, and C combiner values. == [[creating-instance]] Creating Instance SortShuffleWriter takes the following to be created: [[shuffleBlockResolver]] IndexShuffleBlockResolver.md[IndexShuffleBlockResolver] [[handle]] spark-shuffle-BaseShuffleHandle.md[BaseShuffleHandle] [[mapId]] Map ID [[context]] scheduler:spark-TaskContext.md[TaskContext] == [[mapStatus]] MapStatus SortShuffleWriter uses an internal variable for a scheduler:MapStatus.md[MapStatus] after < >. < > itself does not return a value, SortShuffleWriter uses the variable when requested to < > (which allows returning a MapStatus). == [[write]] Writing Records (Into Shuffle Partitioned File In Disk Store)","title":"SortShuffleWriter"},{"location":"shuffle/SortShuffleWriter/#source-scala","text":"write( records: Iterator[Product2[K, V]]): Unit write creates an ExternalSorter.md[ExternalSorter] based on the spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] (of the < >), namely the Map-Size Partial Aggregation flag. The ExternalSorter uses the aggregator and keyOrdering when the flag is enabled. write requests the ExternalSorter to inserts all the records . write requests the < > for a IndexShuffleBlockResolver.md#getDataFile[shuffle output file] (for the shuffle and the < > IDs) and creates a temporary file alongside the shuffle output file (in the same directory). write creates a storage:BlockId.md#ShuffleBlockId[ShuffleBlockId] (for the shuffle and the < > IDs). write requests ExternalSorter to ExternalSorter.md#writePartitionedFile[write all the records (previously inserted in) into the temporary partitioned file in the disk store]. ExternalSorter returns the length of every partition. write requests < > to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write an index file and commit] (with the shuffle and the < > IDs, the temporary shuffle output file). write creates a scheduler:MapStatus.md[MapStatus] (with the storage:BlockManager.md#shuffleServerId[location of the shuffle server] that serves the shuffle files and the sizes of the shuffle partitions). The MapStatus is later available as the < > internal attribute. write does not handle exceptions so when they occur, they will break the processing. In the end, write deletes the temporary shuffle output file. write prints out the following ERROR message to the logs if the file count not be deleted: Error while deleting temp file [path] write is part of the ShuffleWriter.md#write[ShuffleWriter] abstraction. == [[stop]] Closing SortShuffleWriter (and Calculating MapStatus)","title":"[source, scala]"},{"location":"shuffle/SortShuffleWriter/#source-scala_1","text":"stop( success: Boolean): Option[MapStatus] stop turns < > flag on and returns the internal < > if the input success is enabled. Otherwise, when < > flag is already enabled or the input success is disabled, stop returns no MapStatus (i.e. None ). In the end, stop requests the ExternalSorter to ExternalSorter.md#stop[stop] and increments the shuffle write time task metrics. stop is part of the ShuffleWriter.md#contract[ShuffleWriter] abstraction. == [[shouldBypassMergeSort]] Requirements of BypassMergeSortShuffleHandle (as ShuffleHandle)","title":"[source, scala]"},{"location":"shuffle/SortShuffleWriter/#source-scala_2","text":"shouldBypassMergeSort( conf: SparkConf, dep: ShuffleDependency[_, _, _]): Boolean shouldBypassMergeSort returns true when all of the following hold: . No map-side aggregation (the mapSideCombine flag of the given ShuffleDependency is off) . Number of partitions (of the Partitioner of the given ShuffleDependency ) is not greater than spark.shuffle.sort.bypassMergeThreshold configuration property Otherwise, shouldBypassMergeSort does not hold (i.e. false ). shouldBypassMergeSort is used when SortShuffleManager is requested to register a shuffle (and creates a ShuffleHandle) . == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.SortShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"shuffle/SortShuffleWriter/#source","text":"","title":"[source]"},{"location":"shuffle/SortShuffleWriter/#log4jloggerorgapachesparkshufflesortsortshufflewriterall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[stopping]] stopping | Internal flag to mark that < >. |===","title":"log4j.logger.org.apache.spark.shuffle.sort.SortShuffleWriter=ALL"},{"location":"shuffle/Spillable/","text":"Spillable \u00b6 Spillable is an extension of the MemoryConsumer abstraction for spillable collections that can spill to disk . Spillable[C] is a parameterized type of C combiner (partial) values. Contract \u00b6 forceSpill \u00b6 forceSpill () : Boolean Force spilling the current in-memory collection to disk to release memory. Used when Spillable is requested to spill spill \u00b6 spill ( collection : C ) : Unit Spills the current in-memory collection to disk, and releases the memory. Used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to spilling to disk if necessary Implementations \u00b6 ExternalAppendOnlyMap ExternalSorter Memory Threshold \u00b6 Spillable uses a threshold for the memory size (in bytes) to know when to spill to disk . When the size of the in-memory collection is above the threshold, Spillable will try to acquire more memory. Unless given all requested memory, Spillable spills to disk. The memory threshold starts as spark.shuffle.spill.initialMemoryThreshold configuration property and is increased every time Spillable is requested to spill to disk if needed , but managed to acquire required memory. The threshold goes back to the initial value when requested to release all memory . Used when Spillable is requested to spill and releaseMemory . Creating Instance \u00b6 Spillable takes the following to be created: TaskMemoryManager Abstract Class Spillable is an abstract class and cannot be created directly. It is created indirectly for the concrete Spillables . Configuration Properties \u00b6 spark.shuffle.spill.numElementsForceSpillThreshold \u00b6 Spillable uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property to force spilling in-memory objects to disk when requested to maybeSpill . spark.shuffle.spill.initialMemoryThreshold \u00b6 Spillable uses spark.shuffle.spill.initialMemoryThreshold configuration property as the initial threshold for the size of a collection (and the minimum memory required to operate properly). Spillable uses it when requested to spill and releaseMemory . Releasing All Memory \u00b6 releaseMemory () : Unit releaseMemory ...FIXME releaseMemory is used when: ExternalAppendOnlyMap is requested to freeCurrentMap ExternalSorter is requested to stop Spillable is requested to maybeSpill and spill (and spilled to disk in either case) Spilling In-Memory Collection to Disk (to Release Memory) \u00b6 spill ( collection : C ) : Unit spill spills the given in-memory collection to disk to release memory. spill is used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to maybeSpill forceSpill \u00b6 forceSpill () : Boolean forceSpill forcefully spills the Spillable to disk to release memory. forceSpill is used when Spillable is requested to spill an in-memory collection to disk . Spilling to Disk if Necessary \u00b6 maybeSpill ( collection : C , currentMemory : Long ) : Boolean maybeSpill ...FIXME maybeSpill is used when: ExternalAppendOnlyMap is requested to insertAll ExternalSorter is requested to attempt to spill an in-memory collection to disk if needed","title":"Spillable"},{"location":"shuffle/Spillable/#spillable","text":"Spillable is an extension of the MemoryConsumer abstraction for spillable collections that can spill to disk . Spillable[C] is a parameterized type of C combiner (partial) values.","title":"Spillable"},{"location":"shuffle/Spillable/#contract","text":"","title":"Contract"},{"location":"shuffle/Spillable/#forcespill","text":"forceSpill () : Boolean Force spilling the current in-memory collection to disk to release memory. Used when Spillable is requested to spill","title":" forceSpill"},{"location":"shuffle/Spillable/#spill","text":"spill ( collection : C ) : Unit Spills the current in-memory collection to disk, and releases the memory. Used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to spilling to disk if necessary","title":" spill"},{"location":"shuffle/Spillable/#implementations","text":"ExternalAppendOnlyMap ExternalSorter","title":"Implementations"},{"location":"shuffle/Spillable/#memory-threshold","text":"Spillable uses a threshold for the memory size (in bytes) to know when to spill to disk . When the size of the in-memory collection is above the threshold, Spillable will try to acquire more memory. Unless given all requested memory, Spillable spills to disk. The memory threshold starts as spark.shuffle.spill.initialMemoryThreshold configuration property and is increased every time Spillable is requested to spill to disk if needed , but managed to acquire required memory. The threshold goes back to the initial value when requested to release all memory . Used when Spillable is requested to spill and releaseMemory .","title":" Memory Threshold"},{"location":"shuffle/Spillable/#creating-instance","text":"Spillable takes the following to be created: TaskMemoryManager Abstract Class Spillable is an abstract class and cannot be created directly. It is created indirectly for the concrete Spillables .","title":"Creating Instance"},{"location":"shuffle/Spillable/#configuration-properties","text":"","title":"Configuration Properties"},{"location":"shuffle/Spillable/#sparkshufflespillnumelementsforcespillthreshold","text":"Spillable uses spark.shuffle.spill.numElementsForceSpillThreshold configuration property to force spilling in-memory objects to disk when requested to maybeSpill .","title":" spark.shuffle.spill.numElementsForceSpillThreshold"},{"location":"shuffle/Spillable/#sparkshufflespillinitialmemorythreshold","text":"Spillable uses spark.shuffle.spill.initialMemoryThreshold configuration property as the initial threshold for the size of a collection (and the minimum memory required to operate properly). Spillable uses it when requested to spill and releaseMemory .","title":" spark.shuffle.spill.initialMemoryThreshold"},{"location":"shuffle/Spillable/#releasing-all-memory","text":"releaseMemory () : Unit releaseMemory ...FIXME releaseMemory is used when: ExternalAppendOnlyMap is requested to freeCurrentMap ExternalSorter is requested to stop Spillable is requested to maybeSpill and spill (and spilled to disk in either case)","title":" Releasing All Memory"},{"location":"shuffle/Spillable/#spilling-in-memory-collection-to-disk-to-release-memory","text":"spill ( collection : C ) : Unit spill spills the given in-memory collection to disk to release memory. spill is used when: ExternalAppendOnlyMap is requested to forceSpill Spillable is requested to maybeSpill","title":" Spilling In-Memory Collection to Disk (to Release Memory)"},{"location":"shuffle/Spillable/#forcespill_1","text":"forceSpill () : Boolean forceSpill forcefully spills the Spillable to disk to release memory. forceSpill is used when Spillable is requested to spill an in-memory collection to disk .","title":" forceSpill"},{"location":"shuffle/Spillable/#spilling-to-disk-if-necessary","text":"maybeSpill ( collection : C , currentMemory : Long ) : Boolean maybeSpill ...FIXME maybeSpill is used when: ExternalAppendOnlyMap is requested to insertAll ExternalSorter is requested to attempt to spill an in-memory collection to disk if needed","title":" Spilling to Disk if Necessary"},{"location":"shuffle/UnsafeShuffleWriter/","text":"UnsafeShuffleWriter \u00b6 UnsafeShuffleWriter<K, V> is a ShuffleWriter for SerializedShuffleHandle s. UnsafeShuffleWriter opens resources (a ShuffleExternalSorter and the buffers) while being created . Creating Instance \u00b6 UnsafeShuffleWriter takes the following to be created: BlockManager TaskMemoryManager SerializedShuffleHandle Map ID TaskContext SparkConf ShuffleWriteMetricsReporter ShuffleExecutorComponents UnsafeShuffleWriter is created when SortShuffleManager is requested for a ShuffleWriter for a SerializedShuffleHandle . UnsafeShuffleWriter makes sure that the number of partitions at most 16MB reduce partitions ( 1 << 24 ) (as the upper bound of the partition identifiers that can be encoded ) or throws an IllegalArgumentException : UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions UnsafeShuffleWriter uses the number of partitions of the Partitioner that is used for the ShuffleDependency of the SerializedShuffleHandle . Note The number of shuffle output partitions is first enforced when SortShuffleManager is requested to check out whether a SerializedShuffleHandle can be used for ShuffleHandle (that eventually leads to UnsafeShuffleWriter ). In the end, UnsafeShuffleWriter creates a ShuffleExternalSorter and a SerializationStream . ShuffleExternalSorter \u00b6 UnsafeShuffleWriter uses a ShuffleExternalSorter . ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open (while being created ) and dereferenced ( null ed) when requested to close internal resources and merge spill files . Used when UnsafeShuffleWriter is requested for the following: Updating peak memory used Writing records Closing internal resources and merging spill files Inserting a record Stopping IndexShuffleBlockResolver \u00b6 UnsafeShuffleWriter is given a IndexShuffleBlockResolver when created . UnsafeShuffleWriter uses the IndexShuffleBlockResolver for...FIXME Initial Serialized Buffer Size \u00b6 UnsafeShuffleWriter uses a fixed buffer size for the output stream of serialized data written into a byte array (default: 1024 * 1024 ). inputBufferSizeInBytes \u00b6 UnsafeShuffleWriter uses the spark.shuffle.file.buffer configuration property for...FIXME outputBufferSizeInBytes \u00b6 UnsafeShuffleWriter uses the spark.shuffle.unsafe.file.output.buffer configuration property for...FIXME transferToEnabled \u00b6 UnsafeShuffleWriter can use a specialized NIO-based fast merge procedure that avoids extra serialization/deserialization when spark.file.transferTo configuration property is enabled. initialSortBufferSize \u00b6 UnsafeShuffleWriter uses the initial buffer size for sorting (default: 4096 ) when creating a ShuffleExternalSorter (when requested to open ). Tip Use spark.shuffle.sort.initialBufferSize configuration property to change the buffer size. Merging Spills \u00b6 long [] mergeSpills ( SpillInfo [] spills , File outputFile ) Many Spills \u00b6 With multiple SpillInfos to merge, mergeSpills selects between fast and slow merge strategies . The fast merge strategy can be transferTo - or fileStream -based. mergeSpills uses the spark.shuffle.unsafe.fastMergeEnabled configuration property to consider one of the fast merge strategies. A fast merge strategy is supported when spark.shuffle.compress configuration property is disabled or the IO compression codec supports decompression of concatenated compressed streams . With spark.shuffle.compress configuration property enabled, mergeSpills will always use the slow merge strategy. With fast merge strategy enabled and supported, transferToEnabled enabled and encryption disabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithTransferTo . Using transferTo-based fast merge With fast merge strategy enabled and supported, no transferToEnabled or encryption enabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with no compression codec). Using fileStream-based fast merge For slow merge, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with the compression codec). Using slow merge In the end, mergeSpills requests the ShuffleWriteMetrics to decBytesWritten and incBytesWritten , and returns the partition length array. One Spill \u00b6 With one SpillInfo to merge, mergeSpills simply renames the spill file to be the output file and returns the partition length array of the one spill. No Spills \u00b6 With no SpillInfo s to merge, mergeSpills creates an empty output file and returns an array of 0 s of size of the numPartitions of the Partitioner . Usage \u00b6 mergeSpills is used when UnsafeShuffleWriter is requested to close internal resources and merge spill files . mergeSpillsWithTransferTo \u00b6 long [] mergeSpillsWithTransferTo ( SpillInfo [] spills , File outputFile ) mergeSpillsWithTransferTo ...FIXME mergeSpillsWithTransferTo is used when UnsafeShuffleWriter is requested to mergeSpills (with the transferToEnabled flag enabled and no encryption). == [[updatePeakMemoryUsed]] updatePeakMemoryUsed Internal Method [source, java] \u00b6 void updatePeakMemoryUsed() \u00b6 updatePeakMemoryUsed...FIXME updatePeakMemoryUsed is used when UnsafeShuffleWriter is requested for the < > and to < >. Writing Key-Value Records of Partition \u00b6 void write ( Iterator < Product2 < K , V >> records ) write traverses the input sequence of records (for a RDD partition) and insertRecordIntoSorter one by one. When all the records have been processed, write closes internal resources and merges spill files . In the end, write requests ShuffleExternalSorter to clean up . CAUTION: FIXME When requested to < >, UnsafeShuffleWriter simply < > followed by < > (that, among other things, creates the < >). write is part of the ShuffleWriter abstraction. == [[stop]] Stopping ShuffleWriter [source, java] \u00b6 Option stop( boolean success) stop ...FIXME When requested to < >, UnsafeShuffleWriter records the peak execution memory metric and returns the < > (that was created when requested to < >). stop is part of the ShuffleWriter abstraction. == [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter [source, java] \u00b6 void insertRecordIntoSorter( Product2 record) insertRecordIntoSorter requires that the < > is available. insertRecordIntoSorter requests the < > to reset (so that all currently accumulated output in the output stream is discarded and reusing the already allocated buffer space). insertRecordIntoSorter requests the < > to write out the record (write the serializer:SerializationStream.md#writeKey[key] and the serializer:SerializationStream.md#writeValue[value]) and to serializer:SerializationStream.md#flush[flush]. [[insertRecordIntoSorter-serializedRecordSize]] insertRecordIntoSorter requests the < > for the length of the buffer. [[insertRecordIntoSorter-partitionId]] insertRecordIntoSorter requests the < > for the ../rdd/Partitioner.md#getPartition[partition] for the given record (by the key). In the end, insertRecordIntoSorter requests the < > to ShuffleExternalSorter.md#insertRecord[insert] the < > as a byte array (with the < > and the < >). insertRecordIntoSorter is used when UnsafeShuffleWriter is requested to < >. Closing and Writing Output (Merging Spill Files) \u00b6 void closeAndWriteOutput () closeAndWriteOutput asserts that the ShuffleExternalSorter is created (non- null ). closeAndWriteOutput updates peak memory used . closeAndWriteOutput removes the references to the < > and < > output streams ( null s them). closeAndWriteOutput requests the < > to ShuffleExternalSorter.md#closeAndGetSpills[close and return spill metadata]. closeAndWriteOutput removes the reference to the < > ( null s it). closeAndWriteOutput requests the < > for the IndexShuffleBlockResolver.md#getDataFile[output data file] for the < > and < > IDs. [[closeAndWriteOutput-partitionLengths]][[closeAndWriteOutput-tmp]] closeAndWriteOutput creates a temporary file (along the data output file) and uses it to < > (that gives a partition length array). All spill files are then deleted. closeAndWriteOutput requests the < > to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files] (for the < > and < > IDs, the < > and the < >). In the end, closeAndWriteOutput creates a scheduler:MapStatus.md[MapStatus] with the storage:BlockManager.md#shuffleServerId[location of the local BlockManager] and the < >. closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting spill files: Error while deleting spill file [path] closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting the < >: Error while deleting temp file [path] closeAndWriteOutput is used when UnsafeShuffleWriter is requested to write records . == [[mergeSpillsWithFileStream]] mergeSpillsWithFileStream Method [source, java] \u00b6 long[] mergeSpillsWithFileStream( SpillInfo[] spills, File outputFile, CompressionCodec compressionCodec) mergeSpillsWithFileStream will be given an io:CompressionCodec.md[IO compression codec] when shuffle compression is enabled. mergeSpillsWithFileStream...FIXME mergeSpillsWithFileStream requires that there are at least two spills to merge. mergeSpillsWithFileStream is used when UnsafeShuffleWriter is requested to < >. == [[getPeakMemoryUsedBytes]] Getting Peak Memory Used [source, java] \u00b6 long getPeakMemoryUsedBytes() \u00b6 getPeakMemoryUsedBytes simply < > and returns the internal < > registry. getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to < >. == [[open]] Opening UnsafeShuffleWriter and Buffers [source, java] \u00b6 void open() \u00b6 open requires that there is no < > available. open creates a ShuffleExternalSorter.md[ShuffleExternalSorter]. open creates a < > with the capacity of < >. open requests the < > for a serializer:SerializerInstance.md#serializeStream[SerializationStream] to the < > (available internally as the < > reference). open is used when UnsafeShuffleWriter is < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.UnsafeShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. Internal Properties \u00b6 MapStatus \u00b6 MapStatus Created when UnsafeShuffleWriter is requested to < > (with the storage:BlockManagerId.md[] of the < > and partitionLengths ) Returned when UnsafeShuffleWriter is requested to < > Partitioner \u00b6 Partitioner (as used by the spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and create a ShuffleExternalSorter.md[ShuffleExternalSorter] with the given ../rdd/Partitioner.md#numPartitions[number of partitions]) < > (and request the ../rdd/Partitioner.md#getPartition[partition for the key]) < >, < > and < > (for the ../rdd/Partitioner.md#numPartitions[number of partitions] to create partition lengths) Peak Memory Used \u00b6 Peak memory used (in bytes) that is updated exclusively in < > (after requesting the < > for ShuffleExternalSorter.md#getPeakMemoryUsedBytes[getPeakMemoryUsedBytes]) Use < > to access the current value ByteArrayOutputStream for Serialized Data \u00b6 {java-javadoc-url}/java/io/ByteArrayOutputStream.html[java.io.ByteArrayOutputStream] of serialized data (written into a byte array of < > initial size) Used when UnsafeShuffleWriter is requested for the following: < > (and create the internal < >) < > Destroyed ( null ) when requested to < >. === [[serializer]] serializer serializer:SerializerInstance.md[SerializerInstance] (that is a new instance of the Serializer of the spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used exclusively when UnsafeShuffleWriter is requested to < > (and creates the < >) === [[serOutputStream]] serOutputStream serializer:SerializationStream.md[SerializationStream] (that is created when the < > is requested to serializer:SerializerInstance.md#serializeStream[serializeStream] with the < >) Used when UnsafeShuffleWriter is requested to < > Destroyed ( null ) when requested to < >. Shuffle ID \u00b6 Shuffle ID (of the ShuffleDependency of the SerializedShuffleHandle ) Used exclusively when requested to < > === [[writeMetrics]] writeMetrics executor:ShuffleWriteMetrics.md[] (of the scheduler:spark-TaskContext.md#taskMetrics[TaskMetrics] of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and creates the < >) < > < > < >","title":"UnsafeShuffleWriter"},{"location":"shuffle/UnsafeShuffleWriter/#unsafeshufflewriter","text":"UnsafeShuffleWriter<K, V> is a ShuffleWriter for SerializedShuffleHandle s. UnsafeShuffleWriter opens resources (a ShuffleExternalSorter and the buffers) while being created .","title":"UnsafeShuffleWriter"},{"location":"shuffle/UnsafeShuffleWriter/#creating-instance","text":"UnsafeShuffleWriter takes the following to be created: BlockManager TaskMemoryManager SerializedShuffleHandle Map ID TaskContext SparkConf ShuffleWriteMetricsReporter ShuffleExecutorComponents UnsafeShuffleWriter is created when SortShuffleManager is requested for a ShuffleWriter for a SerializedShuffleHandle . UnsafeShuffleWriter makes sure that the number of partitions at most 16MB reduce partitions ( 1 << 24 ) (as the upper bound of the partition identifiers that can be encoded ) or throws an IllegalArgumentException : UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions UnsafeShuffleWriter uses the number of partitions of the Partitioner that is used for the ShuffleDependency of the SerializedShuffleHandle . Note The number of shuffle output partitions is first enforced when SortShuffleManager is requested to check out whether a SerializedShuffleHandle can be used for ShuffleHandle (that eventually leads to UnsafeShuffleWriter ). In the end, UnsafeShuffleWriter creates a ShuffleExternalSorter and a SerializationStream .","title":"Creating Instance"},{"location":"shuffle/UnsafeShuffleWriter/#shuffleexternalsorter","text":"UnsafeShuffleWriter uses a ShuffleExternalSorter . ShuffleExternalSorter is created when UnsafeShuffleWriter is requested to open (while being created ) and dereferenced ( null ed) when requested to close internal resources and merge spill files . Used when UnsafeShuffleWriter is requested for the following: Updating peak memory used Writing records Closing internal resources and merging spill files Inserting a record Stopping","title":" ShuffleExternalSorter"},{"location":"shuffle/UnsafeShuffleWriter/#indexshuffleblockresolver","text":"UnsafeShuffleWriter is given a IndexShuffleBlockResolver when created . UnsafeShuffleWriter uses the IndexShuffleBlockResolver for...FIXME","title":" IndexShuffleBlockResolver"},{"location":"shuffle/UnsafeShuffleWriter/#initial-serialized-buffer-size","text":"UnsafeShuffleWriter uses a fixed buffer size for the output stream of serialized data written into a byte array (default: 1024 * 1024 ).","title":" Initial Serialized Buffer Size"},{"location":"shuffle/UnsafeShuffleWriter/#inputbuffersizeinbytes","text":"UnsafeShuffleWriter uses the spark.shuffle.file.buffer configuration property for...FIXME","title":" inputBufferSizeInBytes"},{"location":"shuffle/UnsafeShuffleWriter/#outputbuffersizeinbytes","text":"UnsafeShuffleWriter uses the spark.shuffle.unsafe.file.output.buffer configuration property for...FIXME","title":" outputBufferSizeInBytes"},{"location":"shuffle/UnsafeShuffleWriter/#transfertoenabled","text":"UnsafeShuffleWriter can use a specialized NIO-based fast merge procedure that avoids extra serialization/deserialization when spark.file.transferTo configuration property is enabled.","title":" transferToEnabled"},{"location":"shuffle/UnsafeShuffleWriter/#initialsortbuffersize","text":"UnsafeShuffleWriter uses the initial buffer size for sorting (default: 4096 ) when creating a ShuffleExternalSorter (when requested to open ). Tip Use spark.shuffle.sort.initialBufferSize configuration property to change the buffer size.","title":" initialSortBufferSize"},{"location":"shuffle/UnsafeShuffleWriter/#merging-spills","text":"long [] mergeSpills ( SpillInfo [] spills , File outputFile )","title":" Merging Spills"},{"location":"shuffle/UnsafeShuffleWriter/#many-spills","text":"With multiple SpillInfos to merge, mergeSpills selects between fast and slow merge strategies . The fast merge strategy can be transferTo - or fileStream -based. mergeSpills uses the spark.shuffle.unsafe.fastMergeEnabled configuration property to consider one of the fast merge strategies. A fast merge strategy is supported when spark.shuffle.compress configuration property is disabled or the IO compression codec supports decompression of concatenated compressed streams . With spark.shuffle.compress configuration property enabled, mergeSpills will always use the slow merge strategy. With fast merge strategy enabled and supported, transferToEnabled enabled and encryption disabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithTransferTo . Using transferTo-based fast merge With fast merge strategy enabled and supported, no transferToEnabled or encryption enabled, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with no compression codec). Using fileStream-based fast merge For slow merge, mergeSpills prints out the following DEBUG message to the logs and mergeSpillsWithFileStream (with the compression codec). Using slow merge In the end, mergeSpills requests the ShuffleWriteMetrics to decBytesWritten and incBytesWritten , and returns the partition length array.","title":" Many Spills"},{"location":"shuffle/UnsafeShuffleWriter/#one-spill","text":"With one SpillInfo to merge, mergeSpills simply renames the spill file to be the output file and returns the partition length array of the one spill.","title":" One Spill"},{"location":"shuffle/UnsafeShuffleWriter/#no-spills","text":"With no SpillInfo s to merge, mergeSpills creates an empty output file and returns an array of 0 s of size of the numPartitions of the Partitioner .","title":" No Spills"},{"location":"shuffle/UnsafeShuffleWriter/#usage","text":"mergeSpills is used when UnsafeShuffleWriter is requested to close internal resources and merge spill files .","title":" Usage"},{"location":"shuffle/UnsafeShuffleWriter/#mergespillswithtransferto","text":"long [] mergeSpillsWithTransferTo ( SpillInfo [] spills , File outputFile ) mergeSpillsWithTransferTo ...FIXME mergeSpillsWithTransferTo is used when UnsafeShuffleWriter is requested to mergeSpills (with the transferToEnabled flag enabled and no encryption). == [[updatePeakMemoryUsed]] updatePeakMemoryUsed Internal Method","title":" mergeSpillsWithTransferTo"},{"location":"shuffle/UnsafeShuffleWriter/#source-java","text":"","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#void-updatepeakmemoryused","text":"updatePeakMemoryUsed...FIXME updatePeakMemoryUsed is used when UnsafeShuffleWriter is requested for the < > and to < >.","title":"void updatePeakMemoryUsed()"},{"location":"shuffle/UnsafeShuffleWriter/#writing-key-value-records-of-partition","text":"void write ( Iterator < Product2 < K , V >> records ) write traverses the input sequence of records (for a RDD partition) and insertRecordIntoSorter one by one. When all the records have been processed, write closes internal resources and merges spill files . In the end, write requests ShuffleExternalSorter to clean up . CAUTION: FIXME When requested to < >, UnsafeShuffleWriter simply < > followed by < > (that, among other things, creates the < >). write is part of the ShuffleWriter abstraction. == [[stop]] Stopping ShuffleWriter","title":" Writing Key-Value Records of Partition"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_1","text":"Option stop( boolean success) stop ...FIXME When requested to < >, UnsafeShuffleWriter records the peak execution memory metric and returns the < > (that was created when requested to < >). stop is part of the ShuffleWriter abstraction. == [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_2","text":"void insertRecordIntoSorter( Product2 record) insertRecordIntoSorter requires that the < > is available. insertRecordIntoSorter requests the < > to reset (so that all currently accumulated output in the output stream is discarded and reusing the already allocated buffer space). insertRecordIntoSorter requests the < > to write out the record (write the serializer:SerializationStream.md#writeKey[key] and the serializer:SerializationStream.md#writeValue[value]) and to serializer:SerializationStream.md#flush[flush]. [[insertRecordIntoSorter-serializedRecordSize]] insertRecordIntoSorter requests the < > for the length of the buffer. [[insertRecordIntoSorter-partitionId]] insertRecordIntoSorter requests the < > for the ../rdd/Partitioner.md#getPartition[partition] for the given record (by the key). In the end, insertRecordIntoSorter requests the < > to ShuffleExternalSorter.md#insertRecord[insert] the < > as a byte array (with the < > and the < >). insertRecordIntoSorter is used when UnsafeShuffleWriter is requested to < >.","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#closing-and-writing-output-merging-spill-files","text":"void closeAndWriteOutput () closeAndWriteOutput asserts that the ShuffleExternalSorter is created (non- null ). closeAndWriteOutput updates peak memory used . closeAndWriteOutput removes the references to the < > and < > output streams ( null s them). closeAndWriteOutput requests the < > to ShuffleExternalSorter.md#closeAndGetSpills[close and return spill metadata]. closeAndWriteOutput removes the reference to the < > ( null s it). closeAndWriteOutput requests the < > for the IndexShuffleBlockResolver.md#getDataFile[output data file] for the < > and < > IDs. [[closeAndWriteOutput-partitionLengths]][[closeAndWriteOutput-tmp]] closeAndWriteOutput creates a temporary file (along the data output file) and uses it to < > (that gives a partition length array). All spill files are then deleted. closeAndWriteOutput requests the < > to IndexShuffleBlockResolver.md#writeIndexFileAndCommit[write shuffle index and data files] (for the < > and < > IDs, the < > and the < >). In the end, closeAndWriteOutput creates a scheduler:MapStatus.md[MapStatus] with the storage:BlockManager.md#shuffleServerId[location of the local BlockManager] and the < >. closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting spill files: Error while deleting spill file [path] closeAndWriteOutput prints out the following ERROR message to the logs if there is an issue with deleting the < >: Error while deleting temp file [path] closeAndWriteOutput is used when UnsafeShuffleWriter is requested to write records . == [[mergeSpillsWithFileStream]] mergeSpillsWithFileStream Method","title":" Closing and Writing Output (Merging Spill Files)"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_3","text":"long[] mergeSpillsWithFileStream( SpillInfo[] spills, File outputFile, CompressionCodec compressionCodec) mergeSpillsWithFileStream will be given an io:CompressionCodec.md[IO compression codec] when shuffle compression is enabled. mergeSpillsWithFileStream...FIXME mergeSpillsWithFileStream requires that there are at least two spills to merge. mergeSpillsWithFileStream is used when UnsafeShuffleWriter is requested to < >. == [[getPeakMemoryUsedBytes]] Getting Peak Memory Used","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_4","text":"","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#long-getpeakmemoryusedbytes","text":"getPeakMemoryUsedBytes simply < > and returns the internal < > registry. getPeakMemoryUsedBytes is used when UnsafeShuffleWriter is requested to < >. == [[open]] Opening UnsafeShuffleWriter and Buffers","title":"long getPeakMemoryUsedBytes()"},{"location":"shuffle/UnsafeShuffleWriter/#source-java_5","text":"","title":"[source, java]"},{"location":"shuffle/UnsafeShuffleWriter/#void-open","text":"open requires that there is no < > available. open creates a ShuffleExternalSorter.md[ShuffleExternalSorter]. open creates a < > with the capacity of < >. open requests the < > for a serializer:SerializerInstance.md#serializeStream[SerializationStream] to the < > (available internally as the < > reference). open is used when UnsafeShuffleWriter is < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.shuffle.sort.UnsafeShuffleWriter logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"void open()"},{"location":"shuffle/UnsafeShuffleWriter/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"shuffle/UnsafeShuffleWriter/#log4jloggerorgapachesparkshufflesortunsafeshufflewriterall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=ALL"},{"location":"shuffle/UnsafeShuffleWriter/#internal-properties","text":"","title":"Internal Properties"},{"location":"shuffle/UnsafeShuffleWriter/#mapstatus","text":"MapStatus Created when UnsafeShuffleWriter is requested to < > (with the storage:BlockManagerId.md[] of the < > and partitionLengths ) Returned when UnsafeShuffleWriter is requested to < >","title":" MapStatus"},{"location":"shuffle/UnsafeShuffleWriter/#partitioner","text":"Partitioner (as used by the spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and create a ShuffleExternalSorter.md[ShuffleExternalSorter] with the given ../rdd/Partitioner.md#numPartitions[number of partitions]) < > (and request the ../rdd/Partitioner.md#getPartition[partition for the key]) < >, < > and < > (for the ../rdd/Partitioner.md#numPartitions[number of partitions] to create partition lengths)","title":" Partitioner"},{"location":"shuffle/UnsafeShuffleWriter/#peak-memory-used","text":"Peak memory used (in bytes) that is updated exclusively in < > (after requesting the < > for ShuffleExternalSorter.md#getPeakMemoryUsedBytes[getPeakMemoryUsedBytes]) Use < > to access the current value","title":" Peak Memory Used"},{"location":"shuffle/UnsafeShuffleWriter/#bytearrayoutputstream-for-serialized-data","text":"{java-javadoc-url}/java/io/ByteArrayOutputStream.html[java.io.ByteArrayOutputStream] of serialized data (written into a byte array of < > initial size) Used when UnsafeShuffleWriter is requested for the following: < > (and create the internal < >) < > Destroyed ( null ) when requested to < >. === [[serializer]] serializer serializer:SerializerInstance.md[SerializerInstance] (that is a new instance of the Serializer of the spark-shuffle-BaseShuffleHandle.md#dependency[ShuffleDependency] of the < >) Used exclusively when UnsafeShuffleWriter is requested to < > (and creates the < >) === [[serOutputStream]] serOutputStream serializer:SerializationStream.md[SerializationStream] (that is created when the < > is requested to serializer:SerializerInstance.md#serializeStream[serializeStream] with the < >) Used when UnsafeShuffleWriter is requested to < > Destroyed ( null ) when requested to < >.","title":" ByteArrayOutputStream for Serialized Data"},{"location":"shuffle/UnsafeShuffleWriter/#shuffle-id","text":"Shuffle ID (of the ShuffleDependency of the SerializedShuffleHandle ) Used exclusively when requested to < > === [[writeMetrics]] writeMetrics executor:ShuffleWriteMetrics.md[] (of the scheduler:spark-TaskContext.md#taskMetrics[TaskMetrics] of the < >) Used when UnsafeShuffleWriter is requested for the following: < > (and creates the < >) < > < > < >","title":" Shuffle ID"},{"location":"storage/BlockData/","text":"= BlockData BlockData is...FIXME","title":"BlockData"},{"location":"storage/BlockDataManager/","text":"= BlockDataManager BlockDataManager is an < > of < > that manage storage for blocks of data (aka block storage management API ). BlockDataManager uses storage:BlockId.md[] to uniquely identify blocks of data and network:ManagedBuffer.md[] to represent them. BlockDataManager is used to initialize a storage:BlockTransferService.md#init[]. BlockDataManager is used to create a storage:NettyBlockRpcServer.md[]. == [[contract]] Contract === [[getBlockData]] getBlockData [source,scala] \u00b6 getBlockData( blockId: BlockId): ManagedBuffer Fetches a block data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md[] Used when: NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#OpenBlocks[handle a OpenBlocks message] ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#fetchLocalBlocks[fetchLocalBlocks] === [[putBlockData]] putBlockData [source, scala] \u00b6 putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean Stores ( puts ) a block data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md[]. Returns true when completed successfully or false when failed. Used when NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#UploadBlock[handle an UploadBlock message] === [[putBlockDataAsStream]] putBlockDataAsStream [source, scala] \u00b6 putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID Stores a block data that will be received as a stream Used when NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#receiveStream[receiveStream] === [[releaseLock]] releaseLock [source, scala] \u00b6 releaseLock( blockId: BlockId, taskAttemptId: Option[Long]): Unit Releases a lock Used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#releaseLock[releaseLock] BlockManager is requested to storage:BlockManager.md#handleLocalReadFailure[handleLocalReadFailure], storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#getOrElseUpdate[getOrElseUpdate], storage:BlockManager.md#doPut[doPut], and storage:BlockManager.md#releaseLockAndDispose[releaseLockAndDispose] == [[implementations]] Available BlockDataManagers storage:BlockManager.md[] is the default and only known BlockDataManager in Apache Spark.","title":"BlockDataManager"},{"location":"storage/BlockDataManager/#sourcescala","text":"getBlockData( blockId: BlockId): ManagedBuffer Fetches a block data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md[] Used when: NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#OpenBlocks[handle a OpenBlocks message] ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#fetchLocalBlocks[fetchLocalBlocks] === [[putBlockData]] putBlockData","title":"[source,scala]"},{"location":"storage/BlockDataManager/#source-scala","text":"putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean Stores ( puts ) a block data (as a network:ManagedBuffer.md[]) for the given storage:BlockId.md[]. Returns true when completed successfully or false when failed. Used when NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#UploadBlock[handle an UploadBlock message] === [[putBlockDataAsStream]] putBlockDataAsStream","title":"[source, scala]"},{"location":"storage/BlockDataManager/#source-scala_1","text":"putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID Stores a block data that will be received as a stream Used when NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#receiveStream[receiveStream] === [[releaseLock]] releaseLock","title":"[source, scala]"},{"location":"storage/BlockDataManager/#source-scala_2","text":"releaseLock( blockId: BlockId, taskAttemptId: Option[Long]): Unit Releases a lock Used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#releaseLock[releaseLock] BlockManager is requested to storage:BlockManager.md#handleLocalReadFailure[handleLocalReadFailure], storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#getOrElseUpdate[getOrElseUpdate], storage:BlockManager.md#doPut[doPut], and storage:BlockManager.md#releaseLockAndDispose[releaseLockAndDispose] == [[implementations]] Available BlockDataManagers storage:BlockManager.md[] is the default and only known BlockDataManager in Apache Spark.","title":"[source, scala]"},{"location":"storage/BlockEvictionHandler/","text":"= BlockEvictionHandler BlockEvictionHandler is an < > of < > that can < >. == [[contract]] Contract === [[dropFromMemory]] dropFromMemory Method [source,scala] \u00b6 dropFromMemory T: ClassTag : StorageLevel Used when MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace]. == [[implementations]] Available BlockEvictionHandlers storage:BlockManager.md[] is the default and only known BlockEvictionHandler in Apache Spark.","title":"BlockEvictionHandler"},{"location":"storage/BlockEvictionHandler/#sourcescala","text":"dropFromMemory T: ClassTag : StorageLevel Used when MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace]. == [[implementations]] Available BlockEvictionHandlers storage:BlockManager.md[] is the default and only known BlockEvictionHandler in Apache Spark.","title":"[source,scala]"},{"location":"storage/BlockId/","text":"= BlockId BlockId is an < > of < > based on an unique < >. BlockId is a Scala sealed abstract class and so all the possible < > are in the single Scala file alongside BlockId. == [[contract]] Contract === [[name]][[toString]] Unique Name [source, scala] \u00b6 name: String \u00b6 Used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#uploadBlock[upload a block] AppStatusListener is requested to core:AppStatusListener.md#updateRDDBlock[updateRDDBlock], core:AppStatusListener.md#updateStreamBlock[updateStreamBlock] BlockManager is requested to storage:BlockManager.md#putBlockDataAsStream[putBlockDataAsStream] UpdateBlockInfo is requested to storage:BlockManagerMasterEndpoint.md#UpdateBlockInfo[writeExternal] DiskBlockManager is requested to storage:DiskBlockManager.md#getFile[getFile] and storage:DiskBlockManager.md#containsBlock[containsBlock] DiskStore is requested to storage:DiskStore.md#getBytes[getBytes] == [[implementations]] Available BlockIds === [[BroadcastBlockId]] BroadcastBlockId BlockId for ROOT:Broadcast.md[]s with broadcastId identifier and optional field name (default: empty ) Uses broadcast_ prefix for the < > Used when: TorrentBroadcast is core:TorrentBroadcast.md#broadcastId[created], requested to core:TorrentBroadcast.md#writeBlocks[store a broadcast and the blocks in a local BlockManager], and < > BlockManager is requested to storage:BlockManager.md#removeBroadcast[remove all the blocks of a broadcast variable] AppStatusListener is requested to core:AppStatusListener.md#updateBroadcastBlock[updateBroadcastBlock] (when core:AppStatusListener.md#onBlockUpdated[onBlockUpdated] for a BroadcastBlockId ) serializer:SerializerManager.md#shouldCompress[Compressed] when core:BroadcastManager.md#spark.broadcast.compress[spark.broadcast.compress] configuration property is enabled === [[RDDBlockId]] RDDBlockId BlockId for RDD partitions with rddId and splitIndex identifiers Uses rdd_ prefix for the < > Used when: StorageStatus is requested to < >, < >, < > LocalCheckpointRDD is requested to compute a partition LocalRDDCheckpointData is requested to rdd:LocalRDDCheckpointData.md#doCheckpoint[doCheckpoint] RDD is requested to rdd:RDD.md#getOrCompute[getOrCompute] DAGScheduler is requested for the scheduler:DAGScheduler.md#getCacheLocs[BlockManagers (executors) for cached RDD partitions] AppStatusListener is requested to core:AppStatusListener.md#updateRDDBlock[updateRDDBlock] (when core:AppStatusListener.md#onBlockUpdated[onBlockUpdated] for a RDDBlockId ) serializer:SerializerManager.md#shouldCompress[Compressed] when ROOT:configuration-properties.md#spark.rdd.compress[spark.rdd.compress] configuration property is enabled (default: false ) === [[ShuffleBlockId]] ShuffleBlockId BlockId for FIXME with shuffleId , mapId , and reduceId identifiers Uses shuffle_ prefix for the < > Used when: ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#throwFetchFailedException[throwFetchFailedException] MapOutputTracker object is requested to scheduler:MapOutputTracker.md#convertMapStatuses[convertMapStatuses] SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write partition records] ShuffleBlockResolver is requested for a shuffle:ShuffleBlockResolver.md#getBlockData[ManagedBuffer to read shuffle block data file] serializer:SerializerManager.md#shouldCompress[Compressed] when ROOT:configuration-properties.md#spark.shuffle.compress[spark.shuffle.compress] configuration property is enabled (default: true ) === [[ShuffleDataBlockId]] ShuffleDataBlockId === [[ShuffleIndexBlockId]] ShuffleIndexBlockId === [[StreamBlockId]] StreamBlockId === [[TaskResultBlockId]] TaskResultBlockId === [[TempLocalBlockId]] TempLocalBlockId === [[TempShuffleBlockId]] TempShuffleBlockId == [[apply]] apply Factory Method [source, scala] \u00b6 apply( name: String): BlockId apply creates one of the available < > by the given name (that uses a prefix to differentiate between different BlockIds). apply is used when: NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#receive[handle an RPC message] and storage:NettyBlockRpcServer.md#receiveStream[receiveStream] UpdateBlockInfo is requested to deserialize (readExternal) DiskBlockManager is requested for storage:DiskBlockManager.md#getAllBlocks[all the blocks (from files stored on disk)] ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[sendRequest] JsonProtocol utility is used to spark-history-server:JsonProtocol.md#accumValueFromJson[accumValueFromJson], spark-history-server:JsonProtocol.md#taskMetricsFromJson[taskMetricsFromJson] and spark-history-server:JsonProtocol.md#blockUpdatedInfoFromJson[blockUpdatedInfoFromJson]","title":"BlockId"},{"location":"storage/BlockId/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/BlockId/#name-string","text":"Used when: NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#uploadBlock[upload a block] AppStatusListener is requested to core:AppStatusListener.md#updateRDDBlock[updateRDDBlock], core:AppStatusListener.md#updateStreamBlock[updateStreamBlock] BlockManager is requested to storage:BlockManager.md#putBlockDataAsStream[putBlockDataAsStream] UpdateBlockInfo is requested to storage:BlockManagerMasterEndpoint.md#UpdateBlockInfo[writeExternal] DiskBlockManager is requested to storage:DiskBlockManager.md#getFile[getFile] and storage:DiskBlockManager.md#containsBlock[containsBlock] DiskStore is requested to storage:DiskStore.md#getBytes[getBytes] == [[implementations]] Available BlockIds === [[BroadcastBlockId]] BroadcastBlockId BlockId for ROOT:Broadcast.md[]s with broadcastId identifier and optional field name (default: empty ) Uses broadcast_ prefix for the < > Used when: TorrentBroadcast is core:TorrentBroadcast.md#broadcastId[created], requested to core:TorrentBroadcast.md#writeBlocks[store a broadcast and the blocks in a local BlockManager], and < > BlockManager is requested to storage:BlockManager.md#removeBroadcast[remove all the blocks of a broadcast variable] AppStatusListener is requested to core:AppStatusListener.md#updateBroadcastBlock[updateBroadcastBlock] (when core:AppStatusListener.md#onBlockUpdated[onBlockUpdated] for a BroadcastBlockId ) serializer:SerializerManager.md#shouldCompress[Compressed] when core:BroadcastManager.md#spark.broadcast.compress[spark.broadcast.compress] configuration property is enabled === [[RDDBlockId]] RDDBlockId BlockId for RDD partitions with rddId and splitIndex identifiers Uses rdd_ prefix for the < > Used when: StorageStatus is requested to < >, < >, < > LocalCheckpointRDD is requested to compute a partition LocalRDDCheckpointData is requested to rdd:LocalRDDCheckpointData.md#doCheckpoint[doCheckpoint] RDD is requested to rdd:RDD.md#getOrCompute[getOrCompute] DAGScheduler is requested for the scheduler:DAGScheduler.md#getCacheLocs[BlockManagers (executors) for cached RDD partitions] AppStatusListener is requested to core:AppStatusListener.md#updateRDDBlock[updateRDDBlock] (when core:AppStatusListener.md#onBlockUpdated[onBlockUpdated] for a RDDBlockId ) serializer:SerializerManager.md#shouldCompress[Compressed] when ROOT:configuration-properties.md#spark.rdd.compress[spark.rdd.compress] configuration property is enabled (default: false ) === [[ShuffleBlockId]] ShuffleBlockId BlockId for FIXME with shuffleId , mapId , and reduceId identifiers Uses shuffle_ prefix for the < > Used when: ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#throwFetchFailedException[throwFetchFailedException] MapOutputTracker object is requested to scheduler:MapOutputTracker.md#convertMapStatuses[convertMapStatuses] SortShuffleWriter is requested to shuffle:SortShuffleWriter.md#write[write partition records] ShuffleBlockResolver is requested for a shuffle:ShuffleBlockResolver.md#getBlockData[ManagedBuffer to read shuffle block data file] serializer:SerializerManager.md#shouldCompress[Compressed] when ROOT:configuration-properties.md#spark.shuffle.compress[spark.shuffle.compress] configuration property is enabled (default: true ) === [[ShuffleDataBlockId]] ShuffleDataBlockId === [[ShuffleIndexBlockId]] ShuffleIndexBlockId === [[StreamBlockId]] StreamBlockId === [[TaskResultBlockId]] TaskResultBlockId === [[TempLocalBlockId]] TempLocalBlockId === [[TempShuffleBlockId]] TempShuffleBlockId == [[apply]] apply Factory Method","title":"name: String"},{"location":"storage/BlockId/#source-scala_1","text":"apply( name: String): BlockId apply creates one of the available < > by the given name (that uses a prefix to differentiate between different BlockIds). apply is used when: NettyBlockRpcServer is requested to storage:NettyBlockRpcServer.md#receive[handle an RPC message] and storage:NettyBlockRpcServer.md#receiveStream[receiveStream] UpdateBlockInfo is requested to deserialize (readExternal) DiskBlockManager is requested for storage:DiskBlockManager.md#getAllBlocks[all the blocks (from files stored on disk)] ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[sendRequest] JsonProtocol utility is used to spark-history-server:JsonProtocol.md#accumValueFromJson[accumValueFromJson], spark-history-server:JsonProtocol.md#taskMetricsFromJson[taskMetricsFromJson] and spark-history-server:JsonProtocol.md#blockUpdatedInfoFromJson[blockUpdatedInfoFromJson]","title":"[source, scala]"},{"location":"storage/BlockInfo/","text":"= BlockInfo BlockInfo is a metadata of storage:BlockId.md[memory block] -- the memory block's < >, the < > and the < >. BlockInfo has a storage:StorageLevel.md[], ClassTag and tellMaster flag. == [[size]] Size size attribute is the size of the memory block. It starts with 0 . It represents the number of bytes that storage:BlockManager.md#putBytes[ BlockManager saved] or storage:BlockManager.md#doPutIterator[BlockManager.doPutIterator]. == [[readerCount]] Reader Count -- readerCount Counter readerCount counter is the number of readers of the memory block, i.e. the number of read locks. It starts with 0 . readerCount is incremented when a storage:BlockInfoManager.md#lockForReading[read lock is acquired] and decreases when the following happens: The storage:BlockManager.md#unlock[memory block is unlocked] storage:BlockInfoManager.md#releaseAllLocksForTask[All locks for the memory block obtained by a task are released]. storage:BlockInfoManager.md#removeBlock[memory block is removed] storage:BlockInfoManager.md#clear[Clearing the current state of BlockInfoManager ]. == [[writerTask]] Writer Task -- writerTask Attribute writerTask attribute is the task that owns the write lock for the memory block. A writer task can be one of the three possible identifiers: [[NO_WRITER]] NO_WRITER (i.e. -1 ) to denote no writers and hence no write lock in use. [[NON_TASK_WRITER]] NON_TASK_WRITER (i.e. -1024 ) for non-task threads, e.g. by a driver thread or by unit test code. the task attempt id of the task which currently holds the write lock for this block. The writer task is assigned in the following scenarios: A storage:BlockInfoManager.md#lockForWriting[write lock is requested for a memory block (with no writer and readers)] A storage:BlockInfoManager.md#unlock[memory block is unlocked] storage:BlockInfoManager.md#releaseAllLocksForTask[All locks obtained by a task are released] A storage:BlockInfoManager.md#removeBlock[memory block is removed] storage:BlockInfoManager.md#clear[Clearing the current state of BlockInfoManager ].","title":"BlockInfo"},{"location":"storage/BlockInfoManager/","text":"= BlockInfoManager BlockInfoManager is used by storage:BlockManager.md[] (and storage:MemoryStore.md#blockInfoManager[MemoryStore]) to manage < > and control concurrent access by locks for < > and < >. NOTE: Locks are the mechanism to control concurrent access to data and prevent destructive interaction between operations that use the same resource. BlockInfoManager is used to create a storage:MemoryStore.md#blockInfoManager[MemoryStore] and a BlockManagerManagedBuffer. == [[creating-instance]] Creating Instance BlockInfoManager takes no parameters to be created. BlockInfoManager is created for storage:BlockManager.md#blockInfoManager[BlockManager]. .BlockInfoManager and BlockManager image::BlockInfoManager-BlockManager.png[align=\"center\"] == [[infos]] Block Metadata [source,scala] \u00b6 infos: Map[BlockId, BlockInfo] \u00b6 BlockInfoManager uses a registry of storage:BlockInfo.md[block metadata]s per storage:BlockId.md[block]. == [[readLocksByTask]][[writeLocksByTask]] Read and Write Locks By Task Tracks tasks (by TaskAttemptId) and the blocks they locked for reading (as storage:BlockId.md[]). Tracks tasks (by TaskAttemptId ) and the blocks they locked for writing (as storage:BlockId.md[]). == [[registerTask]] Registering Task (Start of Execution) [source,scala] \u00b6 registerTask( taskAttemptId: Long): Unit registerTask merely adds a new \"empty\" entry for the given task (by the task attempt ID) to < > internal registry. registerTask is used when: BlockInfoManager is < > BlockManager is requested to storage:BlockManager.md#registerTask[registerTask] == [[downgradeLock]] Downgrading Exclusive Write Lock For Block to Shared Read Lock [source, scala] \u00b6 downgradeLock( blockId: BlockId): Unit downgradeLock prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] downgrading write lock for [blockId] \u00b6 downgradeLock...FIXME downgradeLock is used when BlockManager is requested to storage:BlockManager.md#doPut[doPut] and storage:BlockManager.md#downgradeLock[downgradeLock]. == [[lockForReading]] Obtaining Read Lock For Block [source, scala] \u00b6 lockForReading( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForReading locks blockId memory block for reading when the block was registered earlier and no writer tasks use it. When executed, lockForReading prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] trying to acquire read lock for [blockId] \u00b6 lockForReading looks up the metadata of the blockId block (in < > registry). If no metadata could be found, it returns None which means that the block does not exist or was removed (and anybody could acquire a write lock). Otherwise, when the metadata was found, i.e. registered, it checks so-called writerTask . Only when the storage:BlockInfo.md#NO_WRITER[block has no writer tasks], a read lock can be acquired. If so, the readerCount of the block metadata is incremented and the block is recorded (in the internal < > registry). You should see the following TRACE message in the logs: [source,plaintext] \u00b6 Task [taskAttemptId] acquired read lock for [blockId] \u00b6 The BlockInfo for the blockId block is returned. NOTE: -1024 is a special taskAttemptId , aka storage:BlockInfo.md#NON_TASK_WRITER[NON_TASK_WRITER], used to mark a non-task thread, e.g. by a driver thread or by unit test code. For blocks with storage:BlockInfo.md#NO_WRITER[ writerTask other than NO_WRITER ], when blocking is enabled, lockForReading waits (until another thread invokes the Object.notify method or the Object.notifyAll methods for this object). With blocking enabled, it will repeat the waiting-for-read-lock sequence until either None or the lock is obtained. When blocking is disabled and the lock could not be obtained, None is returned immediately. NOTE: lockForReading is a synchronized method, i.e. no two objects can use this and other instance methods. lockForReading is used when: BlockInfoManager is requested to < > and < > BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#getLocalBytes[getLocalBytes] and storage:BlockManager.md#replicateBlock[replicateBlock] BlockManagerManagedBuffer is requested to retain == [[lockForWriting]] Obtaining Write Lock for Block [source, scala] \u00b6 lockForWriting( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForWriting prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] trying to acquire write lock for [blockId] \u00b6 lockForWriting looks up blockId in the internal < > registry. When no storage:BlockInfo.md[] could be found, None is returned. Otherwise, storage:BlockInfo.md#NO_WRITER[ blockId block is checked for writerTask to be BlockInfo.NO_WRITER ] with no readers (i.e. readerCount is 0 ) and only then the lock is returned. When the write lock can be returned, BlockInfo.writerTask is set to currentTaskAttemptId and a new binding is added to the internal < > registry. You should see the following TRACE message in the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] acquired write lock for [blockId] \u00b6 If, for some reason, storage:BlockInfo.md#writerTask[ blockId has a writer] or the number of readers is positive (i.e. BlockInfo.readerCount is greater than 0 ), the method will wait (based on the input blocking flag) and attempt the write lock acquisition process until it finishes with a write lock. NOTE: (deadlock possible) The method is synchronized and can block, i.e. wait that causes the current thread to wait until another thread invokes Object.notify or Object.notifyAll methods for this object. lockForWriting returns None for no blockId in the internal < > registry or when blocking flag is disabled and the write lock could not be acquired. lockForWriting is used when: BlockInfoManager is requested to < > BlockManager is requested to storage:BlockManager.md#removeBlock[removeBlock] MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[lockNewBlockForWriting]] Obtaining Write Lock for New Block [source, scala] \u00b6 lockNewBlockForWriting( blockId: BlockId, newBlockInfo: BlockInfo): Boolean lockNewBlockForWriting obtains a write lock for blockId but only when the method could register the block. NOTE: lockNewBlockForWriting is similar to < > method but for brand new blocks. When executed, lockNewBlockForWriting prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] trying to put [blockId] \u00b6 If < >, it finishes returning false . Otherwise, when the block does not exist, newBlockInfo is recorded in the internal < > registry and < >. It then returns true . NOTE: lockNewBlockForWriting executes itself in synchronized block so once the BlockInfoManager is locked the other internal registries should be available only for the currently-executing thread. lockNewBlockForWriting is used when BlockManager is requested to storage:BlockManager.md#doPut[doPut]. == [[unlock]] Releasing Lock on Block [source, scala] \u00b6 unlock( blockId: BlockId): Unit unlock prints out the following TRACE message to the logs: [source,plaintext] \u00b6 Task [currentTaskAttemptId] releasing lock for [blockId] \u00b6 unlock gets the metadata for blockId . It may throw a IllegalStateException if the block was not found. If the storage:BlockInfo.md#writerTask[writer task] for the block is not storage:BlockInfo.md#NO_WRITER[NO_WRITER], it becomes so and the blockId block is removed from the internal < > registry for the < >. Otherwise, if the writer task is indeed NO_WRITER , it is assumed that the storage:BlockInfo.md#readerCount[ blockId block is locked for reading]. The readerCount counter is decremented for the blockId block and the read lock removed from the internal < > registry for the < >. In the end, unlock wakes up all the threads waiting for the BlockInfoManager (using Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notifyAll--++[Object.notifyAll ]). CAUTION: FIXME What threads could wait? unlock is used when: BlockInfoManager is requested to < > BlockManager is requested to storage:BlockManager.md#releaseLock[releaseLock] and storage:BlockManager.md#doPut[doPut] BlockManagerManagedBuffer is requested to release MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[releaseAllLocksForTask]] Releasing All Locks Obtained by Task [source,scala] \u00b6 releaseAllLocksForTask( taskAttemptId: TaskAttemptId): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when BlockManager is requested to storage:BlockManager.md#releaseAllLocksForTask[releaseAllLocksForTask]. == [[removeBlock]] Removing Block [source,scala] \u00b6 removeBlock( blockId: BlockId): Unit removeBlock...FIXME removeBlock is used when: BlockManager is requested to storage:BlockManager.md#removeBlockInternal[removeBlockInternal] MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[assertBlockIsLockedForWriting]] assertBlockIsLockedForWriting Method [source,scala] \u00b6 assertBlockIsLockedForWriting( blockId: BlockId): BlockInfo assertBlockIsLockedForWriting...FIXME assertBlockIsLockedForWriting is used when BlockManager is requested to storage:BlockManager.md#dropFromMemory[dropFromMemory] and storage:BlockManager.md#removeBlockInternal[removeBlockInternal]. == [[currentTaskAttemptId]] currentTaskAttemptId Internal Method [source, scala] \u00b6 currentTaskAttemptId: Long /* TaskAttemptId */ \u00b6 currentTaskAttemptId...FIXME currentTaskAttemptId is used when...FIXME == [[clear]] Deleting All State [source,scala] \u00b6 clear(): Unit \u00b6 clear...FIXME clear is used when BlockManager is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockInfoManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockInfoManager=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"BlockInfoManager"},{"location":"storage/BlockInfoManager/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#infos-mapblockid-blockinfo","text":"BlockInfoManager uses a registry of storage:BlockInfo.md[block metadata]s per storage:BlockId.md[block]. == [[readLocksByTask]][[writeLocksByTask]] Read and Write Locks By Task Tracks tasks (by TaskAttemptId) and the blocks they locked for reading (as storage:BlockId.md[]). Tracks tasks (by TaskAttemptId ) and the blocks they locked for writing (as storage:BlockId.md[]). == [[registerTask]] Registering Task (Start of Execution)","title":"infos: Map[BlockId, BlockInfo]"},{"location":"storage/BlockInfoManager/#sourcescala_1","text":"registerTask( taskAttemptId: Long): Unit registerTask merely adds a new \"empty\" entry for the given task (by the task attempt ID) to < > internal registry. registerTask is used when: BlockInfoManager is < > BlockManager is requested to storage:BlockManager.md#registerTask[registerTask] == [[downgradeLock]] Downgrading Exclusive Write Lock For Block to Shared Read Lock","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#source-scala","text":"downgradeLock( blockId: BlockId): Unit downgradeLock prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-downgrading-write-lock-for-blockid","text":"downgradeLock...FIXME downgradeLock is used when BlockManager is requested to storage:BlockManager.md#doPut[doPut] and storage:BlockManager.md#downgradeLock[downgradeLock]. == [[lockForReading]] Obtaining Read Lock For Block","title":"Task [currentTaskAttemptId] downgrading write lock for [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_1","text":"lockForReading( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForReading locks blockId memory block for reading when the block was registered earlier and no writer tasks use it. When executed, lockForReading prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-trying-to-acquire-read-lock-for-blockid","text":"lockForReading looks up the metadata of the blockId block (in < > registry). If no metadata could be found, it returns None which means that the block does not exist or was removed (and anybody could acquire a write lock). Otherwise, when the metadata was found, i.e. registered, it checks so-called writerTask . Only when the storage:BlockInfo.md#NO_WRITER[block has no writer tasks], a read lock can be acquired. If so, the readerCount of the block metadata is incremented and the block is recorded (in the internal < > registry). You should see the following TRACE message in the logs:","title":"Task [currentTaskAttemptId] trying to acquire read lock for [blockId]"},{"location":"storage/BlockInfoManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-taskattemptid-acquired-read-lock-for-blockid","text":"The BlockInfo for the blockId block is returned. NOTE: -1024 is a special taskAttemptId , aka storage:BlockInfo.md#NON_TASK_WRITER[NON_TASK_WRITER], used to mark a non-task thread, e.g. by a driver thread or by unit test code. For blocks with storage:BlockInfo.md#NO_WRITER[ writerTask other than NO_WRITER ], when blocking is enabled, lockForReading waits (until another thread invokes the Object.notify method or the Object.notifyAll methods for this object). With blocking enabled, it will repeat the waiting-for-read-lock sequence until either None or the lock is obtained. When blocking is disabled and the lock could not be obtained, None is returned immediately. NOTE: lockForReading is a synchronized method, i.e. no two objects can use this and other instance methods. lockForReading is used when: BlockInfoManager is requested to < > and < > BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#getLocalBytes[getLocalBytes] and storage:BlockManager.md#replicateBlock[replicateBlock] BlockManagerManagedBuffer is requested to retain == [[lockForWriting]] Obtaining Write Lock for Block","title":"Task [taskAttemptId] acquired read lock for [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_2","text":"lockForWriting( blockId: BlockId, blocking: Boolean = true): Option[BlockInfo] lockForWriting prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-trying-to-acquire-write-lock-for-blockid","text":"lockForWriting looks up blockId in the internal < > registry. When no storage:BlockInfo.md[] could be found, None is returned. Otherwise, storage:BlockInfo.md#NO_WRITER[ blockId block is checked for writerTask to be BlockInfo.NO_WRITER ] with no readers (i.e. readerCount is 0 ) and only then the lock is returned. When the write lock can be returned, BlockInfo.writerTask is set to currentTaskAttemptId and a new binding is added to the internal < > registry. You should see the following TRACE message in the logs:","title":"Task [currentTaskAttemptId] trying to acquire write lock for [blockId]"},{"location":"storage/BlockInfoManager/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-acquired-write-lock-for-blockid","text":"If, for some reason, storage:BlockInfo.md#writerTask[ blockId has a writer] or the number of readers is positive (i.e. BlockInfo.readerCount is greater than 0 ), the method will wait (based on the input blocking flag) and attempt the write lock acquisition process until it finishes with a write lock. NOTE: (deadlock possible) The method is synchronized and can block, i.e. wait that causes the current thread to wait until another thread invokes Object.notify or Object.notifyAll methods for this object. lockForWriting returns None for no blockId in the internal < > registry or when blocking flag is disabled and the write lock could not be acquired. lockForWriting is used when: BlockInfoManager is requested to < > BlockManager is requested to storage:BlockManager.md#removeBlock[removeBlock] MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[lockNewBlockForWriting]] Obtaining Write Lock for New Block","title":"Task [currentTaskAttemptId] acquired write lock for [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_3","text":"lockNewBlockForWriting( blockId: BlockId, newBlockInfo: BlockInfo): Boolean lockNewBlockForWriting obtains a write lock for blockId but only when the method could register the block. NOTE: lockNewBlockForWriting is similar to < > method but for brand new blocks. When executed, lockNewBlockForWriting prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-trying-to-put-blockid","text":"If < >, it finishes returning false . Otherwise, when the block does not exist, newBlockInfo is recorded in the internal < > registry and < >. It then returns true . NOTE: lockNewBlockForWriting executes itself in synchronized block so once the BlockInfoManager is locked the other internal registries should be available only for the currently-executing thread. lockNewBlockForWriting is used when BlockManager is requested to storage:BlockManager.md#doPut[doPut]. == [[unlock]] Releasing Lock on Block","title":"Task [currentTaskAttemptId] trying to put [blockId]"},{"location":"storage/BlockInfoManager/#source-scala_4","text":"unlock( blockId: BlockId): Unit unlock prints out the following TRACE message to the logs:","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#sourceplaintext_6","text":"","title":"[source,plaintext]"},{"location":"storage/BlockInfoManager/#task-currenttaskattemptid-releasing-lock-for-blockid","text":"unlock gets the metadata for blockId . It may throw a IllegalStateException if the block was not found. If the storage:BlockInfo.md#writerTask[writer task] for the block is not storage:BlockInfo.md#NO_WRITER[NO_WRITER], it becomes so and the blockId block is removed from the internal < > registry for the < >. Otherwise, if the writer task is indeed NO_WRITER , it is assumed that the storage:BlockInfo.md#readerCount[ blockId block is locked for reading]. The readerCount counter is decremented for the blockId block and the read lock removed from the internal < > registry for the < >. In the end, unlock wakes up all the threads waiting for the BlockInfoManager (using Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#notifyAll--++[Object.notifyAll ]). CAUTION: FIXME What threads could wait? unlock is used when: BlockInfoManager is requested to < > BlockManager is requested to storage:BlockManager.md#releaseLock[releaseLock] and storage:BlockManager.md#doPut[doPut] BlockManagerManagedBuffer is requested to release MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[releaseAllLocksForTask]] Releasing All Locks Obtained by Task","title":"Task [currentTaskAttemptId] releasing lock for [blockId]"},{"location":"storage/BlockInfoManager/#sourcescala_2","text":"releaseAllLocksForTask( taskAttemptId: TaskAttemptId): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when BlockManager is requested to storage:BlockManager.md#releaseAllLocksForTask[releaseAllLocksForTask]. == [[removeBlock]] Removing Block","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#sourcescala_3","text":"removeBlock( blockId: BlockId): Unit removeBlock...FIXME removeBlock is used when: BlockManager is requested to storage:BlockManager.md#removeBlockInternal[removeBlockInternal] MemoryStore is requested to storage:MemoryStore.md#evictBlocksToFreeSpace[evictBlocksToFreeSpace] == [[assertBlockIsLockedForWriting]] assertBlockIsLockedForWriting Method","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#sourcescala_4","text":"assertBlockIsLockedForWriting( blockId: BlockId): BlockInfo assertBlockIsLockedForWriting...FIXME assertBlockIsLockedForWriting is used when BlockManager is requested to storage:BlockManager.md#dropFromMemory[dropFromMemory] and storage:BlockManager.md#removeBlockInternal[removeBlockInternal]. == [[currentTaskAttemptId]] currentTaskAttemptId Internal Method","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/BlockInfoManager/#currenttaskattemptid-long-taskattemptid","text":"currentTaskAttemptId...FIXME currentTaskAttemptId is used when...FIXME == [[clear]] Deleting All State","title":"currentTaskAttemptId: Long /* TaskAttemptId */"},{"location":"storage/BlockInfoManager/#sourcescala_5","text":"","title":"[source,scala]"},{"location":"storage/BlockInfoManager/#clear-unit","text":"clear...FIXME clear is used when BlockManager is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockInfoManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"clear(): Unit"},{"location":"storage/BlockInfoManager/#source","text":"","title":"[source]"},{"location":"storage/BlockInfoManager/#log4jloggerorgapachesparkstorageblockinfomanagerall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockInfoManager=ALL"},{"location":"storage/BlockManager/","text":"BlockManager \u00b6 BlockManager manages the storage for blocks ( chunks of data ) that can be stored in < > and on < >. BlockManager runs on the driver and executors . BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. < >. [[futureExecutionContext]] BlockManager uses a Scala https://www.scala-lang.org/api/current/scala/concurrent/ExecutionContextExecutorService.html[ExecutionContextExecutorService ] to execute FIXME asynchronously (on a thread pool with block-manager-future prefix and maximum of 128 threads). Cached blocks are blocks with non-zero sum of memory and disk sizes. TIP: Use webui:index.md[Web UI], esp. webui:spark-webui-storage.md[Storage] and webui:spark-webui-executors.md[Executors] tabs, to monitor the memory used. TIP: Use tools:spark-submit.md[spark-submit]'s command-line options, i.e. tools:spark-submit.md#driver-memory[--driver-memory] for the driver and tools:spark-submit.md#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. tools:spark-submit.md#spark.executor.memory[spark.executor.memory] and tools:spark-submit.md#spark_driver_memory[spark.driver.memory], to control the memory for storage memory. When < >, BlockManager uses storage:ExternalShuffleClient.md[ExternalShuffleClient] to read other executors' shuffle files. == [[creating-instance]] Creating Instance BlockManager takes the following to be created: < > < > < > [[serializerManager]] serializer:SerializerManager.md[] [[conf]] ROOT:SparkConf.md[] < > < > < > < > [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores (for an storage:ExternalShuffleClient.md[] with < >) When created, BlockManager sets < > internal flag based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. BlockManager then creates an instance of DiskBlockManager.md[DiskBlockManager] (requesting deleteFilesOnStop when an external shuffle service is not in use). BlockManager creates block-manager-future daemon cached thread pool with 128 threads maximum (as futureExecutionContext ). BlockManager calculates the maximum memory to use (as maxMemory ) by requesting the maximum memory:MemoryManager.md#maxOnHeapStorageMemory[on-heap] and memory:MemoryManager.md#maxOffHeapStorageMemory[off-heap] storage memory from the assigned MemoryManager . BlockManager calculates the port used by the external shuffle service (as externalShuffleServicePort ). NOTE: It is computed specially in Spark on YARN. BlockManager creates a client to read other executors' shuffle files (as shuffleClient ). If the external shuffle service is used an storage:ExternalShuffleClient.md[ExternalShuffleClient] is created or the input storage:BlockTransferService.md[BlockTransferService] is used. BlockManager sets the ROOT:configuration-properties.md#spark.block.failures.beforeLocationRefresh[maximum number of failures] before this block manager refreshes the block locations from the driver (as maxFailuresBeforeLocationRefresh ). BlockManager registers a storage:BlockManagerSlaveEndpoint.md[] with the input ROOT:index.md[RpcEnv], itself, and scheduler:MapOutputTracker.md[MapOutputTracker] (as slaveEndpoint ). BlockManager is created when SparkEnv is core:SparkEnv.md#create-BlockManager[created] (for the driver and executors) when a Spark application starts. .BlockManager and SparkEnv image::BlockManager-SparkEnv.png[align=\"center\"] == [[BlockEvictionHandler]] BlockEvictionHandler BlockManager is a storage:BlockEvictionHandler.md[] that can < > (and store it on a disk when needed). == [[shuffleClient]][[externalShuffleServiceEnabled]] ShuffleClient and External Shuffle Service BlockManager manages the lifecycle of a storage:ShuffleClient.md[]: Creates when < > storage:ShuffleClient.md#init[Inits] (and possibly < >) when requested to < > Closes when requested to < > The ShuffleClient can be an storage:ExternalShuffleClient.md[] or the given < > based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. When enabled, BlockManager uses the storage:ExternalShuffleClient.md[]. The ShuffleClient is available to other Spark services (using shuffleClient value) and is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined key-value records for a reduce task]. When requested for < >, BlockManager simply requests storage:ShuffleClient.md#shuffleMetrics[them] from the ShuffleClient. == [[rpcEnv]] BlockManager and RpcEnv BlockManager is given a rpc:RpcEnv.md[] when < >. The RpcEnv is used to set up a < >. == [[blockInfoManager]] BlockInfoManager BlockManager creates a storage:BlockInfoManager.md[] when < >. BlockManager requests the BlockInfoManager to storage:BlockInfoManager.md#clear[clear] when requested to < >. BlockManager uses the BlockInfoManager to create a < >. BlockManager uses the BlockInfoManager when requested for the following: < > < > < > < > and < > < > < > < > < >, < >, < >, < > < >, < >, < >, < > == [[master]] BlockManager and BlockManagerMaster BlockManager is given a storage:BlockManagerMaster.md[] when < >. == [[BlockDataManager]] BlockManager as BlockDataManager BlockManager is a storage:BlockDataManager.md[]. == [[mapOutputTracker]] BlockManager and MapOutputTracker BlockManager is given a scheduler:MapOutputTracker.md[] when < >. == [[executorId]] Executor ID BlockManager is given an Executor ID when < >. The Executor ID is one of the following: driver ( SparkContext.DRIVER_IDENTIFIER ) for the driver Value of executor:CoarseGrainedExecutorBackend.md#executor-id[--executor-id] command-line argument for executor:CoarseGrainedExecutorBackend.md[] executors (or spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md[MesosExecutorBackend]) == [[slaveEndpoint]] BlockManagerEndpoint RPC Endpoint BlockManager requests the < > to rpc:RpcEnv.md#setupEndpoint[register] a storage:BlockManagerSlaveEndpoint.md[] under the name BlockManagerEndpoint[ID] . The RPC endpoint is used when BlockManager is requested to < > and < > (to register the BlockManager on an executor with the < > on the driver). The endpoint is stopped (by requesting the < > to rpc:RpcEnv.md#stop[stop the reference]) when BlockManager is requested to < >. == [[SparkEnv]] Accessing BlockManager Using SparkEnv BlockManager is available using core:SparkEnv.md#blockManager[SparkEnv] on the driver and executors. [source,plaintext] \u00b6 import org.apache.spark.SparkEnv val bm = SparkEnv.get.blockManager scala> :type bm org.apache.spark.storage.BlockManager == [[blockTransferService]] BlockTransferService BlockManager is given a storage:BlockTransferService.md[BlockTransferService] when < >. BlockTransferService is used as the < > when BlockManager is configured with no external shuffle service (based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). BlockTransferService is storage:BlockTransferService.md#init[initialized] when BlockManager < >. BlockTransferService is storage:BlockTransferService.md#close[closed] when BlockManager is requested to < >. BlockTransferService is used when BlockManager is requested to < > or < > remote block managers. == [[memoryManager]] MemoryManager BlockManager is given a memory:MemoryManager.md[MemoryManager] when < >. BlockManager uses the MemoryManager for the following: Create the < > (that is then assigned to memory:MemoryManager.md#setMemoryStore[MemoryManager] as a \"circular dependency\") Initialize < > and < > (for reporting) == [[shuffleManager]] ShuffleManager BlockManager is given a shuffle:ShuffleManager.md[ShuffleManager] when < >. BlockManager uses the ShuffleManager for the following: < > (for shuffle blocks) < > (for shuffle blocks anyway) < > (when < > on an executor with < >) == [[diskBlockManager]] DiskBlockManager BlockManager creates a DiskBlockManager.md[DiskBlockManager] when < >. .DiskBlockManager and BlockManager image::DiskBlockManager-BlockManager.png[align=\"center\"] BlockManager uses the BlockManager for the following: Creating a < > < > (when < > on an executor with < >) The BlockManager is available as diskBlockManager reference to other Spark systems. [source, scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.blockManager.diskBlockManager == [[memoryStore]] MemoryStore BlockManager creates a storage:MemoryStore.md[] when < > (with the < >, the < >, the < > and itself as a storage:BlockEvictionHandler.md[]). .MemoryStore and BlockManager image::MemoryStore-BlockManager.png[align=\"center\"] BlockManager requests the < > to memory:MemoryManager.md#setMemoryStore[use] the MemoryStore. BlockManager uses the MemoryStore for the following: < > and < > < > < > < > and < > < > and < > < > < > The MemoryStore is requested to storage:MemoryStore.md#clear[clear] when BlockManager is requested to < >. The MemoryStore is available as memoryStore private reference to other Spark services. [source, scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore The MemoryStore is used (via SparkEnv.get.blockManager.memoryStore reference) when Task is requested to scheduler:Task.md#run[run] (that has finished and requests the MemoryStore to storage:MemoryStore.md#releaseUnrollMemoryForThisTask[releaseUnrollMemoryForThisTask]). == [[diskStore]] DiskStore BlockManager creates a DiskStore.md[DiskStore] (with the < >) when < >. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] BlockManager uses the DiskStore when requested to < >, < >, < >, < >, < >, < >, < >, < >. == [[metrics]] Performance Metrics BlockManager uses spark-BlockManager-BlockManagerSource.md[BlockManagerSource] to report metrics under the name BlockManager . == [[getPeers]] getPeers Internal Method [source,scala] \u00b6 getPeers( forceFetch: Boolean): Seq[BlockManagerId] getPeers...FIXME getPeers is used when BlockManager is requested to < > and < >. == [[releaseAllLocksForTask]] Releasing All Locks For Task [source,scala] \u00b6 releaseAllLocksForTask( taskAttemptId: Long): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when TaskRunner is requested to executor:TaskRunner.md#run[run] (at the end of a task). == [[stop]] Stopping BlockManager [source, scala] \u00b6 stop(): Unit \u00b6 stop...FIXME stop is used when SparkEnv is requested to core:SparkEnv.md#stop[stop]. == [[getMatchingBlockIds]] Getting IDs of Existing Blocks (For a Given Filter) [source, scala] \u00b6 getMatchingBlockIds( filter: BlockId => Boolean): Seq[BlockId] getMatchingBlockIds...FIXME getMatchingBlockIds is used when BlockManagerSlaveEndpoint is requested to storage:BlockManagerSlaveEndpoint.md#GetMatchingBlockIds[handle a GetMatchingBlockIds message]. == [[getLocalValues]] Getting Local Block [source, scala] \u00b6 getLocalValues( blockId: BlockId): Option[BlockResult] getLocalValues prints out the following DEBUG message to the logs: Getting local block [blockId] getLocalValues storage:BlockInfoManager.md#lockForReading[obtains a read lock for blockId ]. When no blockId block was found, you should see the following DEBUG message in the logs and getLocalValues returns \"nothing\" (i.e. NONE ). Block [blockId] was not found When the blockId block was found, you should see the following DEBUG message in the logs: Level for block [blockId] is [level] If blockId block has memory level and storage:MemoryStore.md#contains[is registered in MemoryStore ], getLocalValues returns a < > as Memory read method and with a CompletionIterator for an interator: storage:MemoryStore.md#getValues[Values iterator from MemoryStore for blockId ] for \"deserialized\" persistence levels. Iterator from serializer:SerializerManager.md#dataDeserializeStream[ SerializerManager after the data stream has been deserialized] for the blockId block and storage:MemoryStore.md#getBytes[the bytes for blockId block] for \"serialized\" persistence levels. getLocalValues is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#readBroadcastBlock[readBroadcastBlock] BlockManager is requested to < > and < > === [[maybeCacheDiskValuesInMemory]] maybeCacheDiskValuesInMemory Internal Method [source,scala] \u00b6 maybeCacheDiskValuesInMemory T : Iterator[T] maybeCacheDiskValuesInMemory...FIXME maybeCacheDiskValuesInMemory is used when BlockManager is requested to < >. == [[getRemoteValues]] getRemoteValues Internal Method [source, scala] \u00b6 getRemoteValues T: ClassTag : Option[BlockResult] \u00b6 getRemoteValues ...FIXME == [[get]] Retrieving Block from Local or Remote Block Managers [source, scala] \u00b6 get T: ClassTag : Option[BlockResult] \u00b6 get attempts to get the blockId block from a local block manager first before requesting it from remote block managers. Internally, get tries to < >. If the block was found, you should see the following INFO message in the logs and get returns the local < >. INFO Found block [blockId] locally If however the block was not found locally, get tries to < >. If retrieved from a remote block manager, you should see the following INFO message in the logs and get returns the remote < >. INFO Found block [blockId] remotely In the end, get returns \"nothing\" (i.e. NONE ) when the blockId block was not found either in the local BlockManager or any remote BlockManager. [NOTE] \u00b6 get is used when: * BlockManager is requested to < > and < > \u00b6 == [[getBlockData]] Retrieving Block Data [source, scala] \u00b6 getBlockData( blockId: BlockId): ManagedBuffer NOTE: getBlockData is part of the storage:BlockDataManager.md#getBlockData[BlockDataManager] contract. For a BlockId.md[] of a shuffle (a ShuffleBlockId), getBlockData requests the < > for the shuffle:ShuffleManager.md#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for shuffle:ShuffleBlockResolver.md#getBlockData[getBlockData]. Otherwise, getBlockData < > for the given BlockId. If found, getBlockData creates a new BlockManagerManagedBuffer (with the < >, the input BlockId, the retrieved BlockData and the dispose flag enabled). If not found, getBlockData < > that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException. NOTE: getBlockData is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated). == [[getLocalBytes]] Retrieving Non-Shuffle Local Block Data [source, scala] \u00b6 getLocalBytes( blockId: BlockId): Option[BlockData] getLocalBytes ...FIXME [NOTE] \u00b6 getLocalBytes is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#readBlocks[readBlocks] * BlockManager is requested for the < > (of a non-shuffle block) \u00b6 == [[removeBlockInternal]] removeBlockInternal Internal Method [source, scala] \u00b6 removeBlockInternal( blockId: BlockId, tellMaster: Boolean): Unit removeBlockInternal...FIXME removeBlockInternal is used when BlockManager is requested to < > and < >. == [[stores]] Stores A Store is the place where blocks are held. There are the following possible stores: storage:MemoryStore.md[MemoryStore] for memory storage level. DiskStore.md[DiskStore] for disk storage level. ExternalBlockStore for OFF_HEAP storage level. == [[putBlockData]] Storing Block Data Locally [source, scala] \u00b6 putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean putBlockData simply < blockId locally>> (given the given storage level ). NOTE: putBlockData is part of the storage:BlockDataManager.md#putBlockData[BlockDataManager Contract]. Internally, putBlockData wraps ChunkedByteBuffer around data buffer's NIO ByteBuffer and calls < >. == [[putBytes]] Storing Block Bytes Locally [source, scala] \u00b6 putBytes( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean putBytes makes sure that the bytes are not null and < >. [NOTE] \u00b6 putBytes is used when: BlockManager is requested to < > TaskRunner is requested to executor:TaskRunner.md#run-result-sent-via-blockmanager[run] (and the result size is above executor:Executor.md#maxDirectResultSize[maxDirectResultSize]) * TorrentBroadcast is requested to core:TorrentBroadcast.md#writeBlocks[writeBlocks] and core:TorrentBroadcast.md#readBlocks[readBlocks] \u00b6 === [[doPutBytes]] doPutBytes Internal Method [source, scala] \u00b6 doPutBytes T : Boolean doPutBytes calls the internal helper < > with a function that accepts a BlockInfo and does the uploading. Inside the function, if the storage:StorageLevel.md[storage level ]'s replication is greater than 1, it immediately starts < > of the blockId block on a separate thread (from futureExecutionContext thread pool). The replication uses the input bytes and level storage level. For a memory storage level, the function checks whether the storage level is deserialized or not. For a deserialized storage level , BlockManager 's serializer:SerializerManager.md#dataDeserializeStream[ SerializerManager deserializes bytes into an iterator of values] that storage:MemoryStore.md#putIteratorAsValues[ MemoryStore stores]. If however the storage level is not deserialized, the function requests storage:MemoryStore.md#putBytes[ MemoryStore to store the bytes] If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs: WARN BlockManager: Persisting block [blockId] to disk instead. And DiskStore.md#putBytes[ DiskStore stores the bytes]. NOTE: DiskStore.md[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when storage:MemoryStore.md[MemoryStore] has failed. If the storage level is to use disk only, DiskStore.md#putBytes[ DiskStore stores the bytes]. doPutBytes requests < > and if the block was successfully stored, and the driver should know about it ( tellMaster ), the function < >. The executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status] (only when executed inside a task where TaskContext is available). You should see the following DEBUG message in the logs: DEBUG BlockManager: Put block [blockId] locally took [time] ms The function waits till the earlier asynchronous replication finishes for a block with replication level greater than 1 . The final result of doPutBytes is the result of storing the block successful or not (as computed earlier). NOTE: doPutBytes is used exclusively when BlockManager is requested to < >. == [[doPut]] doPut Internal Method [source, scala] \u00b6 doPut T (putBody: BlockInfo => Option[T]): Option[T] doPut executes the input putBody function with a storage:BlockInfo.md[] being a new BlockInfo object (with level storage level) that storage:BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager managed to create a write lock for]. If the block has already been created (and storage:BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager did not manage to create a write lock for]), the following WARN message is printed out to the logs: [source,plaintext] \u00b6 Block [blockId] already exists on this machine; not re-adding it \u00b6 doPut < > when keepReadLock flag is disabled and returns None immediately. If however the write lock has been given, doPut executes putBody . If the result of putBody is None the block is considered saved successfully. For successful save and keepReadLock enabled, storage:BlockInfoManager.md#downgradeLock[ BlockInfoManager is requested to downgrade an exclusive write lock for blockId to a shared read lock]. For successful save and keepReadLock disabled, storage:BlockInfoManager.md#unlock[ BlockInfoManager is requested to release lock on blockId ]. For unsuccessful save, < > and the following WARN message is printed out to the logs: [source,plaintext] \u00b6 Putting block [blockId] failed \u00b6 In the end, doPut prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Putting block [blockId] [withOrWithout] replication took [usedTime] ms \u00b6 doPut is used when BlockManager is requested to < > and < >. == [[removeBlock]] Removing Block From Memory and Disk [source, scala] \u00b6 removeBlock( blockId: BlockId, tellMaster: Boolean = true): Unit removeBlock removes the blockId block from the storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. When executed, it prints out the following DEBUG message to the logs: Removing block [blockId] It requests storage:BlockInfoManager.md[] for lock for writing for the blockId block. If it receives none, it prints out the following WARN message to the logs and quits. Asked to remove block [blockId], which does not exist Otherwise, with a write lock for the block, the block is removed from storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] (see storage:MemoryStore.md#remove[Removing Block in MemoryStore ] and DiskStore.md#remove[Removing Block in DiskStore ]). If both removals fail, it prints out the following WARN message: Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store The block is removed from storage:BlockInfoManager.md[]. removeBlock then < > that is used to < > (if the input tellMaster and the info's tellMaster are both enabled, i.e. true ) and the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. removeBlock is used when: BlockManager is requested to < >, < > and < > BlockManagerSlaveEndpoint is requested to handle a storage:BlockManagerSlaveEndpoint.md#RemoveBlock[RemoveBlock] message == [[removeRdd]] Removing RDD Blocks [source, scala] \u00b6 removeRdd(rddId: Int): Int \u00b6 removeRdd removes all the blocks that belong to the rddId RDD. It prints out the following INFO message to the logs: INFO Removing RDD [rddId] It then requests RDD blocks from storage:BlockInfoManager.md[] and < > (without informing the driver). The number of blocks removed is the final result. NOTE: It is used by storage:BlockManagerSlaveEndpoint.md#RemoveRdd[ BlockManagerSlaveEndpoint while handling RemoveRdd messages]. == [[removeBroadcast]] Removing All Blocks of Broadcast Variable [source, scala] \u00b6 removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int \u00b6 removeBroadcast removes all the blocks of the input broadcastId broadcast. Internally, it starts by printing out the following DEBUG message to the logs: Removing broadcast [broadcastId] It then requests all the storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] objects that belong to the broadcastId broadcast from storage:BlockInfoManager.md[] and < >. The number of blocks removed is the final result. NOTE: It is used by storage:BlockManagerSlaveEndpoint.md#RemoveBroadcast[ BlockManagerSlaveEndpoint while handling RemoveBroadcast messages]. == [[shuffleServerId]] BlockManagerId of Shuffle Server BlockManager uses storage:BlockManagerId.md[] for the location (address) of the server that serves shuffle files of this executor. The BlockManagerId is either the BlockManagerId of the external shuffle service (when < >) or the < >. The BlockManagerId of the Shuffle Server is used for the location of a scheduler:MapStatus.md[shuffle map output] when: BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write partition records to a shuffle file] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[close and write output] == [[getStatus]] getStatus Method [source,scala] \u00b6 getStatus( blockId: BlockId): Option[BlockStatus] getStatus...FIXME getStatus is used when BlockManagerSlaveEndpoint is requested to handle storage:BlockManagerSlaveEndpoint.md#GetBlockStatus[GetBlockStatus] message. == [[initialize]] Initializing BlockManager [source, scala] \u00b6 initialize( appId: String): Unit initialize initializes a BlockManager on the driver and executors (see ROOT:SparkContext.md#creating-instance[Creating SparkContext Instance] and executor:Executor.md#creating-instance[Creating Executor Instance], respectively). NOTE: The method must be called before a BlockManager can be considered fully operable. initialize does the following in order: Initializes storage:BlockTransferService.md#init[BlockTransferService] Initializes the internal shuffle client, be it storage:ExternalShuffleClient.md[ExternalShuffleClient] or storage:BlockTransferService.md[BlockTransferService]. BlockManagerMaster.md#registerBlockManager[Registers itself with the driver's BlockManagerMaster ] (using the id , maxMemory and its slaveEndpoint ). + The BlockManagerMaster reference is passed in when the < > on the driver and executors. Sets < > to an instance of storage:BlockManagerId.md[] given an executor id, host name and port for storage:BlockTransferService.md[BlockTransferService]. It creates the address of the server that serves this executor's shuffle files (using < >) CAUTION: FIXME Review the initialize procedure again CAUTION: FIXME Describe shuffleServerId . Where is it used? If the < >, initialize prints out the following INFO message to the logs: [source,plaintext] \u00b6 external shuffle service port = [externalShuffleServicePort] \u00b6 It BlockManagerMaster.md#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the storage:BlockManagerId.md[], the maximum memory (as maxMemory ), and the storage:BlockManagerSlaveEndpoint.md[]. Ultimately, if the initialization happens on an executor and the < >, it < >. initialize is used when SparkContext is created and when an executor:Executor.md#creating-instance[ Executor is created] (for executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[CoarseGrainedExecutorBackend] and spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md[MesosExecutorBackend]). == [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server [source, scala] \u00b6 registerWithExternalShuffleServer(): Unit \u00b6 registerWithExternalShuffleServer is an internal helper method to register the BlockManager for an executor with an deploy:ExternalShuffleService.md[external shuffle server]. NOTE: It is executed when a < >. When executed, you should see the following INFO message in the logs: Registering executor with local external shuffle service. It uses < > to storage:ExternalShuffleClient.md#registerWithShuffleServer[register the block manager] using < > (i.e. the host, the port and the executorId) and a ExecutorShuffleInfo . NOTE: The ExecutorShuffleInfo uses localDirs and subDirsPerLocalDir from DiskBlockManager.md[DiskBlockManager] and the class name of the constructor shuffle:ShuffleManager.md[ShuffleManager]. It tries to register at most 3 times with 5-second sleeps in-between. NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured. Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs: Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds... registerWithExternalShuffleServer is used when BlockManager is requested to < > (when executed on an executor with < >). == [[reregister]] Re-registering BlockManager with Driver and Reporting Blocks [source, scala] \u00b6 reregister(): Unit \u00b6 When executed, reregister prints the following INFO message to the logs: BlockManager [blockManagerId] re-registering with master reregister then BlockManagerMaster.md#registerBlockManager[registers itself to the driver's BlockManagerMaster ] (just as it was when < >). It passes the storage:BlockManagerId.md[], the maximum memory (as maxMemory ), and the storage:BlockManagerSlaveEndpoint.md[]. reregister will then report all the local blocks to the BlockManagerMaster.md[BlockManagerMaster]. You should see the following INFO message in the logs: Reporting [blockInfoManager.size] blocks to the master. For each block metadata (in storage:BlockInfoManager.md[]) it < > and < >. If there is an issue communicating to the BlockManagerMaster.md[BlockManagerMaster], you should see the following ERROR message in the logs: Failed to report [blockId] to master; giving up. After the ERROR message, reregister stops reporting. reregister is used when a executor:Executor.md#heartbeats-and-active-task-metrics[ Executor was informed to re-register while sending heartbeats]. == [[getCurrentBlockStatus]] Calculate Current Block Status [source, scala] \u00b6 getCurrentBlockStatus( blockId: BlockId, info: BlockInfo): BlockStatus getCurrentBlockStatus gives the current BlockStatus of the BlockId block (with the block's current storage:StorageLevel.md[StorageLevel], memory and disk sizes). It uses storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] for size and other information. NOTE: Most of the information to build BlockStatus is already in BlockInfo except that it may not necessarily reflect the current state per storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. Internally, it uses the input storage:BlockInfo.md[] to know about the block's storage level. If the storage level is not set (i.e. null ), the returned BlockStatus assumes the storage:StorageLevel.md[default NONE storage level] and the memory and disk sizes being 0 . If however the storage level is set, getCurrentBlockStatus uses storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their getSize or assume 0 ). NOTE: It is acceptable that the BlockInfo says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status. getCurrentBlockStatus is used when < >, < > or < > or < >. == [[reportAllBlocks]] reportAllBlocks Internal Method [source, scala] \u00b6 reportAllBlocks(): Unit \u00b6 reportAllBlocks...FIXME reportAllBlocks is used when BlockManager is requested to < >. == [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver [source, scala] \u00b6 reportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Unit reportBlockStatus is an internal method for < > and if told to re-register it prints out the following INFO message to the logs: Got told to re-register updating block [blockId] It does asynchronous reregistration (using asyncReregister ). In either case, it prints out the following DEBUG message to the logs: Told master about block [blockId] reportBlockStatus is used when BlockManager is requested to < >, < >, < >, < > and < >. == [[tryToReportBlockStatus]] Reporting Block Status Update to Driver [source, scala] \u00b6 def tryToReportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean tryToReportBlockStatus BlockManagerMaster.md#updateBlockInfo[reports block status update] to < > and returns its response. tryToReportBlockStatus is used when BlockManager is requested to < > or < >. == [[execution-context]] Execution Context block-manager-future is the execution context for...FIXME == [[ByteBuffer]] ByteBuffer The underlying abstraction for blocks in Spark is a ByteBuffer that limits the size of a block to 2GB ( Integer.MAX_VALUE - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for long ), ser-deser via byte array-backed output streams. == [[BlockResult]] BlockResult BlockResult is a description of a fetched block with the readMethod and bytes . == [[registerTask]] Registering Task [source, scala] \u00b6 registerTask( taskAttemptId: Long): Unit registerTask requests the < > to storage:BlockInfoManager.md#registerTask[register a given task]. registerTask is used when Task is requested to scheduler:Task.md#run[run] (at the start of a task). == [[getDiskWriter]] Creating DiskBlockObjectWriter [source, scala] \u00b6 getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter getDiskWriter creates a storage:DiskBlockObjectWriter.md[DiskBlockObjectWriter] (with ROOT:configuration-properties.md#spark.shuffle.sync[spark.shuffle.sync] configuration property for syncWrites argument). getDiskWriter uses the < > of the BlockManager. getDiskWriter is used when: BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records (of a partition)] ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile] ExternalAppendOnlyMap is requested to shuffle:ExternalAppendOnlyMap.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to shuffle:ExternalSorter.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] and shuffle:ExternalSorter.md#writePartitionedFile[writePartitionedFile] memory:UnsafeSorterSpillWriter.md[UnsafeSorterSpillWriter] is created == [[addUpdatedBlockStatusToTaskMetrics]] Recording Updated BlockStatus In Current Task's TaskMetrics [source, scala] \u00b6 addUpdatedBlockStatusToTaskMetrics( blockId: BlockId, status: BlockStatus): Unit addUpdatedBlockStatusToTaskMetrics spark-TaskContext.md#get[takes an active TaskContext ] (if available) and executor:TaskMetrics.md#incUpdatedBlockStatuses[records updated BlockStatus for Block ] (in the spark-TaskContext.md#taskMetrics[task's TaskMetrics ]). addUpdatedBlockStatusToTaskMetrics is used when BlockManager < > (for a block that was successfully stored), < >, < >, < > (possibly spilling it to disk) and < >. == [[shuffleMetricsSource]] Requesting Shuffle-Related Spark Metrics Source [source, scala] \u00b6 shuffleMetricsSource: Source \u00b6 shuffleMetricsSource requests the < > for the storage:ShuffleClient.md#shuffleMetrics[shuffle metrics] and creates a storage:ShuffleMetricsSource.md[] with the storage:ShuffleMetricsSource.md#sourceName[source name] based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property: ExternalShuffle when ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on ( true ) NettyBlockTransfer when ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off ( false ) shuffleMetricsSource is used when Executor is executor:Executor.md#creating-instance[created] (for non-local / cluster modes). == [[replicate]] Replicating Block To Peers [source, scala] \u00b6 replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty): Unit replicate...FIXME replicate is used when BlockManager is requested to < >, < > and < >. == [[replicateBlock]] replicateBlock Method [source, scala] \u00b6 replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int): Unit replicateBlock...FIXME replicateBlock is used when BlockManagerSlaveEndpoint is requested to storage:BlockManagerSlaveEndpoint.md#ReplicateBlock[handle a ReplicateBlock message]. == [[putIterator]] putIterator Method [source, scala] \u00b6 putIterator T: ClassTag : Boolean putIterator ...FIXME [NOTE] \u00b6 putIterator is used when: BlockManager is requested to < > * Spark Streaming's BlockManagerBasedBlockHandler is requested to storeBlock \u00b6 == [[putSingle]] putSingle Method [source, scala] \u00b6 putSingle T: ClassTag : Boolean putSingle...FIXME putSingle is used when TorrentBroadcast is requested to core:TorrentBroadcast.md#writeBlocks[write the blocks] and core:TorrentBroadcast.md#readBroadcastBlock[readBroadcastBlock]. == [[getRemoteBytes]] Fetching Block From Remote Nodes [source, scala] \u00b6 getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer] \u00b6 getRemoteBytes ...FIXME [NOTE] \u00b6 getRemoteBytes is used when: BlockManager is requested to < > TorrentBroadcast is requested to core:TorrentBroadcast.md#readBlocks[readBlocks] * TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueuing a successful IndirectTaskResult] \u00b6 == [[getRemoteValues]] getRemoteValues Internal Method [source, scala] \u00b6 getRemoteValues T: ClassTag : Option[BlockResult] \u00b6 getRemoteValues ...FIXME NOTE: getRemoteValues is used exclusively when BlockManager is requested to < >. == [[getSingle]] getSingle Method [source, scala] \u00b6 getSingle T: ClassTag : Option[T] \u00b6 getSingle ...FIXME NOTE: getSingle is used exclusively in Spark tests. == [[getOrElseUpdate]] Getting Block From Block Managers Or Computing and Storing It Otherwise [source, scala] \u00b6 getOrElseUpdate T : Either[BlockResult, Iterator[T]] [NOTE] \u00b6 I think it is fair to say that getOrElseUpdate is like ++ https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html#getOrElseUpdate(key:K,op:=%3EV):V++[getOrElseUpdate ] of https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html[scala.collection.mutable.Map ] in Scala. [source, scala] \u00b6 getOrElseUpdate(key: K, op: \u21d2 V): V \u00b6 Quoting the official scaladoc: If given key K is already in this map, getOrElseUpdate returns the associated value V . Otherwise, getOrElseUpdate computes a value V from given expression op , stores with the key K in the map and returns that value. Since BlockManager is a key-value store of blocks of data identified by a block ID that works just fine. \u00b6 getOrElseUpdate first attempts to < > by the BlockId (from the local block manager first and, if unavailable, requesting remote peers). [TIP] \u00b6 Enable INFO logging level for org.apache.spark.storage.BlockManager logger to see what happens when BlockManager tries to < >. See < > in this document. \u00b6 getOrElseUpdate gives the BlockResult of the block if found. If however the block was not found (in any block manager in a Spark cluster), getOrElseUpdate < > (for the input BlockId , the makeIterator function and the StorageLevel ). getOrElseUpdate branches off per the result. For None , getOrElseUpdate < > for the BlockId and eventually returns the BlockResult (unless terminated by a SparkException due to some internal error). For Some(iter) , getOrElseUpdate returns an iterator of T values. NOTE: getOrElseUpdate is used exclusively when RDD is requested to rdd:RDD.md#getOrCompute[get or compute an RDD partition] (for a RDDBlockId with a RDD ID and a partition index). == [[doPutIterator]] doPutIterator Internal Method [source, scala] \u00b6 doPutIterator T : Option[PartiallyUnrolledIterator[T]] doPutIterator simply < > with the putBody function that accepts a BlockInfo and does the following: . putBody branches off per whether the StorageLevel indicates to use a storage:StorageLevel.md#useMemory[memory] or simply a storage:StorageLevel.md#useDisk[disk], i.e. When the input StorageLevel indicates to storage:StorageLevel.md#useMemory[use a memory] for storage in storage:StorageLevel.md#deserialized[deserialized] format, putBody requests < > to storage:MemoryStore.md#putIteratorAsValues[putIteratorAsValues] (for the BlockId and with the iterator factory function). + If the < > returned a correct value, the internal size is set to the value. + If however the < > failed to give a correct value, FIXME When the input StorageLevel indicates to storage:StorageLevel.md#useMemory[use memory] for storage in storage:StorageLevel.md#deserialized[serialized] format, putBody ...FIXME When the input StorageLevel does not indicate to use memory for storage but storage:StorageLevel.md#useDisk[disk] instead, putBody ...FIXME . putBody requests the < > . Only when the block was successfully stored in either the memory or disk store: putBody < > to the < > when the input tellMaster flag (default: enabled) and the tellMaster flag of the block info are both enabled. putBody < > (with the BlockId and BlockStatus ) putBody prints out the following DEBUG message to the logs: + Put block [blockId] locally took [time] ms When the input StorageLevel indicates to use storage:StorageLevel.md#replication[replication], putBody < > followed by < > (with the input BlockId and the StorageLevel as well as the BlockData to replicate) With a successful replication, putBody prints out the following DEBUG message to the logs: + Put block [blockId] remotely took [time] ms . In the end, putBody may or may not give a PartiallyUnrolledIterator if...FIXME NOTE: doPutIterator is used when BlockManager is requested to < > and < >. == [[dropFromMemory]] Dropping Block from Memory [source,scala] \u00b6 dropFromMemory( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel dropFromMemory prints out the following INFO message to the logs: [source,plaintext] \u00b6 Dropping block [blockId] from memory \u00b6 dropFromMemory then asserts that the given block is storage:BlockInfoManager.md#assertBlockIsLockedForWriting[locked for writing]. If the block's storage:StorageLevel.md[StorageLevel] uses disks and the internal DiskStore.md[DiskStore] object ( diskStore ) does not contain the block, it is saved then. You should see the following INFO message in the logs: Writing block [blockId] to disk CAUTION: FIXME Describe the case with saving a block to disk. The block's memory size is fetched and recorded (using MemoryStore.getSize ). The block is storage:MemoryStore.md#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs: Block [blockId] could not be dropped from memory as it does not exist It then < > and < >. It only happens when info.tellMaster . CAUTION: FIXME When would info.tellMaster be true ? A block is considered updated when it was written to disk or removed from memory or both. If either happened, the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. In the end, dropFromMemory returns the current storage level of the block. dropFromMemory is part of the storage:BlockEvictionHandler.md#dropFromMemory[BlockEvictionHandler] abstraction. == [[handleLocalReadFailure]] handleLocalReadFailure Internal Method [source, scala] \u00b6 handleLocalReadFailure(blockId: BlockId): Nothing \u00b6 handleLocalReadFailure ...FIXME NOTE: handleLocalReadFailure is used when...FIXME == [[releaseLockAndDispose]] releaseLockAndDispose Method [source, scala] \u00b6 releaseLockAndDispose( blockId: BlockId, data: BlockData, taskAttemptId: Option[Long] = None): Unit releaseLockAndDispose...FIXME releaseLockAndDispose is used when...FIXME == [[releaseLock]] releaseLock Method [source, scala] \u00b6 releaseLock( blockId: BlockId, taskAttemptId: Option[Long] = None): Unit releaseLock requests the < > to storage:BlockInfoManager.md#unlock[unlock the given block]. releaseLock is part of the storage:BlockDataManager.md#releaseLock[BlockDataManager] abstraction. == [[putBlockDataAsStream]] putBlockDataAsStream Method [source,scala] \u00b6 putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID putBlockDataAsStream...FIXME putBlockDataAsStream is part of the storage:BlockDataManager.md#putBlockDataAsStream[BlockDataManager] abstraction. == [[downgradeLock]] downgradeLock Method [source, scala] \u00b6 downgradeLock( blockId: BlockId): Unit downgradeLock requests the < > to storage:BlockInfoManager.md#downgradeLock[downgradeLock] for the given storage:BlockId.md[block]. downgradeLock seems not to be used. == [[blockIdsToLocations]] blockIdsToLocations Utility [source,scala] \u00b6 blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] blockIdsToLocations...FIXME blockIdsToLocations is used in the now defunct Spark Streaming (when BlockRDD is requested for _locations). === [[getLocationBlockIds]] getLocationBlockIds Internal Method [source,scala] \u00b6 getLocationBlockIds( blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] getLocationBlockIds...FIXME getLocationBlockIds is used when BlockManager utility is requested to < > (for the now defunct Spark Streaming). == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.storage.BlockManager=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[maxMemory]] Maximum Memory Total maximum value that BlockManager can ever possibly use (that depends on < > and may vary over time). Total available memory:MemoryManager.md#maxOnHeapStorageMemory[on-heap] and memory:MemoryManager.md#maxOffHeapStorageMemory[off-heap] memory for storage (in bytes) === [[maxOffHeapMemory]] Maximum Off-Heap Memory === [[maxOnHeapMemory]] Maximum On-Heap Memory","title":"BlockManager"},{"location":"storage/BlockManager/#blockmanager","text":"BlockManager manages the storage for blocks ( chunks of data ) that can be stored in < > and on < >. BlockManager runs on the driver and executors . BlockManager provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. < >. [[futureExecutionContext]] BlockManager uses a Scala https://www.scala-lang.org/api/current/scala/concurrent/ExecutionContextExecutorService.html[ExecutionContextExecutorService ] to execute FIXME asynchronously (on a thread pool with block-manager-future prefix and maximum of 128 threads). Cached blocks are blocks with non-zero sum of memory and disk sizes. TIP: Use webui:index.md[Web UI], esp. webui:spark-webui-storage.md[Storage] and webui:spark-webui-executors.md[Executors] tabs, to monitor the memory used. TIP: Use tools:spark-submit.md[spark-submit]'s command-line options, i.e. tools:spark-submit.md#driver-memory[--driver-memory] for the driver and tools:spark-submit.md#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. tools:spark-submit.md#spark.executor.memory[spark.executor.memory] and tools:spark-submit.md#spark_driver_memory[spark.driver.memory], to control the memory for storage memory. When < >, BlockManager uses storage:ExternalShuffleClient.md[ExternalShuffleClient] to read other executors' shuffle files. == [[creating-instance]] Creating Instance BlockManager takes the following to be created: < > < > < > [[serializerManager]] serializer:SerializerManager.md[] [[conf]] ROOT:SparkConf.md[] < > < > < > < > [[securityManager]] SecurityManager [[numUsableCores]] Number of CPU cores (for an storage:ExternalShuffleClient.md[] with < >) When created, BlockManager sets < > internal flag based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. BlockManager then creates an instance of DiskBlockManager.md[DiskBlockManager] (requesting deleteFilesOnStop when an external shuffle service is not in use). BlockManager creates block-manager-future daemon cached thread pool with 128 threads maximum (as futureExecutionContext ). BlockManager calculates the maximum memory to use (as maxMemory ) by requesting the maximum memory:MemoryManager.md#maxOnHeapStorageMemory[on-heap] and memory:MemoryManager.md#maxOffHeapStorageMemory[off-heap] storage memory from the assigned MemoryManager . BlockManager calculates the port used by the external shuffle service (as externalShuffleServicePort ). NOTE: It is computed specially in Spark on YARN. BlockManager creates a client to read other executors' shuffle files (as shuffleClient ). If the external shuffle service is used an storage:ExternalShuffleClient.md[ExternalShuffleClient] is created or the input storage:BlockTransferService.md[BlockTransferService] is used. BlockManager sets the ROOT:configuration-properties.md#spark.block.failures.beforeLocationRefresh[maximum number of failures] before this block manager refreshes the block locations from the driver (as maxFailuresBeforeLocationRefresh ). BlockManager registers a storage:BlockManagerSlaveEndpoint.md[] with the input ROOT:index.md[RpcEnv], itself, and scheduler:MapOutputTracker.md[MapOutputTracker] (as slaveEndpoint ). BlockManager is created when SparkEnv is core:SparkEnv.md#create-BlockManager[created] (for the driver and executors) when a Spark application starts. .BlockManager and SparkEnv image::BlockManager-SparkEnv.png[align=\"center\"] == [[BlockEvictionHandler]] BlockEvictionHandler BlockManager is a storage:BlockEvictionHandler.md[] that can < > (and store it on a disk when needed). == [[shuffleClient]][[externalShuffleServiceEnabled]] ShuffleClient and External Shuffle Service BlockManager manages the lifecycle of a storage:ShuffleClient.md[]: Creates when < > storage:ShuffleClient.md#init[Inits] (and possibly < >) when requested to < > Closes when requested to < > The ShuffleClient can be an storage:ExternalShuffleClient.md[] or the given < > based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property. When enabled, BlockManager uses the storage:ExternalShuffleClient.md[]. The ShuffleClient is available to other Spark services (using shuffleClient value) and is used when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined key-value records for a reduce task]. When requested for < >, BlockManager simply requests storage:ShuffleClient.md#shuffleMetrics[them] from the ShuffleClient. == [[rpcEnv]] BlockManager and RpcEnv BlockManager is given a rpc:RpcEnv.md[] when < >. The RpcEnv is used to set up a < >. == [[blockInfoManager]] BlockInfoManager BlockManager creates a storage:BlockInfoManager.md[] when < >. BlockManager requests the BlockInfoManager to storage:BlockInfoManager.md#clear[clear] when requested to < >. BlockManager uses the BlockInfoManager to create a < >. BlockManager uses the BlockInfoManager when requested for the following: < > < > < > < > and < > < > < > < > < >, < >, < >, < > < >, < >, < >, < > == [[master]] BlockManager and BlockManagerMaster BlockManager is given a storage:BlockManagerMaster.md[] when < >. == [[BlockDataManager]] BlockManager as BlockDataManager BlockManager is a storage:BlockDataManager.md[]. == [[mapOutputTracker]] BlockManager and MapOutputTracker BlockManager is given a scheduler:MapOutputTracker.md[] when < >. == [[executorId]] Executor ID BlockManager is given an Executor ID when < >. The Executor ID is one of the following: driver ( SparkContext.DRIVER_IDENTIFIER ) for the driver Value of executor:CoarseGrainedExecutorBackend.md#executor-id[--executor-id] command-line argument for executor:CoarseGrainedExecutorBackend.md[] executors (or spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md[MesosExecutorBackend]) == [[slaveEndpoint]] BlockManagerEndpoint RPC Endpoint BlockManager requests the < > to rpc:RpcEnv.md#setupEndpoint[register] a storage:BlockManagerSlaveEndpoint.md[] under the name BlockManagerEndpoint[ID] . The RPC endpoint is used when BlockManager is requested to < > and < > (to register the BlockManager on an executor with the < > on the driver). The endpoint is stopped (by requesting the < > to rpc:RpcEnv.md#stop[stop the reference]) when BlockManager is requested to < >. == [[SparkEnv]] Accessing BlockManager Using SparkEnv BlockManager is available using core:SparkEnv.md#blockManager[SparkEnv] on the driver and executors.","title":"BlockManager"},{"location":"storage/BlockManager/#sourceplaintext","text":"import org.apache.spark.SparkEnv val bm = SparkEnv.get.blockManager scala> :type bm org.apache.spark.storage.BlockManager == [[blockTransferService]] BlockTransferService BlockManager is given a storage:BlockTransferService.md[BlockTransferService] when < >. BlockTransferService is used as the < > when BlockManager is configured with no external shuffle service (based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). BlockTransferService is storage:BlockTransferService.md#init[initialized] when BlockManager < >. BlockTransferService is storage:BlockTransferService.md#close[closed] when BlockManager is requested to < >. BlockTransferService is used when BlockManager is requested to < > or < > remote block managers. == [[memoryManager]] MemoryManager BlockManager is given a memory:MemoryManager.md[MemoryManager] when < >. BlockManager uses the MemoryManager for the following: Create the < > (that is then assigned to memory:MemoryManager.md#setMemoryStore[MemoryManager] as a \"circular dependency\") Initialize < > and < > (for reporting) == [[shuffleManager]] ShuffleManager BlockManager is given a shuffle:ShuffleManager.md[ShuffleManager] when < >. BlockManager uses the ShuffleManager for the following: < > (for shuffle blocks) < > (for shuffle blocks anyway) < > (when < > on an executor with < >) == [[diskBlockManager]] DiskBlockManager BlockManager creates a DiskBlockManager.md[DiskBlockManager] when < >. .DiskBlockManager and BlockManager image::DiskBlockManager-BlockManager.png[align=\"center\"] BlockManager uses the BlockManager for the following: Creating a < > < > (when < > on an executor with < >) The BlockManager is available as diskBlockManager reference to other Spark systems.","title":"[source,plaintext]"},{"location":"storage/BlockManager/#source-scala","text":"import org.apache.spark.SparkEnv SparkEnv.get.blockManager.diskBlockManager == [[memoryStore]] MemoryStore BlockManager creates a storage:MemoryStore.md[] when < > (with the < >, the < >, the < > and itself as a storage:BlockEvictionHandler.md[]). .MemoryStore and BlockManager image::MemoryStore-BlockManager.png[align=\"center\"] BlockManager requests the < > to memory:MemoryManager.md#setMemoryStore[use] the MemoryStore. BlockManager uses the MemoryStore for the following: < > and < > < > < > < > and < > < > and < > < > < > The MemoryStore is requested to storage:MemoryStore.md#clear[clear] when BlockManager is requested to < >. The MemoryStore is available as memoryStore private reference to other Spark services.","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_1","text":"import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore The MemoryStore is used (via SparkEnv.get.blockManager.memoryStore reference) when Task is requested to scheduler:Task.md#run[run] (that has finished and requests the MemoryStore to storage:MemoryStore.md#releaseUnrollMemoryForThisTask[releaseUnrollMemoryForThisTask]). == [[diskStore]] DiskStore BlockManager creates a DiskStore.md[DiskStore] (with the < >) when < >. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] BlockManager uses the DiskStore when requested to < >, < >, < >, < >, < >, < >, < >, < >. == [[metrics]] Performance Metrics BlockManager uses spark-BlockManager-BlockManagerSource.md[BlockManagerSource] to report metrics under the name BlockManager . == [[getPeers]] getPeers Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala","text":"getPeers( forceFetch: Boolean): Seq[BlockManagerId] getPeers...FIXME getPeers is used when BlockManager is requested to < > and < >. == [[releaseAllLocksForTask]] Releasing All Locks For Task","title":"[source,scala]"},{"location":"storage/BlockManager/#sourcescala_1","text":"releaseAllLocksForTask( taskAttemptId: Long): Seq[BlockId] releaseAllLocksForTask...FIXME releaseAllLocksForTask is used when TaskRunner is requested to executor:TaskRunner.md#run[run] (at the end of a task). == [[stop]] Stopping BlockManager","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#stop-unit","text":"stop...FIXME stop is used when SparkEnv is requested to core:SparkEnv.md#stop[stop]. == [[getMatchingBlockIds]] Getting IDs of Existing Blocks (For a Given Filter)","title":"stop(): Unit"},{"location":"storage/BlockManager/#source-scala_3","text":"getMatchingBlockIds( filter: BlockId => Boolean): Seq[BlockId] getMatchingBlockIds...FIXME getMatchingBlockIds is used when BlockManagerSlaveEndpoint is requested to storage:BlockManagerSlaveEndpoint.md#GetMatchingBlockIds[handle a GetMatchingBlockIds message]. == [[getLocalValues]] Getting Local Block","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_4","text":"getLocalValues( blockId: BlockId): Option[BlockResult] getLocalValues prints out the following DEBUG message to the logs: Getting local block [blockId] getLocalValues storage:BlockInfoManager.md#lockForReading[obtains a read lock for blockId ]. When no blockId block was found, you should see the following DEBUG message in the logs and getLocalValues returns \"nothing\" (i.e. NONE ). Block [blockId] was not found When the blockId block was found, you should see the following DEBUG message in the logs: Level for block [blockId] is [level] If blockId block has memory level and storage:MemoryStore.md#contains[is registered in MemoryStore ], getLocalValues returns a < > as Memory read method and with a CompletionIterator for an interator: storage:MemoryStore.md#getValues[Values iterator from MemoryStore for blockId ] for \"deserialized\" persistence levels. Iterator from serializer:SerializerManager.md#dataDeserializeStream[ SerializerManager after the data stream has been deserialized] for the blockId block and storage:MemoryStore.md#getBytes[the bytes for blockId block] for \"serialized\" persistence levels. getLocalValues is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#readBroadcastBlock[readBroadcastBlock] BlockManager is requested to < > and < > === [[maybeCacheDiskValuesInMemory]] maybeCacheDiskValuesInMemory Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_2","text":"maybeCacheDiskValuesInMemory T : Iterator[T] maybeCacheDiskValuesInMemory...FIXME maybeCacheDiskValuesInMemory is used when BlockManager is requested to < >. == [[getRemoteValues]] getRemoteValues Internal Method","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getremotevaluest-classtag-optionblockresult","text":"getRemoteValues ...FIXME == [[get]] Retrieving Block from Local or Remote Block Managers","title":"getRemoteValuesT: ClassTag: Option[BlockResult]"},{"location":"storage/BlockManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#gett-classtag-optionblockresult","text":"get attempts to get the blockId block from a local block manager first before requesting it from remote block managers. Internally, get tries to < >. If the block was found, you should see the following INFO message in the logs and get returns the local < >. INFO Found block [blockId] locally If however the block was not found locally, get tries to < >. If retrieved from a remote block manager, you should see the following INFO message in the logs and get returns the remote < >. INFO Found block [blockId] remotely In the end, get returns \"nothing\" (i.e. NONE ) when the blockId block was not found either in the local BlockManager or any remote BlockManager.","title":"getT: ClassTag: Option[BlockResult]"},{"location":"storage/BlockManager/#note","text":"get is used when:","title":"[NOTE]"},{"location":"storage/BlockManager/#blockmanager-is-requested-to-and","text":"== [[getBlockData]] Retrieving Block Data","title":"* BlockManager is requested to &lt;&gt; and &lt;&gt;"},{"location":"storage/BlockManager/#source-scala_7","text":"getBlockData( blockId: BlockId): ManagedBuffer NOTE: getBlockData is part of the storage:BlockDataManager.md#getBlockData[BlockDataManager] contract. For a BlockId.md[] of a shuffle (a ShuffleBlockId), getBlockData requests the < > for the shuffle:ShuffleManager.md#shuffleBlockResolver[ShuffleBlockResolver] that is then requested for shuffle:ShuffleBlockResolver.md#getBlockData[getBlockData]. Otherwise, getBlockData < > for the given BlockId. If found, getBlockData creates a new BlockManagerManagedBuffer (with the < >, the input BlockId, the retrieved BlockData and the dispose flag enabled). If not found, getBlockData < > that the block could not be found (and that the master should no longer assume the block is available on this executor) and throws a BlockNotFoundException. NOTE: getBlockData is executed for shuffle blocks or local blocks that the BlockManagerMaster knows this executor really has (unless BlockManagerMaster is outdated). == [[getLocalBytes]] Retrieving Non-Shuffle Local Block Data","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_8","text":"getLocalBytes( blockId: BlockId): Option[BlockData] getLocalBytes ...FIXME","title":"[source, scala]"},{"location":"storage/BlockManager/#note_1","text":"getLocalBytes is used when: TorrentBroadcast is requested to core:TorrentBroadcast.md#readBlocks[readBlocks]","title":"[NOTE]"},{"location":"storage/BlockManager/#blockmanager-is-requested-for-the-of-a-non-shuffle-block","text":"== [[removeBlockInternal]] removeBlockInternal Internal Method","title":"* BlockManager is requested for the &lt;&gt; (of a non-shuffle block)"},{"location":"storage/BlockManager/#source-scala_9","text":"removeBlockInternal( blockId: BlockId, tellMaster: Boolean): Unit removeBlockInternal...FIXME removeBlockInternal is used when BlockManager is requested to < > and < >. == [[stores]] Stores A Store is the place where blocks are held. There are the following possible stores: storage:MemoryStore.md[MemoryStore] for memory storage level. DiskStore.md[DiskStore] for disk storage level. ExternalBlockStore for OFF_HEAP storage level. == [[putBlockData]] Storing Block Data Locally","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_10","text":"putBlockData( blockId: BlockId, data: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Boolean putBlockData simply < blockId locally>> (given the given storage level ). NOTE: putBlockData is part of the storage:BlockDataManager.md#putBlockData[BlockDataManager Contract]. Internally, putBlockData wraps ChunkedByteBuffer around data buffer's NIO ByteBuffer and calls < >. == [[putBytes]] Storing Block Bytes Locally","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_11","text":"putBytes( blockId: BlockId, bytes: ChunkedByteBuffer, level: StorageLevel, tellMaster: Boolean = true): Boolean putBytes makes sure that the bytes are not null and < >.","title":"[source, scala]"},{"location":"storage/BlockManager/#note_2","text":"putBytes is used when: BlockManager is requested to < > TaskRunner is requested to executor:TaskRunner.md#run-result-sent-via-blockmanager[run] (and the result size is above executor:Executor.md#maxDirectResultSize[maxDirectResultSize])","title":"[NOTE]"},{"location":"storage/BlockManager/#torrentbroadcast-is-requested-to-coretorrentbroadcastmdwriteblockswriteblocks-and-coretorrentbroadcastmdreadblocksreadblocks","text":"=== [[doPutBytes]] doPutBytes Internal Method","title":"* TorrentBroadcast is requested to core:TorrentBroadcast.md#writeBlocks[writeBlocks] and core:TorrentBroadcast.md#readBlocks[readBlocks]"},{"location":"storage/BlockManager/#source-scala_12","text":"doPutBytes T : Boolean doPutBytes calls the internal helper < > with a function that accepts a BlockInfo and does the uploading. Inside the function, if the storage:StorageLevel.md[storage level ]'s replication is greater than 1, it immediately starts < > of the blockId block on a separate thread (from futureExecutionContext thread pool). The replication uses the input bytes and level storage level. For a memory storage level, the function checks whether the storage level is deserialized or not. For a deserialized storage level , BlockManager 's serializer:SerializerManager.md#dataDeserializeStream[ SerializerManager deserializes bytes into an iterator of values] that storage:MemoryStore.md#putIteratorAsValues[ MemoryStore stores]. If however the storage level is not deserialized, the function requests storage:MemoryStore.md#putBytes[ MemoryStore to store the bytes] If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs: WARN BlockManager: Persisting block [blockId] to disk instead. And DiskStore.md#putBytes[ DiskStore stores the bytes]. NOTE: DiskStore.md[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when storage:MemoryStore.md[MemoryStore] has failed. If the storage level is to use disk only, DiskStore.md#putBytes[ DiskStore stores the bytes]. doPutBytes requests < > and if the block was successfully stored, and the driver should know about it ( tellMaster ), the function < >. The executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the updated block status] (only when executed inside a task where TaskContext is available). You should see the following DEBUG message in the logs: DEBUG BlockManager: Put block [blockId] locally took [time] ms The function waits till the earlier asynchronous replication finishes for a block with replication level greater than 1 . The final result of doPutBytes is the result of storing the block successful or not (as computed earlier). NOTE: doPutBytes is used exclusively when BlockManager is requested to < >. == [[doPut]] doPut Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_13","text":"doPut T (putBody: BlockInfo => Option[T]): Option[T] doPut executes the input putBody function with a storage:BlockInfo.md[] being a new BlockInfo object (with level storage level) that storage:BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager managed to create a write lock for]. If the block has already been created (and storage:BlockInfoManager.md#lockNewBlockForWriting[ BlockInfoManager did not manage to create a write lock for]), the following WARN message is printed out to the logs:","title":"[source, scala]"},{"location":"storage/BlockManager/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#block-blockid-already-exists-on-this-machine-not-re-adding-it","text":"doPut < > when keepReadLock flag is disabled and returns None immediately. If however the write lock has been given, doPut executes putBody . If the result of putBody is None the block is considered saved successfully. For successful save and keepReadLock enabled, storage:BlockInfoManager.md#downgradeLock[ BlockInfoManager is requested to downgrade an exclusive write lock for blockId to a shared read lock]. For successful save and keepReadLock disabled, storage:BlockInfoManager.md#unlock[ BlockInfoManager is requested to release lock on blockId ]. For unsuccessful save, < > and the following WARN message is printed out to the logs:","title":"Block [blockId] already exists on this machine; not re-adding it"},{"location":"storage/BlockManager/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#putting-block-blockid-failed","text":"In the end, doPut prints out the following DEBUG message to the logs:","title":"Putting block [blockId] failed"},{"location":"storage/BlockManager/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#putting-block-blockid-withorwithout-replication-took-usedtime-ms","text":"doPut is used when BlockManager is requested to < > and < >. == [[removeBlock]] Removing Block From Memory and Disk","title":"Putting block [blockId] [withOrWithout] replication took [usedTime] ms"},{"location":"storage/BlockManager/#source-scala_14","text":"removeBlock( blockId: BlockId, tellMaster: Boolean = true): Unit removeBlock removes the blockId block from the storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. When executed, it prints out the following DEBUG message to the logs: Removing block [blockId] It requests storage:BlockInfoManager.md[] for lock for writing for the blockId block. If it receives none, it prints out the following WARN message to the logs and quits. Asked to remove block [blockId], which does not exist Otherwise, with a write lock for the block, the block is removed from storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] (see storage:MemoryStore.md#remove[Removing Block in MemoryStore ] and DiskStore.md#remove[Removing Block in DiskStore ]). If both removals fail, it prints out the following WARN message: Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store The block is removed from storage:BlockInfoManager.md[]. removeBlock then < > that is used to < > (if the input tellMaster and the info's tellMaster are both enabled, i.e. true ) and the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. removeBlock is used when: BlockManager is requested to < >, < > and < > BlockManagerSlaveEndpoint is requested to handle a storage:BlockManagerSlaveEndpoint.md#RemoveBlock[RemoveBlock] message == [[removeRdd]] Removing RDD Blocks","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_15","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#removerddrddid-int-int","text":"removeRdd removes all the blocks that belong to the rddId RDD. It prints out the following INFO message to the logs: INFO Removing RDD [rddId] It then requests RDD blocks from storage:BlockInfoManager.md[] and < > (without informing the driver). The number of blocks removed is the final result. NOTE: It is used by storage:BlockManagerSlaveEndpoint.md#RemoveRdd[ BlockManagerSlaveEndpoint while handling RemoveRdd messages]. == [[removeBroadcast]] Removing All Blocks of Broadcast Variable","title":"removeRdd(rddId: Int): Int"},{"location":"storage/BlockManager/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#removebroadcastbroadcastid-long-tellmaster-boolean-int","text":"removeBroadcast removes all the blocks of the input broadcastId broadcast. Internally, it starts by printing out the following DEBUG message to the logs: Removing broadcast [broadcastId] It then requests all the storage:BlockId.md#BroadcastBlockId[BroadcastBlockId] objects that belong to the broadcastId broadcast from storage:BlockInfoManager.md[] and < >. The number of blocks removed is the final result. NOTE: It is used by storage:BlockManagerSlaveEndpoint.md#RemoveBroadcast[ BlockManagerSlaveEndpoint while handling RemoveBroadcast messages]. == [[shuffleServerId]] BlockManagerId of Shuffle Server BlockManager uses storage:BlockManagerId.md[] for the location (address) of the server that serves shuffle files of this executor. The BlockManagerId is either the BlockManagerId of the external shuffle service (when < >) or the < >. The BlockManagerId of the Shuffle Server is used for the location of a scheduler:MapStatus.md[shuffle map output] when: BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write partition records to a shuffle file] UnsafeShuffleWriter is requested to shuffle:UnsafeShuffleWriter.md#closeAndWriteOutput[close and write output] == [[getStatus]] getStatus Method","title":"removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int"},{"location":"storage/BlockManager/#sourcescala_3","text":"getStatus( blockId: BlockId): Option[BlockStatus] getStatus...FIXME getStatus is used when BlockManagerSlaveEndpoint is requested to handle storage:BlockManagerSlaveEndpoint.md#GetBlockStatus[GetBlockStatus] message. == [[initialize]] Initializing BlockManager","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_17","text":"initialize( appId: String): Unit initialize initializes a BlockManager on the driver and executors (see ROOT:SparkContext.md#creating-instance[Creating SparkContext Instance] and executor:Executor.md#creating-instance[Creating Executor Instance], respectively). NOTE: The method must be called before a BlockManager can be considered fully operable. initialize does the following in order: Initializes storage:BlockTransferService.md#init[BlockTransferService] Initializes the internal shuffle client, be it storage:ExternalShuffleClient.md[ExternalShuffleClient] or storage:BlockTransferService.md[BlockTransferService]. BlockManagerMaster.md#registerBlockManager[Registers itself with the driver's BlockManagerMaster ] (using the id , maxMemory and its slaveEndpoint ). + The BlockManagerMaster reference is passed in when the < > on the driver and executors. Sets < > to an instance of storage:BlockManagerId.md[] given an executor id, host name and port for storage:BlockTransferService.md[BlockTransferService]. It creates the address of the server that serves this executor's shuffle files (using < >) CAUTION: FIXME Review the initialize procedure again CAUTION: FIXME Describe shuffleServerId . Where is it used? If the < >, initialize prints out the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManager/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#external-shuffle-service-port-externalshuffleserviceport","text":"It BlockManagerMaster.md#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the storage:BlockManagerId.md[], the maximum memory (as maxMemory ), and the storage:BlockManagerSlaveEndpoint.md[]. Ultimately, if the initialization happens on an executor and the < >, it < >. initialize is used when SparkContext is created and when an executor:Executor.md#creating-instance[ Executor is created] (for executor:CoarseGrainedExecutorBackend.md#RegisteredExecutor[CoarseGrainedExecutorBackend] and spark-on-mesos:spark-executor-backends-MesosExecutorBackend.md[MesosExecutorBackend]). == [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server","title":"external shuffle service port = [externalShuffleServicePort]"},{"location":"storage/BlockManager/#source-scala_18","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#registerwithexternalshuffleserver-unit","text":"registerWithExternalShuffleServer is an internal helper method to register the BlockManager for an executor with an deploy:ExternalShuffleService.md[external shuffle server]. NOTE: It is executed when a < >. When executed, you should see the following INFO message in the logs: Registering executor with local external shuffle service. It uses < > to storage:ExternalShuffleClient.md#registerWithShuffleServer[register the block manager] using < > (i.e. the host, the port and the executorId) and a ExecutorShuffleInfo . NOTE: The ExecutorShuffleInfo uses localDirs and subDirsPerLocalDir from DiskBlockManager.md[DiskBlockManager] and the class name of the constructor shuffle:ShuffleManager.md[ShuffleManager]. It tries to register at most 3 times with 5-second sleeps in-between. NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured. Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs: Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds... registerWithExternalShuffleServer is used when BlockManager is requested to < > (when executed on an executor with < >). == [[reregister]] Re-registering BlockManager with Driver and Reporting Blocks","title":"registerWithExternalShuffleServer(): Unit"},{"location":"storage/BlockManager/#source-scala_19","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#reregister-unit","text":"When executed, reregister prints the following INFO message to the logs: BlockManager [blockManagerId] re-registering with master reregister then BlockManagerMaster.md#registerBlockManager[registers itself to the driver's BlockManagerMaster ] (just as it was when < >). It passes the storage:BlockManagerId.md[], the maximum memory (as maxMemory ), and the storage:BlockManagerSlaveEndpoint.md[]. reregister will then report all the local blocks to the BlockManagerMaster.md[BlockManagerMaster]. You should see the following INFO message in the logs: Reporting [blockInfoManager.size] blocks to the master. For each block metadata (in storage:BlockInfoManager.md[]) it < > and < >. If there is an issue communicating to the BlockManagerMaster.md[BlockManagerMaster], you should see the following ERROR message in the logs: Failed to report [blockId] to master; giving up. After the ERROR message, reregister stops reporting. reregister is used when a executor:Executor.md#heartbeats-and-active-task-metrics[ Executor was informed to re-register while sending heartbeats]. == [[getCurrentBlockStatus]] Calculate Current Block Status","title":"reregister(): Unit"},{"location":"storage/BlockManager/#source-scala_20","text":"getCurrentBlockStatus( blockId: BlockId, info: BlockInfo): BlockStatus getCurrentBlockStatus gives the current BlockStatus of the BlockId block (with the block's current storage:StorageLevel.md[StorageLevel], memory and disk sizes). It uses storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] for size and other information. NOTE: Most of the information to build BlockStatus is already in BlockInfo except that it may not necessarily reflect the current state per storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore]. Internally, it uses the input storage:BlockInfo.md[] to know about the block's storage level. If the storage level is not set (i.e. null ), the returned BlockStatus assumes the storage:StorageLevel.md[default NONE storage level] and the memory and disk sizes being 0 . If however the storage level is set, getCurrentBlockStatus uses storage:MemoryStore.md[MemoryStore] and DiskStore.md[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their getSize or assume 0 ). NOTE: It is acceptable that the BlockInfo says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status. getCurrentBlockStatus is used when < >, < > or < > or < >. == [[reportAllBlocks]] reportAllBlocks Internal Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_21","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#reportallblocks-unit","text":"reportAllBlocks...FIXME reportAllBlocks is used when BlockManager is requested to < >. == [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver","title":"reportAllBlocks(): Unit"},{"location":"storage/BlockManager/#source-scala_22","text":"reportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Unit reportBlockStatus is an internal method for < > and if told to re-register it prints out the following INFO message to the logs: Got told to re-register updating block [blockId] It does asynchronous reregistration (using asyncReregister ). In either case, it prints out the following DEBUG message to the logs: Told master about block [blockId] reportBlockStatus is used when BlockManager is requested to < >, < >, < >, < > and < >. == [[tryToReportBlockStatus]] Reporting Block Status Update to Driver","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_23","text":"def tryToReportBlockStatus( blockId: BlockId, info: BlockInfo, status: BlockStatus, droppedMemorySize: Long = 0L): Boolean tryToReportBlockStatus BlockManagerMaster.md#updateBlockInfo[reports block status update] to < > and returns its response. tryToReportBlockStatus is used when BlockManager is requested to < > or < >. == [[execution-context]] Execution Context block-manager-future is the execution context for...FIXME == [[ByteBuffer]] ByteBuffer The underlying abstraction for blocks in Spark is a ByteBuffer that limits the size of a block to 2GB ( Integer.MAX_VALUE - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for long ), ser-deser via byte array-backed output streams. == [[BlockResult]] BlockResult BlockResult is a description of a fetched block with the readMethod and bytes . == [[registerTask]] Registering Task","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_24","text":"registerTask( taskAttemptId: Long): Unit registerTask requests the < > to storage:BlockInfoManager.md#registerTask[register a given task]. registerTask is used when Task is requested to scheduler:Task.md#run[run] (at the start of a task). == [[getDiskWriter]] Creating DiskBlockObjectWriter","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_25","text":"getDiskWriter( blockId: BlockId, file: File, serializerInstance: SerializerInstance, bufferSize: Int, writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter getDiskWriter creates a storage:DiskBlockObjectWriter.md[DiskBlockObjectWriter] (with ROOT:configuration-properties.md#spark.shuffle.sync[spark.shuffle.sync] configuration property for syncWrites argument). getDiskWriter uses the < > of the BlockManager. getDiskWriter is used when: BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records (of a partition)] ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile] ExternalAppendOnlyMap is requested to shuffle:ExternalAppendOnlyMap.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to shuffle:ExternalSorter.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] and shuffle:ExternalSorter.md#writePartitionedFile[writePartitionedFile] memory:UnsafeSorterSpillWriter.md[UnsafeSorterSpillWriter] is created == [[addUpdatedBlockStatusToTaskMetrics]] Recording Updated BlockStatus In Current Task's TaskMetrics","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_26","text":"addUpdatedBlockStatusToTaskMetrics( blockId: BlockId, status: BlockStatus): Unit addUpdatedBlockStatusToTaskMetrics spark-TaskContext.md#get[takes an active TaskContext ] (if available) and executor:TaskMetrics.md#incUpdatedBlockStatuses[records updated BlockStatus for Block ] (in the spark-TaskContext.md#taskMetrics[task's TaskMetrics ]). addUpdatedBlockStatusToTaskMetrics is used when BlockManager < > (for a block that was successfully stored), < >, < >, < > (possibly spilling it to disk) and < >. == [[shuffleMetricsSource]] Requesting Shuffle-Related Spark Metrics Source","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_27","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#shufflemetricssource-source","text":"shuffleMetricsSource requests the < > for the storage:ShuffleClient.md#shuffleMetrics[shuffle metrics] and creates a storage:ShuffleMetricsSource.md[] with the storage:ShuffleMetricsSource.md#sourceName[source name] based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property: ExternalShuffle when ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on ( true ) NettyBlockTransfer when ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off ( false ) shuffleMetricsSource is used when Executor is executor:Executor.md#creating-instance[created] (for non-local / cluster modes). == [[replicate]] Replicating Block To Peers","title":"shuffleMetricsSource: Source"},{"location":"storage/BlockManager/#source-scala_28","text":"replicate( blockId: BlockId, data: BlockData, level: StorageLevel, classTag: ClassTag[_], existingReplicas: Set[BlockManagerId] = Set.empty): Unit replicate...FIXME replicate is used when BlockManager is requested to < >, < > and < >. == [[replicateBlock]] replicateBlock Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_29","text":"replicateBlock( blockId: BlockId, existingReplicas: Set[BlockManagerId], maxReplicas: Int): Unit replicateBlock...FIXME replicateBlock is used when BlockManagerSlaveEndpoint is requested to storage:BlockManagerSlaveEndpoint.md#ReplicateBlock[handle a ReplicateBlock message]. == [[putIterator]] putIterator Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_30","text":"putIterator T: ClassTag : Boolean putIterator ...FIXME","title":"[source, scala]"},{"location":"storage/BlockManager/#note_3","text":"putIterator is used when: BlockManager is requested to < >","title":"[NOTE]"},{"location":"storage/BlockManager/#spark-streamings-blockmanagerbasedblockhandler-is-requested-to-storeblock","text":"== [[putSingle]] putSingle Method","title":"* Spark Streaming's BlockManagerBasedBlockHandler is requested to storeBlock"},{"location":"storage/BlockManager/#source-scala_31","text":"putSingle T: ClassTag : Boolean putSingle...FIXME putSingle is used when TorrentBroadcast is requested to core:TorrentBroadcast.md#writeBlocks[write the blocks] and core:TorrentBroadcast.md#readBroadcastBlock[readBroadcastBlock]. == [[getRemoteBytes]] Fetching Block From Remote Nodes","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_32","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getremotebytesblockid-blockid-optionchunkedbytebuffer","text":"getRemoteBytes ...FIXME","title":"getRemoteBytes(blockId: BlockId): Option[ChunkedByteBuffer]"},{"location":"storage/BlockManager/#note_4","text":"getRemoteBytes is used when: BlockManager is requested to < > TorrentBroadcast is requested to core:TorrentBroadcast.md#readBlocks[readBlocks]","title":"[NOTE]"},{"location":"storage/BlockManager/#taskresultgetter-is-requested-to-schedulertaskresultgettermdenqueuesuccessfultaskenqueuing-a-successful-indirecttaskresult","text":"== [[getRemoteValues]] getRemoteValues Internal Method","title":"* TaskResultGetter is requested to scheduler:TaskResultGetter.md#enqueueSuccessfulTask[enqueuing a successful IndirectTaskResult]"},{"location":"storage/BlockManager/#source-scala_33","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getremotevaluest-classtag-optionblockresult_1","text":"getRemoteValues ...FIXME NOTE: getRemoteValues is used exclusively when BlockManager is requested to < >. == [[getSingle]] getSingle Method","title":"getRemoteValuesT: ClassTag: Option[BlockResult]"},{"location":"storage/BlockManager/#source-scala_34","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getsinglet-classtag-optiont","text":"getSingle ...FIXME NOTE: getSingle is used exclusively in Spark tests. == [[getOrElseUpdate]] Getting Block From Block Managers Or Computing and Storing It Otherwise","title":"getSingleT: ClassTag: Option[T]"},{"location":"storage/BlockManager/#source-scala_35","text":"getOrElseUpdate T : Either[BlockResult, Iterator[T]]","title":"[source, scala]"},{"location":"storage/BlockManager/#note_5","text":"I think it is fair to say that getOrElseUpdate is like ++ https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html#getOrElseUpdate(key:K,op:=%3EV):V++[getOrElseUpdate ] of https://www.scala-lang.org/api/current/scala/collection/mutable/Map.html[scala.collection.mutable.Map ] in Scala.","title":"[NOTE]"},{"location":"storage/BlockManager/#source-scala_36","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#getorelseupdatekey-k-op-v-v","text":"Quoting the official scaladoc: If given key K is already in this map, getOrElseUpdate returns the associated value V . Otherwise, getOrElseUpdate computes a value V from given expression op , stores with the key K in the map and returns that value.","title":"getOrElseUpdate(key: K, op: \u21d2 V): V"},{"location":"storage/BlockManager/#since-blockmanager-is-a-key-value-store-of-blocks-of-data-identified-by-a-block-id-that-works-just-fine","text":"getOrElseUpdate first attempts to < > by the BlockId (from the local block manager first and, if unavailable, requesting remote peers).","title":"Since BlockManager is a key-value store of blocks of data identified by a block ID that works just fine."},{"location":"storage/BlockManager/#tip","text":"Enable INFO logging level for org.apache.spark.storage.BlockManager logger to see what happens when BlockManager tries to < >.","title":"[TIP]"},{"location":"storage/BlockManager/#see-in-this-document","text":"getOrElseUpdate gives the BlockResult of the block if found. If however the block was not found (in any block manager in a Spark cluster), getOrElseUpdate < > (for the input BlockId , the makeIterator function and the StorageLevel ). getOrElseUpdate branches off per the result. For None , getOrElseUpdate < > for the BlockId and eventually returns the BlockResult (unless terminated by a SparkException due to some internal error). For Some(iter) , getOrElseUpdate returns an iterator of T values. NOTE: getOrElseUpdate is used exclusively when RDD is requested to rdd:RDD.md#getOrCompute[get or compute an RDD partition] (for a RDDBlockId with a RDD ID and a partition index). == [[doPutIterator]] doPutIterator Internal Method","title":"See &lt;&gt; in this document."},{"location":"storage/BlockManager/#source-scala_37","text":"doPutIterator T : Option[PartiallyUnrolledIterator[T]] doPutIterator simply < > with the putBody function that accepts a BlockInfo and does the following: . putBody branches off per whether the StorageLevel indicates to use a storage:StorageLevel.md#useMemory[memory] or simply a storage:StorageLevel.md#useDisk[disk], i.e. When the input StorageLevel indicates to storage:StorageLevel.md#useMemory[use a memory] for storage in storage:StorageLevel.md#deserialized[deserialized] format, putBody requests < > to storage:MemoryStore.md#putIteratorAsValues[putIteratorAsValues] (for the BlockId and with the iterator factory function). + If the < > returned a correct value, the internal size is set to the value. + If however the < > failed to give a correct value, FIXME When the input StorageLevel indicates to storage:StorageLevel.md#useMemory[use memory] for storage in storage:StorageLevel.md#deserialized[serialized] format, putBody ...FIXME When the input StorageLevel does not indicate to use memory for storage but storage:StorageLevel.md#useDisk[disk] instead, putBody ...FIXME . putBody requests the < > . Only when the block was successfully stored in either the memory or disk store: putBody < > to the < > when the input tellMaster flag (default: enabled) and the tellMaster flag of the block info are both enabled. putBody < > (with the BlockId and BlockStatus ) putBody prints out the following DEBUG message to the logs: + Put block [blockId] locally took [time] ms When the input StorageLevel indicates to use storage:StorageLevel.md#replication[replication], putBody < > followed by < > (with the input BlockId and the StorageLevel as well as the BlockData to replicate) With a successful replication, putBody prints out the following DEBUG message to the logs: + Put block [blockId] remotely took [time] ms . In the end, putBody may or may not give a PartiallyUnrolledIterator if...FIXME NOTE: doPutIterator is used when BlockManager is requested to < > and < >. == [[dropFromMemory]] Dropping Block from Memory","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_4","text":"dropFromMemory( blockId: BlockId, data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel dropFromMemory prints out the following INFO message to the logs:","title":"[source,scala]"},{"location":"storage/BlockManager/#sourceplaintext_5","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#dropping-block-blockid-from-memory","text":"dropFromMemory then asserts that the given block is storage:BlockInfoManager.md#assertBlockIsLockedForWriting[locked for writing]. If the block's storage:StorageLevel.md[StorageLevel] uses disks and the internal DiskStore.md[DiskStore] object ( diskStore ) does not contain the block, it is saved then. You should see the following INFO message in the logs: Writing block [blockId] to disk CAUTION: FIXME Describe the case with saving a block to disk. The block's memory size is fetched and recorded (using MemoryStore.getSize ). The block is storage:MemoryStore.md#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs: Block [blockId] could not be dropped from memory as it does not exist It then < > and < >. It only happens when info.tellMaster . CAUTION: FIXME When would info.tellMaster be true ? A block is considered updated when it was written to disk or removed from memory or both. If either happened, the executor:TaskMetrics.md#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change]. In the end, dropFromMemory returns the current storage level of the block. dropFromMemory is part of the storage:BlockEvictionHandler.md#dropFromMemory[BlockEvictionHandler] abstraction. == [[handleLocalReadFailure]] handleLocalReadFailure Internal Method","title":"Dropping block [blockId] from memory"},{"location":"storage/BlockManager/#source-scala_38","text":"","title":"[source, scala]"},{"location":"storage/BlockManager/#handlelocalreadfailureblockid-blockid-nothing","text":"handleLocalReadFailure ...FIXME NOTE: handleLocalReadFailure is used when...FIXME == [[releaseLockAndDispose]] releaseLockAndDispose Method","title":"handleLocalReadFailure(blockId: BlockId): Nothing"},{"location":"storage/BlockManager/#source-scala_39","text":"releaseLockAndDispose( blockId: BlockId, data: BlockData, taskAttemptId: Option[Long] = None): Unit releaseLockAndDispose...FIXME releaseLockAndDispose is used when...FIXME == [[releaseLock]] releaseLock Method","title":"[source, scala]"},{"location":"storage/BlockManager/#source-scala_40","text":"releaseLock( blockId: BlockId, taskAttemptId: Option[Long] = None): Unit releaseLock requests the < > to storage:BlockInfoManager.md#unlock[unlock the given block]. releaseLock is part of the storage:BlockDataManager.md#releaseLock[BlockDataManager] abstraction. == [[putBlockDataAsStream]] putBlockDataAsStream Method","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_5","text":"putBlockDataAsStream( blockId: BlockId, level: StorageLevel, classTag: ClassTag[_]): StreamCallbackWithID putBlockDataAsStream...FIXME putBlockDataAsStream is part of the storage:BlockDataManager.md#putBlockDataAsStream[BlockDataManager] abstraction. == [[downgradeLock]] downgradeLock Method","title":"[source,scala]"},{"location":"storage/BlockManager/#source-scala_41","text":"downgradeLock( blockId: BlockId): Unit downgradeLock requests the < > to storage:BlockInfoManager.md#downgradeLock[downgradeLock] for the given storage:BlockId.md[block]. downgradeLock seems not to be used. == [[blockIdsToLocations]] blockIdsToLocations Utility","title":"[source, scala]"},{"location":"storage/BlockManager/#sourcescala_6","text":"blockIdsToLocations( blockIds: Array[BlockId], env: SparkEnv, blockManagerMaster: BlockManagerMaster = null): Map[BlockId, Seq[String]] blockIdsToLocations...FIXME blockIdsToLocations is used in the now defunct Spark Streaming (when BlockRDD is requested for _locations). === [[getLocationBlockIds]] getLocationBlockIds Internal Method","title":"[source,scala]"},{"location":"storage/BlockManager/#sourcescala_7","text":"getLocationBlockIds( blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] getLocationBlockIds...FIXME getLocationBlockIds is used when BlockManager utility is requested to < > (for the now defunct Spark Streaming). == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source,scala]"},{"location":"storage/BlockManager/#sourceplaintext_6","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManager/#log4jloggerorgapachesparkstorageblockmanagerall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[maxMemory]] Maximum Memory Total maximum value that BlockManager can ever possibly use (that depends on < > and may vary over time). Total available memory:MemoryManager.md#maxOnHeapStorageMemory[on-heap] and memory:MemoryManager.md#maxOffHeapStorageMemory[off-heap] memory for storage (in bytes) === [[maxOffHeapMemory]] Maximum Off-Heap Memory === [[maxOnHeapMemory]] Maximum On-Heap Memory","title":"log4j.logger.org.apache.spark.storage.BlockManager=ALL"},{"location":"storage/BlockManagerId/","text":"= BlockManagerId BlockManagerId is a unique identifier of a storage:BlockManager.md[].","title":"BlockManagerId"},{"location":"storage/BlockManagerInfo/","text":"= BlockManagerInfo BlockManagerInfo is...FIXME","title":"BlockManagerInfo"},{"location":"storage/BlockManagerMaster/","text":"= BlockManagerMaster BlockManagerMaster core:SparkEnv.md#BlockManagerMaster[runs on the driver]. BlockManagerMaster uses storage:BlockManagerMasterEndpoint.md[] registered under BlockManagerMaster RPC endpoint name on the driver (with the endpoint references on executors) to allow executors for sending block status updates to it and hence keep track of block statuses. == [[creating-instance]] Creating Instance BlockManagerMaster takes the following to be created: [[driverEndpoint]] rpc:RpcEndpointRef.md[] [[conf]] ROOT:SparkConf.md[] [[isDriver]] Flag whether BlockManagerMaster is created for the driver or executors BlockManagerMaster is created when SparkEnv utility is used to core:SparkEnv.md#create[create a SparkEnv (for the driver and executors)] (to create a storage:BlockManager.md[]). == [[removeExecutorAsync]] removeExecutorAsync Method CAUTION: FIXME == [[contains]] contains Method CAUTION: FIXME == [[removeExecutor]] Removing Executor -- removeExecutor Method [source, scala] \u00b6 removeExecutor(execId: String): Unit \u00b6 removeExecutor posts storage:BlockManagerMasterEndpoint.md#RemoveExecutor[ RemoveExecutor to BlockManagerMaster RPC endpoint] and waits for a response. If false in response comes in, a SparkException is thrown with the following message: BlockManagerMasterEndpoint returned false, expected true. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: Removed executor [execId] NOTE: removeExecutor is executed when scheduler:DAGSchedulerEventProcessLoop.md#handleExecutorLost[ DAGScheduler processes ExecutorLost event]. == [[removeBlock]] Removing Block -- removeBlock Method [source, scala] \u00b6 removeBlock(blockId: BlockId): Unit \u00b6 removeBlock simply posts a RemoveBlock blocking message to storage:BlockManagerMasterEndpoint.md[] (and ultimately disregards the reponse). == [[removeRdd]] Removing RDD Blocks -- removeRdd Method [source, scala] \u00b6 removeRdd(rddId: Int, blocking: Boolean) \u00b6 removeRdd removes all the blocks of rddId RDD, possibly in blocking fashion. Internally, removeRdd posts a RemoveRdd(rddId) message to storage:BlockManagerMasterEndpoint.md[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove RDD [rddId] - [exception] If it is a blocking operation, it waits for a result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. == [[removeShuffle]] Removing Shuffle Blocks -- removeShuffle Method [source, scala] \u00b6 removeShuffle(shuffleId: Int, blocking: Boolean) \u00b6 removeShuffle removes all the blocks of shuffleId shuffle, possibly in a blocking fashion. It posts a RemoveShuffle(shuffleId) message to storage:BlockManagerMasterEndpoint.md[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove shuffle [shuffleId] - [exception] If it is a blocking operation, it waits for the result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. NOTE: removeShuffle is used exclusively when core:ContextCleaner.md#doCleanupShuffle[ ContextCleaner removes a shuffle]. == [[removeBroadcast]] Removing Broadcast Blocks -- removeBroadcast Method [source, scala] \u00b6 removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean) \u00b6 removeBroadcast removes all the blocks of broadcastId broadcast, possibly in a blocking fashion. It posts a RemoveBroadcast(broadcastId, removeFromMaster) message to storage:BlockManagerMasterEndpoint.md[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove broadcast [broadcastId] with removeFromMaster = [removeFromMaster] - [exception] If it is a blocking operation, it waits for the result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. == [[stop]] Stopping BlockManagerMaster -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop sends a StopBlockManagerMaster message to storage:BlockManagerMasterEndpoint.md[] and waits for a response. NOTE: It is only executed for the driver. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: BlockManagerMaster stopped Otherwise, a SparkException is thrown. BlockManagerMasterEndpoint returned false, expected true. == [[registerBlockManager]] Registering BlockManager with Driver [source, scala] \u00b6 registerBlockManager( blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId registerBlockManager prints the following INFO message to the logs: [source,plaintext] \u00b6 Registering BlockManager [blockManagerId] \u00b6 .Registering BlockManager with the Driver image::BlockManagerMaster-RegisterBlockManager.png[align=\"center\"] registerBlockManager then notifies the driver that the storage:BlockManagerId.md[] is registering itself. registerBlockManager posts a storage:BlockManagerMasterEndpoint.md#RegisterBlockManager[blocking RegisterBlockManager message to BlockManagerMaster RPC endpoint]. NOTE: The input maxMemSize is the storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. registerBlockManager waits until a confirmation comes (as storage:BlockManagerId.md[]). In the end, registerBlockManager prints the following INFO message to the logs and returns the storage:BlockManagerId.md[] received. [source,plaintext] \u00b6 Registered BlockManager [updatedId] \u00b6 registerBlockManager is used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] and storage:BlockManager.md#reregister[re-register itself with the driver]. == [[updateBlockInfo]] Relaying Block Status Update From BlockManager to Driver [source, scala] \u00b6 updateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long): Boolean updateBlockInfo sends a blocking storage:BlockManagerMasterEndpoint.md#UpdateBlockInfo[UpdateBlockInfo] event to < > (and waits for a response). updateBlockInfo prints out the following DEBUG message to the logs: DEBUG BlockManagerMaster: Updated info of block [blockId] updateBlockInfo returns the response from the < >. NOTE: updateBlockInfo is used exclusively when BlockManager is requested to storage:BlockManager.md#tryToReportBlockStatus[report a block status update to the driver]. == [[getLocations-block]] Get Block Locations of One Block -- getLocations Method [source, scala] \u00b6 getLocations(blockId: BlockId): Seq[BlockManagerId] \u00b6 getLocations storage:BlockManagerMasterEndpoint.md#GetLocations[posts a blocking GetLocations message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when < > and storage:BlockManager.md#getLocations[ BlockManager getLocations]. == [[getLocations-block-array]] Get Block Locations for Multiple Blocks -- getLocations Method [source, scala] \u00b6 getLocations(blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] \u00b6 getLocations storage:BlockManagerMasterEndpoint.md#GetLocationsMultipleBlockIds[posts a blocking GetLocationsMultipleBlockIds message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when scheduler:DAGScheduler.md#getCacheLocs[ DAGScheduler finds BlockManagers (and so executors) for cached RDD partitions] and when BlockManager storage:BlockManager.md#getLocationBlockIds[getLocationBlockIds] and storage:BlockManager.md#blockIdsToHosts[blockIdsToHosts]. == [[getPeers]] Finding Peers of BlockManager -- getPeers Internal Method [source, scala] \u00b6 getPeers(blockManagerId: BlockManagerId): Seq[BlockManagerId] \u00b6 getPeers storage:BlockManagerMasterEndpoint.md#GetPeers[posts a blocking GetPeers message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. NOTE: getPeers is used when storage:BlockManager.md#getPeers[ BlockManager finds the peers of a BlockManager ], Structured Streaming's KafkaSource and Spark Streaming's KafkaRDD . == [[getExecutorEndpointRef]] getExecutorEndpointRef Method [source, scala] \u00b6 getExecutorEndpointRef(executorId: String): Option[RpcEndpointRef] \u00b6 getExecutorEndpointRef posts GetExecutorEndpointRef(executorId) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the return value. == [[getMemoryStatus]] getMemoryStatus Method [source, scala] \u00b6 getMemoryStatus: Map[BlockManagerId, (Long, Long)] \u00b6 getMemoryStatus posts a GetMemoryStatus message storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the return value. == [[getStorageStatus]] Storage Status (Posting GetStorageStatus to BlockManagerMaster RPC endpoint) -- getStorageStatus Method [source, scala] \u00b6 getStorageStatus: Array[StorageStatus] \u00b6 getStorageStatus posts a GetStorageStatus message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the return value. == [[getBlockStatus]] getBlockStatus Method [source, scala] \u00b6 getBlockStatus( blockId: BlockId, askSlaves: Boolean = true): Map[BlockManagerId, BlockStatus] getBlockStatus posts a GetBlockStatus(blockId, askSlaves) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response (of type Map[BlockManagerId, Future[Option[BlockStatus]]] ). It then builds a sequence of future results that are BlockStatus statuses and waits for a result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. No result leads to a SparkException with the following message: BlockManager returned null for BlockStatus query: [blockId] == [[getMatchingBlockIds]] getMatchingBlockIds Method [source, scala] \u00b6 getMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean): Seq[BlockId] getMatchingBlockIds posts a GetMatchingBlockIds(filter, askSlaves) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. == [[hasCachedBlocks]] hasCachedBlocks Method [source, scala] \u00b6 hasCachedBlocks(executorId: String): Boolean \u00b6 hasCachedBlocks posts a HasCachedBlocks(executorId) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the result. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMaster logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerMaster=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"BlockManagerMaster"},{"location":"storage/BlockManagerMaster/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removeexecutorexecid-string-unit","text":"removeExecutor posts storage:BlockManagerMasterEndpoint.md#RemoveExecutor[ RemoveExecutor to BlockManagerMaster RPC endpoint] and waits for a response. If false in response comes in, a SparkException is thrown with the following message: BlockManagerMasterEndpoint returned false, expected true. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: Removed executor [execId] NOTE: removeExecutor is executed when scheduler:DAGSchedulerEventProcessLoop.md#handleExecutorLost[ DAGScheduler processes ExecutorLost event]. == [[removeBlock]] Removing Block -- removeBlock Method","title":"removeExecutor(execId: String): Unit"},{"location":"storage/BlockManagerMaster/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removeblockblockid-blockid-unit","text":"removeBlock simply posts a RemoveBlock blocking message to storage:BlockManagerMasterEndpoint.md[] (and ultimately disregards the reponse). == [[removeRdd]] Removing RDD Blocks -- removeRdd Method","title":"removeBlock(blockId: BlockId): Unit"},{"location":"storage/BlockManagerMaster/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removerddrddid-int-blocking-boolean","text":"removeRdd removes all the blocks of rddId RDD, possibly in blocking fashion. Internally, removeRdd posts a RemoveRdd(rddId) message to storage:BlockManagerMasterEndpoint.md[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove RDD [rddId] - [exception] If it is a blocking operation, it waits for a result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. == [[removeShuffle]] Removing Shuffle Blocks -- removeShuffle Method","title":"removeRdd(rddId: Int, blocking: Boolean)"},{"location":"storage/BlockManagerMaster/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removeshuffleshuffleid-int-blocking-boolean","text":"removeShuffle removes all the blocks of shuffleId shuffle, possibly in a blocking fashion. It posts a RemoveShuffle(shuffleId) message to storage:BlockManagerMasterEndpoint.md[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove shuffle [shuffleId] - [exception] If it is a blocking operation, it waits for the result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. NOTE: removeShuffle is used exclusively when core:ContextCleaner.md#doCleanupShuffle[ ContextCleaner removes a shuffle]. == [[removeBroadcast]] Removing Broadcast Blocks -- removeBroadcast Method","title":"removeShuffle(shuffleId: Int, blocking: Boolean)"},{"location":"storage/BlockManagerMaster/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#removebroadcastbroadcastid-long-removefrommaster-boolean-blocking-boolean","text":"removeBroadcast removes all the blocks of broadcastId broadcast, possibly in a blocking fashion. It posts a RemoveBroadcast(broadcastId, removeFromMaster) message to storage:BlockManagerMasterEndpoint.md[] on a separate thread. If there is an issue, you should see the following WARN message in the logs and the entire exception: WARN Failed to remove broadcast [broadcastId] with removeFromMaster = [removeFromMaster] - [exception] If it is a blocking operation, it waits for the result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. == [[stop]] Stopping BlockManagerMaster -- stop Method","title":"removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean)"},{"location":"storage/BlockManagerMaster/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#stop-unit","text":"stop sends a StopBlockManagerMaster message to storage:BlockManagerMasterEndpoint.md[] and waits for a response. NOTE: It is only executed for the driver. If all goes fine, you should see the following INFO message in the logs: INFO BlockManagerMaster: BlockManagerMaster stopped Otherwise, a SparkException is thrown. BlockManagerMasterEndpoint returned false, expected true. == [[registerBlockManager]] Registering BlockManager with Driver","title":"stop(): Unit"},{"location":"storage/BlockManagerMaster/#source-scala_6","text":"registerBlockManager( blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId registerBlockManager prints the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMaster/#registering-blockmanager-blockmanagerid","text":".Registering BlockManager with the Driver image::BlockManagerMaster-RegisterBlockManager.png[align=\"center\"] registerBlockManager then notifies the driver that the storage:BlockManagerId.md[] is registering itself. registerBlockManager posts a storage:BlockManagerMasterEndpoint.md#RegisterBlockManager[blocking RegisterBlockManager message to BlockManagerMaster RPC endpoint]. NOTE: The input maxMemSize is the storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. registerBlockManager waits until a confirmation comes (as storage:BlockManagerId.md[]). In the end, registerBlockManager prints the following INFO message to the logs and returns the storage:BlockManagerId.md[] received.","title":"Registering BlockManager [blockManagerId]"},{"location":"storage/BlockManagerMaster/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMaster/#registered-blockmanager-updatedid","text":"registerBlockManager is used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] and storage:BlockManager.md#reregister[re-register itself with the driver]. == [[updateBlockInfo]] Relaying Block Status Update From BlockManager to Driver","title":"Registered BlockManager [updatedId]"},{"location":"storage/BlockManagerMaster/#source-scala_7","text":"updateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long): Boolean updateBlockInfo sends a blocking storage:BlockManagerMasterEndpoint.md#UpdateBlockInfo[UpdateBlockInfo] event to < > (and waits for a response). updateBlockInfo prints out the following DEBUG message to the logs: DEBUG BlockManagerMaster: Updated info of block [blockId] updateBlockInfo returns the response from the < >. NOTE: updateBlockInfo is used exclusively when BlockManager is requested to storage:BlockManager.md#tryToReportBlockStatus[report a block status update to the driver]. == [[getLocations-block]] Get Block Locations of One Block -- getLocations Method","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getlocationsblockid-blockid-seqblockmanagerid","text":"getLocations storage:BlockManagerMasterEndpoint.md#GetLocations[posts a blocking GetLocations message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when < > and storage:BlockManager.md#getLocations[ BlockManager getLocations]. == [[getLocations-block-array]] Get Block Locations for Multiple Blocks -- getLocations Method","title":"getLocations(blockId: BlockId): Seq[BlockManagerId]"},{"location":"storage/BlockManagerMaster/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getlocationsblockids-arrayblockid-indexedseqseqblockmanagerid","text":"getLocations storage:BlockManagerMasterEndpoint.md#GetLocationsMultipleBlockIds[posts a blocking GetLocationsMultipleBlockIds message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: getLocations is used when scheduler:DAGScheduler.md#getCacheLocs[ DAGScheduler finds BlockManagers (and so executors) for cached RDD partitions] and when BlockManager storage:BlockManager.md#getLocationBlockIds[getLocationBlockIds] and storage:BlockManager.md#blockIdsToHosts[blockIdsToHosts]. == [[getPeers]] Finding Peers of BlockManager -- getPeers Internal Method","title":"getLocations(blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]]"},{"location":"storage/BlockManagerMaster/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getpeersblockmanagerid-blockmanagerid-seqblockmanagerid","text":"getPeers storage:BlockManagerMasterEndpoint.md#GetPeers[posts a blocking GetPeers message to BlockManagerMaster RPC endpoint] and returns the response. NOTE: Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. NOTE: getPeers is used when storage:BlockManager.md#getPeers[ BlockManager finds the peers of a BlockManager ], Structured Streaming's KafkaSource and Spark Streaming's KafkaRDD . == [[getExecutorEndpointRef]] getExecutorEndpointRef Method","title":"getPeers(blockManagerId: BlockManagerId): Seq[BlockManagerId]"},{"location":"storage/BlockManagerMaster/#source-scala_11","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getexecutorendpointrefexecutorid-string-optionrpcendpointref","text":"getExecutorEndpointRef posts GetExecutorEndpointRef(executorId) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the return value. == [[getMemoryStatus]] getMemoryStatus Method","title":"getExecutorEndpointRef(executorId: String): Option[RpcEndpointRef]"},{"location":"storage/BlockManagerMaster/#source-scala_12","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getmemorystatus-mapblockmanagerid-long-long","text":"getMemoryStatus posts a GetMemoryStatus message storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the return value. == [[getStorageStatus]] Storage Status (Posting GetStorageStatus to BlockManagerMaster RPC endpoint) -- getStorageStatus Method","title":"getMemoryStatus: Map[BlockManagerId, (Long, Long)]"},{"location":"storage/BlockManagerMaster/#source-scala_13","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#getstoragestatus-arraystoragestatus","text":"getStorageStatus posts a GetStorageStatus message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the return value. == [[getBlockStatus]] getBlockStatus Method","title":"getStorageStatus: Array[StorageStatus]"},{"location":"storage/BlockManagerMaster/#source-scala_14","text":"getBlockStatus( blockId: BlockId, askSlaves: Boolean = true): Map[BlockManagerId, BlockStatus] getBlockStatus posts a GetBlockStatus(blockId, askSlaves) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response (of type Map[BlockManagerId, Future[Option[BlockStatus]]] ). It then builds a sequence of future results that are BlockStatus statuses and waits for a result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. No result leads to a SparkException with the following message: BlockManager returned null for BlockStatus query: [blockId] == [[getMatchingBlockIds]] getMatchingBlockIds Method","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#source-scala_15","text":"getMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean): Seq[BlockId] getMatchingBlockIds posts a GetMatchingBlockIds(filter, askSlaves) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the result for rpc:index.md#spark.rpc.askTimeout[spark.rpc.askTimeout], rpc:index.md#spark.network.timeout[spark.network.timeout] or 120 secs. == [[hasCachedBlocks]] hasCachedBlocks Method","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMaster/#hascachedblocksexecutorid-string-boolean","text":"hasCachedBlocks posts a HasCachedBlocks(executorId) message to storage:BlockManagerMasterEndpoint.md[] and waits for a response which becomes the result. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMaster logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"hasCachedBlocks(executorId: String): Boolean"},{"location":"storage/BlockManagerMaster/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerMaster/#log4jloggerorgapachesparkstorageblockmanagermasterall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockManagerMaster=ALL"},{"location":"storage/BlockManagerMasterEndpoint/","text":"= BlockManagerMasterEndpoint -- BlockManagerMaster RPC Endpoint :navtitle: BlockManagerMasterEndpoint BlockManagerMasterEndpoint is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] for storage:BlockManagerMaster.md[BlockManagerMaster]. BlockManagerMasterEndpoint is registered under BlockManagerMaster name. BlockManagerMasterEndpoint tracks status of the storage:BlockManager.md[BlockManagers] (on the executors) in a Spark application. == [[creating-instance]] Creating Instance BlockManagerMasterEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[isLocal]] Flag whether BlockManagerMasterEndpoint works in local or cluster mode [[conf]] ROOT:SparkConf.md[] [[listenerBus]] scheduler:LiveListenerBus.md[] BlockManagerMasterEndpoint is created for the core:SparkEnv.md#create[SparkEnv] on the driver (to create a storage:BlockManagerMaster.md[] for a storage:BlockManager.md#master[BlockManager]). When created, BlockManagerMasterEndpoint prints out the following INFO message to the logs: [source,plaintext] \u00b6 BlockManagerMasterEndpoint up \u00b6 == [[messages]][[receiveAndReply]] Messages As an rpc:RpcEndpoint.md[], BlockManagerMasterEndpoint handles RPC messages. === [[BlockManagerHeartbeat]] BlockManagerHeartbeat [source, scala] \u00b6 BlockManagerHeartbeat( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocations]] GetLocations [source, scala] \u00b6 GetLocations( blockId: BlockId) When received, BlockManagerMasterEndpoint replies with the < > of blockId . Posted when BlockManagerMaster.md#getLocations-block[ BlockManagerMaster requests the block locations of a single block]. === [[GetLocationsAndStatus]] GetLocationsAndStatus [source, scala] \u00b6 GetLocationsAndStatus( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocationsMultipleBlockIds]] GetLocationsMultipleBlockIds [source, scala] \u00b6 GetLocationsMultipleBlockIds( blockIds: Array[BlockId]) When received, BlockManagerMasterEndpoint replies with the < > for the given storage:BlockId.md[]. Posted when BlockManagerMaster.md#getLocations[ BlockManagerMaster requests the block locations for multiple blocks]. === [[GetPeers]] GetPeers [source, scala] \u00b6 GetPeers( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint replies with the < > of blockManagerId . Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. Posted when BlockManagerMaster.md#getPeers[ BlockManagerMaster requests the peers of a BlockManager ]. === [[GetExecutorEndpointRef]] GetExecutorEndpointRef [source, scala] \u00b6 GetExecutorEndpointRef( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetMemoryStatus]] GetMemoryStatus [source, scala] \u00b6 GetMemoryStatus \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetStorageStatus]] GetStorageStatus [source, scala] \u00b6 GetStorageStatus \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetBlockStatus]] GetBlockStatus [source, scala] \u00b6 GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint is requested to < >. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds [source, scala] \u00b6 GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[HasCachedBlocks]] HasCachedBlocks [source, scala] \u00b6 HasCachedBlocks( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RegisterBlockManager]] RegisterBlockManager [source,scala] \u00b6 RegisterBlockManager( blockManagerId: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, sender: RpcEndpointRef) When received, BlockManagerMasterEndpoint is requested to < > (by the given storage:BlockManagerId.md[]). Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#registerBlockManager[register a BlockManager] === [[RemoveRdd]] RemoveRdd [source, scala] \u00b6 RemoveRdd( rddId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveShuffle]] RemoveShuffle [source, scala] \u00b6 RemoveShuffle( shuffleId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBroadcast]] RemoveBroadcast [source, scala] \u00b6 RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBlock]] RemoveBlock [source, scala] \u00b6 RemoveBlock( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveExecutor]] RemoveExecutor [source, scala] \u00b6 RemoveExecutor( execId: String) When received, BlockManagerMasterEndpoint < execId is removed>> and the response true sent back. Posted when BlockManagerMaster.md#removeExecutor[ BlockManagerMaster removes an executor]. === [[StopBlockManagerMaster]] StopBlockManagerMaster [source, scala] \u00b6 StopBlockManagerMaster \u00b6 When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[UpdateBlockInfo]] UpdateBlockInfo [source, scala] \u00b6 UpdateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long) When received, BlockManagerMasterEndpoint...FIXME Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#updateBlockInfo[handle a block status update (from BlockManager on an executor)]. == [[storageStatus]] storageStatus Internal Method [source,scala] \u00b6 storageStatus: Array[StorageStatus] \u00b6 storageStatus...FIXME storageStatus is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getLocationsMultipleBlockIds]] getLocationsMultipleBlockIds Internal Method [source,scala] \u00b6 getLocationsMultipleBlockIds( blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] getLocationsMultipleBlockIds...FIXME getLocationsMultipleBlockIds is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeShuffle]] removeShuffle Internal Method [source,scala] \u00b6 removeShuffle( shuffleId: Int): Future[Seq[Boolean]] removeShuffle...FIXME removeShuffle is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getPeers]] getPeers Internal Method [source, scala] \u00b6 getPeers( blockManagerId: BlockManagerId): Seq[BlockManagerId] getPeers finds all the registered BlockManagers (using < > internal registry) and checks if the input blockManagerId is amongst them. If the input blockManagerId is registered, getPeers returns all the registered BlockManagers but the one on the driver and blockManagerId . Otherwise, getPeers returns no BlockManagers . NOTE: Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. getPeers is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[register]] register Internal Method [source, scala] \u00b6 register( idWithoutTopologyInfo: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId register registers a storage:BlockManager.md[] (based on the given storage:BlockManagerId.md[]) in the < > and < > registries and posts a SparkListenerBlockManagerAdded message (to the < >). NOTE: The input maxMemSize is the storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. NOTE: Registering a BlockManager can only happen once for an executor (identified by BlockManagerId.executorId in < > internal registry). If another BlockManager has earlier been registered for the executor, you should see the following ERROR message in the logs: [source,plaintext] \u00b6 Got two different block manager registrations on same executor - will replace old one [oldId] with new one [id] \u00b6 And then < >. register prints out the following INFO message to the logs: [source,plaintext] \u00b6 Registering block manager [hostPort] with [bytes] RAM, [id] \u00b6 The BlockManager is recorded in the internal registries: < > < > In the end, register requests the < > to scheduler:LiveListenerBus.md#post[post] a ROOT:SparkListener.md#SparkListenerBlockManagerAdded[SparkListenerBlockManagerAdded] message. register is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeExecutor]] removeExecutor Internal Method [source, scala] \u00b6 removeExecutor( execId: String): Unit removeExecutor prints the following INFO message to the logs: [source,plaintext] \u00b6 Trying to remove executor [execId] from BlockManagerMaster. \u00b6 If the execId executor is registered (in the internal < > internal registry), removeExecutor < BlockManager >>. removeExecutor is used when BlockManagerMasterEndpoint is requested to handle < > or < > messages. == [[removeBlockManager]] removeBlockManager Internal Method [source, scala] \u00b6 removeBlockManager( blockManagerId: BlockManagerId): Unit removeBlockManager looks up blockManagerId and removes the executor it was working on from the internal registries: < > < > It then goes over all the blocks for the BlockManager , and removes the executor for each block from blockLocations registry. ROOT:SparkListener.md#SparkListenerBlockManagerRemoved[SparkListenerBlockManagerRemoved(System.currentTimeMillis(), blockManagerId)] is posted to ROOT:SparkContext.md#listenerBus[listenerBus]. You should then see the following INFO message in the logs: [source,plaintext] \u00b6 Removing block manager [blockManagerId] \u00b6 removeBlockManager is used when BlockManagerMasterEndpoint is requested to < > (to handle < > or < > messages). == [[getLocations]] getLocations Internal Method [source, scala] \u00b6 getLocations( blockId: BlockId): Seq[BlockManagerId] getLocations looks up the given storage:BlockId.md[] in the blockLocations internal registry and returns the locations (as a collection of BlockManagerId ) or an empty collection. getLocations is used when BlockManagerMasterEndpoint is requested to handle < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerMasterEndpoint=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[blockManagerIdByExecutor]] blockManagerIdByExecutor Lookup Table [source,scala] \u00b6 blockManagerIdByExecutor: Map[String, BlockManagerId] \u00b6 Lookup table of storage:BlockManagerId.md[]s by executor ID A new executor is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). An executor is removed when BlockManagerMasterEndpoint is requested to handle a < > and a < > messages (via < >) Used when BlockManagerMasterEndpoint is requested to handle < > message, < >, < > and < >. === [[blockManagerInfo]] blockManagerInfo Lookup Table [source,scala] \u00b6 blockManagerIdByExecutor: Map[String, BlockManagerId] \u00b6 Lookup table of storage:BlockManagerInfo.md[] by storage:BlockManagerId.md[] A new BlockManagerInfo is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). A BlockManagerInfo is removed when BlockManagerMasterEndpoint is requested to < > (to handle < > and < > messages). === [[blockLocations]] blockLocations [source,scala] \u00b6 blockLocations: Map[BlockId, Set[BlockManagerId]] \u00b6 Collection of storage:BlockId.md[] and their locations (as BlockManagerId ). Used in removeRdd to remove blocks for a RDD, removeBlockManager to remove blocks after a BlockManager gets removed, removeBlockFromWorkers , updateBlockInfo , and < >.","title":"BlockManagerMasterEndpoint"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanagermasterendpoint-up","text":"== [[messages]][[receiveAndReply]] Messages As an rpc:RpcEndpoint.md[], BlockManagerMasterEndpoint handles RPC messages. === [[BlockManagerHeartbeat]] BlockManagerHeartbeat","title":"BlockManagerMasterEndpoint up"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala","text":"BlockManagerHeartbeat( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocations]] GetLocations","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_1","text":"GetLocations( blockId: BlockId) When received, BlockManagerMasterEndpoint replies with the < > of blockId . Posted when BlockManagerMaster.md#getLocations-block[ BlockManagerMaster requests the block locations of a single block]. === [[GetLocationsAndStatus]] GetLocationsAndStatus","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_2","text":"GetLocationsAndStatus( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetLocationsMultipleBlockIds]] GetLocationsMultipleBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_3","text":"GetLocationsMultipleBlockIds( blockIds: Array[BlockId]) When received, BlockManagerMasterEndpoint replies with the < > for the given storage:BlockId.md[]. Posted when BlockManagerMaster.md#getLocations[ BlockManagerMaster requests the block locations for multiple blocks]. === [[GetPeers]] GetPeers","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_4","text":"GetPeers( blockManagerId: BlockManagerId) When received, BlockManagerMasterEndpoint replies with the < > of blockManagerId . Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. Posted when BlockManagerMaster.md#getPeers[ BlockManagerMaster requests the peers of a BlockManager ]. === [[GetExecutorEndpointRef]] GetExecutorEndpointRef","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_5","text":"GetExecutorEndpointRef( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetMemoryStatus]] GetMemoryStatus","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#getmemorystatus","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetStorageStatus]] GetStorageStatus","title":"GetMemoryStatus"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#getstoragestatus","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[GetBlockStatus]] GetBlockStatus","title":"GetStorageStatus"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_8","text":"GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint is requested to < >. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_9","text":"GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[HasCachedBlocks]] HasCachedBlocks","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_10","text":"HasCachedBlocks( executorId: String) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RegisterBlockManager]] RegisterBlockManager","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala","text":"RegisterBlockManager( blockManagerId: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, sender: RpcEndpointRef) When received, BlockManagerMasterEndpoint is requested to < > (by the given storage:BlockManagerId.md[]). Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#registerBlockManager[register a BlockManager] === [[RemoveRdd]] RemoveRdd","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_11","text":"RemoveRdd( rddId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveShuffle]] RemoveShuffle","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_12","text":"RemoveShuffle( shuffleId: Int) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBroadcast]] RemoveBroadcast","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_13","text":"RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveBlock]] RemoveBlock","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_14","text":"RemoveBlock( blockId: BlockId) When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[RemoveExecutor]] RemoveExecutor","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_15","text":"RemoveExecutor( execId: String) When received, BlockManagerMasterEndpoint < execId is removed>> and the response true sent back. Posted when BlockManagerMaster.md#removeExecutor[ BlockManagerMaster removes an executor]. === [[StopBlockManagerMaster]] StopBlockManagerMaster","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#stopblockmanagermaster","text":"When received, BlockManagerMasterEndpoint...FIXME Posted when...FIXME === [[UpdateBlockInfo]] UpdateBlockInfo","title":"StopBlockManagerMaster"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_17","text":"UpdateBlockInfo( blockManagerId: BlockManagerId, blockId: BlockId, storageLevel: StorageLevel, memSize: Long, diskSize: Long) When received, BlockManagerMasterEndpoint...FIXME Posted when BlockManagerMaster is requested to storage:BlockManagerMaster.md#updateBlockInfo[handle a block status update (from BlockManager on an executor)]. == [[storageStatus]] storageStatus Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_1","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#storagestatus-arraystoragestatus","text":"storageStatus...FIXME storageStatus is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getLocationsMultipleBlockIds]] getLocationsMultipleBlockIds Internal Method","title":"storageStatus: Array[StorageStatus]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_2","text":"getLocationsMultipleBlockIds( blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]] getLocationsMultipleBlockIds...FIXME getLocationsMultipleBlockIds is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeShuffle]] removeShuffle Internal Method","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_3","text":"removeShuffle( shuffleId: Int): Future[Seq[Boolean]] removeShuffle...FIXME removeShuffle is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[getPeers]] getPeers Internal Method","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_18","text":"getPeers( blockManagerId: BlockManagerId): Seq[BlockManagerId] getPeers finds all the registered BlockManagers (using < > internal registry) and checks if the input blockManagerId is amongst them. If the input blockManagerId is registered, getPeers returns all the registered BlockManagers but the one on the driver and blockManagerId . Otherwise, getPeers returns no BlockManagers . NOTE: Peers of a storage:BlockManager.md[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application. getPeers is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[register]] register Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_19","text":"register( idWithoutTopologyInfo: BlockManagerId, maxOnHeapMemSize: Long, maxOffHeapMemSize: Long, slaveEndpoint: RpcEndpointRef): BlockManagerId register registers a storage:BlockManager.md[] (based on the given storage:BlockManagerId.md[]) in the < > and < > registries and posts a SparkListenerBlockManagerAdded message (to the < >). NOTE: The input maxMemSize is the storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on a BlockManager ]. NOTE: Registering a BlockManager can only happen once for an executor (identified by BlockManagerId.executorId in < > internal registry). If another BlockManager has earlier been registered for the executor, you should see the following ERROR message in the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#got-two-different-block-manager-registrations-on-same-executor-will-replace-old-one-oldid-with-new-one-id","text":"And then < >. register prints out the following INFO message to the logs:","title":"Got two different block manager registrations on same executor - will replace old one [oldId] with new one [id]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_2","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#registering-block-manager-hostport-with-bytes-ram-id","text":"The BlockManager is recorded in the internal registries: < > < > In the end, register requests the < > to scheduler:LiveListenerBus.md#post[post] a ROOT:SparkListener.md#SparkListenerBlockManagerAdded[SparkListenerBlockManagerAdded] message. register is used when BlockManagerMasterEndpoint is requested to handle < > message. == [[removeExecutor]] removeExecutor Internal Method","title":"Registering block manager [hostPort] with [bytes] RAM, [id]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_20","text":"removeExecutor( execId: String): Unit removeExecutor prints the following INFO message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#trying-to-remove-executor-execid-from-blockmanagermaster","text":"If the execId executor is registered (in the internal < > internal registry), removeExecutor < BlockManager >>. removeExecutor is used when BlockManagerMasterEndpoint is requested to handle < > or < > messages. == [[removeBlockManager]] removeBlockManager Internal Method","title":"Trying to remove executor [execId] from BlockManagerMaster."},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_21","text":"removeBlockManager( blockManagerId: BlockManagerId): Unit removeBlockManager looks up blockManagerId and removes the executor it was working on from the internal registries: < > < > It then goes over all the blocks for the BlockManager , and removes the executor for each block from blockLocations registry. ROOT:SparkListener.md#SparkListenerBlockManagerRemoved[SparkListenerBlockManagerRemoved(System.currentTimeMillis(), blockManagerId)] is posted to ROOT:SparkContext.md#listenerBus[listenerBus]. You should then see the following INFO message in the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#sourceplaintext_4","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerMasterEndpoint/#removing-block-manager-blockmanagerid","text":"removeBlockManager is used when BlockManagerMasterEndpoint is requested to < > (to handle < > or < > messages). == [[getLocations]] getLocations Internal Method","title":"Removing block manager [blockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#source-scala_22","text":"getLocations( blockId: BlockId): Seq[BlockManagerId] getLocations looks up the given storage:BlockId.md[] in the blockLocations internal registry and returns the locations (as a collection of BlockManagerId ) or an empty collection. getLocations is used when BlockManagerMasterEndpoint is requested to handle < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerMasterEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/BlockManagerMasterEndpoint/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerMasterEndpoint/#log4jloggerorgapachesparkstorageblockmanagermasterendpointall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-properties]] Internal Properties === [[blockManagerIdByExecutor]] blockManagerIdByExecutor Lookup Table","title":"log4j.logger.org.apache.spark.storage.BlockManagerMasterEndpoint=ALL"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_4","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanageridbyexecutor-mapstring-blockmanagerid","text":"Lookup table of storage:BlockManagerId.md[]s by executor ID A new executor is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). An executor is removed when BlockManagerMasterEndpoint is requested to handle a < > and a < > messages (via < >) Used when BlockManagerMasterEndpoint is requested to handle < > message, < >, < > and < >. === [[blockManagerInfo]] blockManagerInfo Lookup Table","title":"blockManagerIdByExecutor: Map[String, BlockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_5","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blockmanageridbyexecutor-mapstring-blockmanagerid_1","text":"Lookup table of storage:BlockManagerInfo.md[] by storage:BlockManagerId.md[] A new BlockManagerInfo is added when BlockManagerMasterEndpoint is requested to handle a < > message (and < >). A BlockManagerInfo is removed when BlockManagerMasterEndpoint is requested to < > (to handle < > and < > messages). === [[blockLocations]] blockLocations","title":"blockManagerIdByExecutor: Map[String, BlockManagerId]"},{"location":"storage/BlockManagerMasterEndpoint/#sourcescala_6","text":"","title":"[source,scala]"},{"location":"storage/BlockManagerMasterEndpoint/#blocklocations-mapblockid-setblockmanagerid","text":"Collection of storage:BlockId.md[] and their locations (as BlockManagerId ). Used in removeRdd to remove blocks for a RDD, removeBlockManager to remove blocks after a BlockManager gets removed, removeBlockFromWorkers , updateBlockInfo , and < >.","title":"blockLocations: Map[BlockId, Set[BlockManagerId]]"},{"location":"storage/BlockManagerSlaveEndpoint/","text":"= BlockManagerSlaveEndpoint BlockManagerSlaveEndpoint is a rpc:RpcEndpoint.md#ThreadSafeRpcEndpoint[ThreadSafeRpcEndpoint] for storage:BlockManager.md#slaveEndpoint[BlockManager]. == [[creating-instance]] Creating Instance BlockManagerSlaveEndpoint takes the following to be created: [[rpcEnv]] rpc:RpcEnv.md[] [[blockManager]] Parent storage:BlockManager.md[] [[mapOutputTracker]] scheduler:MapOutputTracker.md[] BlockManagerSlaveEndpoint is created for storage:BlockManager.md#slaveEndpoint[BlockManager] (and registered under the name BlockManagerEndpoint[ID] ). == [[messages]] Messages === [[GetBlockStatus]] GetBlockStatus [source, scala] \u00b6 GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > for the storage:BlockManager.md#getStatus[status of a given block] (by storage:BlockId.md[]) and sends it back to a sender. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds [source, scala] \u00b6 GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > to storage:BlockManager.md#getMatchingBlockIds[find IDs of existing blocks for a given filter] and sends them back to a sender. Posted when...FIXME === [[RemoveBlock]] RemoveBlock [source, scala] \u00b6 RemoveBlock( blockId: BlockId) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 removing block [blockId] \u00b6 BlockManagerSlaveEndpoint then < blockId block>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing block [blockId], response is [response] And true response is sent back. You should see the following DEBUG in the logs: Sent response: true to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing block [blockId] === [[RemoveBroadcast]] RemoveBroadcast [source, scala] \u00b6 RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 removing broadcast [broadcastId] \u00b6 It then calls < broadcastId broadcast>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing broadcast [broadcastId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing broadcast [broadcastId] === [[RemoveRdd]] RemoveRdd [source, scala] \u00b6 RemoveRdd( rddId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing RDD [rddId] It then calls < rddId RDD>>. NOTE: Handling RemoveRdd messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing RDD [rddId], response is [response] And the number of blocks removed is sent back. You should see the following DEBUG in the logs: Sent response: [#blocks] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing RDD [rddId] === [[RemoveShuffle]] RemoveShuffle [source, scala] \u00b6 RemoveShuffle( shuffleId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing shuffle [shuffleId] If scheduler:MapOutputTracker.md[MapOutputTracker] was given (when the RPC endpoint was created), it calls scheduler:MapOutputTracker.md#unregisterShuffle[MapOutputTracker to unregister the shuffleId shuffle]. It then calls shuffle:ShuffleManager.md#unregisterShuffle[ShuffleManager to unregister the shuffleId shuffle]. NOTE: Handling RemoveShuffle messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing shuffle [shuffleId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing shuffle [shuffleId] Posted when BlockManagerMaster.md#removeShuffle[BlockManagerMaster] and storage:BlockManagerMasterEndpoint.md#removeShuffle[BlockManagerMasterEndpoint] are requested to remove all blocks of a shuffle. === [[ReplicateBlock]] ReplicateBlock [source, scala] \u00b6 ReplicateBlock( blockId: BlockId, replicas: Seq[BlockManagerId], maxReplicas: Int) When received, BlockManagerSlaveEndpoint...FIXME Posted when...FIXME === [[TriggerThreadDump]] TriggerThreadDump When received, BlockManagerSlaveEndpoint is requested for the thread info for all live threads with stack trace and synchronization information. == [[asyncThreadPool]][[asyncExecutionContext]] block-manager-slave-async-thread-pool Thread Pool BlockManagerSlaveEndpoint creates a thread pool of maximum 100 daemon threads with block-manager-slave-async-thread-pool thread prefix (using {java-javadoc-url}/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor]). BlockManagerSlaveEndpoint uses the thread pool (as a Scala implicit value) when requested to < > to communicate in a non-blocking, asynchronous way. The thread pool is shut down when BlockManagerSlaveEndpoint is requested to < >. The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients. == [[doAsync]] doAsync Internal Method [source,scala] \u00b6 doAsync T ( body: => T) doAsync creates a Scala Future to execute the following asynchronously (i.e. on a separate thread from the < >): . Prints out the given actionMessage as a DEBUG message to the logs . Executes the given body When completed successfully, doAsync prints out the following DEBUG messages to the logs and requests the given RpcCallContext to reply the response to the sender. [source,plaintext] \u00b6 Done [actionMessage], response is [response] Sent response: [response] to [senderAddress] In case of a failure, doAsync prints out the following ERROR message to the logs and requests the given RpcCallContext to send the failure to the sender. [source,plaintext] \u00b6 Error in [actionMessage] \u00b6 doAsync is used when BlockManagerSlaveEndpoint is requested to handle < >, < >, < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerSlaveEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"BlockManagerSlaveEndpoint"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala","text":"GetBlockStatus( blockId: BlockId, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > for the storage:BlockManager.md#getStatus[status of a given block] (by storage:BlockId.md[]) and sends it back to a sender. Posted when...FIXME === [[GetMatchingBlockIds]] GetMatchingBlockIds","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_1","text":"GetMatchingBlockIds( filter: BlockId => Boolean, askSlaves: Boolean = true) When received, BlockManagerSlaveEndpoint requests the < > to storage:BlockManager.md#getMatchingBlockIds[find IDs of existing blocks for a given filter] and sends them back to a sender. Posted when...FIXME === [[RemoveBlock]] RemoveBlock","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_2","text":"RemoveBlock( blockId: BlockId) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#removing-block-blockid","text":"BlockManagerSlaveEndpoint then < blockId block>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing block [blockId], response is [response] And true response is sent back. You should see the following DEBUG in the logs: Sent response: true to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing block [blockId] === [[RemoveBroadcast]] RemoveBroadcast","title":"removing block [blockId]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_3","text":"RemoveBroadcast( broadcastId: Long, removeFromDriver: Boolean = true) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#removing-broadcast-broadcastid","text":"It then calls < broadcastId broadcast>>. When the computation is successful, you should see the following DEBUG in the logs: Done removing broadcast [broadcastId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing broadcast [broadcastId] === [[RemoveRdd]] RemoveRdd","title":"removing broadcast [broadcastId]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_4","text":"RemoveRdd( rddId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing RDD [rddId] It then calls < rddId RDD>>. NOTE: Handling RemoveRdd messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing RDD [rddId], response is [response] And the number of blocks removed is sent back. You should see the following DEBUG in the logs: Sent response: [#blocks] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing RDD [rddId] === [[RemoveShuffle]] RemoveShuffle","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_5","text":"RemoveShuffle( shuffleId: Int) When received, BlockManagerSlaveEndpoint prints out the following DEBUG message to the logs: removing shuffle [shuffleId] If scheduler:MapOutputTracker.md[MapOutputTracker] was given (when the RPC endpoint was created), it calls scheduler:MapOutputTracker.md#unregisterShuffle[MapOutputTracker to unregister the shuffleId shuffle]. It then calls shuffle:ShuffleManager.md#unregisterShuffle[ShuffleManager to unregister the shuffleId shuffle]. NOTE: Handling RemoveShuffle messages happens on a separate thread. See < >. When the computation is successful, you should see the following DEBUG in the logs: Done removing shuffle [shuffleId], response is [response] And the result is sent back. You should see the following DEBUG in the logs: Sent response: [response] to [senderAddress] In case of failure, you should see the following ERROR in the logs and the stack trace. Error in removing shuffle [shuffleId] Posted when BlockManagerMaster.md#removeShuffle[BlockManagerMaster] and storage:BlockManagerMasterEndpoint.md#removeShuffle[BlockManagerMasterEndpoint] are requested to remove all blocks of a shuffle. === [[ReplicateBlock]] ReplicateBlock","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#source-scala_6","text":"ReplicateBlock( blockId: BlockId, replicas: Seq[BlockManagerId], maxReplicas: Int) When received, BlockManagerSlaveEndpoint...FIXME Posted when...FIXME === [[TriggerThreadDump]] TriggerThreadDump When received, BlockManagerSlaveEndpoint is requested for the thread info for all live threads with stack trace and synchronization information. == [[asyncThreadPool]][[asyncExecutionContext]] block-manager-slave-async-thread-pool Thread Pool BlockManagerSlaveEndpoint creates a thread pool of maximum 100 daemon threads with block-manager-slave-async-thread-pool thread prefix (using {java-javadoc-url}/java/util/concurrent/ThreadPoolExecutor.html[java.util.concurrent.ThreadPoolExecutor]). BlockManagerSlaveEndpoint uses the thread pool (as a Scala implicit value) when requested to < > to communicate in a non-blocking, asynchronous way. The thread pool is shut down when BlockManagerSlaveEndpoint is requested to < >. The reason for the async thread pool is that the block-related operations might take quite some time and to release the main RPC thread other threads are spawned to talk to the external services and pass responses on to the clients. == [[doAsync]] doAsync Internal Method","title":"[source, scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourcescala","text":"doAsync T ( body: => T) doAsync creates a Scala Future to execute the following asynchronously (i.e. on a separate thread from the < >): . Prints out the given actionMessage as a DEBUG message to the logs . Executes the given body When completed successfully, doAsync prints out the following DEBUG messages to the logs and requests the given RpcCallContext to reply the response to the sender.","title":"[source,scala]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_2","text":"Done [actionMessage], response is [response] Sent response: [response] to [senderAddress] In case of a failure, doAsync prints out the following ERROR message to the logs and requests the given RpcCallContext to send the failure to the sender.","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#sourceplaintext_3","text":"","title":"[source,plaintext]"},{"location":"storage/BlockManagerSlaveEndpoint/#error-in-actionmessage","text":"doAsync is used when BlockManagerSlaveEndpoint is requested to handle < >, < >, < > and < > messages. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.BlockManagerSlaveEndpoint logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"Error in [actionMessage]"},{"location":"storage/BlockManagerSlaveEndpoint/#source","text":"","title":"[source]"},{"location":"storage/BlockManagerSlaveEndpoint/#log4jloggerorgapachesparkstorageblockmanagerslaveendpointall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.BlockManagerSlaveEndpoint=ALL"},{"location":"storage/BlockManagerSource/","text":"BlockManagerSource -- Metrics Source for BlockManager \u00b6 BlockManagerSource is the spark-metrics-Source.md[metrics source] of a storage:BlockManager.md[BlockManager]. [[sourceName]] BlockManagerSource is registered under the name BlockManager (when SparkContext is created). [[metrics]] .BlockManagerSource's Gauge Metrics (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | Name | Type | Description | disk.diskSpaceUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their disk space used ( diskUsed ). | memory.maxMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their maximum memory limit ( maxMem ). | memory.maxOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( maxOffHeapMem ). | memory.maxOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( maxOnHeapMem ). | memory.memUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory used ( memUsed ). | memory.offHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory used ( offHeapMemUsed ). | memory.onHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory used ( onHeapMemUsed ). | memory.remainingMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory remaining ( memRemaining ). | memory.remainingOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( offHeapMemRemaining ). | memory.remainingOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( onHeapMemRemaining ). |=== You can access the BlockManagerSource < > using the web UI's port (as spark-webui-properties.md#spark.ui.port[spark.ui.port] configuration property). $ http --follow http://localhost:4040/metrics/json \\ | jq '.gauges | keys | .[] | select(test(\".driver.BlockManager\"; \"g\"))' \"local-1528725411625.driver.BlockManager.disk.diskSpaceUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.maxMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOnHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.memUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.offHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.onHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOnHeapMem_MB\" [[creating-instance]] [[blockManager]] BlockManagerSource takes a storage:BlockManager.md[BlockManager] when created. BlockManagerSource is created when SparkContext is created.","title":"BlockManagerSource"},{"location":"storage/BlockManagerSource/#blockmanagersource-metrics-source-for-blockmanager","text":"BlockManagerSource is the spark-metrics-Source.md[metrics source] of a storage:BlockManager.md[BlockManager]. [[sourceName]] BlockManagerSource is registered under the name BlockManager (when SparkContext is created). [[metrics]] .BlockManagerSource's Gauge Metrics (in alphabetical order) [width=\"100%\",cols=\"1,1,2\",options=\"header\"] |=== | Name | Type | Description | disk.diskSpaceUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their disk space used ( diskUsed ). | memory.maxMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their maximum memory limit ( maxMem ). | memory.maxOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( maxOffHeapMem ). | memory.maxOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( maxOnHeapMem ). | memory.memUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory used ( memUsed ). | memory.offHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory used ( offHeapMemUsed ). | memory.onHeapMemUsed_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory used ( onHeapMemUsed ). | memory.remainingMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their memory remaining ( memRemaining ). | memory.remainingOffHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their off-heap memory remaining ( offHeapMemRemaining ). | memory.remainingOnHeapMem_MB | long | Requests BlockManagerMaster for BlockManagerMaster.md#getStorageStatus[storage status] (for every storage:BlockManager.md[BlockManager]) and sums up their on-heap memory remaining ( onHeapMemRemaining ). |=== You can access the BlockManagerSource < > using the web UI's port (as spark-webui-properties.md#spark.ui.port[spark.ui.port] configuration property). $ http --follow http://localhost:4040/metrics/json \\ | jq '.gauges | keys | .[] | select(test(\".driver.BlockManager\"; \"g\"))' \"local-1528725411625.driver.BlockManager.disk.diskSpaceUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.maxMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.maxOnHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.memUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.offHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.onHeapMemUsed_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOffHeapMem_MB\" \"local-1528725411625.driver.BlockManager.memory.remainingOnHeapMem_MB\" [[creating-instance]] [[blockManager]] BlockManagerSource takes a storage:BlockManager.md[BlockManager] when created. BlockManagerSource is created when SparkContext is created.","title":"BlockManagerSource -- Metrics Source for BlockManager"},{"location":"storage/BlockTransferService/","text":"= BlockTransferService BlockTransferService is an < > of the storage:ShuffleClient.md[] abstraction for < > that can < > and < > blocks of data synchronously or asynchronously. BlockTransferService is a networking service available by a < > and a < >. BlockTransferService was introduced in https://issues.apache.org/jira/browse/SPARK-3019[SPARK-3019 Pluggable block transfer interface (BlockTransferService)]. == [[contract]] Contract === [[close]] close [source,scala] \u00b6 close(): Unit \u00b6 Used when BlockManager is requested to storage:BlockManager.md#stop[stop] === [[fetchBlocks]] fetchBlocks [source,scala] \u00b6 fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener, tempFileManager: DownloadFileManager): Unit Fetches a sequence of blocks from a remote node asynchronously fetchBlocks is part of the storage:ShuffleClient.md#fetchBlocks[ShuffleClient] abstraction. Used when BlockTransferService is requested to < > === [[hostName]] hostName [source,scala] \u00b6 hostName: String \u00b6 Used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] === [[init]] init [source,scala] \u00b6 init( blockDataManager: BlockDataManager): Unit Used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] (with the storage:BlockDataManager.md[] being the BlockManager itself) === [[port]] port [source,scala] \u00b6 port: Int \u00b6 Used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] === [[uploadBlock]] uploadBlock [source,scala] \u00b6 uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Used when BlockTransferService is requested to < > == [[implementations]] BlockTransferServices storage:NettyBlockTransferService.md[] is the default and only known BlockTransferService. == [[fetchBlockSync]] fetchBlockSync Method [source, scala] \u00b6 fetchBlockSync( host: String, port: Int, execId: String, blockId: String, tempFileManager: TempFileManager): ManagedBuffer fetchBlockSync...FIXME Synchronous (and hence blocking) fetchBlockSync to fetch one block blockId (that corresponds to the storage:ShuffleClient.md[] parent's asynchronous storage:ShuffleClient.md#fetchBlocks[fetchBlocks]). fetchBlockSync is a mere wrapper around storage:ShuffleClient.md#fetchBlocks[fetchBlocks] to fetch one blockId block that waits until the fetch finishes. fetchBlockSync is used when...FIXME == [[uploadBlockSync]] Uploading Single Block to Remote Node (Blocking Fashion) [source, scala] \u00b6 uploadBlockSync( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Unit uploadBlockSync...FIXME uploadBlockSync is a mere blocking wrapper around < > that waits until the upload finishes. uploadBlockSync is used when BlockManager is requested to storage:BlockManager.md#replicate[replicate] (when a storage:StorageLevel.md[replication level is greater than 1]).","title":"BlockTransferService"},{"location":"storage/BlockTransferService/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/BlockTransferService/#close-unit","text":"Used when BlockManager is requested to storage:BlockManager.md#stop[stop] === [[fetchBlocks]] fetchBlocks","title":"close(): Unit"},{"location":"storage/BlockTransferService/#sourcescala_1","text":"fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener, tempFileManager: DownloadFileManager): Unit Fetches a sequence of blocks from a remote node asynchronously fetchBlocks is part of the storage:ShuffleClient.md#fetchBlocks[ShuffleClient] abstraction. Used when BlockTransferService is requested to < > === [[hostName]] hostName","title":"[source,scala]"},{"location":"storage/BlockTransferService/#sourcescala_2","text":"","title":"[source,scala]"},{"location":"storage/BlockTransferService/#hostname-string","text":"Used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] === [[init]] init","title":"hostName: String"},{"location":"storage/BlockTransferService/#sourcescala_3","text":"init( blockDataManager: BlockDataManager): Unit Used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] (with the storage:BlockDataManager.md[] being the BlockManager itself) === [[port]] port","title":"[source,scala]"},{"location":"storage/BlockTransferService/#sourcescala_4","text":"","title":"[source,scala]"},{"location":"storage/BlockTransferService/#port-int","text":"Used when BlockManager is requested to storage:BlockManager.md#initialize[initialize] === [[uploadBlock]] uploadBlock","title":"port: Int"},{"location":"storage/BlockTransferService/#sourcescala_5","text":"uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Used when BlockTransferService is requested to < > == [[implementations]] BlockTransferServices storage:NettyBlockTransferService.md[] is the default and only known BlockTransferService. == [[fetchBlockSync]] fetchBlockSync Method","title":"[source,scala]"},{"location":"storage/BlockTransferService/#source-scala","text":"fetchBlockSync( host: String, port: Int, execId: String, blockId: String, tempFileManager: TempFileManager): ManagedBuffer fetchBlockSync...FIXME Synchronous (and hence blocking) fetchBlockSync to fetch one block blockId (that corresponds to the storage:ShuffleClient.md[] parent's asynchronous storage:ShuffleClient.md#fetchBlocks[fetchBlocks]). fetchBlockSync is a mere wrapper around storage:ShuffleClient.md#fetchBlocks[fetchBlocks] to fetch one blockId block that waits until the fetch finishes. fetchBlockSync is used when...FIXME == [[uploadBlockSync]] Uploading Single Block to Remote Node (Blocking Fashion)","title":"[source, scala]"},{"location":"storage/BlockTransferService/#source-scala_1","text":"uploadBlockSync( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Unit uploadBlockSync...FIXME uploadBlockSync is a mere blocking wrapper around < > that waits until the upload finishes. uploadBlockSync is used when BlockManager is requested to storage:BlockManager.md#replicate[replicate] (when a storage:StorageLevel.md[replication level is greater than 1]).","title":"[source, scala]"},{"location":"storage/DiskBlockManager/","text":"= [[DiskBlockManager]] DiskBlockManager DiskBlockManager creates and maintains the logical mapping between logical blocks and physical on-disk locations for a storage:BlockManager.md#diskBlockManager[BlockManager]. .DiskBlockManager and BlockManager image::DiskBlockManager-BlockManager.png[align=\"center\"] By default, one block is mapped to one file with a name given by its BlockId . It is however possible to have a block map to only a segment of a file. Block files are hashed among the < >. DiskBlockManager is used to create a DiskStore.md[DiskStore]. TIP: Consult demo-diskblockmanager-and-block-data.md[Demo: DiskBlockManager and Block Data]. == [[creating-instance]] Creating Instance DiskBlockManager takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] [[deleteFilesOnStop]] deleteFilesOnStop flag When created, DiskBlockManager < > and initializes the internal < > collection of locks for every local directory. In the end, DiskBlockManager < > to clean up the local directories for blocks. == [[localDirs]] Local Directories for Blocks [source,scala] \u00b6 localDirs: Array[File] \u00b6 While being created, DiskBlockManager < > for block data. DiskBlockManager expects at least one local directory or prints out the following ERROR message to the logs and exits the JVM (with exit code 53). Failed to create any local dir. localDirs is used when: DiskBlockManager is requested to < >, initialize the < > internal registry, and to < > BlockManager is requested to storage:BlockManager.md#registerWithExternalShuffleServer[register with an external shuffle server] == [[subDirsPerLocalDir]][[subDirs]] File Locks for Local Block Store Directories [source, scala] \u00b6 subDirs: Array[Array[File]] \u00b6 subDirs is a lookup table for file locks of every < > (with the first dimension for local directories and the second for locks). The number of block subdirectories is controlled by ROOT:configuration-properties.md#spark.diskStore.subDirectories[spark.diskStore.subDirectories] configuration property (default: 64 ). subDirs(dirId)(subDirId) is used to access subDirId subdirectory in dirId local directory. subDirs is used when DiskBlockManager is requested for a < > or < >. == [[createLocalDirs]] Creating Local Directories for Block Data [source, scala] \u00b6 createLocalDirs( conf: SparkConf): Array[File] createLocalDirs creates blockmgr-[random UUID] directory under local directories to store block data. Internally, createLocalDirs < > and creates a subdirectory blockmgr-[UUID] under every configured parent directory. For every local directory, createLocalDirs prints out the following INFO message to the logs: Created local directory at [localDir] In case of an exception, createLocalDirs prints out the following ERROR message to the logs and skips the directory. Failed to create local dir in [rootDir]. Ignoring this directory. createLocalDirs is used when the < > internal registry is initialized. == [[getFile]] Finding Block File (and Creating Parent Directories) [source, scala] \u00b6 getFile( blockId: BlockId): File // <1> getFile( filename: String): File <1> Uses the name of the given BlockId getFile computes a hash of the file name of the input storage:BlockId.md[] that is used for the name of the parent directory and subdirectory. getFile creates the subdirectory unless it already exists. getFile is used when: DiskBlockManager is requested to < >, < >, < > DiskStore is requested to DiskStore.md#getBytes[getBytes], DiskStore.md#remove[remove], DiskStore.md#contains[contains], and DiskStore.md#put[put] IndexShuffleBlockResolver is requested to shuffle:IndexShuffleBlockResolver.md#getDataFile[getDataFile] and shuffle:IndexShuffleBlockResolver.md#getIndexFile[getIndexFile] == [[createTempShuffleBlock]] createTempShuffleBlock Method [source, scala] \u00b6 createTempShuffleBlock(): (TempShuffleBlockId, File) \u00b6 createTempShuffleBlock creates a temporary TempShuffleBlockId block. CAUTION: FIXME == [[getAllFiles]] All Block Files [source, scala] \u00b6 getAllFiles(): Seq[File] \u00b6 getAllFiles ...FIXME NOTE: getAllFiles is used exclusively when DiskBlockManager is requested to < >. == [[addShutdownHook]] Registering Shutdown Hook -- addShutdownHook Internal Method [source, scala] \u00b6 addShutdownHook(): AnyRef \u00b6 addShutdownHook registers a shutdown hook to execute < > at shutdown. When executed, you should see the following DEBUG message in the logs: DEBUG DiskBlockManager: Adding shutdown hook addShutdownHook adds the shutdown hook so it prints the following INFO message and executes < >. INFO DiskBlockManager: Shutdown hook called == [[doStop]] Stopping DiskBlockManager (Removing Local Directories for Blocks) -- doStop Internal Method [source, scala] \u00b6 doStop(): Unit \u00b6 doStop deletes the local directories recursively (only when the constructor's deleteFilesOnStop is enabled and the parent directories are not registered to be removed at shutdown). NOTE: doStop is used when DiskBlockManager is requested to < > or < >. == [[getConfiguredLocalDirs]] Getting Local Directories for Spark to Write Files -- Utils.getConfiguredLocalDirs Internal Method [source, scala] \u00b6 getConfiguredLocalDirs(conf: SparkConf): Array[String] \u00b6 getConfiguredLocalDirs returns the local directories where Spark can write files. Internally, getConfiguredLocalDirs uses conf ROOT:SparkConf.md[SparkConf] to know if deploy:ExternalShuffleService.md[External Shuffle Service] is enabled (based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). getConfiguredLocalDirs checks if < > and if so, returns < LOCAL_DIRS -controlled local directories>>. In non-YARN mode (or for the driver in yarn-client mode), getConfiguredLocalDirs checks the following environment variables (in the order) and returns the value of the first met: SPARK_EXECUTOR_DIRS environment variable SPARK_LOCAL_DIRS environment variable MESOS_DIRECTORY environment variable (only when External Shuffle Service is not used) In the end, when no earlier environment variables were found, getConfiguredLocalDirs uses spark-properties.md#spark.local.dir[spark.local.dir] Spark property or falls back on java.io.tmpdir System property. [NOTE] \u00b6 getConfiguredLocalDirs is used when: DiskBlockManager is requested to < > * Utils helper is requested to spark-Utils.md#getLocalDir[getLocalDir] and spark-Utils.md#getOrCreateLocalRootDirsImpl[getOrCreateLocalRootDirsImpl] \u00b6 == [[getYarnLocalDirs]] Getting Writable Directories in YARN -- getYarnLocalDirs Internal Method [source, scala] \u00b6 getYarnLocalDirs(conf: SparkConf): String \u00b6 getYarnLocalDirs uses conf ROOT:SparkConf.md[SparkConf] to read LOCAL_DIRS environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them). getYarnLocalDirs throws an Exception with the message Yarn Local dirs can't be empty if LOCAL_DIRS environment variable was not set. == [[isRunningInYarnContainer]] Checking If Spark Runs on YARN -- isRunningInYarnContainer Internal Method [source, scala] \u00b6 isRunningInYarnContainer(conf: SparkConf): Boolean \u00b6 isRunningInYarnContainer uses conf ROOT:SparkConf.md[SparkConf] to read Hadoop YARN's http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-api/apidocs/org/apache/hadoop/yarn/api/ApplicationConstants.Environment.html#CONTAINER_ID [ CONTAINER_ID environment variable] to find out if Spark runs in a YARN container. NOTE: CONTAINER_ID environment variable is exported by YARN NodeManager. == [[getAllBlocks]] Getting All Blocks (From Files Stored On Disk) [source, scala] \u00b6 getAllBlocks(): Seq[BlockId] \u00b6 getAllBlocks gets all the blocks stored on disk. Internally, getAllBlocks takes the < > and returns their names (as BlockId ). getAllBlocks is used when BlockManager is requested to storage:BlockManager.md#getMatchingBlockIds[find IDs of existing blocks for a given filter]. == [[stop]] stop Internal Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop is used exclusively when BlockManager is requested to storage:BlockManager.md#stop[stop]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskBlockManager logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"DiskBlockManager"},{"location":"storage/DiskBlockManager/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/DiskBlockManager/#localdirs-arrayfile","text":"While being created, DiskBlockManager < > for block data. DiskBlockManager expects at least one local directory or prints out the following ERROR message to the logs and exits the JVM (with exit code 53). Failed to create any local dir. localDirs is used when: DiskBlockManager is requested to < >, initialize the < > internal registry, and to < > BlockManager is requested to storage:BlockManager.md#registerWithExternalShuffleServer[register with an external shuffle server] == [[subDirsPerLocalDir]][[subDirs]] File Locks for Local Block Store Directories","title":"localDirs: Array[File]"},{"location":"storage/DiskBlockManager/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#subdirs-arrayarrayfile","text":"subDirs is a lookup table for file locks of every < > (with the first dimension for local directories and the second for locks). The number of block subdirectories is controlled by ROOT:configuration-properties.md#spark.diskStore.subDirectories[spark.diskStore.subDirectories] configuration property (default: 64 ). subDirs(dirId)(subDirId) is used to access subDirId subdirectory in dirId local directory. subDirs is used when DiskBlockManager is requested for a < > or < >. == [[createLocalDirs]] Creating Local Directories for Block Data","title":"subDirs: Array[Array[File]]"},{"location":"storage/DiskBlockManager/#source-scala_1","text":"createLocalDirs( conf: SparkConf): Array[File] createLocalDirs creates blockmgr-[random UUID] directory under local directories to store block data. Internally, createLocalDirs < > and creates a subdirectory blockmgr-[UUID] under every configured parent directory. For every local directory, createLocalDirs prints out the following INFO message to the logs: Created local directory at [localDir] In case of an exception, createLocalDirs prints out the following ERROR message to the logs and skips the directory. Failed to create local dir in [rootDir]. Ignoring this directory. createLocalDirs is used when the < > internal registry is initialized. == [[getFile]] Finding Block File (and Creating Parent Directories)","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#source-scala_2","text":"getFile( blockId: BlockId): File // <1> getFile( filename: String): File <1> Uses the name of the given BlockId getFile computes a hash of the file name of the input storage:BlockId.md[] that is used for the name of the parent directory and subdirectory. getFile creates the subdirectory unless it already exists. getFile is used when: DiskBlockManager is requested to < >, < >, < > DiskStore is requested to DiskStore.md#getBytes[getBytes], DiskStore.md#remove[remove], DiskStore.md#contains[contains], and DiskStore.md#put[put] IndexShuffleBlockResolver is requested to shuffle:IndexShuffleBlockResolver.md#getDataFile[getDataFile] and shuffle:IndexShuffleBlockResolver.md#getIndexFile[getIndexFile] == [[createTempShuffleBlock]] createTempShuffleBlock Method","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#createtempshuffleblock-tempshuffleblockid-file","text":"createTempShuffleBlock creates a temporary TempShuffleBlockId block. CAUTION: FIXME == [[getAllFiles]] All Block Files","title":"createTempShuffleBlock(): (TempShuffleBlockId, File)"},{"location":"storage/DiskBlockManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getallfiles-seqfile","text":"getAllFiles ...FIXME NOTE: getAllFiles is used exclusively when DiskBlockManager is requested to < >. == [[addShutdownHook]] Registering Shutdown Hook -- addShutdownHook Internal Method","title":"getAllFiles(): Seq[File]"},{"location":"storage/DiskBlockManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#addshutdownhook-anyref","text":"addShutdownHook registers a shutdown hook to execute < > at shutdown. When executed, you should see the following DEBUG message in the logs: DEBUG DiskBlockManager: Adding shutdown hook addShutdownHook adds the shutdown hook so it prints the following INFO message and executes < >. INFO DiskBlockManager: Shutdown hook called == [[doStop]] Stopping DiskBlockManager (Removing Local Directories for Blocks) -- doStop Internal Method","title":"addShutdownHook(): AnyRef"},{"location":"storage/DiskBlockManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#dostop-unit","text":"doStop deletes the local directories recursively (only when the constructor's deleteFilesOnStop is enabled and the parent directories are not registered to be removed at shutdown). NOTE: doStop is used when DiskBlockManager is requested to < > or < >. == [[getConfiguredLocalDirs]] Getting Local Directories for Spark to Write Files -- Utils.getConfiguredLocalDirs Internal Method","title":"doStop(): Unit"},{"location":"storage/DiskBlockManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getconfiguredlocaldirsconf-sparkconf-arraystring","text":"getConfiguredLocalDirs returns the local directories where Spark can write files. Internally, getConfiguredLocalDirs uses conf ROOT:SparkConf.md[SparkConf] to know if deploy:ExternalShuffleService.md[External Shuffle Service] is enabled (based on ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property). getConfiguredLocalDirs checks if < > and if so, returns < LOCAL_DIRS -controlled local directories>>. In non-YARN mode (or for the driver in yarn-client mode), getConfiguredLocalDirs checks the following environment variables (in the order) and returns the value of the first met: SPARK_EXECUTOR_DIRS environment variable SPARK_LOCAL_DIRS environment variable MESOS_DIRECTORY environment variable (only when External Shuffle Service is not used) In the end, when no earlier environment variables were found, getConfiguredLocalDirs uses spark-properties.md#spark.local.dir[spark.local.dir] Spark property or falls back on java.io.tmpdir System property.","title":"getConfiguredLocalDirs(conf: SparkConf): Array[String]"},{"location":"storage/DiskBlockManager/#note","text":"getConfiguredLocalDirs is used when: DiskBlockManager is requested to < >","title":"[NOTE]"},{"location":"storage/DiskBlockManager/#utils-helper-is-requested-to-spark-utilsmdgetlocaldirgetlocaldir-and-spark-utilsmdgetorcreatelocalrootdirsimplgetorcreatelocalrootdirsimpl","text":"== [[getYarnLocalDirs]] Getting Writable Directories in YARN -- getYarnLocalDirs Internal Method","title":"* Utils helper is requested to spark-Utils.md#getLocalDir[getLocalDir] and spark-Utils.md#getOrCreateLocalRootDirsImpl[getOrCreateLocalRootDirsImpl]"},{"location":"storage/DiskBlockManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getyarnlocaldirsconf-sparkconf-string","text":"getYarnLocalDirs uses conf ROOT:SparkConf.md[SparkConf] to read LOCAL_DIRS environment variable with comma-separated local directories (that have already been created and secured so that only the user has access to them). getYarnLocalDirs throws an Exception with the message Yarn Local dirs can't be empty if LOCAL_DIRS environment variable was not set. == [[isRunningInYarnContainer]] Checking If Spark Runs on YARN -- isRunningInYarnContainer Internal Method","title":"getYarnLocalDirs(conf: SparkConf): String"},{"location":"storage/DiskBlockManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#isrunninginyarncontainerconf-sparkconf-boolean","text":"isRunningInYarnContainer uses conf ROOT:SparkConf.md[SparkConf] to read Hadoop YARN's http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-api/apidocs/org/apache/hadoop/yarn/api/ApplicationConstants.Environment.html#CONTAINER_ID [ CONTAINER_ID environment variable] to find out if Spark runs in a YARN container. NOTE: CONTAINER_ID environment variable is exported by YARN NodeManager. == [[getAllBlocks]] Getting All Blocks (From Files Stored On Disk)","title":"isRunningInYarnContainer(conf: SparkConf): Boolean"},{"location":"storage/DiskBlockManager/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#getallblocks-seqblockid","text":"getAllBlocks gets all the blocks stored on disk. Internally, getAllBlocks takes the < > and returns their names (as BlockId ). getAllBlocks is used when BlockManager is requested to storage:BlockManager.md#getMatchingBlockIds[find IDs of existing blocks for a given filter]. == [[stop]] stop Internal Method","title":"getAllBlocks(): Seq[BlockId]"},{"location":"storage/DiskBlockManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockManager/#stop-unit","text":"stop ...FIXME NOTE: stop is used exclusively when BlockManager is requested to storage:BlockManager.md#stop[stop]. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskBlockManager logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"stop(): Unit"},{"location":"storage/DiskBlockManager/#source","text":"","title":"[source]"},{"location":"storage/DiskBlockManager/#log4jloggerorgapachesparkstoragediskblockmanagerall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.DiskBlockManager=ALL"},{"location":"storage/DiskBlockObjectWriter/","text":"= [[DiskBlockObjectWriter]] DiskBlockObjectWriter DiskBlockObjectWriter is a custom {java-javadoc-url}/java/io/OutputStream.html[java.io.OutputStream] that storage:BlockManager.md#getDiskWriter[BlockManager] offers for < >. DiskBlockObjectWriter is used when: BypassMergeSortShuffleWriter is requested for shuffle:BypassMergeSortShuffleWriter.md#partitionWriters[partition writers] UnsafeSorterSpillWriter is requested for a memory:UnsafeSorterSpillWriter.md#writer[partition writer] ShuffleExternalSorter is requested to shuffle:ShuffleExternalSorter.md#writeSortedFile[writeSortedFile] ExternalSorter is requested to shuffle:ExternalSorter.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] == [[creating-instance]] Creating Instance DiskBlockObjectWriter takes the following to be created: [[file]] Java {java-javadoc-url}/java/io/File.html[File] [[serializerManager]] serializer:SerializerManager.md[] [[serializerInstance]] serializer:SerializerInstance.md[] [[bufferSize]] Buffer size [[syncWrites]] syncWrites flag [[writeMetrics]] executor:ShuffleWriteMetrics.md[] [[blockId]] storage:BlockId.md[] (default: null ) DiskBlockObjectWriter is created when: BlockManager is requested for storage:BlockManager.md#getDiskWriter[one] BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records] (as shuffle:BypassMergeSortShuffleWriter.md#partitionWriters[partition writers]) == [[objOut]] SerializationStream DiskBlockObjectWriter manages a serializer:SerializationStream.md[SerializationStream] for < >: Opens it when requested to < > Closes it when requested to < > Dereferences it ( null s it) when < > == [[states]][[streamOpen]] States DiskBlockObjectWriter can be in the following states (that match the state of the underlying output streams): . Initialized . Open . Closed == [[write]] Writing Key and Value (of Record) [source, scala] \u00b6 write( key: Any, value: Any): Unit write < > unless < > already. write requests the < > to serializer:SerializationStream.md#writeKey[write the key] and then the serializer:SerializationStream.md#writeValue[value]. In the end, write < >. write is used when: BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records of a partition] ExternalAppendOnlyMap is requested to shuffle:ExternalAppendOnlyMap.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to shuffle:ExternalSorter.md#writePartitionedFile[write all records into a partitioned file] ** SpillableIterator is requested to spill WritablePartitionedPairCollection is requested for a destructiveSortedWritablePartitionedIterator == [[commitAndGet]] commitAndGet Method [source, scala] \u00b6 commitAndGet(): FileSegment \u00b6 commitAndGet...FIXME commitAndGet is used when...FIXME == [[close]] Committing Writes and Closing Resources [source, scala] \u00b6 close(): Unit \u00b6 close...FIXME close is used when...FIXME == [[revertPartialWritesAndClose]] revertPartialWritesAndClose Method [source, scala] \u00b6 revertPartialWritesAndClose(): File \u00b6 revertPartialWritesAndClose...FIXME revertPartialWritesAndClose is used when...FIXME == [[updateBytesWritten]] updateBytesWritten Method CAUTION: FIXME == [[initialize]] initialize Method CAUTION: FIXME == [[write-bytes]] Writing Bytes (From Byte Array Starting From Offset) [source, scala] \u00b6 write(kvBytes: Array[Byte], offs: Int, len: Int): Unit \u00b6 write...FIXME CAUTION: FIXME == [[recordWritten]] recordWritten Method CAUTION: FIXME == [[open]] Opening DiskBlockObjectWriter [source, scala] \u00b6 open(): DiskBlockObjectWriter \u00b6 open opens DiskBlockObjectWriter, i.e. < > and re-sets < > and < > internal output streams. Internally, open makes sure that DiskBlockObjectWriter is not closed (i.e. < > flag is disabled). If it was, open throws a IllegalStateException : Writer already closed. Cannot be reopened. Unless DiskBlockObjectWriter has already been initialized (i.e. < > flag is enabled), open < > it (and turns < > flag on). Regardless of whether DiskBlockObjectWriter was already initialized or not, open serializer:SerializerManager.md#wrapStream[requests SerializerManager to wrap mcs output stream for encryption and compression] (for < >) and sets it as < >. open requests the < > to serializer:SerializerInstance.md#serializeStream[serialize bs output stream] and sets it as < >. NOTE: open uses SerializerInstance that was specified when < > In the end, open turns < > flag on. NOTE: open is used exclusively when DiskBlockObjectWriter < > or < > but the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialized | [[initialized]] Internal flag...FIXME Used when...FIXME | hasBeenClosed | [[hasBeenClosed]] Internal flag...FIXME Used when...FIXME | mcs | [[mcs]] FIXME Used when...FIXME | bs | [[bs]] FIXME Used when...FIXME |===","title":"DiskBlockObjectWriter"},{"location":"storage/DiskBlockObjectWriter/#source-scala","text":"write( key: Any, value: Any): Unit write < > unless < > already. write requests the < > to serializer:SerializationStream.md#writeKey[write the key] and then the serializer:SerializationStream.md#writeValue[value]. In the end, write < >. write is used when: BypassMergeSortShuffleWriter is requested to shuffle:BypassMergeSortShuffleWriter.md#write[write records of a partition] ExternalAppendOnlyMap is requested to shuffle:ExternalAppendOnlyMap.md#spillMemoryIteratorToDisk[spillMemoryIteratorToDisk] ExternalSorter is requested to shuffle:ExternalSorter.md#writePartitionedFile[write all records into a partitioned file] ** SpillableIterator is requested to spill WritablePartitionedPairCollection is requested for a destructiveSortedWritablePartitionedIterator == [[commitAndGet]] commitAndGet Method","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#commitandget-filesegment","text":"commitAndGet...FIXME commitAndGet is used when...FIXME == [[close]] Committing Writes and Closing Resources","title":"commitAndGet(): FileSegment"},{"location":"storage/DiskBlockObjectWriter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#close-unit","text":"close...FIXME close is used when...FIXME == [[revertPartialWritesAndClose]] revertPartialWritesAndClose Method","title":"close(): Unit"},{"location":"storage/DiskBlockObjectWriter/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#revertpartialwritesandclose-file","text":"revertPartialWritesAndClose...FIXME revertPartialWritesAndClose is used when...FIXME == [[updateBytesWritten]] updateBytesWritten Method CAUTION: FIXME == [[initialize]] initialize Method CAUTION: FIXME == [[write-bytes]] Writing Bytes (From Byte Array Starting From Offset)","title":"revertPartialWritesAndClose(): File"},{"location":"storage/DiskBlockObjectWriter/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#writekvbytes-arraybyte-offs-int-len-int-unit","text":"write...FIXME CAUTION: FIXME == [[recordWritten]] recordWritten Method CAUTION: FIXME == [[open]] Opening DiskBlockObjectWriter","title":"write(kvBytes: Array[Byte], offs: Int, len: Int): Unit"},{"location":"storage/DiskBlockObjectWriter/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/DiskBlockObjectWriter/#open-diskblockobjectwriter","text":"open opens DiskBlockObjectWriter, i.e. < > and re-sets < > and < > internal output streams. Internally, open makes sure that DiskBlockObjectWriter is not closed (i.e. < > flag is disabled). If it was, open throws a IllegalStateException : Writer already closed. Cannot be reopened. Unless DiskBlockObjectWriter has already been initialized (i.e. < > flag is enabled), open < > it (and turns < > flag on). Regardless of whether DiskBlockObjectWriter was already initialized or not, open serializer:SerializerManager.md#wrapStream[requests SerializerManager to wrap mcs output stream for encryption and compression] (for < >) and sets it as < >. open requests the < > to serializer:SerializerInstance.md#serializeStream[serialize bs output stream] and sets it as < >. NOTE: open uses SerializerInstance that was specified when < > In the end, open turns < > flag on. NOTE: open is used exclusively when DiskBlockObjectWriter < > or < > but the < >. == [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | initialized | [[initialized]] Internal flag...FIXME Used when...FIXME | hasBeenClosed | [[hasBeenClosed]] Internal flag...FIXME Used when...FIXME | mcs | [[mcs]] FIXME Used when...FIXME | bs | [[bs]] FIXME Used when...FIXME |===","title":"open(): DiskBlockObjectWriter"},{"location":"storage/DiskStore/","text":"= DiskStore DiskStore manages data blocks on disk for storage:BlockManager.md#diskStore[BlockManager]. .DiskStore and BlockManager image::DiskStore-BlockManager.png[align=\"center\"] == [[creating-instance]] Creating Instance DiskStore takes the following to be created: [[conf]] ROOT:SparkConf.md[] [[diskManager]] storage:DiskBlockManager.md[] [[securityManager]] SecurityManager == [[getBytes]] getBytes Method [source,scala] \u00b6 getBytes( blockId: BlockId): BlockData getBytes...FIXME getBytes is used when BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes]. == [[blockSizes]] blockSizes Internal Registry [source, scala] \u00b6 blockSizes: ConcurrentHashMap[BlockId, Long] \u00b6 blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track storage:BlockId.md[]s by their size on disk. == [[contains]] Checking if Block File Exists [source, scala] \u00b6 contains( blockId: BlockId): Boolean contains requests the < > for the storage:DiskBlockManager.md#getFile[block file] by (the name of) the input storage:BlockId.md[] and check whether the file actually exists or not. contains is used when: BlockManager is requested to storage:BlockManager.md#getStatus[getStatus], storage:BlockManager.md#getCurrentBlockStatus[getCurrentBlockStatus], storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[put]] Writing Block to Disk [source, scala] \u00b6 put( blockId: BlockId)( writeFunc: WritableByteChannel => Unit): Unit put prints out the following DEBUG message to the logs: Attempting to put block [blockId] put requests the < > for the storage:DiskBlockManager.md#getFile[block file] for the input storage:BlockId.md[]. put < > (wrapped into a CountingWritableChannel to count the bytes written). put executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the < > internal registry. In the end, put prints out the following DEBUG message to the logs: Block [fileName] stored as [size] file on disk in [time] ms In case of any exception, put < >. put throws an IllegalStateException when the BlockId is already < >: Block [blockId] is already present in the disk store put is used when: BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator] and storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[putBytes]] putBytes Method [source, scala] \u00b6 putBytes( blockId: BlockId, bytes: ChunkedByteBuffer): Unit putBytes ...FIXME putBytes is used when BlockManager is requested to storage:BlockManager.md#doPutBytes[doPutBytes] and storage:BlockManager.md#dropFromMemory[dropFromMemory]. == [[remove]] Removing Block [source, scala] \u00b6 remove( blockId: BlockId): Boolean remove ...FIXME remove is used when: BlockManager is requested to storage:BlockManager.md#removeBlockInternal[removeBlockInternal] DiskStore is requested to < > (when an exception was thrown) == [[openForWrite]] Opening Block File For Writing [source, scala] \u00b6 openForWrite( file: File): WritableByteChannel openForWrite ...FIXME openForWrite is used when DiskStore is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskStore logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.DiskStore=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"DiskStore"},{"location":"storage/DiskStore/#sourcescala","text":"getBytes( blockId: BlockId): BlockData getBytes...FIXME getBytes is used when BlockManager is requested to storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes]. == [[blockSizes]] blockSizes Internal Registry","title":"[source,scala]"},{"location":"storage/DiskStore/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/DiskStore/#blocksizes-concurrenthashmapblockid-long","text":"blockSizes is a Java {java-javadoc-url}/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] that DiskStore uses to track storage:BlockId.md[]s by their size on disk. == [[contains]] Checking if Block File Exists","title":"blockSizes: ConcurrentHashMap[BlockId, Long]"},{"location":"storage/DiskStore/#source-scala_1","text":"contains( blockId: BlockId): Boolean contains requests the < > for the storage:DiskBlockManager.md#getFile[block file] by (the name of) the input storage:BlockId.md[] and check whether the file actually exists or not. contains is used when: BlockManager is requested to storage:BlockManager.md#getStatus[getStatus], storage:BlockManager.md#getCurrentBlockStatus[getCurrentBlockStatus], storage:BlockManager.md#getLocalValues[getLocalValues], storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[put]] Writing Block to Disk","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_2","text":"put( blockId: BlockId)( writeFunc: WritableByteChannel => Unit): Unit put prints out the following DEBUG message to the logs: Attempting to put block [blockId] put requests the < > for the storage:DiskBlockManager.md#getFile[block file] for the input storage:BlockId.md[]. put < > (wrapped into a CountingWritableChannel to count the bytes written). put executes the given writeFunc function with the WritableByteChannel of the block file and registers the bytes written to the < > internal registry. In the end, put prints out the following DEBUG message to the logs: Block [fileName] stored as [size] file on disk in [time] ms In case of any exception, put < >. put throws an IllegalStateException when the BlockId is already < >: Block [blockId] is already present in the disk store put is used when: BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator] and storage:BlockManager.md#dropFromMemory[dropFromMemory] DiskStore is requested to < > == [[putBytes]] putBytes Method","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_3","text":"putBytes( blockId: BlockId, bytes: ChunkedByteBuffer): Unit putBytes ...FIXME putBytes is used when BlockManager is requested to storage:BlockManager.md#doPutBytes[doPutBytes] and storage:BlockManager.md#dropFromMemory[dropFromMemory]. == [[remove]] Removing Block","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_4","text":"remove( blockId: BlockId): Boolean remove ...FIXME remove is used when: BlockManager is requested to storage:BlockManager.md#removeBlockInternal[removeBlockInternal] DiskStore is requested to < > (when an exception was thrown) == [[openForWrite]] Opening Block File For Writing","title":"[source, scala]"},{"location":"storage/DiskStore/#source-scala_5","text":"openForWrite( file: File): WritableByteChannel openForWrite ...FIXME openForWrite is used when DiskStore is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.DiskStore logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/DiskStore/#source","text":"","title":"[source]"},{"location":"storage/DiskStore/#log4jloggerorgapachesparkstoragediskstoreall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.storage.DiskStore=ALL"},{"location":"storage/ExternalShuffleClient/","text":"= ExternalShuffleClient ExternalShuffleClient is a storage:ShuffleClient.md[] that...FIXME == [[init]] Initializing ExternalShuffleClient [source,java] \u00b6 void init( String appId) init...FIXME init is part of the storage:ShuffleClient.md#init[ShuffleClient] abstraction. == [[registerWithShuffleServer]] Register Block Manager with Shuffle Server [source, java] \u00b6 void registerWithShuffleServer( String host, int port, String execId, ExecutorShuffleInfo executorInfo) registerWithShuffleServer...FIXME registerWithShuffleServer is used when...FIXME == [[fetchBlocks]] Fetching Blocks [source, java] \u00b6 void fetchBlocks( String host, int port, String execId, String[] blockIds, BlockFetchingListener listener, TempFileManager tempFileManager) fetchBlocks...FIXME fetchBlocks is part of storage:ShuffleClient.md#fetchBlocks[ShuffleClient] abstraction.","title":"ExternalShuffleClient"},{"location":"storage/ExternalShuffleClient/#sourcejava","text":"void init( String appId) init...FIXME init is part of the storage:ShuffleClient.md#init[ShuffleClient] abstraction. == [[registerWithShuffleServer]] Register Block Manager with Shuffle Server","title":"[source,java]"},{"location":"storage/ExternalShuffleClient/#source-java","text":"void registerWithShuffleServer( String host, int port, String execId, ExecutorShuffleInfo executorInfo) registerWithShuffleServer...FIXME registerWithShuffleServer is used when...FIXME == [[fetchBlocks]] Fetching Blocks","title":"[source, java]"},{"location":"storage/ExternalShuffleClient/#source-java_1","text":"void fetchBlocks( String host, int port, String execId, String[] blockIds, BlockFetchingListener listener, TempFileManager tempFileManager) fetchBlocks...FIXME fetchBlocks is part of storage:ShuffleClient.md#fetchBlocks[ShuffleClient] abstraction.","title":"[source, java]"},{"location":"storage/MemoryStore/","text":"= MemoryStore MemoryStore manages blocks of data in memory for storage:BlockManager.md#memoryStore[BlockManager]. .MemoryStore and BlockManager image::MemoryStore-BlockManager.png[align=\"center\"] == [[creating-instance]] Creating Instance MemoryStore takes the following to be created: [[conf]] ROOT:SparkConf.md[] < > [[serializerManager]] serializer:SerializerManager.md[] [[memoryManager]] memory:MemoryManager.md[] [[blockEvictionHandler]] storage:BlockEvictionHandler.md[] MemoryStore is created for storage:BlockManager.md#memoryStore[BlockManager]. .Creating MemoryStore image::spark-MemoryStore.png[align=\"center\"] == [[blockInfoManager]] BlockInfoManager MemoryStore is given a storage:BlockInfoManager.md[] when < >. MemoryStore uses the BlockInfoManager when requested to < >. == [[memoryStore]] Accessing MemoryStore MemoryStore is available using storage:BlockManager.md#memoryStore[BlockManager.memoryStore] reference to other Spark services. [source,scala] \u00b6 import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore == [[unrollMemoryThreshold]][[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Configuration Property MemoryStore uses ROOT:configuration-properties.md#spark.storage.unrollMemoryThreshold[spark.storage.unrollMemoryThreshold] configuration property for < > and < >. == [[releaseUnrollMemoryForThisTask]] releaseUnrollMemoryForThisTask Method [source, scala] \u00b6 releaseUnrollMemoryForThisTask( memoryMode: MemoryMode, memory: Long = Long.MaxValue): Unit releaseUnrollMemoryForThisTask...FIXME releaseUnrollMemoryForThisTask is used when: Task is requested to scheduler:Task.md#run[run] (and cleans up after itself) MemoryStore is requested to < > PartiallyUnrolledIterator is requested to releaseUnrollMemory PartiallySerializedBlock is requested to discard and finishWritingToStream == [[getValues]] getValues Method [source, scala] \u00b6 getValues( blockId: BlockId): Option[Iterator[_]] getValues...FIXME getValues is used when BlockManager is requested to storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[getBytes]] getBytes Method [source, scala] \u00b6 getBytes( blockId: BlockId): Option[ChunkedByteBuffer] getBytes...FIXME getBytes is used when BlockManager is requested to storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[putIteratorAsBytes]] putIteratorAsBytes Method [source, scala] \u00b6 putIteratorAsBytes T : Either[PartiallySerializedBlock[T], Long] putIteratorAsBytes...FIXME putIteratorAsBytes is used when BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator]. == [[remove]] Dropping Block from Memory [source, scala] \u00b6 remove( blockId: BlockId): Boolean remove removes the given storage:BlockId.md[] from the < > internal registry and branches off based on whether the < > or < >. === [[remove-block-removed]] Block Removed When found and removed, remove requests the < > to memory:MemoryManager.md#releaseStorageMemory[releaseStorageMemory] and prints out the following DEBUG message to the logs: [source,plaintext] \u00b6 Block [blockId] of size [size] dropped from memory (free [memory]) \u00b6 remove returns true . === [[remove-no-block]] No Block Removed If no BlockId was registered and removed, remove returns false . === [[remove-usage]] Usage remove is used when BlockManager is requested to storage:BlockManager.md#dropFromMemory[dropFromMemory] and storage:BlockManager.md#removeBlockInternal[removeBlockInternal]. == [[putBytes]] Acquiring Storage Memory for Blocks [source, scala] \u00b6 putBytes T: ClassTag : Boolean putBytes requests memory:MemoryManager.md#acquireStorageMemory[storage memory for blockId from MemoryManager ] and registers the block in < > internal registry. Internally, putBytes first makes sure that blockId block has not been registered already in < > internal registry. putBytes then requests memory:MemoryManager.md#acquireStorageMemory[ size memory for the blockId block in a given memoryMode from the current MemoryManager ]. [NOTE] \u00b6 memoryMode can be ON_HEAP or OFF_HEAP and is a property of a storage:StorageLevel.md[StorageLevel]. import org.apache.spark.storage.StorageLevel._ scala> MEMORY_AND_DISK.useOffHeap res0: Boolean = false scala> OFF_HEAP.useOffHeap res1: Boolean = true \u00b6 If successful, putBytes \"materializes\" _bytes byte buffer and makes sure that the size is exactly size . It then registers a SerializedMemoryEntry (for the bytes and memoryMode ) for blockId in the internal < > registry. You should see the following INFO message in the logs: Block [blockId] stored as bytes in memory (estimated size [size], free [bytes]) putBytes returns true only after blockId was successfully registered in the internal < > registry. putBytes is used when BlockManager is requested to storage:BlockManager.md#doPutBytes[doPutBytes] and storage:BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[evictBlocksToFreeSpace]] Evicting Blocks [source, scala] \u00b6 evictBlocksToFreeSpace( blockId: Option[BlockId], space: Long, memoryMode: MemoryMode): Long evictBlocksToFreeSpace...FIXME evictBlocksToFreeSpace is used when StorageMemoryPool is requested to memory:StorageMemoryPool.md#acquireMemory[acquireMemory] and memory:StorageMemoryPool.md#freeSpaceToShrinkPool[freeSpaceToShrinkPool]. == [[contains]] Checking Whether Block Exists In MemoryStore [source, scala] \u00b6 contains( blockId: BlockId): Boolean contains is positive ( true ) when the < > internal registry contains blockId key. contains is used when...FIXME == [[putIteratorAsValues]] putIteratorAsValues Method [source, scala] \u00b6 putIteratorAsValues T : Either[PartiallyUnrolledIterator[T], Long] putIteratorAsValues makes sure that the BlockId does not exist or throws an IllegalArgumentException : requirement failed: Block [blockId] is already present in the MemoryStore putIteratorAsValues < > (with the < > and ON_HEAP memory mode). CAUTION: FIXME putIteratorAsValues tries to put the blockId block in memory store as values . putIteratorAsValues is used when BlockManager is requested to store storage:BlockManager.md#doPutBytes[bytes] or storage:BlockManager.md#doPutIterator[values] of a block or when storage:BlockManager.md#maybeCacheDiskValuesInMemory[attempting to cache spilled values read from disk]. == [[reserveUnrollMemoryForThisTask]] reserveUnrollMemoryForThisTask Method [source, scala] \u00b6 reserveUnrollMemoryForThisTask( blockId: BlockId, memory: Long, memoryMode: MemoryMode): Boolean reserveUnrollMemoryForThisTask acquires a lock on < > and requests it to memory:MemoryManager.md#acquireUnrollMemory[acquireUnrollMemory]. NOTE: reserveUnrollMemoryForThisTask is used when MemoryStore is requested to < > and < >. == [[maxMemory]] Total Amount Of Memory Available For Storage [source, scala] \u00b6 maxMemory: Long \u00b6 maxMemory requests the < > for the current memory:MemoryManager.md#maxOnHeapStorageMemory[maxOnHeapStorageMemory] and memory:MemoryManager.md#maxOffHeapStorageMemory[maxOffHeapStorageMemory], and simply returns their sum. [TIP] \u00b6 Enable INFO < > to find the maxMemory in the logs when MemoryStore is < >: MemoryStore started with capacity [maxMemory] MB \u00b6 NOTE: maxMemory is used for < > purposes only. == [[putIterator]] putIterator Internal Method [source, scala] \u00b6 putIterator T : Either[Long, Long] putIterator...FIXME putIterator is used when MemoryStore is requested to < > and < >. == [[logUnrollFailureMessage]] logUnrollFailureMessage Internal Method [source, scala] \u00b6 logUnrollFailureMessage( blockId: BlockId, finalVectorSize: Long): Unit logUnrollFailureMessage...FIXME logUnrollFailureMessage is used when MemoryStore is requested to < >. == [[logMemoryUsage]] logMemoryUsage Internal Method [source, scala] \u00b6 logMemoryUsage(): Unit \u00b6 logMemoryUsage...FIXME logMemoryUsage is used when MemoryStore is requested to < >. == [[memoryUsed]] Total Memory Used [source, scala] \u00b6 memoryUsed: Long \u00b6 memoryUsed requests the < > for the memory:MemoryManager.md#storageMemoryUsed[storageMemoryUsed]. memoryUsed is used when MemoryStore is requested for < > and to < >. == [[blocksMemoryUsed]] Memory Used for Caching Blocks [source, scala] \u00b6 blocksMemoryUsed: Long \u00b6 blocksMemoryUsed is the < > without the < >. blocksMemoryUsed is used for logging purposes when MemoryStore is requested to < >, < >, < >, < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.memory.MemoryStore logger to see what happens inside. Add the following line to conf/log4j.properties : [source] \u00b6 log4j.logger.org.apache.spark.storage.memory.MemoryStore=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging]. == [[internal-registries]] Internal Registries === [[entries]] MemoryEntries by BlockId [source, scala] \u00b6 entries: LinkedHashMap[BlockId, MemoryEntry[_]] \u00b6 MemoryStore creates a Java {java-javadoc-url}/java/util/LinkedHashMap.html[LinkedHashMap] of MemoryEntries per storage:BlockId.md[] (with the initial capacity of 32 and the load factor of 0.75 ) when < >. entries uses access-order ordering mode where the order of iteration is the order in which the entries were last accessed (from least-recently accessed to most-recently). That gives LRU cache behaviour when MemoryStore is requested to < >.","title":"MemoryStore"},{"location":"storage/MemoryStore/#sourcescala","text":"import org.apache.spark.SparkEnv SparkEnv.get.blockManager.memoryStore == [[unrollMemoryThreshold]][[spark.storage.unrollMemoryThreshold]] spark.storage.unrollMemoryThreshold Configuration Property MemoryStore uses ROOT:configuration-properties.md#spark.storage.unrollMemoryThreshold[spark.storage.unrollMemoryThreshold] configuration property for < > and < >. == [[releaseUnrollMemoryForThisTask]] releaseUnrollMemoryForThisTask Method","title":"[source,scala]"},{"location":"storage/MemoryStore/#source-scala","text":"releaseUnrollMemoryForThisTask( memoryMode: MemoryMode, memory: Long = Long.MaxValue): Unit releaseUnrollMemoryForThisTask...FIXME releaseUnrollMemoryForThisTask is used when: Task is requested to scheduler:Task.md#run[run] (and cleans up after itself) MemoryStore is requested to < > PartiallyUnrolledIterator is requested to releaseUnrollMemory PartiallySerializedBlock is requested to discard and finishWritingToStream == [[getValues]] getValues Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_1","text":"getValues( blockId: BlockId): Option[Iterator[_]] getValues...FIXME getValues is used when BlockManager is requested to storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[getBytes]] getBytes Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_2","text":"getBytes( blockId: BlockId): Option[ChunkedByteBuffer] getBytes...FIXME getBytes is used when BlockManager is requested to storage:BlockManager.md#doGetLocalBytes[doGetLocalBytes], storage:BlockManager.md#getLocalValues[getLocalValues] and storage:BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[putIteratorAsBytes]] putIteratorAsBytes Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_3","text":"putIteratorAsBytes T : Either[PartiallySerializedBlock[T], Long] putIteratorAsBytes...FIXME putIteratorAsBytes is used when BlockManager is requested to storage:BlockManager.md#doPutIterator[doPutIterator]. == [[remove]] Dropping Block from Memory","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_4","text":"remove( blockId: BlockId): Boolean remove removes the given storage:BlockId.md[] from the < > internal registry and branches off based on whether the < > or < >. === [[remove-block-removed]] Block Removed When found and removed, remove requests the < > to memory:MemoryManager.md#releaseStorageMemory[releaseStorageMemory] and prints out the following DEBUG message to the logs:","title":"[source, scala]"},{"location":"storage/MemoryStore/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/MemoryStore/#block-blockid-of-size-size-dropped-from-memory-free-memory","text":"remove returns true . === [[remove-no-block]] No Block Removed If no BlockId was registered and removed, remove returns false . === [[remove-usage]] Usage remove is used when BlockManager is requested to storage:BlockManager.md#dropFromMemory[dropFromMemory] and storage:BlockManager.md#removeBlockInternal[removeBlockInternal]. == [[putBytes]] Acquiring Storage Memory for Blocks","title":"Block [blockId] of size [size] dropped from memory (free [memory])"},{"location":"storage/MemoryStore/#source-scala_5","text":"putBytes T: ClassTag : Boolean putBytes requests memory:MemoryManager.md#acquireStorageMemory[storage memory for blockId from MemoryManager ] and registers the block in < > internal registry. Internally, putBytes first makes sure that blockId block has not been registered already in < > internal registry. putBytes then requests memory:MemoryManager.md#acquireStorageMemory[ size memory for the blockId block in a given memoryMode from the current MemoryManager ].","title":"[source, scala]"},{"location":"storage/MemoryStore/#note","text":"memoryMode can be ON_HEAP or OFF_HEAP and is a property of a storage:StorageLevel.md[StorageLevel].","title":"[NOTE]"},{"location":"storage/MemoryStore/#import-orgapachesparkstoragestoragelevel_-scala-memory_and_diskuseoffheap-res0-boolean-false-scala-off_heapuseoffheap-res1-boolean-true","text":"If successful, putBytes \"materializes\" _bytes byte buffer and makes sure that the size is exactly size . It then registers a SerializedMemoryEntry (for the bytes and memoryMode ) for blockId in the internal < > registry. You should see the following INFO message in the logs: Block [blockId] stored as bytes in memory (estimated size [size], free [bytes]) putBytes returns true only after blockId was successfully registered in the internal < > registry. putBytes is used when BlockManager is requested to storage:BlockManager.md#doPutBytes[doPutBytes] and storage:BlockManager.md#maybeCacheDiskBytesInMemory[maybeCacheDiskBytesInMemory]. == [[evictBlocksToFreeSpace]] Evicting Blocks","title":"import org.apache.spark.storage.StorageLevel._\nscala&gt; MEMORY_AND_DISK.useOffHeap\nres0: Boolean = false\n\nscala&gt; OFF_HEAP.useOffHeap\nres1: Boolean = true\n"},{"location":"storage/MemoryStore/#source-scala_6","text":"evictBlocksToFreeSpace( blockId: Option[BlockId], space: Long, memoryMode: MemoryMode): Long evictBlocksToFreeSpace...FIXME evictBlocksToFreeSpace is used when StorageMemoryPool is requested to memory:StorageMemoryPool.md#acquireMemory[acquireMemory] and memory:StorageMemoryPool.md#freeSpaceToShrinkPool[freeSpaceToShrinkPool]. == [[contains]] Checking Whether Block Exists In MemoryStore","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_7","text":"contains( blockId: BlockId): Boolean contains is positive ( true ) when the < > internal registry contains blockId key. contains is used when...FIXME == [[putIteratorAsValues]] putIteratorAsValues Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_8","text":"putIteratorAsValues T : Either[PartiallyUnrolledIterator[T], Long] putIteratorAsValues makes sure that the BlockId does not exist or throws an IllegalArgumentException : requirement failed: Block [blockId] is already present in the MemoryStore putIteratorAsValues < > (with the < > and ON_HEAP memory mode). CAUTION: FIXME putIteratorAsValues tries to put the blockId block in memory store as values . putIteratorAsValues is used when BlockManager is requested to store storage:BlockManager.md#doPutBytes[bytes] or storage:BlockManager.md#doPutIterator[values] of a block or when storage:BlockManager.md#maybeCacheDiskValuesInMemory[attempting to cache spilled values read from disk]. == [[reserveUnrollMemoryForThisTask]] reserveUnrollMemoryForThisTask Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_9","text":"reserveUnrollMemoryForThisTask( blockId: BlockId, memory: Long, memoryMode: MemoryMode): Boolean reserveUnrollMemoryForThisTask acquires a lock on < > and requests it to memory:MemoryManager.md#acquireUnrollMemory[acquireUnrollMemory]. NOTE: reserveUnrollMemoryForThisTask is used when MemoryStore is requested to < > and < >. == [[maxMemory]] Total Amount Of Memory Available For Storage","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#maxmemory-long","text":"maxMemory requests the < > for the current memory:MemoryManager.md#maxOnHeapStorageMemory[maxOnHeapStorageMemory] and memory:MemoryManager.md#maxOffHeapStorageMemory[maxOffHeapStorageMemory], and simply returns their sum.","title":"maxMemory: Long"},{"location":"storage/MemoryStore/#tip","text":"Enable INFO < > to find the maxMemory in the logs when MemoryStore is < >:","title":"[TIP]"},{"location":"storage/MemoryStore/#memorystore-started-with-capacity-maxmemory-mb","text":"NOTE: maxMemory is used for < > purposes only. == [[putIterator]] putIterator Internal Method","title":"MemoryStore started with capacity [maxMemory] MB\n"},{"location":"storage/MemoryStore/#source-scala_11","text":"putIterator T : Either[Long, Long] putIterator...FIXME putIterator is used when MemoryStore is requested to < > and < >. == [[logUnrollFailureMessage]] logUnrollFailureMessage Internal Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_12","text":"logUnrollFailureMessage( blockId: BlockId, finalVectorSize: Long): Unit logUnrollFailureMessage...FIXME logUnrollFailureMessage is used when MemoryStore is requested to < >. == [[logMemoryUsage]] logMemoryUsage Internal Method","title":"[source, scala]"},{"location":"storage/MemoryStore/#source-scala_13","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#logmemoryusage-unit","text":"logMemoryUsage...FIXME logMemoryUsage is used when MemoryStore is requested to < >. == [[memoryUsed]] Total Memory Used","title":"logMemoryUsage(): Unit"},{"location":"storage/MemoryStore/#source-scala_14","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#memoryused-long","text":"memoryUsed requests the < > for the memory:MemoryManager.md#storageMemoryUsed[storageMemoryUsed]. memoryUsed is used when MemoryStore is requested for < > and to < >. == [[blocksMemoryUsed]] Memory Used for Caching Blocks","title":"memoryUsed: Long"},{"location":"storage/MemoryStore/#source-scala_15","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#blocksmemoryused-long","text":"blocksMemoryUsed is the < > without the < >. blocksMemoryUsed is used for logging purposes when MemoryStore is requested to < >, < >, < >, < > and < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.storage.memory.MemoryStore logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"blocksMemoryUsed: Long"},{"location":"storage/MemoryStore/#source","text":"","title":"[source]"},{"location":"storage/MemoryStore/#log4jloggerorgapachesparkstoragememorymemorystoreall","text":"Refer to ROOT:spark-logging.md[Logging]. == [[internal-registries]] Internal Registries === [[entries]] MemoryEntries by BlockId","title":"log4j.logger.org.apache.spark.storage.memory.MemoryStore=ALL"},{"location":"storage/MemoryStore/#source-scala_16","text":"","title":"[source, scala]"},{"location":"storage/MemoryStore/#entries-linkedhashmapblockid-memoryentry_","text":"MemoryStore creates a Java {java-javadoc-url}/java/util/LinkedHashMap.html[LinkedHashMap] of MemoryEntries per storage:BlockId.md[] (with the initial capacity of 32 and the load factor of 0.75 ) when < >. entries uses access-order ordering mode where the order of iteration is the order in which the entries were last accessed (from least-recently accessed to most-recently). That gives LRU cache behaviour when MemoryStore is requested to < >.","title":"entries: LinkedHashMap[BlockId, MemoryEntry[_]]"},{"location":"storage/NettyBlockRpcServer/","text":"= NettyBlockRpcServer NettyBlockRpcServer is an network:RpcHandler.md[] to handle < > for storage:NettyBlockTransferService.md[NettyBlockTransferService]. .NettyBlockRpcServer and NettyBlockTransferService image::NettyBlockRpcServer.png[align=\"center\"] == [[creating-instance]] Creating Instance NettyBlockRpcServer takes the following to be created: [[appId]] Application ID [[serializer]] serializer:Serializer.md[] [[blockManager]] storage:BlockDataManager.md[] NettyBlockRpcServer is created when NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#init[initialize]. == [[streamManager]] OneForOneStreamManager NettyBlockRpcServer uses a network:OneForOneStreamManager.md[] for...FIXME == [[receive]] Receiving RPC Messages [source, scala] \u00b6 receive( client: TransportClient, rpcMessage: ByteBuffer, responseContext: RpcResponseCallback): Unit receive...FIXME receive is part of network:RpcHandler.md#receive[RpcHandler] abstraction. == [[messages]] Messages === [[OpenBlocks]] OpenBlocks [source,java] \u00b6 OpenBlocks( String appId, String execId, String[] blockIds) When received, NettyBlockRpcServer requests the < > for storage:BlockDataManager.md#getBlockData[block data] for every block id in the message. The block data is a collection of network:ManagedBuffer.md[] for every block id in the incoming message. NettyBlockRpcServer then network:OneForOneStreamManager.md#registerStream[registers a stream of ManagedBuffer s (for the blocks) with the internal StreamManager ] under streamId . NOTE: The internal StreamManager is network:OneForOneStreamManager.md[OneForOneStreamManager] and is created when < >. You should see the following TRACE message in the logs: [source,plaintext] \u00b6 NettyBlockRpcServer: Registered streamId [streamId] with [size] buffers \u00b6 In the end, NettyBlockRpcServer responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized as a ByteBuffer . Posted when OneForOneBlockFetcher is requested to storage:OneForOneBlockFetcher.md#start[start]. === [[UploadBlock]] UploadBlock [source,java] \u00b6 UploadBlock( String appId, String execId, String blockId, byte[] metadata, byte[] blockData) When received, NettyBlockRpcServer deserializes the metadata of the input message to get the storage:StorageLevel.md[StorageLevel] and ClassTag of the block being uploaded. NettyBlockRpcServer creates a storage:BlockId.md[] for the block id and requests the < > to storage:BlockDataManager.md#putBlockData[store the block]. In the end, NettyBlockRpcServer responds with a 0 -capacity ByteBuffer . Posted when NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#uploadBlock[upload a block]. == [[receiveStream]] Receiving RPC Message with Streamed Data [source, scala] \u00b6 receiveStream( client: TransportClient, messageHeader: ByteBuffer, responseContext: RpcResponseCallback): StreamCallbackWithID receiveStream...FIXME receiveStream is part of network:RpcHandler.md#receive[RpcHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockRpcServer logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"NettyBlockRpcServer"},{"location":"storage/NettyBlockRpcServer/#source-scala","text":"receive( client: TransportClient, rpcMessage: ByteBuffer, responseContext: RpcResponseCallback): Unit receive...FIXME receive is part of network:RpcHandler.md#receive[RpcHandler] abstraction. == [[messages]] Messages === [[OpenBlocks]] OpenBlocks","title":"[source, scala]"},{"location":"storage/NettyBlockRpcServer/#sourcejava","text":"OpenBlocks( String appId, String execId, String[] blockIds) When received, NettyBlockRpcServer requests the < > for storage:BlockDataManager.md#getBlockData[block data] for every block id in the message. The block data is a collection of network:ManagedBuffer.md[] for every block id in the incoming message. NettyBlockRpcServer then network:OneForOneStreamManager.md#registerStream[registers a stream of ManagedBuffer s (for the blocks) with the internal StreamManager ] under streamId . NOTE: The internal StreamManager is network:OneForOneStreamManager.md[OneForOneStreamManager] and is created when < >. You should see the following TRACE message in the logs:","title":"[source,java]"},{"location":"storage/NettyBlockRpcServer/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/NettyBlockRpcServer/#nettyblockrpcserver-registered-streamid-streamid-with-size-buffers","text":"In the end, NettyBlockRpcServer responds with a StreamHandle (with the streamId and the number of blocks). The response is serialized as a ByteBuffer . Posted when OneForOneBlockFetcher is requested to storage:OneForOneBlockFetcher.md#start[start]. === [[UploadBlock]] UploadBlock","title":"NettyBlockRpcServer: Registered streamId [streamId]  with [size] buffers"},{"location":"storage/NettyBlockRpcServer/#sourcejava_1","text":"UploadBlock( String appId, String execId, String blockId, byte[] metadata, byte[] blockData) When received, NettyBlockRpcServer deserializes the metadata of the input message to get the storage:StorageLevel.md[StorageLevel] and ClassTag of the block being uploaded. NettyBlockRpcServer creates a storage:BlockId.md[] for the block id and requests the < > to storage:BlockDataManager.md#putBlockData[store the block]. In the end, NettyBlockRpcServer responds with a 0 -capacity ByteBuffer . Posted when NettyBlockTransferService is requested to storage:NettyBlockTransferService.md#uploadBlock[upload a block]. == [[receiveStream]] Receiving RPC Message with Streamed Data","title":"[source,java]"},{"location":"storage/NettyBlockRpcServer/#source-scala_1","text":"receiveStream( client: TransportClient, messageHeader: ByteBuffer, responseContext: RpcResponseCallback): StreamCallbackWithID receiveStream...FIXME receiveStream is part of network:RpcHandler.md#receive[RpcHandler] abstraction. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockRpcServer logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/NettyBlockRpcServer/#sourceplaintext_1","text":"","title":"[source,plaintext]"},{"location":"storage/NettyBlockRpcServer/#log4jloggerorgapachesparknetworknettynettyblockrpcserverall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.network.netty.NettyBlockRpcServer=ALL"},{"location":"storage/NettyBlockTransferService/","text":"= NettyBlockTransferService NettyBlockTransferService is a storage:BlockTransferService.md[] that uses Netty for < > or < > blocks of data. .NettyBlockTransferService, SparkEnv and BlockManager image::NettyBlockTransferService.png[align=\"center\"] == [[creating-instance]] Creating Instance NettyBlockTransferService takes the following to be created: [[conf]] ROOT:SparkConf.md[SparkConf] [[securityManager]] SecurityManager [[bindAddress]] Bind address [[hostName]] Host name [[_port]] Port number [[numCores]] Number of CPU cores NettyBlockTransferService is created when SparkEnv is core:SparkEnv.md#create-NettyBlockTransferService[created] for the driver and executors (and core:SparkEnv.md#create-BlockManager[creates the BlockManager]). == [[transportConf]][[transportContext]] TransportConf, TransportContext NettyBlockTransferService creates a network:TransportConf.md[] for shuffle module (using network:SparkTransportConf.md#fromSparkConf[SparkTransportConf] utility) when < >. NettyBlockTransferService uses the TransportConf for the following: Create a network:TransportContext.md[] when requested to < > Create a storage:OneForOneBlockFetcher.md[] and a core:RetryingBlockFetcher.md[RetryingBlockFetcher] when requested to < > NettyBlockTransferService uses the TransportContext to create the < > and the < >. == [[clientFactory]] TransportClientFactory NettyBlockTransferService creates a network:TransportClientFactory.md[] when requested to < >. NettyBlockTransferService uses the TransportClientFactory for the following: < > < > < > NettyBlockTransferService requests the TransportClientFactory to network:TransportClientFactory.md#close[close] when requested to < >. == [[server]] TransportServer NettyBlockTransferService < > when requested to < >. NettyBlockTransferService uses the TransportServer for the following: < > < > NettyBlockTransferService requests the TransportServer to network:TransportServer.md#close[close] when requested to < >. == [[port]] Port NettyBlockTransferService simply requests the < > for the network:TransportServer.md#getPort[port]. == [[shuffleMetrics]] Shuffle Metrics [source,scala] \u00b6 shuffleMetrics(): MetricSet \u00b6 shuffleMetrics...FIXME shuffleMetrics is part of the storage:ShuffleClient.md#shuffleMetrics[ShuffleClient] abstraction. == [[fetchBlocks]] Fetching Blocks [source, scala] \u00b6 fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener): Unit When executed, fetchBlocks prints out the following TRACE message in the logs: TRACE Fetch blocks from [host]:[port] (executor id [execId]) fetchBlocks then creates a RetryingBlockFetcher.BlockFetchStarter where createAndStart method...FIXME Depending on the maximum number of acceptable IO exceptions (such as connection timeouts) per request, if the number is greater than 0 , fetchBlocks creates a core:RetryingBlockFetcher.md#creating-instance[RetryingBlockFetcher] and core:RetryingBlockFetcher.md#start[starts] it immediately. NOTE: RetryingBlockFetcher is created with the RetryingBlockFetcher.BlockFetchStarter created earlier, the input blockIds and listener . If however the number of retries is not greater than 0 (it could be 0 or less), the RetryingBlockFetcher.BlockFetchStarter created earlier is started (with the input blockIds and listener ). In case of any Exception , you should see the following ERROR message in the logs and the input BlockFetchingListener gets notified (using onBlockFetchFailure for every block id). ERROR Exception while beginning fetchBlocks fetchBlocks is part of storage:BlockTransferService.md#fetchBlocks[BlockTransferService] abstraction. == [[appId]] Application Id == [[close]] Closing NettyBlockTransferService [source, scala] \u00b6 close(): Unit \u00b6 close...FIXME close is part of the storage:BlockTransferService.md#close[BlockTransferService] abstraction. == [[init]] Initializing NettyBlockTransferService [source, scala] \u00b6 init( blockDataManager: BlockDataManager): Unit init creates a storage:NettyBlockRpcServer.md[] (for the ROOT:SparkConf.md#getAppId[application id], a JavaSerializer and the given storage:BlockDataManager.md[BlockDataManager]) that is used to create a < >. init creates the internal clientFactory and a server. CAUTION: FIXME What's the \"a server\"? In the end, you should see the INFO message in the logs: Server created on [hostName]:[port] NOTE: hostname is given when core:SparkEnv.md#NettyBlockTransferService[NettyBlockTransferService is created] and is controlled by spark-driver.md#spark_driver_host[ spark.driver.host Spark property] for the driver and differs per deployment environment for executors (as controlled by executor:CoarseGrainedExecutorBackend.md#main[ --hostname for CoarseGrainedExecutorBackend ]). init is part of the storage:BlockTransferService.md#init[BlockTransferService] abstraction. == [[uploadBlock]] Uploading Block [source, scala] \u00b6 uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Internally, uploadBlock creates a TransportClient client to send a < UploadBlock message>> (to the input hostname and port ). NOTE: UploadBlock message is processed by storage:NettyBlockRpcServer.md[NettyBlockRpcServer]. The UploadBlock message holds the < >, the input execId and blockId . It also holds the serialized bytes for block metadata with level and classTag serialized (using the internal JavaSerializer ) as well as the serialized bytes for the input blockData itself (this time however the serialization uses storage:BlockDataManager.md#ManagedBuffer[ ManagedBuffer.nioByteBuffer method]). The entire UploadBlock message is further serialized before sending (using TransportClient.sendRpc ). CAUTION: FIXME Describe TransportClient and clientFactory.createClient . When blockId block was successfully uploaded, you should see the following TRACE message in the logs: TRACE NettyBlockTransferService: Successfully uploaded block [blockId] When an upload failed, you should see the following ERROR message in the logs: ERROR Error while uploading block [blockId] uploadBlock is part of the storage:BlockTransferService.md#uploadBlock[BlockTransferService] abstraction. == [[UploadBlock]] UploadBlock Message UploadBlock is a BlockTransferMessage that describes a block being uploaded, i.e. send over the wire from a < > to a storage:NettyBlockRpcServer.md#UploadBlock[NettyBlockRpcServer]. . UploadBlock Attributes [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Attribute | Description | appId | The application id (the block belongs to) | execId | The executor id | blockId | The block id | metadata | | blockData | The block data as an array of bytes |=== As an Encodable , UploadBlock can calculate the encoded size and do encoding and decoding itself to or from a ByteBuf , respectively. == [[createServer]] createServer Internal Method [source, scala] \u00b6 createServer( bootstraps: List[TransportServerBootstrap]): TransportServer createServer...FIXME createServer is used when NettyBlockTransferService is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockTransferService logger to see what happens inside. Add the following line to conf/log4j.properties : [source,plaintext] \u00b6 log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=ALL \u00b6 Refer to ROOT:spark-logging.md[Logging].","title":"NettyBlockTransferService"},{"location":"storage/NettyBlockTransferService/#sourcescala","text":"","title":"[source,scala]"},{"location":"storage/NettyBlockTransferService/#shufflemetrics-metricset","text":"shuffleMetrics...FIXME shuffleMetrics is part of the storage:ShuffleClient.md#shuffleMetrics[ShuffleClient] abstraction. == [[fetchBlocks]] Fetching Blocks","title":"shuffleMetrics(): MetricSet"},{"location":"storage/NettyBlockTransferService/#source-scala","text":"fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener): Unit When executed, fetchBlocks prints out the following TRACE message in the logs: TRACE Fetch blocks from [host]:[port] (executor id [execId]) fetchBlocks then creates a RetryingBlockFetcher.BlockFetchStarter where createAndStart method...FIXME Depending on the maximum number of acceptable IO exceptions (such as connection timeouts) per request, if the number is greater than 0 , fetchBlocks creates a core:RetryingBlockFetcher.md#creating-instance[RetryingBlockFetcher] and core:RetryingBlockFetcher.md#start[starts] it immediately. NOTE: RetryingBlockFetcher is created with the RetryingBlockFetcher.BlockFetchStarter created earlier, the input blockIds and listener . If however the number of retries is not greater than 0 (it could be 0 or less), the RetryingBlockFetcher.BlockFetchStarter created earlier is started (with the input blockIds and listener ). In case of any Exception , you should see the following ERROR message in the logs and the input BlockFetchingListener gets notified (using onBlockFetchFailure for every block id). ERROR Exception while beginning fetchBlocks fetchBlocks is part of storage:BlockTransferService.md#fetchBlocks[BlockTransferService] abstraction. == [[appId]] Application Id == [[close]] Closing NettyBlockTransferService","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#close-unit","text":"close...FIXME close is part of the storage:BlockTransferService.md#close[BlockTransferService] abstraction. == [[init]] Initializing NettyBlockTransferService","title":"close(): Unit"},{"location":"storage/NettyBlockTransferService/#source-scala_2","text":"init( blockDataManager: BlockDataManager): Unit init creates a storage:NettyBlockRpcServer.md[] (for the ROOT:SparkConf.md#getAppId[application id], a JavaSerializer and the given storage:BlockDataManager.md[BlockDataManager]) that is used to create a < >. init creates the internal clientFactory and a server. CAUTION: FIXME What's the \"a server\"? In the end, you should see the INFO message in the logs: Server created on [hostName]:[port] NOTE: hostname is given when core:SparkEnv.md#NettyBlockTransferService[NettyBlockTransferService is created] and is controlled by spark-driver.md#spark_driver_host[ spark.driver.host Spark property] for the driver and differs per deployment environment for executors (as controlled by executor:CoarseGrainedExecutorBackend.md#main[ --hostname for CoarseGrainedExecutorBackend ]). init is part of the storage:BlockTransferService.md#init[BlockTransferService] abstraction. == [[uploadBlock]] Uploading Block","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#source-scala_3","text":"uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel, classTag: ClassTag[_]): Future[Unit] Internally, uploadBlock creates a TransportClient client to send a < UploadBlock message>> (to the input hostname and port ). NOTE: UploadBlock message is processed by storage:NettyBlockRpcServer.md[NettyBlockRpcServer]. The UploadBlock message holds the < >, the input execId and blockId . It also holds the serialized bytes for block metadata with level and classTag serialized (using the internal JavaSerializer ) as well as the serialized bytes for the input blockData itself (this time however the serialization uses storage:BlockDataManager.md#ManagedBuffer[ ManagedBuffer.nioByteBuffer method]). The entire UploadBlock message is further serialized before sending (using TransportClient.sendRpc ). CAUTION: FIXME Describe TransportClient and clientFactory.createClient . When blockId block was successfully uploaded, you should see the following TRACE message in the logs: TRACE NettyBlockTransferService: Successfully uploaded block [blockId] When an upload failed, you should see the following ERROR message in the logs: ERROR Error while uploading block [blockId] uploadBlock is part of the storage:BlockTransferService.md#uploadBlock[BlockTransferService] abstraction. == [[UploadBlock]] UploadBlock Message UploadBlock is a BlockTransferMessage that describes a block being uploaded, i.e. send over the wire from a < > to a storage:NettyBlockRpcServer.md#UploadBlock[NettyBlockRpcServer]. . UploadBlock Attributes [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Attribute | Description | appId | The application id (the block belongs to) | execId | The executor id | blockId | The block id | metadata | | blockData | The block data as an array of bytes |=== As an Encodable , UploadBlock can calculate the encoded size and do encoding and decoding itself to or from a ByteBuf , respectively. == [[createServer]] createServer Internal Method","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#source-scala_4","text":"createServer( bootstraps: List[TransportServerBootstrap]): TransportServer createServer...FIXME createServer is used when NettyBlockTransferService is requested to < >. == [[logging]] Logging Enable ALL logging level for org.apache.spark.network.netty.NettyBlockTransferService logger to see what happens inside. Add the following line to conf/log4j.properties :","title":"[source, scala]"},{"location":"storage/NettyBlockTransferService/#sourceplaintext","text":"","title":"[source,plaintext]"},{"location":"storage/NettyBlockTransferService/#log4jloggerorgapachesparknetworknettynettyblocktransferserviceall","text":"Refer to ROOT:spark-logging.md[Logging].","title":"log4j.logger.org.apache.spark.network.netty.NettyBlockTransferService=ALL"},{"location":"storage/OneForOneBlockFetcher/","text":"= OneForOneBlockFetcher OneForOneBlockFetcher is...FIXME == [[creating-instance]] Creating Instance OneForOneBlockFetcher takes the following to be created: [[client]] TransportClient [[appId]] Application ID [[execId]] Executor ID [[blockIds]] Block IDs [[listener]] core:BlockFetchingListener.md[] [[transportConf]] network:TransportConf.md[] [[downloadFileManager]] DownloadFileManager OneForOneBlockFetcher is created when storage:NettyBlockTransferService.md#fetchBlocks[NettyBlockTransferService] and storage:ExternalShuffleClient.md#fetchBlocks[ExternalShuffleClient] are requested to fetch blocks. == [[openMessage]] OpenBlocks Message OneForOneBlockFetcher creates a OpenBlocks message (for the given < >, < > and < >) when < >. The OpenBlocks message is posted when OneForOneBlockFetcher is requested to < >. == [[start]] start Method [source,java] \u00b6 void start() \u00b6 start...FIXME start is used when storage:NettyBlockTransferService.md#fetchBlocks[NettyBlockTransferService] and storage:ExternalShuffleClient.md#fetchBlocks[ExternalShuffleClient] are requested to fetch blocks.","title":"OneForOneBlockFetcher"},{"location":"storage/OneForOneBlockFetcher/#sourcejava","text":"","title":"[source,java]"},{"location":"storage/OneForOneBlockFetcher/#void-start","text":"start...FIXME start is used when storage:NettyBlockTransferService.md#fetchBlocks[NettyBlockTransferService] and storage:ExternalShuffleClient.md#fetchBlocks[ExternalShuffleClient] are requested to fetch blocks.","title":"void start()"},{"location":"storage/RDDInfo/","text":"= RDDInfo RDDInfo is...FIXME","title":"RDDInfo"},{"location":"storage/ShuffleBlockFetcherIterator/","text":"= ShuffleBlockFetcherIterator ShuffleBlockFetcherIterator is a Scala http://www.scala-lang.org/api/current/scala/collection/Iterator.html[Iterator ] that fetches shuffle blocks (aka shuffle map outputs ) from block managers. ShuffleBlockFetcherIterator is < > exclusively when BlockStoreShuffleReader is requested to shuffle:BlockStoreShuffleReader.md#read[read combined key-value records for a reduce task]. ShuffleBlockFetcherIterator allows for < > as (BlockId, InputStream) pairs so a caller can handle shuffle blocks in a pipelined fashion as they are received. ShuffleBlockFetcherIterator is exhausted (i.e. < >) when the < > is at least the < >. ShuffleBlockFetcherIterator < > to avoid consuming too much memory. [[internal-registries]] .ShuffleBlockFetcherIterator's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | numBlocksProcessed | [[numBlocksProcessed]] The number of blocks < >. | numBlocksToFetch a| [[numBlocksToFetch]] Total number of blocks to < >. ShuffleBlockFetcherIterator can < > up to numBlocksToFetch elements. numBlocksToFetch is increased every time ShuffleBlockFetcherIterator is requested to < > that prints it out as the INFO message to the logs: Getting [numBlocksToFetch] non-empty blocks out of [totalBlocks] blocks | [[results]] results | Internal FIFO blocking queue (using Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html[java.util.concurrent.LinkedBlockingQueue ]) to hold FetchResult remote and local fetch results. Used in: < > to take one FetchResult off the queue, < > to put SuccessFetchResult or FailureFetchResult remote fetch results (as part of BlockFetchingListener callback), < > (similarly to < >) to put local fetch results, < > to release managed buffers for SuccessFetchResult results. | [[maxBytesInFlight]] maxBytesInFlight | The maximum size (in bytes) of all the remote shuffle blocks to fetch. Set when < >. | [[maxReqsInFlight]] maxReqsInFlight | The maximum number of remote requests to fetch shuffle blocks. Set when < >. | [[bytesInFlight]] bytesInFlight | The bytes of fetched remote shuffle blocks in flight Starts at 0 when < >. Incremented every < > and decremented every < >. ShuffleBlockFetcherIterator makes sure that the invariant of bytesInFlight below < > holds every < >. | [[reqsInFlight]] reqsInFlight | The number of remote shuffle block fetch requests in flight. Starts at 0 when < >. Incremented every < > and decremented every < >. ShuffleBlockFetcherIterator makes sure that the invariant of reqsInFlight below < > holds every < >. | [[isZombie]] isZombie | Flag whether ShuffleBlockFetcherIterator is still active. It is disabled, i.e. false , when < >. < > (when the task using ShuffleBlockFetcherIterator finishes), the < > (registered in sendRequest ) will no longer add fetched remote shuffle blocks into < > internal queue. | [[currentResult]] currentResult | The currently-processed SuccessFetchResult Set when ShuffleBlockFetcherIterator < (BlockId, InputStream) tuple>> and < > (on < >). |=== [TIP] \u00b6 Enable ERROR , WARN , INFO , DEBUG or TRACE logging levels for org.apache.spark.storage.ShuffleBlockFetcherIterator logger to see what happens in ShuffleBlockFetcherIterator. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE Refer to spark-logging.md[Logging]. \u00b6 == [[fetchUpToMaxBytes]] fetchUpToMaxBytes Method CAUTION: FIXME == [[creating-instance]] Creating ShuffleBlockFetcherIterator Instance When created, ShuffleBlockFetcherIterator takes the following: [[context]] spark-TaskContext.md[TaskContext] [[shuffleClient]] storage:ShuffleClient.md[] [[blockManager]] storage:BlockManager.md[BlockManager] [[blocksByAddress]] Blocks to fetch per storage:BlockManager.md[BlockManager] (as Seq[(BlockManagerId, Seq[(BlockId, Long)])] ) [[streamWrapper]] Function to wrap the returned input stream (as (BlockId, InputStream) => InputStream ) < > -- the maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task (controlled by shuffle:BlockStoreShuffleReader.md#spark_reducer_maxSizeInFlight[spark.reducer.maxSizeInFlight] Spark property) < > -- the maximum number of remote requests to fetch blocks at any given point (controlled by shuffle:BlockStoreShuffleReader.md#spark_reducer_maxReqsInFlight[spark.reducer.maxReqsInFlight] Spark property) [[maxBlocksInFlightPerAddress]] maxBlocksInFlightPerAddress [[maxReqSizeShuffleToMem]] maxReqSizeShuffleToMem [[detectCorrupt]] detectCorrupt flag to detect any corruption in fetched blocks (controlled by shuffle:BlockStoreShuffleReader.md#spark_shuffle_detectCorrupt[spark.shuffle.detectCorrupt] Spark property) == [[initialize]] Initializing ShuffleBlockFetcherIterator -- initialize Internal Method [source, scala] \u00b6 initialize(): Unit \u00b6 initialize registers a task cleanup and fetches shuffle blocks from remote and local storage:BlockManager.md[BlockManagers]. Internally, initialize spark-TaskContext.md#addTaskCompletionListener[registers a TaskCompletionListener ] (that will < > right after the task finishes). initialize < >. initialize < fetchRequests internal registry)>>. As ShuffleBlockFetcherIterator is in initialization phase, initialize makes sure that < > and < > internal counters are both 0 . Otherwise, initialize throws an exception. initialize < > (from remote storage:BlockManager.md[BlockManagers]). You should see the following INFO message in the logs: INFO ShuffleBlockFetcherIterator: Started [numFetches] remote fetches in [time] ms initialize < >. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Got local blocks in [time] ms NOTE: initialize is used exclusively when ShuffleBlockFetcherIterator is < >. == [[sendRequest]] Sending Remote Shuffle Block Fetch Request -- sendRequest Internal Method [source, scala] \u00b6 sendRequest(req: FetchRequest): Unit \u00b6 Internally, when sendRequest runs, you should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Sending request for [blocks.size] blocks ([size] B) from [hostPort] sendRequest increments < > and < > internal counters. NOTE: The input FetchRequest contains the remote storage:BlockManagerId.md[] address and the shuffle blocks to fetch (as a sequence of storage:BlockId.md[] and their sizes). sendRequest storage:ShuffleClient.md#fetchBlocks[requests ShuffleClient to fetch shuffle blocks] (from the host, the port, and the executor as defined in the input FetchRequest ). NOTE: ShuffleClient was defined when < >. sendRequest registers a BlockFetchingListener with ShuffleClient that: < > adds it as SuccessFetchResult to < > internal queue. < > adds it as FailureFetchResult to < > internal queue. NOTE: sendRequest is used exclusively when ShuffleBlockFetcherIterator is requested to < >. === [[sendRequest-BlockFetchingListener-onBlockFetchSuccess]] onBlockFetchSuccess Callback [source, scala] \u00b6 onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit \u00b6 Internally, onBlockFetchSuccess checks if the < > and does the further processing if it is not. onBlockFetchSuccess marks the input blockId as received (i.e. removes it from all the blocks to fetch as requested in < >). onBlockFetchSuccess adds the managed buf (as SuccessFetchResult ) to < > internal queue. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: remainingBlocks: [blocks] Regardless of zombie state of ShuffleBlockFetcherIterator, you should see the following TRACE message in the logs: TRACE ShuffleBlockFetcherIterator: Got remote block [blockId] after [time] ms === [[sendRequest-BlockFetchingListener-onBlockFetchFailure]] onBlockFetchFailure Callback [source, scala] \u00b6 onBlockFetchFailure(blockId: String, e: Throwable): Unit \u00b6 When onBlockFetchFailure is called, you should see the following ERROR message in the logs: ERROR ShuffleBlockFetcherIterator: Failed to get block(s) from [hostPort] onBlockFetchFailure adds the block (as FailureFetchResult ) to < > internal queue. == [[throwFetchFailedException]] Throwing FetchFailedException (for ShuffleBlockId) -- throwFetchFailedException Internal Method [source, scala] \u00b6 throwFetchFailedException( blockId: BlockId, address: BlockManagerId, e: Throwable): Nothing throwFetchFailedException throws a shuffle:FetchFailedException.md[FetchFailedException] when the input blockId is a ShuffleBlockId . NOTE: throwFetchFailedException creates a FetchFailedException passing on the root cause of a failure, i.e. the input e . Otherwise, throwFetchFailedException throws a SparkException : Failed to get block [blockId], which is not a shuffle block NOTE: throwFetchFailedException is used when ShuffleBlockFetcherIterator is requested for the < >. == [[cleanup]] Releasing Resources -- cleanup Internal Method [source, scala] \u00b6 cleanup(): Unit \u00b6 Internally, cleanup marks ShuffleBlockFetcherIterator a < >. cleanup < >. cleanup iterates over < > internal queue and for every SuccessFetchResult , increments remote bytes read and blocks fetched shuffle task metrics, and eventually releases the managed buffer. NOTE: cleanup is used when < >. == [[releaseCurrentResultBuffer]] Decrementing Reference Count Of and Releasing Result Buffer (for SuccessFetchResult) -- releaseCurrentResultBuffer Internal Method [source, scala] \u00b6 releaseCurrentResultBuffer(): Unit \u00b6 releaseCurrentResultBuffer decrements the < SuccessFetchResult reference>>'s buffer reference count if there is any. releaseCurrentResultBuffer releases < >. NOTE: releaseCurrentResultBuffer is used when < > and BufferReleasingInputStream closes. == [[fetchLocalBlocks]] fetchLocalBlocks Internal Method [source, scala] \u00b6 fetchLocalBlocks(): Unit \u00b6 fetchLocalBlocks ...FIXME NOTE: fetchLocalBlocks is used when...FIXME == [[hasNext]] hasNext Method [source, scala] \u00b6 hasNext: Boolean \u00b6 NOTE: hasNext is part of Scala's ++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext is positive ( true ) when < > is less than < >. Otherwise, hasNext is negative ( false ). == [[splitLocalRemoteBlocks]] splitLocalRemoteBlocks Internal Method [source, scala] \u00b6 splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest] \u00b6 splitLocalRemoteBlocks ...FIXME NOTE: splitLocalRemoteBlocks is used exclusively when ShuffleBlockFetcherIterator is requested to < >. == [[next]] Retrieving Next Element -- next Method [source, scala] \u00b6 next(): (BlockId, InputStream) \u00b6 NOTE: next is part of Scala's ++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#next():A++[Iterator Contract] to produce the next element of this iterator. next ...FIXME","title":"ShuffleBlockFetcherIterator"},{"location":"storage/ShuffleBlockFetcherIterator/#tip","text":"Enable ERROR , WARN , INFO , DEBUG or TRACE logging levels for org.apache.spark.storage.ShuffleBlockFetcherIterator logger to see what happens in ShuffleBlockFetcherIterator. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.storage.ShuffleBlockFetcherIterator=TRACE","title":"[TIP]"},{"location":"storage/ShuffleBlockFetcherIterator/#refer-to-spark-loggingmdlogging","text":"== [[fetchUpToMaxBytes]] fetchUpToMaxBytes Method CAUTION: FIXME == [[creating-instance]] Creating ShuffleBlockFetcherIterator Instance When created, ShuffleBlockFetcherIterator takes the following: [[context]] spark-TaskContext.md[TaskContext] [[shuffleClient]] storage:ShuffleClient.md[] [[blockManager]] storage:BlockManager.md[BlockManager] [[blocksByAddress]] Blocks to fetch per storage:BlockManager.md[BlockManager] (as Seq[(BlockManagerId, Seq[(BlockId, Long)])] ) [[streamWrapper]] Function to wrap the returned input stream (as (BlockId, InputStream) => InputStream ) < > -- the maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task (controlled by shuffle:BlockStoreShuffleReader.md#spark_reducer_maxSizeInFlight[spark.reducer.maxSizeInFlight] Spark property) < > -- the maximum number of remote requests to fetch blocks at any given point (controlled by shuffle:BlockStoreShuffleReader.md#spark_reducer_maxReqsInFlight[spark.reducer.maxReqsInFlight] Spark property) [[maxBlocksInFlightPerAddress]] maxBlocksInFlightPerAddress [[maxReqSizeShuffleToMem]] maxReqSizeShuffleToMem [[detectCorrupt]] detectCorrupt flag to detect any corruption in fetched blocks (controlled by shuffle:BlockStoreShuffleReader.md#spark_shuffle_detectCorrupt[spark.shuffle.detectCorrupt] Spark property) == [[initialize]] Initializing ShuffleBlockFetcherIterator -- initialize Internal Method","title":"Refer to spark-logging.md[Logging]."},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#initialize-unit","text":"initialize registers a task cleanup and fetches shuffle blocks from remote and local storage:BlockManager.md[BlockManagers]. Internally, initialize spark-TaskContext.md#addTaskCompletionListener[registers a TaskCompletionListener ] (that will < > right after the task finishes). initialize < >. initialize < fetchRequests internal registry)>>. As ShuffleBlockFetcherIterator is in initialization phase, initialize makes sure that < > and < > internal counters are both 0 . Otherwise, initialize throws an exception. initialize < > (from remote storage:BlockManager.md[BlockManagers]). You should see the following INFO message in the logs: INFO ShuffleBlockFetcherIterator: Started [numFetches] remote fetches in [time] ms initialize < >. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Got local blocks in [time] ms NOTE: initialize is used exclusively when ShuffleBlockFetcherIterator is < >. == [[sendRequest]] Sending Remote Shuffle Block Fetch Request -- sendRequest Internal Method","title":"initialize(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#sendrequestreq-fetchrequest-unit","text":"Internally, when sendRequest runs, you should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: Sending request for [blocks.size] blocks ([size] B) from [hostPort] sendRequest increments < > and < > internal counters. NOTE: The input FetchRequest contains the remote storage:BlockManagerId.md[] address and the shuffle blocks to fetch (as a sequence of storage:BlockId.md[] and their sizes). sendRequest storage:ShuffleClient.md#fetchBlocks[requests ShuffleClient to fetch shuffle blocks] (from the host, the port, and the executor as defined in the input FetchRequest ). NOTE: ShuffleClient was defined when < >. sendRequest registers a BlockFetchingListener with ShuffleClient that: < > adds it as SuccessFetchResult to < > internal queue. < > adds it as FailureFetchResult to < > internal queue. NOTE: sendRequest is used exclusively when ShuffleBlockFetcherIterator is requested to < >. === [[sendRequest-BlockFetchingListener-onBlockFetchSuccess]] onBlockFetchSuccess Callback","title":"sendRequest(req: FetchRequest): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#onblockfetchsuccessblockid-string-buf-managedbuffer-unit","text":"Internally, onBlockFetchSuccess checks if the < > and does the further processing if it is not. onBlockFetchSuccess marks the input blockId as received (i.e. removes it from all the blocks to fetch as requested in < >). onBlockFetchSuccess adds the managed buf (as SuccessFetchResult ) to < > internal queue. You should see the following DEBUG message in the logs: DEBUG ShuffleBlockFetcherIterator: remainingBlocks: [blocks] Regardless of zombie state of ShuffleBlockFetcherIterator, you should see the following TRACE message in the logs: TRACE ShuffleBlockFetcherIterator: Got remote block [blockId] after [time] ms === [[sendRequest-BlockFetchingListener-onBlockFetchFailure]] onBlockFetchFailure Callback","title":"onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#onblockfetchfailureblockid-string-e-throwable-unit","text":"When onBlockFetchFailure is called, you should see the following ERROR message in the logs: ERROR ShuffleBlockFetcherIterator: Failed to get block(s) from [hostPort] onBlockFetchFailure adds the block (as FailureFetchResult ) to < > internal queue. == [[throwFetchFailedException]] Throwing FetchFailedException (for ShuffleBlockId) -- throwFetchFailedException Internal Method","title":"onBlockFetchFailure(blockId: String, e: Throwable): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_4","text":"throwFetchFailedException( blockId: BlockId, address: BlockManagerId, e: Throwable): Nothing throwFetchFailedException throws a shuffle:FetchFailedException.md[FetchFailedException] when the input blockId is a ShuffleBlockId . NOTE: throwFetchFailedException creates a FetchFailedException passing on the root cause of a failure, i.e. the input e . Otherwise, throwFetchFailedException throws a SparkException : Failed to get block [blockId], which is not a shuffle block NOTE: throwFetchFailedException is used when ShuffleBlockFetcherIterator is requested for the < >. == [[cleanup]] Releasing Resources -- cleanup Internal Method","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_5","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#cleanup-unit","text":"Internally, cleanup marks ShuffleBlockFetcherIterator a < >. cleanup < >. cleanup iterates over < > internal queue and for every SuccessFetchResult , increments remote bytes read and blocks fetched shuffle task metrics, and eventually releases the managed buffer. NOTE: cleanup is used when < >. == [[releaseCurrentResultBuffer]] Decrementing Reference Count Of and Releasing Result Buffer (for SuccessFetchResult) -- releaseCurrentResultBuffer Internal Method","title":"cleanup(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_6","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#releasecurrentresultbuffer-unit","text":"releaseCurrentResultBuffer decrements the < SuccessFetchResult reference>>'s buffer reference count if there is any. releaseCurrentResultBuffer releases < >. NOTE: releaseCurrentResultBuffer is used when < > and BufferReleasingInputStream closes. == [[fetchLocalBlocks]] fetchLocalBlocks Internal Method","title":"releaseCurrentResultBuffer(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_7","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#fetchlocalblocks-unit","text":"fetchLocalBlocks ...FIXME NOTE: fetchLocalBlocks is used when...FIXME == [[hasNext]] hasNext Method","title":"fetchLocalBlocks(): Unit"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_8","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#hasnext-boolean","text":"NOTE: hasNext is part of Scala's ++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#hasNext:Boolean++[Iterator Contract] to test whether this iterator can provide another element. hasNext is positive ( true ) when < > is less than < >. Otherwise, hasNext is negative ( false ). == [[splitLocalRemoteBlocks]] splitLocalRemoteBlocks Internal Method","title":"hasNext: Boolean"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_9","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#splitlocalremoteblocks-arraybufferfetchrequest","text":"splitLocalRemoteBlocks ...FIXME NOTE: splitLocalRemoteBlocks is used exclusively when ShuffleBlockFetcherIterator is requested to < >. == [[next]] Retrieving Next Element -- next Method","title":"splitLocalRemoteBlocks(): ArrayBuffer[FetchRequest]"},{"location":"storage/ShuffleBlockFetcherIterator/#source-scala_10","text":"","title":"[source, scala]"},{"location":"storage/ShuffleBlockFetcherIterator/#next-blockid-inputstream","text":"NOTE: next is part of Scala's ++ https://www.scala-lang.org/api/current/scala/collection/Iterator.html#next():A++[Iterator Contract] to produce the next element of this iterator. next ...FIXME","title":"next(): (BlockId, InputStream)"},{"location":"storage/ShuffleClient/","text":"= ShuffleClient ShuffleClient is the < > of < > that can < >. ShuffleClient can optionally be < > with an appId (that actually does nothing by default) ShuffleClient has < > that are used when BlockManager is requested for a storage:BlockManager.md#shuffleMetricsSource[shuffle-related Spark metrics source] (only when Executor is executor:Executor.md#creating-instance[created] for a non-local / cluster mode). [[contract]] [source, java] package org.apache.spark.network.shuffle; abstract class ShuffleClient implements Closeable { // only required methods that have no implementation // the others follow abstract void fetchBlocks( String host, int port, String execId, String[] blockIds, BlockFetchingListener listener, TempFileManager tempFileManager); } .(Subset of) ShuffleClient Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | fetchBlocks | [[fetchBlocks]] Fetches a sequence of blocks from a remote block manager node asynchronously Used exclusively when ShuffleBlockFetcherIterator is requested to storage:ShuffleBlockFetcherIterator.md#sendRequest[sendRequest] |=== [[implementations]] .ShuffleClients [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ShuffleClient | Description | storage:BlockTransferService.md[] | [[BlockTransferService]] | storage:ExternalShuffleClient.md[] | [[ExternalShuffleClient]] |=== == [[init]] init Method [source, java] \u00b6 void init( String appId) init does nothing by default. [NOTE] \u00b6 init is used when: BlockManager is requested to storage:BlockManager.md#initialize[initialize] * Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to registered \u00b6 == [[shuffleMetrics]] Shuffle Metrics [source, java] \u00b6 MetricSet shuffleMetrics() \u00b6 shuffleMetrics returns an empty Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricSet.html[MetricSet ] by default. NOTE: shuffleMetrics is used exclusively when BlockManager is requested for a storage:BlockManager.md#shuffleMetricsSource[shuffle-related Spark metrics source] (only when Executor is executor:Executor.md#creating-instance[created] for a non-local / cluster mode).","title":"ShuffleClient"},{"location":"storage/ShuffleClient/#source-java","text":"void init( String appId) init does nothing by default.","title":"[source, java]"},{"location":"storage/ShuffleClient/#note","text":"init is used when: BlockManager is requested to storage:BlockManager.md#initialize[initialize]","title":"[NOTE]"},{"location":"storage/ShuffleClient/#spark-on-mesos-mesoscoarsegrainedschedulerbackend-is-requested-to-registered","text":"== [[shuffleMetrics]] Shuffle Metrics","title":"* Spark on Mesos' MesosCoarseGrainedSchedulerBackend is requested to registered"},{"location":"storage/ShuffleClient/#source-java_1","text":"","title":"[source, java]"},{"location":"storage/ShuffleClient/#metricset-shufflemetrics","text":"shuffleMetrics returns an empty Dropwizard Metrics' https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricSet.html[MetricSet ] by default. NOTE: shuffleMetrics is used exclusively when BlockManager is requested for a storage:BlockManager.md#shuffleMetricsSource[shuffle-related Spark metrics source] (only when Executor is executor:Executor.md#creating-instance[created] for a non-local / cluster mode).","title":"MetricSet shuffleMetrics()"},{"location":"storage/ShuffleMetricsSource/","text":"= ShuffleMetricsSource ShuffleMetricsSource is the metrics:spark-metrics-Source.md[metrics source] of a storage:BlockManager.md[] for < >. ShuffleMetricsSource lives on a Spark executor and is executor:Executor.md#creating-instance-BlockManager-shuffleMetricsSource[registered only when a Spark application runs in a non-local / cluster mode]. .Registering ShuffleMetricsSource with \"executor\" MetricsSystem image::ShuffleMetricsSource.png[align=\"center\"] == [[creating-instance]] Creating Instance ShuffleMetricsSource takes the following to be created: < > < > ShuffleMetricsSource is created when BlockManager is requested for the storage:BlockManager.md#shuffleMetricsSource[shuffle metrics source]. == [[metricSet]] MetricSet ShuffleMetricsSource is given a Dropwizard Metrics https://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricSet.html[MetricSet ] when < >. The MetricSet is requested from the storage:ShuffleClient.md#shuffleMetrics[ShuffleClient] (of storage:BlockManager.md#shuffleClient[BlockManager]). == [[sourceName]] Source Name ShuffleMetricsSource is given a name when < > that is one of the following: NettyBlockTransfer when ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is off ( false ) ExternalShuffle when ROOT:configuration-properties.md#spark.shuffle.service.enabled[spark.shuffle.service.enabled] configuration property is on ( true )","title":"ShuffleMetricsSource"},{"location":"storage/StorageLevel/","text":"= StorageLevel StorageLevel describes how an RDD is persisted (and addresses the following concerns): Does RDD use disk? Does RDD < > to store data? How much of RDD is in memory? Does RDD use off-heap memory? Should an RDD be serialized or < > (while storing the data)? How many replicas (default: 1 ) to use (can only be less than 40 )? There are the following StorageLevel (number _2 in the name denotes 2 replicas): [[NONE]] NONE (default) DISK_ONLY DISK_ONLY_2 [[MEMORY_ONLY]] MEMORY_ONLY (default for spark-rdd-caching.md#cache[ cache operation] for RDDs) MEMORY_ONLY_2 MEMORY_ONLY_SER MEMORY_ONLY_SER_2 [[MEMORY_AND_DISK]] MEMORY_AND_DISK MEMORY_AND_DISK_2 MEMORY_AND_DISK_SER MEMORY_AND_DISK_SER_2 OFF_HEAP You can check out the storage level using getStorageLevel() operation. val lines = sc.textFile(\"README.md\") scala> lines.getStorageLevel res0: org.apache.spark.storage.StorageLevel = StorageLevel(disk=false, memory=false, offheap=false, deserialized=false, replication=1) [[useMemory]] StorageLevel can indicate to use memory for data storage using useMemory flag. [source, scala] \u00b6 useMemory: Boolean \u00b6 [[useDisk]] StorageLevel can indicate to use disk for data storage using useDisk flag. [source, scala] \u00b6 useDisk: Boolean \u00b6 [[deserialized]] StorageLevel can indicate to store data in deserialized format using deserialized flag. [source, scala] \u00b6 deserialized: Boolean \u00b6 [[replication]] StorageLevel can indicate to replicate the data to other block managers using replication property. [source, scala] \u00b6 replication: Int \u00b6","title":"StorageLevel"},{"location":"storage/StorageLevel/#source-scala","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#usememory-boolean","text":"[[useDisk]] StorageLevel can indicate to use disk for data storage using useDisk flag.","title":"useMemory: Boolean"},{"location":"storage/StorageLevel/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#usedisk-boolean","text":"[[deserialized]] StorageLevel can indicate to store data in deserialized format using deserialized flag.","title":"useDisk: Boolean"},{"location":"storage/StorageLevel/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#deserialized-boolean","text":"[[replication]] StorageLevel can indicate to replicate the data to other block managers using replication property.","title":"deserialized: Boolean"},{"location":"storage/StorageLevel/#source-scala_3","text":"","title":"[source, scala]"},{"location":"storage/StorageLevel/#replication-int","text":"","title":"replication: Int"},{"location":"storage/StorageStatus/","text":"== [[StorageStatus]] StorageStatus StorageStatus is a developer API that Spark uses to pass \"just enough\" information about registered storage:BlockManager.md[BlockManagers] in a Spark application between Spark services (mostly for monitoring purposes like spark-webui.md[web UI] or ROOT:SparkListener.md[]s). [NOTE] \u00b6 There are two ways to access StorageStatus about all the known BlockManagers in a Spark application: ROOT:SparkContext.md#getExecutorStorageStatus[SparkContext.getExecutorStorageStatus] * Being a ROOT:SparkListener.md[] and intercepting ROOT:SparkListener.md#onBlockManagerAdded[onBlockManagerAdded] and ROOT:SparkListener.md#onBlockManagerRemoved[onBlockManagerRemoved] events \u00b6 StorageStatus is < > when: BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#storageStatus[is requested for storage status] (of every storage:BlockManager.md[BlockManager] in a Spark application) StorageStatusListener spark-webui-StorageStatusListener.md#onBlockManagerAdded[gets notified about a new BlockManager ] (in a Spark application) [[internal-registries]] .StorageStatus's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_nonRddBlocks]] _nonRddBlocks | Lookup table of BlockIds per BlockId . Used when...FIXME | [[_rddBlocks]] _rddBlocks | Lookup table of BlockIds with BlockStatus per RDD id. Used when...FIXME |=== === [[updateStorageInfo]] updateStorageInfo Internal Method [source, scala] \u00b6 updateStorageInfo( blockId: BlockId, newBlockStatus: BlockStatus): Unit updateStorageInfo ...FIXME NOTE: updateStorageInfo is used when...FIXME === [[creating-instance]] Creating StorageStatus Instance StorageStatus takes the following when created: [[blockManagerId]] storage:BlockManagerId.md[] [[maxMem]] Maximum memory -- storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on the BlockManager ] StorageStatus initializes the < >. === [[rddBlocksById]] Getting RDD Blocks For RDD -- rddBlocksById Method [source, scala] \u00b6 rddBlocksById(rddId: Int): Map[BlockId, BlockStatus] \u00b6 rddBlocksById gives the blocks (as BlockId with their status as BlockStatus ) that belong to rddId RDD. [NOTE] \u00b6 rddBlocksById is used when: StorageStatusListener spark-webui-StorageStatusListener.md#updateStorageStatus-unpersistedRDD[removes the RDD blocks of an unpersisted RDD]. AllRDDResource does getRDDStorageInfo StorageUtils does getRddBlockLocations \u00b6 === [[removeBlock]] Removing Block (From Internal Registries) -- removeBlock Internal Method [source, scala] \u00b6 removeBlock(blockId: BlockId): Option[BlockStatus] \u00b6 removeBlock removes blockId from <<_rddBlocks, _rddBlocks>> registry and returns it. Internally, removeBlock < > of blockId (to be empty, i.e. removed). removeBlock branches off per the type of storage:BlockId.md[], i.e. RDDBlockId or not. For a RDDBlockId , removeBlock finds the RDD in <<_rddBlocks, _rddBlocks>> and removes the blockId . removeBlock removes the RDD (from <<_rddBlocks, _rddBlocks>>) completely, if there are no more blocks registered. For a non- RDDBlockId , removeBlock removes blockId from <<_nonRddBlocks, _nonRddBlocks>> registry. NOTE: removeBlock is used when StorageStatusListener spark-webui-StorageStatusListener.md#updateStorageStatus-unpersistedRDD[removes RDD blocks for an unpersisted RDD] or spark-webui-StorageStatusListener.md#updateStorageStatus-executor[updates storage status for an executor]. === [[addBlock]] Registering Status of Data Block -- addBlock Method [source, scala] \u00b6 addBlock( blockId: BlockId, blockStatus: BlockStatus): Unit addBlock ...FIXME NOTE: addBlock is used when...FIXME === [[getBlock]] getBlock Method [source, scala] \u00b6 getBlock(blockId: BlockId): Option[BlockStatus] \u00b6 getBlock ...FIXME NOTE: getBlock is used when...FIXME","title":"StorageStatus"},{"location":"storage/StorageStatus/#note","text":"There are two ways to access StorageStatus about all the known BlockManagers in a Spark application: ROOT:SparkContext.md#getExecutorStorageStatus[SparkContext.getExecutorStorageStatus]","title":"[NOTE]"},{"location":"storage/StorageStatus/#being-a-rootsparklistenermd-and-intercepting-rootsparklistenermdonblockmanageraddedonblockmanageradded-and-rootsparklistenermdonblockmanagerremovedonblockmanagerremoved-events","text":"StorageStatus is < > when: BlockManagerMasterEndpoint storage:BlockManagerMasterEndpoint.md#storageStatus[is requested for storage status] (of every storage:BlockManager.md[BlockManager] in a Spark application) StorageStatusListener spark-webui-StorageStatusListener.md#onBlockManagerAdded[gets notified about a new BlockManager ] (in a Spark application) [[internal-registries]] .StorageStatus's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_nonRddBlocks]] _nonRddBlocks | Lookup table of BlockIds per BlockId . Used when...FIXME | [[_rddBlocks]] _rddBlocks | Lookup table of BlockIds with BlockStatus per RDD id. Used when...FIXME |=== === [[updateStorageInfo]] updateStorageInfo Internal Method","title":"* Being a ROOT:SparkListener.md[] and intercepting ROOT:SparkListener.md#onBlockManagerAdded[onBlockManagerAdded] and ROOT:SparkListener.md#onBlockManagerRemoved[onBlockManagerRemoved] events"},{"location":"storage/StorageStatus/#source-scala","text":"updateStorageInfo( blockId: BlockId, newBlockStatus: BlockStatus): Unit updateStorageInfo ...FIXME NOTE: updateStorageInfo is used when...FIXME === [[creating-instance]] Creating StorageStatus Instance StorageStatus takes the following when created: [[blockManagerId]] storage:BlockManagerId.md[] [[maxMem]] Maximum memory -- storage:BlockManager.md#maxMemory[total available on-heap and off-heap memory for storage on the BlockManager ] StorageStatus initializes the < >. === [[rddBlocksById]] Getting RDD Blocks For RDD -- rddBlocksById Method","title":"[source, scala]"},{"location":"storage/StorageStatus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#rddblocksbyidrddid-int-mapblockid-blockstatus","text":"rddBlocksById gives the blocks (as BlockId with their status as BlockStatus ) that belong to rddId RDD.","title":"rddBlocksById(rddId: Int): Map[BlockId, BlockStatus]"},{"location":"storage/StorageStatus/#note_1","text":"rddBlocksById is used when: StorageStatusListener spark-webui-StorageStatusListener.md#updateStorageStatus-unpersistedRDD[removes the RDD blocks of an unpersisted RDD]. AllRDDResource does getRDDStorageInfo","title":"[NOTE]"},{"location":"storage/StorageStatus/#storageutils-does-getrddblocklocations","text":"=== [[removeBlock]] Removing Block (From Internal Registries) -- removeBlock Internal Method","title":"StorageUtils does getRddBlockLocations"},{"location":"storage/StorageStatus/#source-scala_2","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#removeblockblockid-blockid-optionblockstatus","text":"removeBlock removes blockId from <<_rddBlocks, _rddBlocks>> registry and returns it. Internally, removeBlock < > of blockId (to be empty, i.e. removed). removeBlock branches off per the type of storage:BlockId.md[], i.e. RDDBlockId or not. For a RDDBlockId , removeBlock finds the RDD in <<_rddBlocks, _rddBlocks>> and removes the blockId . removeBlock removes the RDD (from <<_rddBlocks, _rddBlocks>>) completely, if there are no more blocks registered. For a non- RDDBlockId , removeBlock removes blockId from <<_nonRddBlocks, _nonRddBlocks>> registry. NOTE: removeBlock is used when StorageStatusListener spark-webui-StorageStatusListener.md#updateStorageStatus-unpersistedRDD[removes RDD blocks for an unpersisted RDD] or spark-webui-StorageStatusListener.md#updateStorageStatus-executor[updates storage status for an executor]. === [[addBlock]] Registering Status of Data Block -- addBlock Method","title":"removeBlock(blockId: BlockId): Option[BlockStatus]"},{"location":"storage/StorageStatus/#source-scala_3","text":"addBlock( blockId: BlockId, blockStatus: BlockStatus): Unit addBlock ...FIXME NOTE: addBlock is used when...FIXME === [[getBlock]] getBlock Method","title":"[source, scala]"},{"location":"storage/StorageStatus/#source-scala_4","text":"","title":"[source, scala]"},{"location":"storage/StorageStatus/#getblockblockid-blockid-optionblockstatus","text":"getBlock ...FIXME NOTE: getBlock is used when...FIXME","title":"getBlock(blockId: BlockId): Option[BlockStatus]"},{"location":"tools/AbstractCommandBuilder/","text":"== [[AbstractCommandBuilder]] AbstractCommandBuilder AbstractCommandBuilder is the base command builder for spark-submit-SparkSubmitCommandBuilder.md[SparkSubmitCommandBuilder] and SparkClassCommandBuilder specialized command builders. AbstractCommandBuilder expects that command builders define buildCommand . . AbstractCommandBuilder Methods [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | buildCommand | The only abstract method that subclasses have to define. | < > | | < > | | < > | Loads the configuration file for a Spark application, be it the user-specified properties file or spark-defaults.conf file under the Spark configuration directory. |=== === [[buildJavaCommand]] buildJavaCommand Internal Method [source, java] \u00b6 List buildJavaCommand(String extraClassPath) \u00b6 buildJavaCommand builds the Java command for a Spark application (which is a collection of elements with the path to java executable, JVM options from java-opts file, and a class path). If javaHome is set, buildJavaCommand adds [javaHome]/bin/java to the result Java command. Otherwise, it uses JAVA_HOME or, when no earlier checks succeeded, falls through to java.home Java's system property. CAUTION: FIXME Who sets javaHome internal property and when? buildJavaCommand loads extra Java options from the java-opts file in < > if the file exists and adds them to the result Java command. Eventually, buildJavaCommand < > (with the extra class path if non-empty) and adds it as -cp to the result Java command. === [[buildClassPath]] buildClassPath method [source, java] \u00b6 List buildClassPath(String appClassPath) \u00b6 buildClassPath builds the classpath for a Spark application. NOTE: Directories always end up with the OS-specific file separator at the end of their paths. buildClassPath adds the following in that order: SPARK_CLASSPATH environment variable The input appClassPath The spark-AbstractCommandBuilder.md#getConfDir[configuration directory] (only with SPARK_PREPEND_CLASSES set or SPARK_TESTING being 1 ) Locally compiled Spark classes in classes , test-classes and Core's jars. + CAUTION: FIXME Elaborate on \"locally compiled Spark classes\". (only with SPARK_SQL_TESTING being 1 ) ... + CAUTION: FIXME Elaborate on the SQL testing case HADOOP_CONF_DIR environment variable YARN_CONF_DIR environment variable SPARK_DIST_CLASSPATH environment variable NOTE: childEnv is queried first before System properties. It is always empty for AbstractCommandBuilder (and SparkSubmitCommandBuilder , too). === [[loadPropertiesFile]] Loading Properties File -- loadPropertiesFile Internal Method [source, java] \u00b6 Properties loadPropertiesFile() \u00b6 loadPropertiesFile is part of AbstractCommandBuilder private API that loads Spark settings from a properties file (when specified on the command line) or spark-properties.md#spark-defaults-conf[spark-defaults.conf] in the < >. It loads the settings from the following files starting from the first and checking every location until the first properties file is found: propertiesFile (if specified using --properties-file command-line option or set by AbstractCommandBuilder.setPropertiesFile ). [SPARK_CONF_DIR]/spark-defaults.conf [SPARK_HOME]/conf/spark-defaults.conf NOTE: loadPropertiesFile reads a properties file using UTF-8 . === [[getConfDir]][[configuration-directory]] Spark's Configuration Directory -- getConfDir Internal Method AbstractCommandBuilder uses getConfDir to compute the current configuration directory of a Spark application. It uses SPARK_CONF_DIR (from childEnv which is always empty anyway or as a environment variable) and falls through to [SPARK_HOME]/conf (with SPARK_HOME from < getSparkHome internal method>>). === [[getSparkHome]][[home-directory]] Spark's Home Directory -- getSparkHome Internal Method AbstractCommandBuilder uses getSparkHome to compute Spark's home directory for a Spark application. It uses SPARK_HOME (from childEnv which is always empty anyway or as a environment variable). If SPARK_HOME is not set, Spark throws a IllegalStateException : Spark home not found; set it explicitly or use the SPARK_HOME environment variable.","title":"AbstractCommandBuilder"},{"location":"tools/AbstractCommandBuilder/#source-java","text":"","title":"[source, java]"},{"location":"tools/AbstractCommandBuilder/#list-buildjavacommandstring-extraclasspath","text":"buildJavaCommand builds the Java command for a Spark application (which is a collection of elements with the path to java executable, JVM options from java-opts file, and a class path). If javaHome is set, buildJavaCommand adds [javaHome]/bin/java to the result Java command. Otherwise, it uses JAVA_HOME or, when no earlier checks succeeded, falls through to java.home Java's system property. CAUTION: FIXME Who sets javaHome internal property and when? buildJavaCommand loads extra Java options from the java-opts file in < > if the file exists and adds them to the result Java command. Eventually, buildJavaCommand < > (with the extra class path if non-empty) and adds it as -cp to the result Java command. === [[buildClassPath]] buildClassPath method","title":"List buildJavaCommand(String extraClassPath)"},{"location":"tools/AbstractCommandBuilder/#source-java_1","text":"","title":"[source, java]"},{"location":"tools/AbstractCommandBuilder/#list-buildclasspathstring-appclasspath","text":"buildClassPath builds the classpath for a Spark application. NOTE: Directories always end up with the OS-specific file separator at the end of their paths. buildClassPath adds the following in that order: SPARK_CLASSPATH environment variable The input appClassPath The spark-AbstractCommandBuilder.md#getConfDir[configuration directory] (only with SPARK_PREPEND_CLASSES set or SPARK_TESTING being 1 ) Locally compiled Spark classes in classes , test-classes and Core's jars. + CAUTION: FIXME Elaborate on \"locally compiled Spark classes\". (only with SPARK_SQL_TESTING being 1 ) ... + CAUTION: FIXME Elaborate on the SQL testing case HADOOP_CONF_DIR environment variable YARN_CONF_DIR environment variable SPARK_DIST_CLASSPATH environment variable NOTE: childEnv is queried first before System properties. It is always empty for AbstractCommandBuilder (and SparkSubmitCommandBuilder , too). === [[loadPropertiesFile]] Loading Properties File -- loadPropertiesFile Internal Method","title":"List buildClassPath(String appClassPath)"},{"location":"tools/AbstractCommandBuilder/#source-java_2","text":"","title":"[source, java]"},{"location":"tools/AbstractCommandBuilder/#properties-loadpropertiesfile","text":"loadPropertiesFile is part of AbstractCommandBuilder private API that loads Spark settings from a properties file (when specified on the command line) or spark-properties.md#spark-defaults-conf[spark-defaults.conf] in the < >. It loads the settings from the following files starting from the first and checking every location until the first properties file is found: propertiesFile (if specified using --properties-file command-line option or set by AbstractCommandBuilder.setPropertiesFile ). [SPARK_CONF_DIR]/spark-defaults.conf [SPARK_HOME]/conf/spark-defaults.conf NOTE: loadPropertiesFile reads a properties file using UTF-8 . === [[getConfDir]][[configuration-directory]] Spark's Configuration Directory -- getConfDir Internal Method AbstractCommandBuilder uses getConfDir to compute the current configuration directory of a Spark application. It uses SPARK_CONF_DIR (from childEnv which is always empty anyway or as a environment variable) and falls through to [SPARK_HOME]/conf (with SPARK_HOME from < getSparkHome internal method>>). === [[getSparkHome]][[home-directory]] Spark's Home Directory -- getSparkHome Internal Method AbstractCommandBuilder uses getSparkHome to compute Spark's home directory for a Spark application. It uses SPARK_HOME (from childEnv which is always empty anyway or as a environment variable). If SPARK_HOME is not set, Spark throws a IllegalStateException : Spark home not found; set it explicitly or use the SPARK_HOME environment variable.","title":"Properties loadPropertiesFile()"},{"location":"tools/SparkLauncher/","text":"== [[SparkLauncher]] SparkLauncher -- Launching Spark Applications Programmatically SparkLauncher is an interface to launch Spark applications programmatically, i.e. from a code (not spark-submit.md[spark-submit] directly). It uses a builder pattern to configure a Spark application and launch it as a child process using spark-submit.md[spark-submit]. SparkLauncher belongs to org.apache.spark.launcher Scala package in spark-launcher build module. SparkLauncher uses spark-submit-SparkSubmitCommandBuilder.md[SparkSubmitCommandBuilder] to build the Spark command of a Spark application to launch. . SparkLauncher 's Builder Methods to Set Up Invocation of Spark Application [options=\"header\",width=\"100%\"] |=== | Setter | Description | addAppArgs(String... args) | Adds command line arguments for a Spark application. | addFile(String file) | Adds a file to be submitted with a Spark application. | addJar(String jar) | Adds a jar file to be submitted with the application. | addPyFile(String file) | Adds a python file / zip / egg to be submitted with a Spark application. | addSparkArg(String arg) | Adds a no-value argument to the Spark invocation. | addSparkArg(String name, String value) | Adds an argument with a value to the Spark invocation. It recognizes known command-line arguments, i.e. --master , --properties-file , --conf , --class , --jars , --files , and --py-files . | directory(File dir) | Sets the working directory of spark-submit. | redirectError() | Redirects stderr to stdout. | redirectError(File errFile) | Redirects error output to the specified errFile file. | redirectError(ProcessBuilder.Redirect to) | Redirects error output to the specified to Redirect. | redirectOutput(File outFile) | Redirects output to the specified outFile file. | redirectOutput(ProcessBuilder.Redirect to) | Redirects standard output to the specified to Redirect. | redirectToLog(String loggerName) | Sets all output to be logged and redirected to a logger with the specified name. | setAppName(String appName) | Sets the name of an Spark application | setAppResource(String resource) | Sets the main application resource, i.e. the location of a jar file for Scala/Java applications. | setConf(String key, String value) | Sets a Spark property. Expects key starting with spark. prefix. | setDeployMode(String mode) | Sets the deploy mode. | setJavaHome(String javaHome) | Sets a custom JAVA_HOME . | setMainClass(String mainClass) | Sets the main class. | setMaster(String master) | Sets the master URL. | setPropertiesFile(String path) | Sets the internal propertiesFile . See spark-AbstractCommandBuilder.md#loadPropertiesFile[ loadPropertiesFile Internal Method]. | setSparkHome(String sparkHome) | Sets a custom SPARK_HOME . | setVerbose(boolean verbose) | Enables verbose reporting for SparkSubmit. |=== After the invocation of a Spark application is set up, use launch() method to launch a sub-process that will start the configured Spark application. It is however recommended to use startApplication method instead. [source, scala] \u00b6 import org.apache.spark.launcher.SparkLauncher val command = new SparkLauncher() .setAppResource(\"SparkPi\") .setVerbose(true) val appHandle = command.startApplication() \u00b6","title":"SparkLauncher"},{"location":"tools/SparkLauncher/#source-scala","text":"import org.apache.spark.launcher.SparkLauncher val command = new SparkLauncher() .setAppResource(\"SparkPi\") .setVerbose(true)","title":"[source, scala]"},{"location":"tools/SparkLauncher/#val-apphandle-commandstartapplication","text":"","title":"val appHandle = command.startApplication()"},{"location":"tools/SparkSubmitArguments/","text":"== [[SparkSubmitArguments]] SparkSubmitArguments -- spark-submit's Command-Line Argument Parser SparkSubmitArguments is a custom SparkSubmitArgumentsParser to < > the command-line arguments of spark-submit.md[ spark-submit script] that the spark-submit.md#actions[actions] (i.e. spark-submit.md#submit[submit], spark-submit.md#kill[kill] and spark-submit.md#status[status]) use for their execution (possibly with the explicit env environment). NOTE: SparkSubmitArguments is created when < spark-submit script>> with only args passed in and later used for printing the arguments in < >. === [[loadEnvironmentArguments]] Calculating Spark Properties -- loadEnvironmentArguments internal method [source, scala] \u00b6 loadEnvironmentArguments(): Unit \u00b6 loadEnvironmentArguments calculates the Spark properties for the current execution of spark-submit.md[spark-submit]. loadEnvironmentArguments reads command-line options first followed by Spark properties and System's environment variables. NOTE: Spark config properties start with spark. prefix and can be set using --conf [key=value] command-line option. === [[handle]] handle Method [source, scala] \u00b6 protected def handle(opt: String, value: String): Boolean \u00b6 handle parses the input opt argument and returns true or throws an IllegalArgumentException when it finds an unknown opt . handle sets the internal properties in the table spark-submit.md#options-properties-variables[Command-Line Options, Spark Properties and Environment Variables]. === [[mergeDefaultSparkProperties]] mergeDefaultSparkProperties Internal Method [source, scala] \u00b6 mergeDefaultSparkProperties(): Unit \u00b6 mergeDefaultSparkProperties merges Spark properties from the spark-properties.md#spark-defaults-conf[default Spark properties file, i.e. spark-defaults.conf ] with those specified through --conf command-line option.","title":"SparkSubmitArguments"},{"location":"tools/SparkSubmitArguments/#source-scala","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitArguments/#loadenvironmentarguments-unit","text":"loadEnvironmentArguments calculates the Spark properties for the current execution of spark-submit.md[spark-submit]. loadEnvironmentArguments reads command-line options first followed by Spark properties and System's environment variables. NOTE: Spark config properties start with spark. prefix and can be set using --conf [key=value] command-line option. === [[handle]] handle Method","title":"loadEnvironmentArguments(): Unit"},{"location":"tools/SparkSubmitArguments/#source-scala_1","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitArguments/#protected-def-handleopt-string-value-string-boolean","text":"handle parses the input opt argument and returns true or throws an IllegalArgumentException when it finds an unknown opt . handle sets the internal properties in the table spark-submit.md#options-properties-variables[Command-Line Options, Spark Properties and Environment Variables]. === [[mergeDefaultSparkProperties]] mergeDefaultSparkProperties Internal Method","title":"protected def handle(opt: String, value: String): Boolean"},{"location":"tools/SparkSubmitArguments/#source-scala_2","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitArguments/#mergedefaultsparkproperties-unit","text":"mergeDefaultSparkProperties merges Spark properties from the spark-properties.md#spark-defaults-conf[default Spark properties file, i.e. spark-defaults.conf ] with those specified through --conf command-line option.","title":"mergeDefaultSparkProperties(): Unit"},{"location":"tools/SparkSubmitCommandBuilder/","text":"== [[SparkSubmitCommandBuilder]] SparkSubmitCommandBuilder Command Builder SparkSubmitCommandBuilder is used to build a command that spark-submit.md#main[spark-submit] and spark-SparkLauncher.md[SparkLauncher] use to launch a Spark application. SparkSubmitCommandBuilder uses the first argument to distinguish between shells: pyspark-shell-main sparkr-shell-main run-example CAUTION: FIXME Describe run-example SparkSubmitCommandBuilder parses command-line arguments using OptionParser (which is a spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser]). OptionParser comes with the following methods: handle to handle the known options (see the table below). It sets up master , deployMode , propertiesFile , conf , mainClass , sparkArgs internal properties. handleUnknown to handle unrecognized options that usually lead to Unrecognized option error message. handleExtraArgs to handle extra arguments that are considered a Spark application's arguments. NOTE: For spark-shell it assumes that the application arguments are after spark-submit 's arguments. === [[buildCommand]] SparkSubmitCommandBuilder.buildCommand / buildSparkSubmitCommand [source, java] \u00b6 public List buildCommand(Map env) \u00b6 NOTE: buildCommand is part of the spark-AbstractCommandBuilder.md[AbstractCommandBuilder] public API. SparkSubmitCommandBuilder.buildCommand simply passes calls on to < > private method (unless it was executed for pyspark or sparkr scripts which we are not interested in in this document). ==== [[buildSparkSubmitCommand]] buildSparkSubmitCommand Internal Method [source, java] \u00b6 private List buildSparkSubmitCommand(Map env) \u00b6 buildSparkSubmitCommand starts by < >. When in < >, buildSparkSubmitCommand adds spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] to the result Spark command. NOTE: Use spark-submit to have spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] in effect. buildSparkSubmitCommand spark-AbstractCommandBuilder.md#buildJavaCommand[builds the first part of the Java command] passing in the extra classpath (only for client deploy mode). CAUTION: FIXME Add isThriftServer case. buildSparkSubmitCommand appends SPARK_SUBMIT_OPTS and SPARK_JAVA_OPTS environment variables. (only for client deploy mode) ... CAUTION: FIXME Elaborate on the client deply mode case. addPermGenSizeOpt case...elaborate CAUTION: FIXME Elaborate on addPermGenSizeOpt buildSparkSubmitCommand appends org.apache.spark.deploy.SparkSubmit and the command-line arguments (using < >). ==== [[buildSparkSubmitArgs]] buildSparkSubmitArgs method [source, java] \u00b6 List buildSparkSubmitArgs() \u00b6 buildSparkSubmitArgs builds a list of command-line arguments for spark-submit.md[spark-submit]. buildSparkSubmitArgs uses a spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser] to add the command-line arguments that spark-submit recognizes (when it is executed later on and uses the very same SparkSubmitOptionParser parser to parse command-line arguments). . SparkSubmitCommandBuilder Properties and Corresponding SparkSubmitOptionParser Attributes [options=\"header\",width=\"100%\"] |=== | SparkSubmitCommandBuilder Property | SparkSubmitOptionParser Attribute | verbose | VERBOSE | master | MASTER [master] | deployMode | DEPLOY_MODE [deployMode] | appName | NAME [appName] | conf | CONF [key=value]* | propertiesFile | PROPERTIES_FILE [propertiesFile] | jars | JARS [comma-separated jars] | files | FILES [comma-separated files] | pyFiles | PY_FILES [comma-separated pyFiles] | mainClass | CLASS [mainClass] | sparkArgs | sparkArgs (passed straight through) | appResource | appResource (passed straight through) | appArgs | appArgs (passed straight through) |=== ==== [[getEffectiveConfig]] getEffectiveConfig Internal Method [source, java] \u00b6 Map getEffectiveConfig() \u00b6 getEffectiveConfig internal method builds effectiveConfig that is conf with the Spark properties file loaded (using spark-AbstractCommandBuilder.md#loadPropertiesFile[loadPropertiesFile] internal method) skipping keys that have already been loaded (it happened when the command-line options were parsed in < > method). NOTE: Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. charset and trims white spaces around values. ==== [[isClientMode]] isClientMode Internal Method [source, java] \u00b6 private boolean isClientMode(Map userProps) \u00b6 isClientMode checks master first (from the command-line options) and then spark.master Spark property. Same with deployMode and spark.submit.deployMode . CAUTION: FIXME Review master and deployMode . How are they set? isClientMode responds positive when no explicit master and client deploy mode set explicitly. === [[OptionParser]] OptionParser OptionParser is a custom spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser] that SparkSubmitCommandBuilder uses to parse command-line arguments. It defines all the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks], i.e. < >, < >, and < >, for command-line argument handling. ==== [[OptionParser-handle]] OptionParser's handle Callback [source, scala] \u00b6 boolean handle(String opt, String value) \u00b6 OptionParser comes with a custom handle callback (from the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks]). . handle Method [options=\"header\",width=\"100%\"] |=== | Command-Line Option | Property / Behaviour | --master | master | --deploy-mode | deployMode | --properties-file | propertiesFile | --driver-memory | Sets spark.driver.memory (in conf ) | --driver-java-options | Sets spark.driver.extraJavaOptions (in conf ) | --driver-library-path | Sets spark.driver.extraLibraryPath (in conf ) | --driver-class-path | Sets spark.driver.extraClassPath (in conf ) | --conf | Expects a key=value pair that it puts in conf | --class | Sets mainClass (in conf ). It may also set allowsMixedArguments and appResource if the execution is for one of the special classes, i.e. spark-shell.md[spark-shell], SparkSQLCLIDriver , or spark-sql-thrift-server.md[HiveThriftServer2]. | --kill | --status | Disables isAppResourceReq and adds itself with the value to sparkArgs . | --help | --usage-error | Disables isAppResourceReq and adds itself to sparkArgs . | --version | Disables isAppResourceReq and adds itself to sparkArgs . | anything else | Adds an element to sparkArgs |=== ==== [[OptionParser-handleUnknown]] OptionParser's handleUnknown Method [source, scala] \u00b6 boolean handleUnknown(String opt) \u00b6 If allowsMixedArguments is enabled, handleUnknown simply adds the input opt to appArgs and allows for further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's allowsMixedArguments enabled? If isExample is enabled, handleUnknown sets mainClass to be org.apache.spark.examples.[opt] (unless the input opt has already the package prefix) and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's isExample enabled? Otherwise, handleUnknown sets appResource and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. ==== [[OptionParser-handleExtraArgs]] OptionParser's handleExtraArgs Method [source, scala] \u00b6 void handleExtraArgs(List extra) \u00b6 handleExtraArgs adds all the extra arguments to appArgs .","title":"SparkSubmitCommandBuilder"},{"location":"tools/SparkSubmitCommandBuilder/#source-java","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#public-list-buildcommandmap-env","text":"NOTE: buildCommand is part of the spark-AbstractCommandBuilder.md[AbstractCommandBuilder] public API. SparkSubmitCommandBuilder.buildCommand simply passes calls on to < > private method (unless it was executed for pyspark or sparkr scripts which we are not interested in in this document). ==== [[buildSparkSubmitCommand]] buildSparkSubmitCommand Internal Method","title":"public List buildCommand(Map env)"},{"location":"tools/SparkSubmitCommandBuilder/#source-java_1","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#private-list-buildsparksubmitcommandmap-env","text":"buildSparkSubmitCommand starts by < >. When in < >, buildSparkSubmitCommand adds spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] to the result Spark command. NOTE: Use spark-submit to have spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] in effect. buildSparkSubmitCommand spark-AbstractCommandBuilder.md#buildJavaCommand[builds the first part of the Java command] passing in the extra classpath (only for client deploy mode). CAUTION: FIXME Add isThriftServer case. buildSparkSubmitCommand appends SPARK_SUBMIT_OPTS and SPARK_JAVA_OPTS environment variables. (only for client deploy mode) ... CAUTION: FIXME Elaborate on the client deply mode case. addPermGenSizeOpt case...elaborate CAUTION: FIXME Elaborate on addPermGenSizeOpt buildSparkSubmitCommand appends org.apache.spark.deploy.SparkSubmit and the command-line arguments (using < >). ==== [[buildSparkSubmitArgs]] buildSparkSubmitArgs method","title":"private List buildSparkSubmitCommand(Map env)"},{"location":"tools/SparkSubmitCommandBuilder/#source-java_2","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#list-buildsparksubmitargs","text":"buildSparkSubmitArgs builds a list of command-line arguments for spark-submit.md[spark-submit]. buildSparkSubmitArgs uses a spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser] to add the command-line arguments that spark-submit recognizes (when it is executed later on and uses the very same SparkSubmitOptionParser parser to parse command-line arguments). . SparkSubmitCommandBuilder Properties and Corresponding SparkSubmitOptionParser Attributes [options=\"header\",width=\"100%\"] |=== | SparkSubmitCommandBuilder Property | SparkSubmitOptionParser Attribute | verbose | VERBOSE | master | MASTER [master] | deployMode | DEPLOY_MODE [deployMode] | appName | NAME [appName] | conf | CONF [key=value]* | propertiesFile | PROPERTIES_FILE [propertiesFile] | jars | JARS [comma-separated jars] | files | FILES [comma-separated files] | pyFiles | PY_FILES [comma-separated pyFiles] | mainClass | CLASS [mainClass] | sparkArgs | sparkArgs (passed straight through) | appResource | appResource (passed straight through) | appArgs | appArgs (passed straight through) |=== ==== [[getEffectiveConfig]] getEffectiveConfig Internal Method","title":"List buildSparkSubmitArgs()"},{"location":"tools/SparkSubmitCommandBuilder/#source-java_3","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#map-geteffectiveconfig","text":"getEffectiveConfig internal method builds effectiveConfig that is conf with the Spark properties file loaded (using spark-AbstractCommandBuilder.md#loadPropertiesFile[loadPropertiesFile] internal method) skipping keys that have already been loaded (it happened when the command-line options were parsed in < > method). NOTE: Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. charset and trims white spaces around values. ==== [[isClientMode]] isClientMode Internal Method","title":"Map getEffectiveConfig()"},{"location":"tools/SparkSubmitCommandBuilder/#source-java_4","text":"","title":"[source, java]"},{"location":"tools/SparkSubmitCommandBuilder/#private-boolean-isclientmodemap-userprops","text":"isClientMode checks master first (from the command-line options) and then spark.master Spark property. Same with deployMode and spark.submit.deployMode . CAUTION: FIXME Review master and deployMode . How are they set? isClientMode responds positive when no explicit master and client deploy mode set explicitly. === [[OptionParser]] OptionParser OptionParser is a custom spark-submit-SparkSubmitOptionParser.md[SparkSubmitOptionParser] that SparkSubmitCommandBuilder uses to parse command-line arguments. It defines all the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks], i.e. < >, < >, and < >, for command-line argument handling. ==== [[OptionParser-handle]] OptionParser's handle Callback","title":"private boolean isClientMode(Map userProps)"},{"location":"tools/SparkSubmitCommandBuilder/#source-scala","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitCommandBuilder/#boolean-handlestring-opt-string-value","text":"OptionParser comes with a custom handle callback (from the spark-submit-SparkSubmitOptionParser.md#callbacks[SparkSubmitOptionParser callbacks]). . handle Method [options=\"header\",width=\"100%\"] |=== | Command-Line Option | Property / Behaviour | --master | master | --deploy-mode | deployMode | --properties-file | propertiesFile | --driver-memory | Sets spark.driver.memory (in conf ) | --driver-java-options | Sets spark.driver.extraJavaOptions (in conf ) | --driver-library-path | Sets spark.driver.extraLibraryPath (in conf ) | --driver-class-path | Sets spark.driver.extraClassPath (in conf ) | --conf | Expects a key=value pair that it puts in conf | --class | Sets mainClass (in conf ). It may also set allowsMixedArguments and appResource if the execution is for one of the special classes, i.e. spark-shell.md[spark-shell], SparkSQLCLIDriver , or spark-sql-thrift-server.md[HiveThriftServer2]. | --kill | --status | Disables isAppResourceReq and adds itself with the value to sparkArgs . | --help | --usage-error | Disables isAppResourceReq and adds itself to sparkArgs . | --version | Disables isAppResourceReq and adds itself to sparkArgs . | anything else | Adds an element to sparkArgs |=== ==== [[OptionParser-handleUnknown]] OptionParser's handleUnknown Method","title":"boolean handle(String opt, String value)"},{"location":"tools/SparkSubmitCommandBuilder/#source-scala_1","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitCommandBuilder/#boolean-handleunknownstring-opt","text":"If allowsMixedArguments is enabled, handleUnknown simply adds the input opt to appArgs and allows for further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's allowsMixedArguments enabled? If isExample is enabled, handleUnknown sets mainClass to be org.apache.spark.examples.[opt] (unless the input opt has already the package prefix) and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. CAUTION: FIXME Where's isExample enabled? Otherwise, handleUnknown sets appResource and stops further spark-submit-SparkSubmitOptionParser.md#parse[parsing of the argument list]. ==== [[OptionParser-handleExtraArgs]] OptionParser's handleExtraArgs Method","title":"boolean handleUnknown(String opt)"},{"location":"tools/SparkSubmitCommandBuilder/#source-scala_2","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitCommandBuilder/#void-handleextraargslist-extra","text":"handleExtraArgs adds all the extra arguments to appArgs .","title":"void handleExtraArgs(List extra)"},{"location":"tools/SparkSubmitOptionParser/","text":"== [[SparkSubmitOptionParser]] SparkSubmitOptionParser -- spark-submit's Command-Line Parser SparkSubmitOptionParser is the parser of spark-submit.md[spark-submit]'s command-line options. . spark-submit Command-Line Options [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Command-Line Option | Description | --archives | | --class | The main class to run (as mainClass internal attribute). | --conf [prop=value] or -c [prop=value] | All = -separated values end up in conf potentially overriding existing settings. Order on command-line matters. | --deploy-mode | deployMode internal property | --driver-class-path | spark.driver.extraClassPath in conf -- the driver class path | --driver-cores | | --driver-java-options | spark.driver.extraJavaOptions in conf -- the driver VM options | --driver-library-path | spark.driver.extraLibraryPath in conf -- the driver native library path | --driver-memory | spark.driver.memory in conf | --exclude-packages | | --executor-cores | | --executor-memory | | --files | | --help or -h | The option is added to sparkArgs | --jars | | --keytab | | --kill | The option and a value are added to sparkArgs | --master | master internal property | --name | | --num-executors | | --packages | | --principal | | --properties-file [FILE] | propertiesFile internal property. Refer to spark-submit.md#properties-file[Custom Spark Properties File -- --properties-file command-line option]. | --proxy-user | | --py-files | | --queue | | --repositories | | --status | The option and a value are added to sparkArgs | --supervise | | --total-executor-cores | | --usage-error | The option is added to sparkArgs | --verbose or -v | | --version | The option is added to sparkArgs |=== === [[callbacks]] SparkSubmitOptionParser Callbacks SparkSubmitOptionParser is supposed to be overriden for the following capabilities (as callbacks). .Callbacks [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Callback | Description | handle | Executed when an option with an argument is parsed. | handleUnknown | Executed when an unrecognized option is parsed. | handleExtraArgs | Executed for the command-line arguments that handle and handleUnknown callbacks have not processed. |=== SparkSubmitOptionParser belongs to org.apache.spark.launcher Scala package and spark-launcher Maven/sbt module. NOTE: org.apache.spark.launcher.SparkSubmitArgumentsParser is a custom SparkSubmitOptionParser . === [[parse]] Parsing Command-Line Arguments -- parse Method [source, scala] \u00b6 final void parse(List args) \u00b6 parse parses a list of command-line arguments. parse calls handle callback whenever it finds a known command-line option or a switch (a command-line option with no parameter). It calls handleUnknown callback for unrecognized command-line options. parse keeps processing command-line arguments until handle or handleUnknown callback return false or all command-line arguments have been consumed. Ultimately, parse calls handleExtraArgs callback.","title":"SparkSubmitOptionParser"},{"location":"tools/SparkSubmitOptionParser/#source-scala","text":"","title":"[source, scala]"},{"location":"tools/SparkSubmitOptionParser/#final-void-parselist-args","text":"parse parses a list of command-line arguments. parse calls handle callback whenever it finds a known command-line option or a switch (a command-line option with no parameter). It calls handleUnknown callback for unrecognized command-line options. parse keeps processing command-line arguments until handle or handleUnknown callback return false or all command-line arguments have been consumed. Ultimately, parse calls handleExtraArgs callback.","title":"final void parse(List args)"},{"location":"tools/spark-class/","text":"== spark-class shell script spark-class shell script is the Spark application command-line launcher that is responsible for setting up JVM environment and executing a Spark application. NOTE: Ultimately, any shell script in Spark, e.g. link:spark-submit.adoc[spark-submit], calls spark-class script. You can find spark-class script in bin directory of the Spark distribution. When started, spark-class first loads $SPARK_HOME/bin/load-spark-env.sh , collects the Spark assembly jars, and executes < >. Depending on the Spark distribution (or rather lack thereof), i.e. whether RELEASE file exists or not, it sets SPARK_JARS_DIR environment variable to [SPARK_HOME]/jars or [SPARK_HOME]/assembly/target/scala-[SPARK_SCALA_VERSION]/jars , respectively (with the latter being a local build). If SPARK_JARS_DIR does not exist, spark-class prints the following error message and exits with the code 1 . Failed to find Spark jars directory ([SPARK_JARS_DIR]). You need to build Spark with the target \"package\" before running this program. spark-class sets LAUNCH_CLASSPATH environment variable to include all the jars under SPARK_JARS_DIR . If SPARK_PREPEND_CLASSES is enabled, [SPARK_HOME]/launcher/target/scala-[SPARK_SCALA_VERSION]/classes directory is added to LAUNCH_CLASSPATH as the first entry. NOTE: Use SPARK_PREPEND_CLASSES to have the Spark launcher classes (from [SPARK_HOME]/launcher/target/scala-[SPARK_SCALA_VERSION]/classes ) to appear before the other Spark assembly jars. It is useful for development so your changes don't require rebuilding Spark again. SPARK_TESTING and SPARK_SQL_TESTING environment variables enable test special mode . CAUTION: FIXME What's so special about the env vars? spark-class uses < > command-line application to compute the Spark command to launch. The Main class programmatically computes the command that spark-class executes afterwards. TIP: Use JAVA_HOME to point at the JVM to use. === [[main]] Launching org.apache.spark.launcher.Main Standalone Application org.apache.spark.launcher.Main is a Scala standalone application used in spark-class to prepare the Spark command to execute. Main expects that the first parameter is the class name that is the \"operation mode\": org.apache.spark.deploy.SparkSubmit -- Main uses link:spark-submit-SparkSubmitCommandBuilder.adoc[SparkSubmitCommandBuilder] to parse command-line arguments. This is the mode link:spark-submit.adoc[spark-submit] uses. anything -- Main uses SparkClassCommandBuilder to parse command-line arguments. $ ./bin/spark-class org.apache.spark.launcher.Main Exception in thread \"main\" java.lang.IllegalArgumentException: Not enough arguments: missing class name. at org.apache.spark.launcher.CommandBuilderUtils.checkArgument(CommandBuilderUtils.java:241) at org.apache.spark.launcher.Main.main(Main.java:51) Main uses buildCommand method on the builder to build a Spark command. If SPARK_PRINT_LAUNCH_COMMAND environment variable is enabled, Main prints the final Spark command to standard error. Spark Command: [cmd] ======================================== If on Windows it calls prepareWindowsCommand while on non-Windows OSes prepareBashCommand with tokens separated by \u0000\u0000\\0 . CAUTION: FIXME What's prepareWindowsCommand ? prepareBashCommand ? Main uses the following environment variables: SPARK_DAEMON_JAVA_OPTS and SPARK_MASTER_OPTS to be added to the command line of the command. SPARK_DAEMON_MEMORY (default: 1g ) for -Xms and -Xmx .","title":"spark-class"},{"location":"tools/spark-shell/","text":"== [[spark-shell]] Spark Shell -- spark-shell shell script Spark shell is an interactive environment where you can learn how to make the most out of Apache Spark quickly and conveniently. TIP: Spark shell is particularly helpful for fast interactive prototyping. Under the covers, Spark shell is a standalone Spark application written in Scala that offers environment with auto-completion (using TAB key) where you can run ad-hoc queries and get familiar with the features of Spark (that help you in developing your own standalone Spark applications). It is a very convenient tool to explore the many things available in Spark with immediate feedback. It is one of the many reasons why spark-overview.md#why-spark[Spark is so helpful for tasks to process datasets of any size]. There are variants of Spark shell for different languages: spark-shell for Scala, pyspark for Python and sparkR for R. NOTE: This document (and the book in general) uses spark-shell for Scala only. You can start Spark shell using < spark-shell script>>. $ ./bin/spark-shell scala> spark-shell is an extension of Scala REPL with automatic instantiation of spark-sql-SparkSession.md[SparkSession] as spark (and ROOT:SparkContext.md[] as sc ). [source, scala] \u00b6 scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports spark-sql-SparkSession.md#implicits[Scala SQL's implicits] and spark-sql-SparkSession.md#sql[ sql method]. [source, scala] \u00b6 scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms) [NOTE] \u00b6 When you execute spark-shell you actually execute spark-submit.md[Spark submit] as follows: [options=\"wrap\"] \u00b6 org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell \u00b6 Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to spark-tips-and-tricks.md#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]. \u00b6 === [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of spark-sql-SparkSession.md[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of ROOT:SparkContext.md[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_repl_class_uri]] spark.repl.class.uri | null | Used in spark-shell to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code. Enable INFO logging level for executor:Executor.md[org.apache.spark.executor.Executor] logger to have the value printed out to the logs: INFO Using REPL class URI: [classUri] |===","title":"spark-shell"},{"location":"tools/spark-shell/#source-scala","text":"scala> :type spark org.apache.spark.sql.SparkSession // Learn the current version of Spark in use scala> spark.version res0: String = 2.1.0-SNAPSHOT spark-shell also imports spark-sql-SparkSession.md#implicits[Scala SQL's implicits] and spark-sql-SparkSession.md#sql[ sql method].","title":"[source, scala]"},{"location":"tools/spark-shell/#source-scala_1","text":"scala> :imports 1) import spark.implicits._ (59 terms, 38 are implicit) 2) import spark.sql (1 terms)","title":"[source, scala]"},{"location":"tools/spark-shell/#note","text":"When you execute spark-shell you actually execute spark-submit.md[Spark submit] as follows:","title":"[NOTE]"},{"location":"tools/spark-shell/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"tools/spark-shell/#orgapachesparkdeploysparksubmit-class-orgapachesparkreplmain-name-spark-shell-spark-shell","text":"","title":"org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name Spark shell spark-shell"},{"location":"tools/spark-shell/#set-spark_print_launch_command-to-see-the-entire-command-to-be-executed-refer-to-spark-tips-and-tricksmdspark_print_launch_commandprint-launch-command-of-spark-scripts","text":"=== [[using-spark-shell]] Using Spark shell You start Spark shell using spark-shell script (available in bin directory). $ ./bin/spark-shell Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException Spark context Web UI available at http://10.47.71.138:4040 Spark context available as 'sc' (master = local[*], app id = local-1477858597347). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112) Type in expressions to have them evaluated. Type :help for more information. scala> Spark shell creates an instance of spark-sql-SparkSession.md[SparkSession] under the name spark for you (so you don't have to know the details how to do it yourself on day 1). scala> :type spark org.apache.spark.sql.SparkSession Besides, there is also sc value created which is an instance of ROOT:SparkContext.md[]. scala> :type sc org.apache.spark.SparkContext To close Spark shell, you press Ctrl+D or type in :q (or any subset of :quit ). scala> :q === [[settings]] Settings .Spark Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Spark Property | Default Value | Description | [[spark_repl_class_uri]] spark.repl.class.uri | null | Used in spark-shell to create REPL ClassLoader to load new classes defined in the Scala REPL as a user types code. Enable INFO logging level for executor:Executor.md[org.apache.spark.executor.Executor] logger to have the value printed out to the logs: INFO Using REPL class URI: [classUri] |===","title":"Set SPARK_PRINT_LAUNCH_COMMAND to see the entire command to be executed. Refer to spark-tips-and-tricks.md#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts]."},{"location":"tools/spark-submit/","text":"== Spark Submit -- spark-submit shell script spark-submit shell script allows you to manage your Spark applications. You can < > to a Spark deployment environment for execution, < > or < > of Spark applications. You can find spark-submit script in bin directory of the Spark distribution. $ ./bin/spark-submit Usage: spark-submit [options] <app jar | python file> [app arguments] Usage: spark-submit --kill [submission ID] --master [spark://...] Usage: spark-submit --status [submission ID] --master [spark://...] Usage: spark-submit run-example [options] example-class [example args] ... When executed, spark-submit script first checks whether SPARK_HOME environment variable is set and sets it to the directory that contains bin/spark-submit shell script if not. It then executes spark-class.md[ spark-class shell script] to run < SparkSubmit standalone application>>. CAUTION: FIXME Add Cluster Manager and Deploy Mode to the table below (see options value) [[options-properties-variables]] .Command-Line Options, Spark Properties and Environment Variables (from SparkSubmitArguments 's spark-submit-SparkSubmitArguments.md#loadEnvironmentArguments[loadEnvironmentArguments] and spark-submit-SparkSubmitArguments.md#handle[handle]) [cols=\"1,1,1,2,1\", options=\"header\",width=\"100%\"] |=== | Command-Line Option | Spark Property | Environment Variable | Description | Internal Property | action | | | Defaults to SUBMIT | | --archives | | | | archives | --conf | | | | sparkProperties | --deploy-mode | spark.submit.deployMode | DEPLOY_MODE | Deploy mode | deployMode | --driver-class-path | spark.driver.extraClassPath | | The driver's class path | driverExtraClassPath | --driver-java-options | spark.driver.extraJavaOptions | | The driver's JVM options | driverExtraJavaOptions | --driver-library-path | spark.driver.extraLibraryPath | | The driver's native library path | driverExtraLibraryPath | [[driver-memory]] --driver-memory | [[spark_driver_memory]] spark.driver.memory | SPARK_DRIVER_MEMORY | The driver's memory | driverMemory | --driver-cores | spark.driver.cores | | | driverCores | --exclude-packages | spark.jars.excludes | | | packagesExclusions | --executor-cores | spark.executor.cores | SPARK_EXECUTOR_CORES | The number of executor CPU cores | executorCores | [[executor-memory]] --executor-memory | [[spark.executor.memory]] spark.executor.memory | SPARK_EXECUTOR_MEMORY | An executor's memory | executorMemory | --files | spark.files | | | files | ivyRepoPath | spark.jars.ivy | | | | --jars | spark.jars | | | jars | --keytab | spark.yarn.keytab | | | keytab | --kill | | | submissionToKill and action set to KILL | | --master | spark.master | MASTER | Master URL. Defaults to local[*] | master | --class | | | | mainClass | --name | spark.app.name | SPARK_YARN_APP_NAME (YARN only) | Uses mainClass or the directory off primaryResource when no other ways set it | name | --num-executors | executor:Executor.md#spark.executor.instances[spark.executor.instances] | | | numExecutors | [[packages]] --packages | spark.jars.packages | | | packages | --principal | spark.yarn.principal | | | principal | --properties-file | spark.yarn.principal | | | propertiesFile | --proxy-user | | | | proxyUser | --py-files | | | | pyFiles | --queue | | | | queue | --repositories | | | | repositories | --status | | | submissionToRequestStatusFor and action set to REQUEST_STATUS | | --supervise | | | | supervise | --total-executor-cores | spark.cores.max | | | totalExecutorCores | --verbose | | | | verbose | --version | | | SparkSubmit.printVersionAndExit() | | --help | | | printUsageAndExit(0) | | --usage-error | | | printUsageAndExit(1) | |=== [TIP] \u00b6 Set SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the console, e.g. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja... Refer to spark-tips-and-tricks.md#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts] (or spark-class.md#main[ org.apache.spark.launcher.Main Standalone Application] where this environment variable is actually used). \u00b6 [TIP] \u00b6 Avoid using scala.App trait for a Spark application's main class in Scala as reported in https://issues.apache.org/jira/browse/SPARK-4170[SPARK-4170 Closure problems when running Scala app that \"extends App\"]. Refer to < runMain internal method>> in this document. \u00b6 === [[prepareSubmitEnvironment]] Preparing Submit Environment -- prepareSubmitEnvironment Internal Method [source, scala] \u00b6 prepareSubmitEnvironment(args: SparkSubmitArguments) : (Seq[String], Seq[String], Map[String, String], String) prepareSubmitEnvironment creates a 4-element tuple, i.e. (childArgs, childClasspath, sysProps, childMainClass) . . prepareSubmitEnvironment 's Four-Element Return Tuple [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Element | Description | childArgs | Arguments | childClasspath | Classpath elements | sysProps | spark-properties.md[Spark properties] | childMainClass | Main class |=== prepareSubmitEnvironment uses options to... CAUTION: FIXME NOTE: prepareSubmitEnvironment is used in SparkSubmit object. TIP: See the elements of the return tuple using < --verbose command-line option>>. === [[properties-file]] Custom Spark Properties File -- --properties-file command-line option --properties-file [FILE] --properties-file command-line option sets the path to a file FILE from which Spark loads extra spark-properties.md[Spark properties]. TIP: Spark uses spark-properties.md#spark-defaults-conf[conf/spark-defaults.conf] by default. === [[driver-cores]] Driver Cores in Cluster Deploy Mode -- --driver-cores command-line option --driver-cores NUM --driver-cores command-line option sets the number of cores to NUM for the spark-driver.md[driver] in the spark-deploy-mode.md#cluster[cluster deploy mode]. NOTE: --driver-cores switch is only available for cluster mode (for Standalone, Mesos, and YARN). NOTE: It corresponds to spark-driver.md#spark_driver_cores[spark.driver.cores] setting. NOTE: It is printed out to the standard error output in < >. === [[jars]] Additional JAR Files to Distribute -- --jars command-line option --jars JARS --jars is a comma-separated list of local jars to include on the driver's and executors' classpaths. CAUTION: FIXME === [[files]] Additional Files to Distribute --files command-line option --files FILES CAUTION: FIXME === [[archives]] Additional Archives to Distribute -- --archives command-line option --archives ARCHIVES CAUTION: FIXME === [[queue]] Specifying YARN Resource Queue -- --queue command-line option --queue QUEUE_NAME With --queue you can choose the YARN resource queue to spark-yarn-client.md#createApplicationSubmissionContext[submit a Spark application to]. The yarn/spark-yarn-settings.md#spark.yarn.queue[default queue name is default ]. CAUTION: FIXME What is a queue ? NOTE: It corresponds to yarn/spark-yarn-settings.md#spark.yarn.queue[spark.yarn.queue] Spark's setting. TIP: It is printed out to the standard error output in < >. === [[actions]] Actions ==== [[submit]] Submitting Applications for Execution -- submit method The default action of spark-submit script is to submit a Spark application to a deployment environment for execution. TIP: Use < > command-line switch to know the main class to be executed, arguments, system properties, and classpath (to ensure that the command-line arguments and switches were processed properly). When executed, spark-submit executes submit method. [source, scala] \u00b6 submit(args: SparkSubmitArguments): Unit \u00b6 If proxyUser is set it will...FIXME CAUTION: FIXME Review why and when to use proxyUser . It passes the execution on to < >. ===== [[runMain]] Executing Main -- runMain internal method [source, scala] \u00b6 runMain( childArgs: Seq[String], childClasspath: Seq[String], sysProps: Map[String, String], childMainClass: String, verbose: Boolean): Unit runMain is an internal method to build execution environment and invoke the main method of the Spark application that has been submitted for execution. NOTE: It is exclusively used when < >. When verbose input flag is enabled (i.e. true ) runMain prints out all the input parameters, i.e. childMainClass , childArgs , sysProps , and childClasspath (in that order). Main class: [childMainClass] Arguments: [childArgs one per line] System properties: [sysProps one per line] Classpath elements: [childClasspath one per line] NOTE: Use spark-submit 's < > to enable verbose flag. runMain builds the context classloader (as loader ) depending on spark.driver.userClassPathFirst flag. CAUTION: FIXME Describe spark.driver.userClassPathFirst It < > specified in childClasspath input parameter to the context classloader (that is later responsible for loading the childMainClass main class). NOTE: childClasspath input parameter corresponds to < > with the primary resource if specified in spark-deploy-mode.md#client[client deploy mode]. It sets all the system properties specified in sysProps input parameter (using Java's https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#setProperty-java.lang.String-java.lang.String-[System.setProperty ] method). It creates an instance of childMainClass main class (as mainClass ). NOTE: childMainClass is the main class spark-submit has been invoked with. TIP: Avoid using scala.App trait for a Spark application's main class in Scala as reported in https://issues.apache.org/jira/browse/SPARK-4170[SPARK-4170 Closure problems when running Scala app that \"extends App\"]. If you use scala.App for the main class, you should see the following warning message in the logs: Warning: Subclasses of scala.App may not work correctly. Use a main() method instead. Finally, runMain executes the main method of the Spark application passing in the childArgs arguments. Any SparkUserAppException exceptions lead to System.exit while the others are simply re-thrown. ===== [[addJarToClasspath]] Adding Local Jars to ClassLoader -- addJarToClasspath internal method [source, scala] \u00b6 addJarToClasspath(localJar: String, loader: MutableURLClassLoader) \u00b6 addJarToClasspath is an internal method to add file or local jars (as localJar ) to the loader classloader. Internally, addJarToClasspath resolves the URI of localJar . If the URI is file or local and the file denoted by localJar exists, localJar is added to loader . Otherwise, the following warning is printed out to the logs: Warning: Local jar /path/to/fake.jar does not exist, skipping. For all other URIs, the following warning is printed out to the logs: Warning: Skip remote jar hdfs://fake.jar. NOTE: addJarToClasspath assumes file URI when localJar has no URI specified, e.g. /path/to/local.jar . CAUTION: FIXME What is a URI fragment? How does this change re YARN distributed cache? See Utils#resolveURI . ==== [[kill]] Killing Applications -- --kill command-line option --kill ==== [[status]][[requestStatus]] Requesting Application Status -- --status command-line option --status === [[command-line-options]] Command-line Options Execute spark-submit --help to know about the command-line options supported. \u279c spark git:(master) \u2717 ./bin/spark-submit --help Usage: spark-submit [options] <app jar | python file> [app arguments] Usage: spark-submit --kill [submission ID] --master [spark://...] Usage: spark-submit --status [submission ID] --master [spark://...] Usage: spark-submit run-example [options] example-class [example args] Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (\"client\") or on one of the worker machines inside the cluster (\"cluster\") (Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: \"default\"). --num-executors NUM Number of executors to launch (Default: 2). --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. --class --conf or -c --deploy-mode (see < >) --driver-class-path (see < --driver-class-path command-line option>>) --driver-cores (see < >) --driver-java-options --driver-library-path --driver-memory --executor-memory --files --jars --kill for spark-standalone.md[Standalone cluster mode] only --master --name --packages --exclude-packages --properties-file (see < >) --proxy-user --py-files --repositories --status for spark-standalone.md[Standalone cluster mode] only --total-executor-cores List of switches, i.e. command-line options that do not take parameters: --help or -h --supervise for spark-standalone.md[Standalone cluster mode] only --usage-error --verbose or -v (see < >) --version (see < >) YARN-only options: --archives --executor-cores --keytab --num-executors --principal --queue (see < >) === [[driver-class-path]] --driver-class-path command-line option --driver-class-path command-line option sets the extra class path entries (e.g. jars and directories) that should be added to a driver's JVM. TIP: You should use --driver-class-path in client deploy mode (not ROOT:SparkConf.md[SparkConf]) to ensure that the CLASSPATH is set up with the entries. client deploy mode uses the same JVM for the driver as spark-submit 's. --driver-class-path sets the internal driverExtraClassPath property (when spark-submit-SparkSubmitArguments.md#handle[SparkSubmitArguments.handle] called). It works for all cluster managers and deploy modes. If driverExtraClassPath not set on command-line, the spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] setting is used. NOTE: Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. .Spark Settings in Spark Properties File and on Command Line [options=\"header\",width=\"100%\"] |=== | Setting / System Property | Command-Line Option | Description | spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] | --driver-class-path | Extra class path entries (e.g. jars and directories) to pass to a driver's JVM. |=== === [[version]] Version -- --version command-line option $ ./bin/spark-submit --version Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Branch master Compiled by user jacek on 2016-09-30T07:08:39Z Revision 1fad5596885aab8b32d2307c0edecbae50d5bd7a Url https://github.com/apache/spark.git Type --help for more information. === [[verbose-mode]] Verbose Mode -- --verbose command-line option When spark-submit is executed with --verbose command-line option, it enters verbose mode . In verbose mode, the parsed arguments are printed out to the System error output. FIXME It also prints out propertiesFile and the properties from the file. FIXME === [[deploy-mode]] Deploy Mode -- --deploy-mode command-line option You use spark-submit's --deploy-mode command-line option to specify the spark-deploy-mode.md[deploy mode] for a Spark application. === [[environment-variables]] Environment Variables The following is the list of environment variables that are considered when command-line options are not specified: MASTER for --master SPARK_DRIVER_MEMORY for --driver-memory SPARK_EXECUTOR_MEMORY (see ROOT:SparkContext.md#environment-variables[Environment Variables] in the SparkContext document) SPARK_EXECUTOR_CORES DEPLOY_MODE SPARK_YARN_APP_NAME _SPARK_CMD_USAGE === External packages and custom repositories The spark-submit utility supports specifying external packages using Maven coordinates using --packages and custom repositories using --repositories . ./bin/spark-submit \\ --packages my:awesome:package \\ --repositories s3n://$aws_ak:$aws_sak@bucket/path/to/repo FIXME Why should I care? === [[main]] Launching SparkSubmit Standalone Application -- main method TIP: The source code of the script lives in https://github.com/apache/spark/blob/master/bin/spark-submit . When executed, spark-submit script simply passes the call to spark-class.md[spark-class] with org.apache.spark.deploy.SparkSubmit class followed by command-line arguments. [TIP] \u00b6 spark-class uses the class name -- org.apache.spark.deploy.SparkSubmit -- to parse command-line arguments appropriately. Refer to spark-class.md#main[ org.apache.spark.launcher.Main Standalone Application] \u00b6 It creates an instance of spark-submit-SparkSubmitArguments.md[SparkSubmitArguments]. If in < >, it prints out the application arguments. It then relays the execution to < > (with the application arguments): When no action was explicitly given, it is assumed < > action. < > (when --kill switch is used) < > (when --status switch is used) NOTE: The action can only have one of the three available values: SUBMIT , KILL , or REQUEST_STATUS . ==== [[sparkenv]] spark-env.sh - load additional environment settings spark-env.sh consists of environment settings to configure Spark for your site. export JAVA_HOME=/your/directory/java export HADOOP_HOME=/usr/lib/hadoop export SPARK_WORKER_CORES=2 export SPARK_WORKER_MEMORY=1G spark-env.sh is loaded at the startup of Spark's command line scripts. SPARK_ENV_LOADED env var is to ensure the spark-env.sh script is loaded once. SPARK_CONF_DIR points at the directory with spark-env.sh or $SPARK_HOME/conf is used. spark-env.sh is executed if it exists. $SPARK_HOME/conf directory has spark-env.sh.template file that serves as a template for your own custom configuration. Consult http://spark.apache.org/docs/latest/configuration.html#environment-variables[Environment Variables] in the official documentation.","title":"spark-submit"},{"location":"tools/spark-submit/#tip","text":"Set SPARK_PRINT_LAUNCH_COMMAND environment variable to have the complete Spark command printed out to the console, e.g. $ SPARK_PRINT_LAUNCH_COMMAND=1 ./bin/spark-shell Spark Command: /Library/Ja...","title":"[TIP]"},{"location":"tools/spark-submit/#refer-to-spark-tips-and-tricksmdspark_print_launch_commandprint-launch-command-of-spark-scripts-or-spark-classmdmainorgapachesparklaunchermain-standalone-application-where-this-environment-variable-is-actually-used","text":"","title":"Refer to spark-tips-and-tricks.md#SPARK_PRINT_LAUNCH_COMMAND[Print Launch Command of Spark Scripts] (or spark-class.md#main[org.apache.spark.launcher.Main Standalone Application] where this environment variable is actually used)."},{"location":"tools/spark-submit/#tip_1","text":"Avoid using scala.App trait for a Spark application's main class in Scala as reported in https://issues.apache.org/jira/browse/SPARK-4170[SPARK-4170 Closure problems when running Scala app that \"extends App\"].","title":"[TIP]"},{"location":"tools/spark-submit/#refer-to-wzxhzdk136-internal-method-in-this-document","text":"=== [[prepareSubmitEnvironment]] Preparing Submit Environment -- prepareSubmitEnvironment Internal Method","title":"Refer to &lt;\u0002wzxhzdk:136\u0003 internal method>&gt; in this document."},{"location":"tools/spark-submit/#source-scala","text":"prepareSubmitEnvironment(args: SparkSubmitArguments) : (Seq[String], Seq[String], Map[String, String], String) prepareSubmitEnvironment creates a 4-element tuple, i.e. (childArgs, childClasspath, sysProps, childMainClass) . . prepareSubmitEnvironment 's Four-Element Return Tuple [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Element | Description | childArgs | Arguments | childClasspath | Classpath elements | sysProps | spark-properties.md[Spark properties] | childMainClass | Main class |=== prepareSubmitEnvironment uses options to... CAUTION: FIXME NOTE: prepareSubmitEnvironment is used in SparkSubmit object. TIP: See the elements of the return tuple using < --verbose command-line option>>. === [[properties-file]] Custom Spark Properties File -- --properties-file command-line option --properties-file [FILE] --properties-file command-line option sets the path to a file FILE from which Spark loads extra spark-properties.md[Spark properties]. TIP: Spark uses spark-properties.md#spark-defaults-conf[conf/spark-defaults.conf] by default. === [[driver-cores]] Driver Cores in Cluster Deploy Mode -- --driver-cores command-line option --driver-cores NUM --driver-cores command-line option sets the number of cores to NUM for the spark-driver.md[driver] in the spark-deploy-mode.md#cluster[cluster deploy mode]. NOTE: --driver-cores switch is only available for cluster mode (for Standalone, Mesos, and YARN). NOTE: It corresponds to spark-driver.md#spark_driver_cores[spark.driver.cores] setting. NOTE: It is printed out to the standard error output in < >. === [[jars]] Additional JAR Files to Distribute -- --jars command-line option --jars JARS --jars is a comma-separated list of local jars to include on the driver's and executors' classpaths. CAUTION: FIXME === [[files]] Additional Files to Distribute --files command-line option --files FILES CAUTION: FIXME === [[archives]] Additional Archives to Distribute -- --archives command-line option --archives ARCHIVES CAUTION: FIXME === [[queue]] Specifying YARN Resource Queue -- --queue command-line option --queue QUEUE_NAME With --queue you can choose the YARN resource queue to spark-yarn-client.md#createApplicationSubmissionContext[submit a Spark application to]. The yarn/spark-yarn-settings.md#spark.yarn.queue[default queue name is default ]. CAUTION: FIXME What is a queue ? NOTE: It corresponds to yarn/spark-yarn-settings.md#spark.yarn.queue[spark.yarn.queue] Spark's setting. TIP: It is printed out to the standard error output in < >. === [[actions]] Actions ==== [[submit]] Submitting Applications for Execution -- submit method The default action of spark-submit script is to submit a Spark application to a deployment environment for execution. TIP: Use < > command-line switch to know the main class to be executed, arguments, system properties, and classpath (to ensure that the command-line arguments and switches were processed properly). When executed, spark-submit executes submit method.","title":"[source, scala]"},{"location":"tools/spark-submit/#source-scala_1","text":"","title":"[source, scala]"},{"location":"tools/spark-submit/#submitargs-sparksubmitarguments-unit","text":"If proxyUser is set it will...FIXME CAUTION: FIXME Review why and when to use proxyUser . It passes the execution on to < >. ===== [[runMain]] Executing Main -- runMain internal method","title":"submit(args: SparkSubmitArguments): Unit"},{"location":"tools/spark-submit/#source-scala_2","text":"runMain( childArgs: Seq[String], childClasspath: Seq[String], sysProps: Map[String, String], childMainClass: String, verbose: Boolean): Unit runMain is an internal method to build execution environment and invoke the main method of the Spark application that has been submitted for execution. NOTE: It is exclusively used when < >. When verbose input flag is enabled (i.e. true ) runMain prints out all the input parameters, i.e. childMainClass , childArgs , sysProps , and childClasspath (in that order). Main class: [childMainClass] Arguments: [childArgs one per line] System properties: [sysProps one per line] Classpath elements: [childClasspath one per line] NOTE: Use spark-submit 's < > to enable verbose flag. runMain builds the context classloader (as loader ) depending on spark.driver.userClassPathFirst flag. CAUTION: FIXME Describe spark.driver.userClassPathFirst It < > specified in childClasspath input parameter to the context classloader (that is later responsible for loading the childMainClass main class). NOTE: childClasspath input parameter corresponds to < > with the primary resource if specified in spark-deploy-mode.md#client[client deploy mode]. It sets all the system properties specified in sysProps input parameter (using Java's https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#setProperty-java.lang.String-java.lang.String-[System.setProperty ] method). It creates an instance of childMainClass main class (as mainClass ). NOTE: childMainClass is the main class spark-submit has been invoked with. TIP: Avoid using scala.App trait for a Spark application's main class in Scala as reported in https://issues.apache.org/jira/browse/SPARK-4170[SPARK-4170 Closure problems when running Scala app that \"extends App\"]. If you use scala.App for the main class, you should see the following warning message in the logs: Warning: Subclasses of scala.App may not work correctly. Use a main() method instead. Finally, runMain executes the main method of the Spark application passing in the childArgs arguments. Any SparkUserAppException exceptions lead to System.exit while the others are simply re-thrown. ===== [[addJarToClasspath]] Adding Local Jars to ClassLoader -- addJarToClasspath internal method","title":"[source, scala]"},{"location":"tools/spark-submit/#source-scala_3","text":"","title":"[source, scala]"},{"location":"tools/spark-submit/#addjartoclasspathlocaljar-string-loader-mutableurlclassloader","text":"addJarToClasspath is an internal method to add file or local jars (as localJar ) to the loader classloader. Internally, addJarToClasspath resolves the URI of localJar . If the URI is file or local and the file denoted by localJar exists, localJar is added to loader . Otherwise, the following warning is printed out to the logs: Warning: Local jar /path/to/fake.jar does not exist, skipping. For all other URIs, the following warning is printed out to the logs: Warning: Skip remote jar hdfs://fake.jar. NOTE: addJarToClasspath assumes file URI when localJar has no URI specified, e.g. /path/to/local.jar . CAUTION: FIXME What is a URI fragment? How does this change re YARN distributed cache? See Utils#resolveURI . ==== [[kill]] Killing Applications -- --kill command-line option --kill ==== [[status]][[requestStatus]] Requesting Application Status -- --status command-line option --status === [[command-line-options]] Command-line Options Execute spark-submit --help to know about the command-line options supported. \u279c spark git:(master) \u2717 ./bin/spark-submit --help Usage: spark-submit [options] <app jar | python file> [app arguments] Usage: spark-submit --kill [submission ID] --master [spark://...] Usage: spark-submit --status [submission ID] --master [spark://...] Usage: spark-submit run-example [options] example-class [example args] Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (\"client\") or on one of the worker machines inside the cluster (\"cluster\") (Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: \"default\"). --num-executors NUM Number of executors to launch (Default: 2). --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. --class --conf or -c --deploy-mode (see < >) --driver-class-path (see < --driver-class-path command-line option>>) --driver-cores (see < >) --driver-java-options --driver-library-path --driver-memory --executor-memory --files --jars --kill for spark-standalone.md[Standalone cluster mode] only --master --name --packages --exclude-packages --properties-file (see < >) --proxy-user --py-files --repositories --status for spark-standalone.md[Standalone cluster mode] only --total-executor-cores List of switches, i.e. command-line options that do not take parameters: --help or -h --supervise for spark-standalone.md[Standalone cluster mode] only --usage-error --verbose or -v (see < >) --version (see < >) YARN-only options: --archives --executor-cores --keytab --num-executors --principal --queue (see < >) === [[driver-class-path]] --driver-class-path command-line option --driver-class-path command-line option sets the extra class path entries (e.g. jars and directories) that should be added to a driver's JVM. TIP: You should use --driver-class-path in client deploy mode (not ROOT:SparkConf.md[SparkConf]) to ensure that the CLASSPATH is set up with the entries. client deploy mode uses the same JVM for the driver as spark-submit 's. --driver-class-path sets the internal driverExtraClassPath property (when spark-submit-SparkSubmitArguments.md#handle[SparkSubmitArguments.handle] called). It works for all cluster managers and deploy modes. If driverExtraClassPath not set on command-line, the spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] setting is used. NOTE: Command-line options (e.g. --driver-class-path ) have higher precedence than their corresponding Spark settings in a Spark properties file (e.g. spark.driver.extraClassPath ). You can therefore control the final settings by overriding Spark settings on command line using the command-line options. .Spark Settings in Spark Properties File and on Command Line [options=\"header\",width=\"100%\"] |=== | Setting / System Property | Command-Line Option | Description | spark-driver.md#spark_driver_extraClassPath[spark.driver.extraClassPath] | --driver-class-path | Extra class path entries (e.g. jars and directories) to pass to a driver's JVM. |=== === [[version]] Version -- --version command-line option $ ./bin/spark-submit --version Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.1.0-SNAPSHOT /_/ Branch master Compiled by user jacek on 2016-09-30T07:08:39Z Revision 1fad5596885aab8b32d2307c0edecbae50d5bd7a Url https://github.com/apache/spark.git Type --help for more information. === [[verbose-mode]] Verbose Mode -- --verbose command-line option When spark-submit is executed with --verbose command-line option, it enters verbose mode . In verbose mode, the parsed arguments are printed out to the System error output. FIXME It also prints out propertiesFile and the properties from the file. FIXME === [[deploy-mode]] Deploy Mode -- --deploy-mode command-line option You use spark-submit's --deploy-mode command-line option to specify the spark-deploy-mode.md[deploy mode] for a Spark application. === [[environment-variables]] Environment Variables The following is the list of environment variables that are considered when command-line options are not specified: MASTER for --master SPARK_DRIVER_MEMORY for --driver-memory SPARK_EXECUTOR_MEMORY (see ROOT:SparkContext.md#environment-variables[Environment Variables] in the SparkContext document) SPARK_EXECUTOR_CORES DEPLOY_MODE SPARK_YARN_APP_NAME _SPARK_CMD_USAGE === External packages and custom repositories The spark-submit utility supports specifying external packages using Maven coordinates using --packages and custom repositories using --repositories . ./bin/spark-submit \\ --packages my:awesome:package \\ --repositories s3n://$aws_ak:$aws_sak@bucket/path/to/repo FIXME Why should I care? === [[main]] Launching SparkSubmit Standalone Application -- main method TIP: The source code of the script lives in https://github.com/apache/spark/blob/master/bin/spark-submit . When executed, spark-submit script simply passes the call to spark-class.md[spark-class] with org.apache.spark.deploy.SparkSubmit class followed by command-line arguments.","title":"addJarToClasspath(localJar: String, loader: MutableURLClassLoader)"},{"location":"tools/spark-submit/#tip_2","text":"spark-class uses the class name -- org.apache.spark.deploy.SparkSubmit -- to parse command-line arguments appropriately.","title":"[TIP]"},{"location":"tools/spark-submit/#refer-to-spark-classmdmainorgapachesparklaunchermain-standalone-application","text":"It creates an instance of spark-submit-SparkSubmitArguments.md[SparkSubmitArguments]. If in < >, it prints out the application arguments. It then relays the execution to < > (with the application arguments): When no action was explicitly given, it is assumed < > action. < > (when --kill switch is used) < > (when --status switch is used) NOTE: The action can only have one of the three available values: SUBMIT , KILL , or REQUEST_STATUS . ==== [[sparkenv]] spark-env.sh - load additional environment settings spark-env.sh consists of environment settings to configure Spark for your site. export JAVA_HOME=/your/directory/java export HADOOP_HOME=/usr/lib/hadoop export SPARK_WORKER_CORES=2 export SPARK_WORKER_MEMORY=1G spark-env.sh is loaded at the startup of Spark's command line scripts. SPARK_ENV_LOADED env var is to ensure the spark-env.sh script is loaded once. SPARK_CONF_DIR points at the directory with spark-env.sh or $SPARK_HOME/conf is used. spark-env.sh is executed if it exists. $SPARK_HOME/conf directory has spark-env.sh.template file that serves as a template for your own custom configuration. Consult http://spark.apache.org/docs/latest/configuration.html#environment-variables[Environment Variables] in the official documentation.","title":"Refer to spark-class.md#main[org.apache.spark.launcher.Main Standalone Application]"},{"location":"webui/","text":"Web UI \u2014 Spark Application's Web Console \u00b6 Web UI (aka Application UI or webUI or Spark UI ) is the web interface of a Spark application to monitor and inspect Spark jobs in a web browser. web UI is available at http://[driverHostname]:4040 by default. NOTE: The default port can be changed using spark-webui-properties.md#spark.ui.port[spark.ui.port] configuration property. SparkContext will increase the port if it is already taken until a free one is found. web UI comes with the following tabs ( pages ): . spark-webui-jobs.md[Jobs] . spark-webui-stages.md[Stages] . spark-webui-storage.md[Storage] . spark-webui-environment.md[Environment] . spark-webui-executors.md[Executors] TIP: You can use the web UI after the application has finished by persisting events (using spark-history-server:EventLoggingListener.md[EventLoggingListener]) and using spark-history-server:index.md[Spark History Server]. NOTE: All the information that is displayed in web UI is available thanks to spark-webui-JobProgressListener.md[JobProgressListener] and other ROOT:SparkListener.md[]s. One could say that web UI is a web layer over Spark listeners.","title":"Web UI"},{"location":"webui/#web-ui-spark-applications-web-console","text":"Web UI (aka Application UI or webUI or Spark UI ) is the web interface of a Spark application to monitor and inspect Spark jobs in a web browser. web UI is available at http://[driverHostname]:4040 by default. NOTE: The default port can be changed using spark-webui-properties.md#spark.ui.port[spark.ui.port] configuration property. SparkContext will increase the port if it is already taken until a free one is found. web UI comes with the following tabs ( pages ): . spark-webui-jobs.md[Jobs] . spark-webui-stages.md[Stages] . spark-webui-storage.md[Storage] . spark-webui-environment.md[Environment] . spark-webui-executors.md[Executors] TIP: You can use the web UI after the application has finished by persisting events (using spark-history-server:EventLoggingListener.md[EventLoggingListener]) and using spark-history-server:index.md[Spark History Server]. NOTE: All the information that is displayed in web UI is available thanks to spark-webui-JobProgressListener.md[JobProgressListener] and other ROOT:SparkListener.md[]s. One could say that web UI is a web layer over Spark listeners.","title":"Web UI &mdash; Spark Application's Web Console"},{"location":"webui/AllStagesPage/","text":"== [[AllStagesPage]] Stages for All Jobs Page AllStagesPage is a web page (section) that is registered with the spark-webui-StagesTab.md[Stages tab] that < > - active, pending, completed, and failed stages with their count. .Stages Tab in web UI for FAIR scheduling mode (with pools only) image::spark-webui-stages-alljobs.png[align=\"center\"] [[pool-names]] In spark-scheduler-SchedulingMode.md#FAIR[FAIR scheduling mode] you have access to the table showing the scheduler pools as well as the pool names per stage. NOTE: Pool names are calculated using ROOT:SparkContext.md#getAllPools[SparkContext.getAllPools]. Internally, AllStagesPage is a spark-webui-WebUIPage.md[WebUIPage] with access to the parent spark-webui-StagesTab.md[Stages tab] and more importantly the spark-webui-JobProgressListener.md[JobProgressListener] to have access to current state of the entire Spark application. === [[render]] Rendering AllStagesPage (render method) [source, scala] \u00b6 render(request: HttpServletRequest): Seq[Node] \u00b6 render generates a HTML page to display in a web browser. It uses the parent's spark-webui-JobProgressListener.md[JobProgressListener] to know about: active stages (as activeStages ) pending stages (as pendingStages ) completed stages (as completedStages ) failed stages (as failedStages ) the number of completed stages (as numCompletedStages ) the number of failed stages (as numFailedStages ) NOTE: Stage information is available as scheduler:spark-scheduler-StageInfo.md[StageInfo] object. There are 4 different tables for the different states of stages - active, pending, completed, and failed. They are displayed only when there are stages in a given state. .Stages Tab in web UI for FAIR scheduling mode (with pools and stages) image::spark-webui-stages.png[align=\"center\"] You could also notice \"retry\" for stage when it was retried. CAUTION: FIXME A screenshot","title":"AllStagesPage"},{"location":"webui/AllStagesPage/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/AllStagesPage/#renderrequest-httpservletrequest-seqnode","text":"render generates a HTML page to display in a web browser. It uses the parent's spark-webui-JobProgressListener.md[JobProgressListener] to know about: active stages (as activeStages ) pending stages (as pendingStages ) completed stages (as completedStages ) failed stages (as failedStages ) the number of completed stages (as numCompletedStages ) the number of failed stages (as numFailedStages ) NOTE: Stage information is available as scheduler:spark-scheduler-StageInfo.md[StageInfo] object. There are 4 different tables for the different states of stages - active, pending, completed, and failed. They are displayed only when there are stages in a given state. .Stages Tab in web UI for FAIR scheduling mode (with pools and stages) image::spark-webui-stages.png[align=\"center\"] You could also notice \"retry\" for stage when it was retried. CAUTION: FIXME A screenshot","title":"render(request: HttpServletRequest): Seq[Node]"},{"location":"webui/BlockStatusListener/","text":"== [[BlockStatusListener]] BlockStatusListener Spark Listener BlockStatusListener is a ROOT:SparkListener.md[]s that tracks storage:BlockManager.md[BlockManagers] and the blocks for spark-webui-storage.md[Storage tab] in web UI. . BlockStatusListener Registries [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Registry | Description | [[blockManagers]] blockManagers | The lookup table for a collection of storage:BlockId.md[] and BlockUIData per storage:BlockManagerId.md[] |=== CAUTION: FIXME When are the events posted? . BlockStatusListener Event Handlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Event Handler | Description | onBlockManagerAdded | Registers a BlockManager in < > internal registry (with no blocks). | onBlockManagerRemoved | Removes a BlockManager from < > internal registry. | onBlockUpdated | Puts an updated BlockUIData for BlockId for BlockManagerId in < > internal registry. Ignores updates for unregistered BlockManager s or non- StreamBlockId s. For invalid storage:StorageLevel.md[StorageLevel]s (i.e. they do not use a memory or a disk or no replication) the block is removed. |===","title":"BlockStatusListener"},{"location":"webui/EnvironmentListener/","text":"== [[EnvironmentListener]] EnvironmentListener Spark Listener CAUTION: FIXME","title":"EnvironmentListener"},{"location":"webui/EnvironmentPage/","text":"= EnvironmentPage [[prefix]] EnvironmentPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. EnvironmentPage is < > exclusively when EnvironmentTab is spark-webui-EnvironmentTab.md#creating-instance[created]. == [[creating-instance]] Creating EnvironmentPage Instance EnvironmentPage takes the following when created: [[parent]] Parent spark-webui-EnvironmentTab.md[EnvironmentTab] [[conf]] ROOT:SparkConf.md[SparkConf] [[store]] core:AppStatusStore.md[]","title":"EnvironmentPage"},{"location":"webui/EnvironmentTab/","text":"= EnvironmentTab [[prefix]] EnvironmentTab is a spark-webui-SparkUITab.md[SparkUITab] with environment spark-webui-SparkUITab.md#prefix[prefix]. EnvironmentTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] EnvironmentTab takes the following when created: [[parent]] Parent spark-webui-SparkUI.md[SparkUI] [[store]] core:AppStatusStore.md[] When created, EnvironmentTab creates the spark-webui-EnvironmentPage.md#creating-instance[EnvironmentPage] page and spark-webui-WebUITab.md#attachPage[attaches] it immediately.","title":"EnvironmentTab"},{"location":"webui/ExecutorThreadDumpPage/","text":"== [[ExecutorThreadDumpPage]] ExecutorThreadDumpPage [[prefix]] ExecutorThreadDumpPage is a spark-webui-WebUIPage.md[WebUIPage] with threadDump spark-webui-WebUIPage.md#prefix[prefix]. ExecutorThreadDumpPage is < > exclusively when ExecutorsTab is spark-webui-ExecutorsTab.md#creating-instance[created] (with spark.ui.threadDumpsEnabled configuration property enabled). NOTE: spark.ui.threadDumpsEnabled configuration property is enabled (i.e. true ) by default. === [[creating-instance]] Creating ExecutorThreadDumpPage Instance ExecutorThreadDumpPage takes the following when created: [[parent]] spark-webui-SparkUITab.md[SparkUITab] [[sc]] Optional ROOT:SparkContext.md[]","title":"ExecutorThreadDumpPage"},{"location":"webui/ExecutorsListener/","text":"== [[ExecutorsListener]] ExecutorsListener Spark Listener ExecutorsListener is a ROOT:SparkListener.md[] that tracks < > in a Spark application for spark-webui-StagePage.md[Stage Details] page, spark-webui-jobs.md[Jobs] tab and /allexecutors REST endpoint. [[SparkListener-callbacks]] .ExecutorsListener's SparkListener Callbacks (in alphabetical order) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Event Handler | Description | < > | May create an entry for the driver in < > registry | < > | May create an entry in < > registry. It also makes sure that the number of entries for dead executors does not exceed spark-webui-properties.md#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors] and removes excess. Adds an entry to < > registry and optionally removes the oldest if the number of entries exceeds spark-webui-properties.md#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum]. | < > | FIXME | < > | Marks an executor dead in < > registry. Adds an entry to < > registry and optionally removes the oldest if the number of entries exceeds spark-webui-properties.md#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum]. | < > | FIXME | < > | FIXME | < > | FIXME | < > | May create an entry for an executor in < > registry. | < > | May create an entry for an executor in < > registry. |=== ExecutorsListener requires a spark-webui-StorageStatusListener.md[StorageStatusListener] and ROOT:SparkConf.md[SparkConf]. [[internal-registries]] .ExecutorsListener's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Registry | Description | [[executorToTaskSummary]] executorToTaskSummary | The lookup table for ExecutorTaskSummary per executor id. Used to build a ExecutorSummary for /allexecutors REST endpoint, to display stdout and stderr logs in spark-webui-StagePage.md#tasks[Tasks] and spark-webui-StagePage.md#aggregated-metrics-by-executor[Aggregated Metrics by Executor] sections in spark-webui-StagePage.md[Stage Details] page. | [[executorEvents]] executorEvents | A collection of ROOT:SparkListener.md#SparkListenerEvent[SparkListenerEvent]s. Used to build the event timeline in spark-webui-AllJobsPage.md[AllJobsPage] and spark-webui-jobs.md#JobPage[Details for Job] pages. |=== === [[updateExecutorBlacklist]] updateExecutorBlacklist Method CAUTION: FIXME === [[onExecutorBlacklisted]] Intercepting Executor Was Blacklisted Events -- onExecutorBlacklisted Callback CAUTION: FIXME === [[onExecutorUnblacklisted]] Intercepting Executor Is No Longer Blacklisted Events -- onExecutorUnblacklisted Callback CAUTION: FIXME === [[onNodeBlacklisted]] Intercepting Node Was Blacklisted Events -- onNodeBlacklisted Callback CAUTION: FIXME === [[onNodeUnblacklisted]] Intercepting Node Is No Longer Blacklisted Events -- onNodeUnblacklisted Callback CAUTION: FIXME === [[onApplicationStart]] Intercepting Application Started Events -- onApplicationStart Callback [source, scala] \u00b6 onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit \u00b6 NOTE: onApplicationStart is part of ROOT:SparkListener.md#onApplicationStart[SparkListener contract] to announce that a Spark application has been started. onApplicationStart takes driverLogs property from the input applicationStart (if defined) and finds the driver's active spark-blockmanager-StorageStatus.md[StorageStatus] (using the current spark-webui-StorageStatusListener.md[StorageStatusListener]). onApplicationStart then uses the driver's spark-blockmanager-StorageStatus.md[StorageStatus] (if defined) to set executorLogs . .ExecutorTaskSummary and ExecutorInfo Attributes [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | SparkListenerApplicationStart Attribute | executorLogs | driverLogs (if defined) |=== === [[onExecutorAdded]] Intercepting Executor Added Events -- onExecutorAdded Callback [source, scala] \u00b6 onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit \u00b6 NOTE: onExecutorAdded is part of ROOT:SparkListener.md#onExecutorAdded[SparkListener contract] to announce that a new executor has been registered with the Spark application. onExecutorAdded finds the executor (using the input executorAdded ) in the internal < executorToTaskSummary registry>> and sets the attributes. If not found, onExecutorAdded creates a new entry. .ExecutorTaskSummary and ExecutorInfo Attributes [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | ExecutorInfo Attribute | executorLogs | logUrlMap | totalCores | totalCores | tasksMax | totalCores / ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] |=== onExecutorAdded adds the input executorAdded to < executorEvents collection>>. If the number of elements in executorEvents collection is greater than spark-webui-properties.md#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum] configuration property, the first/oldest event is removed. onExecutorAdded removes the oldest dead executor from < executorToTaskSummary lookup table>> if their number is greater than spark-webui-properties.md#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors]. === [[onExecutorRemoved]] Intercepting Executor Removed Events -- onExecutorRemoved Callback [source, scala] \u00b6 onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit \u00b6 NOTE: onExecutorRemoved is part of ROOT:SparkListener.md#onExecutorRemoved[SparkListener contract] to announce that an executor has been unregistered with the Spark application. onExecutorRemoved adds the input executorRemoved to < executorEvents collection>>. It then removes the oldest event if the number of elements in executorEvents collection is greater than spark-webui-properties.md#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum] configuration property. The executor is marked as removed/inactive in < executorToTaskSummary lookup table>>. === [[onTaskStart]] Intercepting Task Started Events -- onTaskStart Callback [source, scala] \u00b6 onTaskStart(taskStart: SparkListenerTaskStart): Unit \u00b6 NOTE: onTaskStart is part of ROOT:SparkListener.md#onTaskStart[SparkListener contract] to announce that a task has been started. onTaskStart increments tasksActive for the executor (using the input SparkListenerTaskStart ). .ExecutorTaskSummary and SparkListenerTaskStart Attributes [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | Description | tasksActive | Uses taskStart.taskInfo.executorId . |=== === [[onTaskEnd]] Intercepting Task End Events -- onTaskEnd Callback [source, scala] \u00b6 onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit \u00b6 NOTE: onTaskEnd is part of ROOT:SparkListener.md#onTaskEnd[SparkListener contract] to announce that a task has ended. onTaskEnd takes spark-scheduler-TaskInfo.md[TaskInfo] from the input taskEnd (if available). Depending on the reason for SparkListenerTaskEnd onTaskEnd does the following: . onTaskEnd Behaviour per SparkListenerTaskEnd Reason [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | SparkListenerTaskEnd Reason | onTaskEnd Behaviour | Resubmitted | Does nothing | ExceptionFailure | Increment tasksFailed | anything | Increment tasksComplete |=== tasksActive is decremented but only when the number of active tasks for the executor is greater than 0 . .ExecutorTaskSummary and onTaskEnd Behaviour [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | Description | tasksActive | Decremented if greater than 0. | duration | Uses taskEnd.taskInfo.duration |=== If the TaskMetrics (in the input taskEnd ) is available, the metrics are added to the taskSummary for the task's executor. .Task Metrics and Task Summary [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Task Summary | Task Metric | inputBytes | inputMetrics.bytesRead | inputRecords | inputMetrics.recordsRead | outputBytes | outputMetrics.bytesWritten | outputRecords | outputMetrics.recordsWritten | shuffleRead | shuffleReadMetrics.remoteBytesRead | shuffleWrite | executor:ShuffleWriteMetrics.md#bytesWritten[shuffleWriteMetrics.bytesWritten] | jvmGCTime | metrics.jvmGCTime |=== === [[activeStorageStatusList]] Finding Active BlockManagers -- activeStorageStatusList Method [source, scala] \u00b6 activeStorageStatusList: Seq[StorageStatus] \u00b6 activeStorageStatusList requests < > for spark-webui-StorageStatusListener.md#storageStatusList[active BlockManagers (on executors)]. [NOTE] \u00b6 activeStorageStatusList is used when: FIXME AllExecutorListResource does executorList ExecutorListResource does executorList ExecutorsListener gets informed that the < >, < >, and < > \u00b6","title":"ExecutorsListener"},{"location":"webui/ExecutorsListener/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/ExecutorsListener/#onapplicationstartapplicationstart-sparklistenerapplicationstart-unit","text":"NOTE: onApplicationStart is part of ROOT:SparkListener.md#onApplicationStart[SparkListener contract] to announce that a Spark application has been started. onApplicationStart takes driverLogs property from the input applicationStart (if defined) and finds the driver's active spark-blockmanager-StorageStatus.md[StorageStatus] (using the current spark-webui-StorageStatusListener.md[StorageStatusListener]). onApplicationStart then uses the driver's spark-blockmanager-StorageStatus.md[StorageStatus] (if defined) to set executorLogs . .ExecutorTaskSummary and ExecutorInfo Attributes [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | SparkListenerApplicationStart Attribute | executorLogs | driverLogs (if defined) |=== === [[onExecutorAdded]] Intercepting Executor Added Events -- onExecutorAdded Callback","title":"onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit"},{"location":"webui/ExecutorsListener/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/ExecutorsListener/#onexecutoraddedexecutoradded-sparklistenerexecutoradded-unit","text":"NOTE: onExecutorAdded is part of ROOT:SparkListener.md#onExecutorAdded[SparkListener contract] to announce that a new executor has been registered with the Spark application. onExecutorAdded finds the executor (using the input executorAdded ) in the internal < executorToTaskSummary registry>> and sets the attributes. If not found, onExecutorAdded creates a new entry. .ExecutorTaskSummary and ExecutorInfo Attributes [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | ExecutorInfo Attribute | executorLogs | logUrlMap | totalCores | totalCores | tasksMax | totalCores / ROOT:configuration-properties.md#spark.task.cpus[spark.task.cpus] |=== onExecutorAdded adds the input executorAdded to < executorEvents collection>>. If the number of elements in executorEvents collection is greater than spark-webui-properties.md#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum] configuration property, the first/oldest event is removed. onExecutorAdded removes the oldest dead executor from < executorToTaskSummary lookup table>> if their number is greater than spark-webui-properties.md#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors]. === [[onExecutorRemoved]] Intercepting Executor Removed Events -- onExecutorRemoved Callback","title":"onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit"},{"location":"webui/ExecutorsListener/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/ExecutorsListener/#onexecutorremovedexecutorremoved-sparklistenerexecutorremoved-unit","text":"NOTE: onExecutorRemoved is part of ROOT:SparkListener.md#onExecutorRemoved[SparkListener contract] to announce that an executor has been unregistered with the Spark application. onExecutorRemoved adds the input executorRemoved to < executorEvents collection>>. It then removes the oldest event if the number of elements in executorEvents collection is greater than spark-webui-properties.md#spark.ui.timeline.executors.maximum[spark.ui.timeline.executors.maximum] configuration property. The executor is marked as removed/inactive in < executorToTaskSummary lookup table>>. === [[onTaskStart]] Intercepting Task Started Events -- onTaskStart Callback","title":"onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit"},{"location":"webui/ExecutorsListener/#source-scala_3","text":"","title":"[source, scala]"},{"location":"webui/ExecutorsListener/#ontaskstarttaskstart-sparklistenertaskstart-unit","text":"NOTE: onTaskStart is part of ROOT:SparkListener.md#onTaskStart[SparkListener contract] to announce that a task has been started. onTaskStart increments tasksActive for the executor (using the input SparkListenerTaskStart ). .ExecutorTaskSummary and SparkListenerTaskStart Attributes [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | Description | tasksActive | Uses taskStart.taskInfo.executorId . |=== === [[onTaskEnd]] Intercepting Task End Events -- onTaskEnd Callback","title":"onTaskStart(taskStart: SparkListenerTaskStart): Unit"},{"location":"webui/ExecutorsListener/#source-scala_4","text":"","title":"[source, scala]"},{"location":"webui/ExecutorsListener/#ontaskendtaskend-sparklistenertaskend-unit","text":"NOTE: onTaskEnd is part of ROOT:SparkListener.md#onTaskEnd[SparkListener contract] to announce that a task has ended. onTaskEnd takes spark-scheduler-TaskInfo.md[TaskInfo] from the input taskEnd (if available). Depending on the reason for SparkListenerTaskEnd onTaskEnd does the following: . onTaskEnd Behaviour per SparkListenerTaskEnd Reason [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | SparkListenerTaskEnd Reason | onTaskEnd Behaviour | Resubmitted | Does nothing | ExceptionFailure | Increment tasksFailed | anything | Increment tasksComplete |=== tasksActive is decremented but only when the number of active tasks for the executor is greater than 0 . .ExecutorTaskSummary and onTaskEnd Behaviour [options=\"header\",width=\"100%\"] |=== | ExecutorTaskSummary Attribute | Description | tasksActive | Decremented if greater than 0. | duration | Uses taskEnd.taskInfo.duration |=== If the TaskMetrics (in the input taskEnd ) is available, the metrics are added to the taskSummary for the task's executor. .Task Metrics and Task Summary [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Task Summary | Task Metric | inputBytes | inputMetrics.bytesRead | inputRecords | inputMetrics.recordsRead | outputBytes | outputMetrics.bytesWritten | outputRecords | outputMetrics.recordsWritten | shuffleRead | shuffleReadMetrics.remoteBytesRead | shuffleWrite | executor:ShuffleWriteMetrics.md#bytesWritten[shuffleWriteMetrics.bytesWritten] | jvmGCTime | metrics.jvmGCTime |=== === [[activeStorageStatusList]] Finding Active BlockManagers -- activeStorageStatusList Method","title":"onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit"},{"location":"webui/ExecutorsListener/#source-scala_5","text":"","title":"[source, scala]"},{"location":"webui/ExecutorsListener/#activestoragestatuslist-seqstoragestatus","text":"activeStorageStatusList requests < > for spark-webui-StorageStatusListener.md#storageStatusList[active BlockManagers (on executors)].","title":"activeStorageStatusList: Seq[StorageStatus]"},{"location":"webui/ExecutorsListener/#note","text":"activeStorageStatusList is used when: FIXME AllExecutorListResource does executorList ExecutorListResource does executorList","title":"[NOTE]"},{"location":"webui/ExecutorsListener/#executorslistener-gets-informed-that-the-and","text":"","title":"ExecutorsListener gets informed that the &lt;&gt;, &lt;&gt;, and &lt;&gt;"},{"location":"webui/ExecutorsPage/","text":"== [[ExecutorsPage]] ExecutorsPage [[prefix]] ExecutorsPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. ExecutorsPage is < > exclusively when ExecutorsTab is spark-webui-ExecutorsTab.md#creating-instance[created]. === [[creating-instance]] Creating ExecutorsPage Instance ExecutorsPage takes the following when created: [[parent]] Parent spark-webui-SparkUITab.md[SparkUITab] [[threadDumpEnabled]] threadDumpEnabled flag","title":"ExecutorsPage"},{"location":"webui/ExecutorsTab/","text":"== [[ExecutorsTab]] ExecutorsTab [[prefix]] ExecutorsTab is a spark-webui-SparkUITab.md[SparkUITab] with executors spark-webui-SparkUITab.md#prefix[prefix]. ExecutorsTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] [[parent]] ExecutorsTab takes the parent spark-webui-SparkUI.md[SparkUI] when created. When < >, ExecutorsTab creates the following pages and spark-webui-WebUITab.md#attachPage[attaches] them immediately: spark-webui-ExecutorsPage.md[ExecutorsPage] spark-webui-ExecutorThreadDumpPage.md[ExecutorThreadDumpPage] ExecutorsTab uses spark-webui-executors-ExecutorsListener.md[ExecutorsListener] to collect information about executors in a Spark application.","title":"ExecutorsTab"},{"location":"webui/JettyUtils/","text":"== [[JettyUtils]] JettyUtils JettyUtils is a set of < > for creating Jetty HTTP Server-specific components. [[utility-methods]] .JettyUtils's Utility Methods [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | < > | Creates an HttpServlet | < > | Creates a Handler for a static content | < > | Creates a ServletContextHandler for a path < > === === [[createServletHandler]] Creating ServletContextHandler for Path -- createServletHandler Method [source, scala] \u00b6 createServletHandler( path: String, servlet: HttpServlet, basePath: String): ServletContextHandler createServletHandler T <: AnyRef : ServletContextHandler // <1> <1> Uses the first three-argument createServletHandler createServletHandler ...FIXME [NOTE] \u00b6 createServletHandler is used when: WebUI is requested to spark-webui-WebUI.md#attachPage[attachPage] MetricsServlet is requested to getHandlers * Spark Standalone's WorkerWebUI is requested to initialize \u00b6 === [[createServlet]] Creating HttpServlet -- createServlet Method [source, scala] \u00b6 createServlet T <: AnyRef : HttpServlet createServlet creates the X-Frame-Options header that can be either ALLOW-FROM with the value of spark-webui-properties.md#spark.ui.allowFramingFrom[spark.ui.allowFramingFrom] configuration property if defined or SAMEORIGIN . createServlet creates a Java Servlets HttpServlet with support for GET requests. When handling GET requests, the HttpServlet first checks view permissions of the remote user (by requesting the SecurityManager to checkUIViewPermissions of the remote user). [TIP] \u00b6 Enable DEBUG logging level for org.apache.spark.SecurityManager logger to see what happens when SecurityManager does the security check. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SecurityManager=DEBUG You should see the following DEBUG message in the logs: DEBUG SecurityManager: user=[user] aclsEnabled=[aclsEnabled] viewAcls=[viewAcls] viewAclsGroups=[viewAclsGroups] \u00b6 With view permissions check passed, the HttpServlet sends a response with the following: FIXME In case the view permissions didn't allow to view the page, the HttpServlet sends an error response with the following: Status 403 Cache-Control header with \"no-cache, no-store, must-revalidate\" Error message: \"User is not authorized to access this page.\" NOTE: createServlet is used exclusively when JettyUtils is requested to < >. === [[createStaticHandler]] Creating Handler For Static Content -- createStaticHandler Method [source, scala] \u00b6 createStaticHandler(resourceBase: String, path: String): ServletContextHandler \u00b6 createStaticHandler creates a handler for serving files from a static directory Internally, createStaticHandler creates a Jetty ServletContextHandler and sets org.eclipse.jetty.servlet.Default.gzip init parameter to false . createRedirectHandler creates a Jetty https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ]. [NOTE] \u00b6 Quoting the official documentation of Jetty's https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ]: DefaultServlet The default servlet. This servlet, normally mapped to / , provides the handling for static content, OPTION and TRACE methods for the context. The following initParameters are supported, these can be set either on the servlet itself or as ServletContext initParameters with a prefix of org.eclipse.jetty.servlet.Default. With that, org.eclipse.jetty.servlet.Default.gzip is to configure https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[gzip ] init parameter for Jetty's DefaultServlet . gzip If set to true, then static content will be served as gzip content encoded if a matching resource is found ending with \".gz\" (default false ) (deprecated: use precompressed) ==== createRedirectHandler resolves the resourceBase in the Spark classloader and, if successful, sets resourceBase init parameter of the Jetty DefaultServlet to the URL. NOTE: https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[resourceBase ] init parameter is used to replace the context resource base. createRedirectHandler requests the ServletContextHandler to use the path as the context path and register the DefaultServlet to serve it. createRedirectHandler throws an Exception if the input resourceBase could not be resolved. Could not find resource path for Web UI: [resourceBase] NOTE: createStaticHandler is used when spark-webui-SparkUI.md#initialize[SparkUI], spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI are requested to initialize. === [[createRedirectHandler]] createRedirectHandler Method [source, scala] \u00b6 createRedirectHandler( srcPath: String, destPath: String, beforeRedirect: HttpServletRequest => Unit = x => (), basePath: String = \"\", httpMethods: Set[String] = Set(\"GET\")): ServletContextHandler createRedirectHandler ...FIXME NOTE: createRedirectHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and Spark Standalone's MasterWebUI are requested to initialize.","title":"JettyUtils"},{"location":"webui/JettyUtils/#source-scala","text":"createServletHandler( path: String, servlet: HttpServlet, basePath: String): ServletContextHandler createServletHandler T <: AnyRef : ServletContextHandler // <1> <1> Uses the first three-argument createServletHandler createServletHandler ...FIXME","title":"[source, scala]"},{"location":"webui/JettyUtils/#note","text":"createServletHandler is used when: WebUI is requested to spark-webui-WebUI.md#attachPage[attachPage] MetricsServlet is requested to getHandlers","title":"[NOTE]"},{"location":"webui/JettyUtils/#spark-standalones-workerwebui-is-requested-to-initialize","text":"=== [[createServlet]] Creating HttpServlet -- createServlet Method","title":"* Spark Standalone's WorkerWebUI is requested to initialize"},{"location":"webui/JettyUtils/#source-scala_1","text":"createServlet T <: AnyRef : HttpServlet createServlet creates the X-Frame-Options header that can be either ALLOW-FROM with the value of spark-webui-properties.md#spark.ui.allowFramingFrom[spark.ui.allowFramingFrom] configuration property if defined or SAMEORIGIN . createServlet creates a Java Servlets HttpServlet with support for GET requests. When handling GET requests, the HttpServlet first checks view permissions of the remote user (by requesting the SecurityManager to checkUIViewPermissions of the remote user).","title":"[source, scala]"},{"location":"webui/JettyUtils/#tip","text":"Enable DEBUG logging level for org.apache.spark.SecurityManager logger to see what happens when SecurityManager does the security check. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.SecurityManager=DEBUG You should see the following DEBUG message in the logs:","title":"[TIP]"},{"location":"webui/JettyUtils/#debug-securitymanager-useruser-aclsenabledaclsenabled-viewaclsviewacls-viewaclsgroupsviewaclsgroups","text":"With view permissions check passed, the HttpServlet sends a response with the following: FIXME In case the view permissions didn't allow to view the page, the HttpServlet sends an error response with the following: Status 403 Cache-Control header with \"no-cache, no-store, must-revalidate\" Error message: \"User is not authorized to access this page.\" NOTE: createServlet is used exclusively when JettyUtils is requested to < >. === [[createStaticHandler]] Creating Handler For Static Content -- createStaticHandler Method","title":"DEBUG SecurityManager: user=[user] aclsEnabled=[aclsEnabled] viewAcls=[viewAcls] viewAclsGroups=[viewAclsGroups]\n"},{"location":"webui/JettyUtils/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/JettyUtils/#createstatichandlerresourcebase-string-path-string-servletcontexthandler","text":"createStaticHandler creates a handler for serving files from a static directory Internally, createStaticHandler creates a Jetty ServletContextHandler and sets org.eclipse.jetty.servlet.Default.gzip init parameter to false . createRedirectHandler creates a Jetty https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ].","title":"createStaticHandler(resourceBase: String, path: String): ServletContextHandler"},{"location":"webui/JettyUtils/#note_1","text":"Quoting the official documentation of Jetty's https://www.eclipse.org/jetty/javadoc/current/org/eclipse/jetty/servlet/DefaultServlet.html[DefaultServlet ]: DefaultServlet The default servlet. This servlet, normally mapped to / , provides the handling for static content, OPTION and TRACE methods for the context. The following initParameters are supported, these can be set either on the servlet itself or as ServletContext initParameters with a prefix of org.eclipse.jetty.servlet.Default. With that, org.eclipse.jetty.servlet.Default.gzip is to configure https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[gzip ] init parameter for Jetty's DefaultServlet . gzip If set to true, then static content will be served as gzip content encoded if a matching resource is found ending with \".gz\" (default false ) (deprecated: use precompressed) ==== createRedirectHandler resolves the resourceBase in the Spark classloader and, if successful, sets resourceBase init parameter of the Jetty DefaultServlet to the URL. NOTE: https://www.eclipse.org/jetty/documentation/current/advanced-extras.html#default-servlet-init[resourceBase ] init parameter is used to replace the context resource base. createRedirectHandler requests the ServletContextHandler to use the path as the context path and register the DefaultServlet to serve it. createRedirectHandler throws an Exception if the input resourceBase could not be resolved. Could not find resource path for Web UI: [resourceBase] NOTE: createStaticHandler is used when spark-webui-SparkUI.md#initialize[SparkUI], spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI are requested to initialize. === [[createRedirectHandler]] createRedirectHandler Method","title":"[NOTE]"},{"location":"webui/JettyUtils/#source-scala_3","text":"createRedirectHandler( srcPath: String, destPath: String, beforeRedirect: HttpServletRequest => Unit = x => (), basePath: String = \"\", httpMethods: Set[String] = Set(\"GET\")): ServletContextHandler createRedirectHandler ...FIXME NOTE: createRedirectHandler is used when spark-webui-SparkUI.md#initialize[SparkUI] and Spark Standalone's MasterWebUI are requested to initialize.","title":"[source, scala]"},{"location":"webui/JobProgressListener/","text":"== [[JobProgressListener]] JobProgressListener Spark Listener JobProgressListener is a ROOT:SparkListener.md[] for spark-webui.md[web UI]. JobProgressListener intercepts the following ROOT:SparkListener.md#SparkListenerEvent[Spark events]. . JobProgressListener Events [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Handler | Purpose | < > | Creates a < >. It updates < >, < >, < >, < >, < >, < > and < >. | < > | Removes an entry in < >. It also removes entries in < > and < >. It updates < >, < >, < >, < > and < >. | < > | Updates the StageUIData and JobUIData . | < > | Updates the task's StageUIData and JobUIData , and registers a new TaskUIData . | < > | Updates the task's StageUIData (and TaskUIData ), ExecutorSummary , and JobUIData . | < > | | onEnvironmentUpdate | Sets schedulingMode property using the current ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] (from Spark Properties environment details). Used in spark-webui-AllJobsPage.md[AllJobsPage] (for the Scheduling Mode), and to display pools in spark-webui-JobsTab.md[JobsTab] and spark-webui-StagesTab.md[StagesTab]. FIXME : Add the links/screenshots for pools. | onBlockManagerAdded | Records an executor and its block manager in the internal < > registry. | onBlockManagerRemoved | Removes the executor from the internal < > registry. | onApplicationStart | Records a Spark application's start time (in the internal startTime ). Used in spark-webui-jobs.md[Jobs tab] (for a total uptime and the event timeline) and spark-webui-jobs.md[Job page] (for the event timeline). | onApplicationEnd | Records a Spark application's end time (in the internal endTime ). Used in spark-webui-jobs.md[Jobs tab] (for a total uptime). | onTaskGettingResult | Does nothing. FIXME : Why is this event intercepted at all?! |=== === [[updateAggregateMetrics]] updateAggregateMetrics Method CAUTION: FIXME === [[registries]] Registries and Counters JobProgressListener uses registries to collect information about job executions. . JobProgressListener Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[numCompletedStages]] numCompletedStages | | [[numFailedStages]] numFailedStages | | [[stageIdToData]] stageIdToData | Holds < > per stage, i.e. the stage and stage attempt ids. | [[stageIdToInfo]] stageIdToInfo | | [[stageIdToActiveJobIds]] stageIdToActiveJobIds | | [[poolToActiveStages]] poolToActiveStages | | [[activeJobs]] activeJobs | | [[completedJobs]] completedJobs | | [[failedJobs]] failedJobs | | [[jobIdToData]] jobIdToData | | [[jobGroupToJobIds]] jobGroupToJobIds | | [[pendingStages]] pendingStages | | [[activeStages]] activeStages | | [[completedStages]] completedStages | | [[skippedStages]] skippedStages | | [[failedStages]] failedStages | | [[executorIdToBlockManagerId]] executorIdToBlockManagerId | The lookup table of storage:BlockManagerId.md[]s per executor id. Used to track block managers so the Stage page can display Address in spark-webui-StagePage.md#ExecutorTable[Aggregated Metrics by Executor]. FIXME: How does Executors page collect the very same information? |=== === [[onJobStart]] onJobStart Callback [source, scala] \u00b6 onJobStart(jobStart: SparkListenerJobStart): Unit \u00b6 onJobStart creates a < >. It updates < >, < >, < >, < >, < >, < > and < >. onJobStart reads the optional Spark Job group id as spark.jobGroup.id (from properties in the input jobStart ). onJobStart then creates a JobUIData using the input jobStart with status attribute set to JobExecutionStatus.RUNNING and records it in < > and < > registries. onJobStart looks the job ids for the group id (in < > registry) and adds the job id. The internal < > is updated with scheduler:spark-scheduler-StageInfo.md[StageInfo] for the stage id (for every StageInfo in SparkListenerJobStart.stageInfos collection). onJobStart records the stages of the job in < >. onJobStart records scheduler:spark-scheduler-StageInfo.md[StageInfos] in < > and < >. === [[onJobEnd]] onJobEnd Method [source, scala] \u00b6 onJobEnd(jobEnd: SparkListenerJobEnd): Unit \u00b6 onJobEnd removes an entry in < >. It also removes entries in < > and < >. It updates < >, < >, < >, < > and < >. onJobEnd removes the job from < > registry. It removes stages from < > registry. When completed successfully, the job is added to < > registry with status attribute set to JobExecutionStatus.SUCCEEDED . < > gets incremented. When failed, the job is added to < > registry with status attribute set to JobExecutionStatus.FAILED . < > gets incremented. For every stage in the job, the stage is removed from the active jobs (in < >) that can remove the entire entry if no active jobs exist. Every pending stage in < > gets added to < >. === [[onExecutorMetricsUpdate]] onExecutorMetricsUpdate Method [source, scala] \u00b6 onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit \u00b6 === [[onTaskStart]] onTaskStart Method [source, scala] \u00b6 onTaskStart(taskStart: SparkListenerTaskStart): Unit \u00b6 onTaskStart updates StageUIData and JobUIData , and registers a new TaskUIData . onTaskStart takes spark-scheduler-TaskInfo.md[TaskInfo] from the input taskStart . onTaskStart looks the StageUIData for the stage and stage attempt ids up (in < > registry). onTaskStart increments numActiveTasks and puts a TaskUIData for the task in stageData.taskData . Ultimately, onTaskStart looks the stage in the internal < > and for each active job reads its JobUIData (from < >). It then increments numActiveTasks . === [[onTaskEnd]] onTaskEnd Method [source, scala] \u00b6 onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit \u00b6 onTaskEnd updates the StageUIData (and TaskUIData ), ExecutorSummary , and JobUIData . onTaskEnd takes spark-scheduler-TaskInfo.md[TaskInfo] from the input taskEnd . NOTE: onTaskEnd does its processing when the TaskInfo is available and stageAttemptId is not -1 . onTaskEnd looks the StageUIData for the stage and stage attempt ids up (in < > registry). onTaskEnd saves accumulables in the StageUIData . onTaskEnd reads the ExecutorSummary for the executor (the task has finished on). Depending on the task end's reason onTaskEnd increments succeededTasks , killedTasks or failedTasks counters. onTaskEnd adds the task's duration to taskTime . onTaskEnd decrements the number of active tasks (in the StageUIData ). Again , depending on the task end's reason onTaskEnd computes errorMessage and updates StageUIData . CAUTION: FIXME Why is the same information in two different registries -- stageData and execSummary ?! If taskMetrics is available, < > is executed. The task's TaskUIData is looked up in stageData.taskData and updateTaskInfo and updateTaskMetrics are executed. errorMessage is updated. onTaskEnd makes sure that the number of tasks in StageUIData ( stageData.taskData ) is not above < > and drops the excess. Ultimately, onTaskEnd looks the stage in the internal < > and for each active job reads its JobUIData (from < >). It then decrements numActiveTasks and increments numCompletedTasks , numKilledTasks or numFailedTasks depending on the task's end reason. === [[onStageSubmitted]] onStageSubmitted Method [source, scala] \u00b6 onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit \u00b6 === [[onStageCompleted]] onStageCompleted Method [source, scala] \u00b6 onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit \u00b6 onStageCompleted updates the StageUIData and JobUIData . onStageCompleted reads stageInfo from the input stageCompleted and records it in < > registry. onStageCompleted looks the StageUIData for the stage and the stage attempt ids up in < > registry. onStageCompleted records accumulables in StageUIData . onStageCompleted removes the stage from < > and < > registries. If the stage completed successfully (i.e. has no failureReason ), onStageCompleted adds the stage to < > registry and increments < > counter. It trims < >. Otherwise, when the stage failed, onStageCompleted adds the stage to < > registry and increments < > counter. It trims < >. Ultimately, onStageCompleted looks the stage in the internal < > and for each active job reads its JobUIData (from < >). It then decrements numActiveStages . When completed successfully, it adds the stage to completedStageIndices . With failure, numFailedStages gets incremented. === [[JobUIData]] JobUIData CAUTION: FIXME === [[blockManagerIds]] blockManagerIds method [source, scala] \u00b6 blockManagerIds: Seq[BlockManagerId] \u00b6 CAUTION: FIXME === [[StageUIData]] StageUIData CAUTION: FIXME === [[settings]] Settings .Spark Properties [options=\"header\",width=\"100%\"] |=== | Setting | Default Value | Description | [[spark_ui_retainedJobs]] spark.ui.retainedJobs | 1000 | The number of jobs to hold information about | [[spark_ui_retainedStages]] spark.ui.retainedStages | 1000 | The number of stages to hold information about | [[spark_ui_retainedTasks]] spark.ui.retainedTasks | 100000 | The number of tasks to hold information about |===","title":"JobProgressListener"},{"location":"webui/JobProgressListener/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#onjobstartjobstart-sparklistenerjobstart-unit","text":"onJobStart creates a < >. It updates < >, < >, < >, < >, < >, < > and < >. onJobStart reads the optional Spark Job group id as spark.jobGroup.id (from properties in the input jobStart ). onJobStart then creates a JobUIData using the input jobStart with status attribute set to JobExecutionStatus.RUNNING and records it in < > and < > registries. onJobStart looks the job ids for the group id (in < > registry) and adds the job id. The internal < > is updated with scheduler:spark-scheduler-StageInfo.md[StageInfo] for the stage id (for every StageInfo in SparkListenerJobStart.stageInfos collection). onJobStart records the stages of the job in < >. onJobStart records scheduler:spark-scheduler-StageInfo.md[StageInfos] in < > and < >. === [[onJobEnd]] onJobEnd Method","title":"onJobStart(jobStart: SparkListenerJobStart): Unit"},{"location":"webui/JobProgressListener/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#onjobendjobend-sparklistenerjobend-unit","text":"onJobEnd removes an entry in < >. It also removes entries in < > and < >. It updates < >, < >, < >, < > and < >. onJobEnd removes the job from < > registry. It removes stages from < > registry. When completed successfully, the job is added to < > registry with status attribute set to JobExecutionStatus.SUCCEEDED . < > gets incremented. When failed, the job is added to < > registry with status attribute set to JobExecutionStatus.FAILED . < > gets incremented. For every stage in the job, the stage is removed from the active jobs (in < >) that can remove the entire entry if no active jobs exist. Every pending stage in < > gets added to < >. === [[onExecutorMetricsUpdate]] onExecutorMetricsUpdate Method","title":"onJobEnd(jobEnd: SparkListenerJobEnd): Unit"},{"location":"webui/JobProgressListener/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#onexecutormetricsupdateexecutormetricsupdate-sparklistenerexecutormetricsupdate-unit","text":"=== [[onTaskStart]] onTaskStart Method","title":"onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit"},{"location":"webui/JobProgressListener/#source-scala_3","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#ontaskstarttaskstart-sparklistenertaskstart-unit","text":"onTaskStart updates StageUIData and JobUIData , and registers a new TaskUIData . onTaskStart takes spark-scheduler-TaskInfo.md[TaskInfo] from the input taskStart . onTaskStart looks the StageUIData for the stage and stage attempt ids up (in < > registry). onTaskStart increments numActiveTasks and puts a TaskUIData for the task in stageData.taskData . Ultimately, onTaskStart looks the stage in the internal < > and for each active job reads its JobUIData (from < >). It then increments numActiveTasks . === [[onTaskEnd]] onTaskEnd Method","title":"onTaskStart(taskStart: SparkListenerTaskStart): Unit"},{"location":"webui/JobProgressListener/#source-scala_4","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#ontaskendtaskend-sparklistenertaskend-unit","text":"onTaskEnd updates the StageUIData (and TaskUIData ), ExecutorSummary , and JobUIData . onTaskEnd takes spark-scheduler-TaskInfo.md[TaskInfo] from the input taskEnd . NOTE: onTaskEnd does its processing when the TaskInfo is available and stageAttemptId is not -1 . onTaskEnd looks the StageUIData for the stage and stage attempt ids up (in < > registry). onTaskEnd saves accumulables in the StageUIData . onTaskEnd reads the ExecutorSummary for the executor (the task has finished on). Depending on the task end's reason onTaskEnd increments succeededTasks , killedTasks or failedTasks counters. onTaskEnd adds the task's duration to taskTime . onTaskEnd decrements the number of active tasks (in the StageUIData ). Again , depending on the task end's reason onTaskEnd computes errorMessage and updates StageUIData . CAUTION: FIXME Why is the same information in two different registries -- stageData and execSummary ?! If taskMetrics is available, < > is executed. The task's TaskUIData is looked up in stageData.taskData and updateTaskInfo and updateTaskMetrics are executed. errorMessage is updated. onTaskEnd makes sure that the number of tasks in StageUIData ( stageData.taskData ) is not above < > and drops the excess. Ultimately, onTaskEnd looks the stage in the internal < > and for each active job reads its JobUIData (from < >). It then decrements numActiveTasks and increments numCompletedTasks , numKilledTasks or numFailedTasks depending on the task's end reason. === [[onStageSubmitted]] onStageSubmitted Method","title":"onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit"},{"location":"webui/JobProgressListener/#source-scala_5","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#onstagesubmittedstagesubmitted-sparklistenerstagesubmitted-unit","text":"=== [[onStageCompleted]] onStageCompleted Method","title":"onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit"},{"location":"webui/JobProgressListener/#source-scala_6","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#onstagecompletedstagecompleted-sparklistenerstagecompleted-unit","text":"onStageCompleted updates the StageUIData and JobUIData . onStageCompleted reads stageInfo from the input stageCompleted and records it in < > registry. onStageCompleted looks the StageUIData for the stage and the stage attempt ids up in < > registry. onStageCompleted records accumulables in StageUIData . onStageCompleted removes the stage from < > and < > registries. If the stage completed successfully (i.e. has no failureReason ), onStageCompleted adds the stage to < > registry and increments < > counter. It trims < >. Otherwise, when the stage failed, onStageCompleted adds the stage to < > registry and increments < > counter. It trims < >. Ultimately, onStageCompleted looks the stage in the internal < > and for each active job reads its JobUIData (from < >). It then decrements numActiveStages . When completed successfully, it adds the stage to completedStageIndices . With failure, numFailedStages gets incremented. === [[JobUIData]] JobUIData CAUTION: FIXME === [[blockManagerIds]] blockManagerIds method","title":"onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit"},{"location":"webui/JobProgressListener/#source-scala_7","text":"","title":"[source, scala]"},{"location":"webui/JobProgressListener/#blockmanagerids-seqblockmanagerid","text":"CAUTION: FIXME === [[StageUIData]] StageUIData CAUTION: FIXME === [[settings]] Settings .Spark Properties [options=\"header\",width=\"100%\"] |=== | Setting | Default Value | Description | [[spark_ui_retainedJobs]] spark.ui.retainedJobs | 1000 | The number of jobs to hold information about | [[spark_ui_retainedStages]] spark.ui.retainedStages | 1000 | The number of stages to hold information about | [[spark_ui_retainedTasks]] spark.ui.retainedTasks | 100000 | The number of tasks to hold information about |===","title":"blockManagerIds: Seq[BlockManagerId]"},{"location":"webui/LiveEntity/","text":"== [[LiveEntity]] LiveEntity LiveEntity is the < > of a live entity in Spark that...FIXME [[contract]] [source, scala] package org.apache.spark.status abstract class LiveEntity { // only required methods that have no implementation // the others follow protected def doUpdate(): Any } NOTE: LiveEntity is a private[spark] contract. .LiveEntity Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | doUpdate | [[doUpdate]] Used exclusivey when LiveEntity is requested to < >. |=== [[lastWriteTime]] LiveEntity tracks the last < > time (in lastWriteTime internal registry). === [[write]] write Method [source, scala] \u00b6 write(store: ElementTrackingStore, now: Long, checkTriggers: Boolean = false): Unit \u00b6 write requests the input ElementTrackingStore to core:ElementTrackingStore.md#write[write] the < > value. In the end, write records the time in the < >. [NOTE] \u00b6 write is used when: . AppStatusListener is requested to core:AppStatusListener.md#update[update] . SQLAppStatusListener is created (and registers a flush trigger) and requested to update \u00b6","title":"LiveEntity"},{"location":"webui/LiveEntity/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/LiveEntity/#writestore-elementtrackingstore-now-long-checktriggers-boolean-false-unit","text":"write requests the input ElementTrackingStore to core:ElementTrackingStore.md#write[write] the < > value. In the end, write records the time in the < >.","title":"write(store: ElementTrackingStore, now: Long, checkTriggers: Boolean = false): Unit"},{"location":"webui/LiveEntity/#note","text":"write is used when: . AppStatusListener is requested to core:AppStatusListener.md#update[update]","title":"[NOTE]"},{"location":"webui/LiveEntity/#sqlappstatuslistener-is-created-and-registers-a-flush-trigger-and-requested-to-update","text":"","title":". SQLAppStatusListener is created (and registers a flush trigger) and requested to update"},{"location":"webui/LiveRDD/","text":"== [[LiveRDD]] LiveRDD LiveRDD is a spark-core-LiveEntity.md[LiveEntity] that...FIXME LiveRDD is < > exclusively when AppStatusListener is requested to core:AppStatusListener.md#onStageSubmitted[handle onStageSubmitted event] [[creating-instance]] [[info]] LiveRDD takes a storage:RDDInfo.md[RDDInfo] when created. === [[doUpdate]] doUpdate Method [source, scala] \u00b6 doUpdate(): Any \u00b6 NOTE: doUpdate is part of spark-core-LiveEntity.md#doUpdate[LiveEntity Contract] to...FIXME. doUpdate ...FIXME","title":"LiveRDD"},{"location":"webui/LiveRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/LiveRDD/#doupdate-any","text":"NOTE: doUpdate is part of spark-core-LiveEntity.md#doUpdate[LiveEntity Contract] to...FIXME. doUpdate ...FIXME","title":"doUpdate(): Any"},{"location":"webui/PoolPage/","text":"== [[PoolPage]] PoolPage -- Fair Scheduler Pool Details Page [[prefix]] PoolPage is a spark-webui-WebUIPage.md[WebUIPage] with pool spark-webui-WebUIPage.md#prefix[prefix]. The Fair Scheduler Pool Details page shows information about a spark-scheduler-Pool.md[ Schedulable pool] and is only available when a Spark application uses the spark-scheduler-SchedulingMode.md#FAIR[FAIR scheduling mode] (which is controlled by ROOT:configuration-properties.md#spark.scheduler.mode[spark.scheduler.mode] configuration property). .Details Page for production Pool image::spark-webui-pool-details.png[align=\"center\"] PoolPage renders a page under /pool URL and requires one request parameter < > that is the name of the pool to display, e.g. http://localhost:4040/stages/pool/?poolname=production . It is made up of two tables: < > (with the details of the pool) and < > (with the active stages in the pool). PoolPage is < > exclusively when StagesTab is spark-webui-StagesTab.md#creating-instance[created]. [[creating-instance]] [[parent]] PoolPage takes a spark-webui-StagesTab.md[StagesTab] when created. PoolPage uses the parent's ROOT:SparkContext.md#getPoolForName[ SparkContext to access information about the pool] and spark-webui-JobProgressListener.md#poolToActiveStages[ JobProgressListener for active stages in the pool] (sorted by submissionTime in descending order by default). === [[PoolTable]][[pool-summary]] Summary Table The Summary table shows the details of a spark-scheduler-Schedulable.md[ Schedulable pool]. .Summary for production Pool image::spark-webui-pool-summary.png[align=\"center\"] It uses the following columns: Pool Name Minimum Share Pool Weight Active Stages - the number of the active stages in a Schedulable pool. Running Tasks SchedulingMode All the columns are the attributes of a Schedulable but the number of active stages which is calculated using the spark-webui-JobProgressListener.md#poolToActiveStages[list of active stages of a pool] (from the parent's spark-webui-JobProgressListener.md[JobProgressListener]). === [[StageTableBase]][[active-stages]] Active Stages Table The Active Stages table shows the active stages in a pool. .Active Stages for production Pool image::spark-webui-active-stages.png[align=\"center\"] It uses the following columns: Stage Id (optional) Pool Name - only available when in FAIR scheduling mode. Description Submitted Duration Tasks: Succeeded/Total Input -- Bytes and records read from Hadoop or from Spark storage. Output -- Bytes and records written to Hadoop. Shuffle Read -- Total shuffle bytes and records read (includes both data read locally and data read from remote executors). Shuffle Write -- Bytes and records written to disk in order to be read by a shuffle in a future stage. The table uses spark-webui-JobProgressListener.md#stageIdToData[ JobProgressListener for information per stage in the pool]. === [[parameters]] Request Parameters ==== [[poolname]] poolname poolname is the name of the scheduler pool to display on the page. It is a mandatory request parameter.","title":"PoolPage"},{"location":"webui/RDDOperationGraphListener/","text":"== [[RDDOperationGraphListener]] RDDOperationGraphListener Spark Listener CAUTION: FIXME","title":"RDDOperationGraphListener"},{"location":"webui/RDDPage/","text":"== [[RDDPage]] RDDPage [[prefix]] RDDPage is a spark-webui-WebUIPage.md[WebUIPage] with rdd spark-webui-WebUIPage.md#prefix[prefix]. RDDPage is < > exclusively when StorageTab is spark-webui-StorageTab.md#creating-instance[created]. [[creating-instance]] RDDPage takes the following when created: [[parent]] Parent spark-webui-SparkUITab.md[SparkUITab] [[store]] core:AppStatusStore.md[] === [[render]] render Method [source, scala] \u00b6 render(request: HttpServletRequest): Seq[Node] \u00b6 NOTE: render is part of spark-webui-WebUIPage.md#render[WebUIPage Contract] to...FIXME. render ...FIXME","title":"RDDPage"},{"location":"webui/RDDPage/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/RDDPage/#renderrequest-httpservletrequest-seqnode","text":"NOTE: render is part of spark-webui-WebUIPage.md#render[WebUIPage Contract] to...FIXME. render ...FIXME","title":"render(request: HttpServletRequest): Seq[Node]"},{"location":"webui/RDDStorageInfo/","text":"== [[RDDStorageInfo]] RDDStorageInfo [[creating-instance]] RDDStorageInfo contains information about RDD persistence: [[id]] RDD id [[name]] RDD name [[numPartitions]] Number of RDD partitions [[numCachedPartitions]] Number of cached RDD partitions [[storageLevel]] storage:StorageLevel.md[Storage level] ID [[memoryUsed]] Memory used [[diskUsed]] Disk used [[dataDistribution]] Data distribution (as Seq[RDDDataDistribution] ) [[partitions]] Partitions (as Seq[RDDPartitionInfo]] ) RDDStorageInfo is < > exclusively when LiveRDD is requested to webui:spark-core-LiveRDD.md#doUpdate[doUpdate] (when requested to spark-core-LiveEntity.md#write[write]). RDDStorageInfo is used when: . web UI's StoragePage is requested to render an HTML spark-webui-StoragePage.md#rddRow[table row] and an entire spark-webui-StoragePage.md#rddTable[table] for RDD details . REST API's AbstractApplicationResource is requested for spark-api-AbstractApplicationResource.md#rddList[rddList] (at storage/rdd path) . AppStatusStore is requested for core:AppStatusStore.md#rddList[rddList]","title":"RDDStorageInfo"},{"location":"webui/SparkUI/","text":"SparkUI \u00b6 SparkUI is the web UI of a Spark application (aka Application UI ). SparkUI is < > along with the following: SparkContext is created (for a live Spark application with spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] configuration property enabled) FsHistoryProvider is requested for the spark-history-server:FsHistoryProvider.md#getAppUI[application UI] (for a live or completed Spark application) .Creating SparkUI for Live Spark Application image::spark-webui-SparkUI.png[align=\"center\"] When < > (while SparkContext is created for a live Spark application), SparkUI gets the following: Live AppStatusStore (with a core:ElementTrackingStore.md[] using an core:InMemoryStore.md[] and a core:AppStatusListener.md[] for a live Spark application) Name of the Spark application that is exactly the value of ROOT:SparkConf.md#spark.app.name[spark.app.name] configuration property Empty base path When started, SparkUI binds to < > address that you can control using SPARK_PUBLIC_DNS environment variable or spark-driver.md#spark_driver_host[spark.driver.host] Spark property. NOTE: With spark-webui-properties.md#spark.ui.killEnabled[spark.ui.killEnabled] configuration property turned on, SparkUI < > (subject to SecurityManager.checkModifyPermissions permissions). SparkUI gets an < > that is then used for the following: < >, i.e. spark-webui-JobsTab.md#creating-instance[JobsTab], spark-webui-StagesTab.md#creating-instance[StagesTab], spark-webui-StorageTab.md#creating-instance[StorageTab], spark-webui-EnvironmentTab.md#creating-instance[EnvironmentTab] AbstractApplicationResource is requested for spark-api-AbstractApplicationResource.md#jobsList[jobsList], spark-api-AbstractApplicationResource.md#oneJob[oneJob], spark-api-AbstractApplicationResource.md#executorList[executorList], spark-api-AbstractApplicationResource.md#allExecutorList[allExecutorList], spark-api-AbstractApplicationResource.md#rddList[rddList], spark-api-AbstractApplicationResource.md#rddData[rddData], spark-api-AbstractApplicationResource.md#environmentInfo[environmentInfo] StagesResource is requested for spark-api-StagesResource.md#stageList[stageList], spark-api-StagesResource.md#stageData[stageData], spark-api-StagesResource.md#oneAttemptData[oneAttemptData], spark-api-StagesResource.md#taskSummary[taskSummary], spark-api-StagesResource.md#taskList[taskList] SparkUI is requested for the current < > Creating Spark SQL's SQLTab (when SQLHistoryServerPlugin is requested to setupUI ) Spark Streaming's BatchPage is created [[internal-registries]] .SparkUI's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appId | [[appId]] |=== [TIP] \u00b6 Enable INFO logging level for org.apache.spark.ui.SparkUI logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ui.SparkUI=INFO Refer to spark-logging.md[Logging]. \u00b6 == [[setAppId]] Assigning Unique Identifier of Spark Application -- setAppId Method [source, scala] \u00b6 setAppId(id: String): Unit \u00b6 setAppId sets the internal < >. setAppId is used when SparkContext is created. == [[stop]] Stopping SparkUI -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop stops the HTTP server and prints the following INFO message to the logs: INFO SparkUI: Stopped Spark web UI at [appUIAddress] NOTE: appUIAddress in the above INFO message is the result of < > method. == [[appUIAddress]] appUIAddress Method [source, scala] \u00b6 appUIAddress: String \u00b6 appUIAddress returns the entire URL of a Spark application's web UI, including http:// scheme. Internally, appUIAddress uses < >. == [[getSparkUser]] Accessing Spark User -- getSparkUser Method [source, scala] \u00b6 getSparkUser: String \u00b6 getSparkUser returns the name of the user a Spark application runs as. Internally, getSparkUser requests user.name System property from spark-webui-EnvironmentListener.md[EnvironmentListener] Spark listener. NOTE: getSparkUser is used...FIXME == [[createLiveUI]] createLiveUI Method [source, scala] \u00b6 createLiveUI( sc: SparkContext, conf: SparkConf, listenerBus: SparkListenerBus, jobProgressListener: JobProgressListener, securityManager: SecurityManager, appName: String, startTime: Long): SparkUI createLiveUI creates a SparkUI for a live running Spark application. Internally, createLiveUI simply forwards the call to < >. createLiveUI is used when SparkContext is created. == [[createHistoryUI]] createHistoryUI Method CAUTION: FIXME == [[appUIHostPort]] appUIHostPort Method [source, scala] \u00b6 appUIHostPort: String \u00b6 appUIHostPort returns the Spark application's web UI which is the public hostname and port, excluding the scheme. NOTE: < > uses appUIHostPort and adds http:// scheme. == [[getAppName]] getAppName Method [source, scala] \u00b6 getAppName: String \u00b6 getAppName returns the name of the Spark application (of a SparkUI instance). NOTE: getAppName is used when...FIXME == [[create]] Creating SparkUI Instance -- create Factory Method [source, scala] \u00b6 create( sc: Option[SparkContext], store: AppStatusStore, conf: SparkConf, securityManager: SecurityManager, appName: String, basePath: String = \"\", startTime: Long, appSparkVersion: String = org.apache.spark.SPARK_VERSION): SparkUI create creates a SparkUI backed by a core:AppStatusStore.md[]. Internally, create simply creates a new < > (with the predefined Spark version). create is used when: SparkContext is created FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[getAppUI] (for a Spark application that already finished) Creating Instance \u00b6 SparkUI takes the following when created: [[store]] core:AppStatusStore.md[] [[sc]] ROOT:SparkContext.md[] [[conf]] ROOT:SparkConf.md[SparkConf] [[securityManager]] SecurityManager [[appName]] Application name [[basePath]] basePath [[startTime]] Start time [[appSparkVersion]] appSparkVersion SparkUI initializes the < > and < >. == [[initialize]] Attaching Tabs and Context Handlers -- initialize Method [source, scala] \u00b6 initialize(): Unit \u00b6 NOTE: initialize is part of spark-webui-WebUI.md#initialize[WebUI Contract] to initialize web components. initialize creates and < > the following tabs (with the reference to the SparkUI and its < >): . spark-webui-JobsTab.md[JobsTab] . spark-webui-StagesTab.md[StagesTab] . spark-webui-StorageTab.md[StorageTab] . spark-webui-EnvironmentTab.md[EnvironmentTab] . spark-webui-ExecutorsTab.md[ExecutorsTab] In the end, initialize creates and spark-webui-WebUI.md#attachHandler[attaches] the following ServletContextHandlers : . spark-webui-JettyUtils.md#createStaticHandler[Creates a static handler] for serving files from a static directory, i.e. /static to serve static files from org/apache/spark/ui/static directory (on CLASSPATH) . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect / to /jobs/ (and so the spark-webui-jobs.md[Jobs tab] is the welcome tab when you open the web UI) . spark-api-ApiRootResource.md#getServletHandler[Creates the /api/* context handler] for the spark-api.md[Status REST API] . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /jobs/job/kill to /jobs/ and request the JobsTab to execute spark-webui-JobsTab.md#handleKillRequest[handleKillRequest] before redirection . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /stages/stage/kill to /stages/ and request the StagesTab to execute spark-webui-StagesTab.md#handleKillRequest[handleKillRequest] before redirection","title":"SparkUI"},{"location":"webui/SparkUI/#sparkui","text":"SparkUI is the web UI of a Spark application (aka Application UI ). SparkUI is < > along with the following: SparkContext is created (for a live Spark application with spark-webui-properties.md#spark.ui.enabled[spark.ui.enabled] configuration property enabled) FsHistoryProvider is requested for the spark-history-server:FsHistoryProvider.md#getAppUI[application UI] (for a live or completed Spark application) .Creating SparkUI for Live Spark Application image::spark-webui-SparkUI.png[align=\"center\"] When < > (while SparkContext is created for a live Spark application), SparkUI gets the following: Live AppStatusStore (with a core:ElementTrackingStore.md[] using an core:InMemoryStore.md[] and a core:AppStatusListener.md[] for a live Spark application) Name of the Spark application that is exactly the value of ROOT:SparkConf.md#spark.app.name[spark.app.name] configuration property Empty base path When started, SparkUI binds to < > address that you can control using SPARK_PUBLIC_DNS environment variable or spark-driver.md#spark_driver_host[spark.driver.host] Spark property. NOTE: With spark-webui-properties.md#spark.ui.killEnabled[spark.ui.killEnabled] configuration property turned on, SparkUI < > (subject to SecurityManager.checkModifyPermissions permissions). SparkUI gets an < > that is then used for the following: < >, i.e. spark-webui-JobsTab.md#creating-instance[JobsTab], spark-webui-StagesTab.md#creating-instance[StagesTab], spark-webui-StorageTab.md#creating-instance[StorageTab], spark-webui-EnvironmentTab.md#creating-instance[EnvironmentTab] AbstractApplicationResource is requested for spark-api-AbstractApplicationResource.md#jobsList[jobsList], spark-api-AbstractApplicationResource.md#oneJob[oneJob], spark-api-AbstractApplicationResource.md#executorList[executorList], spark-api-AbstractApplicationResource.md#allExecutorList[allExecutorList], spark-api-AbstractApplicationResource.md#rddList[rddList], spark-api-AbstractApplicationResource.md#rddData[rddData], spark-api-AbstractApplicationResource.md#environmentInfo[environmentInfo] StagesResource is requested for spark-api-StagesResource.md#stageList[stageList], spark-api-StagesResource.md#stageData[stageData], spark-api-StagesResource.md#oneAttemptData[oneAttemptData], spark-api-StagesResource.md#taskSummary[taskSummary], spark-api-StagesResource.md#taskList[taskList] SparkUI is requested for the current < > Creating Spark SQL's SQLTab (when SQLHistoryServerPlugin is requested to setupUI ) Spark Streaming's BatchPage is created [[internal-registries]] .SparkUI's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | appId | [[appId]] |===","title":"SparkUI"},{"location":"webui/SparkUI/#tip","text":"Enable INFO logging level for org.apache.spark.ui.SparkUI logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ui.SparkUI=INFO","title":"[TIP]"},{"location":"webui/SparkUI/#refer-to-spark-loggingmdlogging","text":"== [[setAppId]] Assigning Unique Identifier of Spark Application -- setAppId Method","title":"Refer to spark-logging.md[Logging]."},{"location":"webui/SparkUI/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#setappidid-string-unit","text":"setAppId sets the internal < >. setAppId is used when SparkContext is created. == [[stop]] Stopping SparkUI -- stop Method","title":"setAppId(id: String): Unit"},{"location":"webui/SparkUI/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#stop-unit","text":"stop stops the HTTP server and prints the following INFO message to the logs: INFO SparkUI: Stopped Spark web UI at [appUIAddress] NOTE: appUIAddress in the above INFO message is the result of < > method. == [[appUIAddress]] appUIAddress Method","title":"stop(): Unit"},{"location":"webui/SparkUI/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#appuiaddress-string","text":"appUIAddress returns the entire URL of a Spark application's web UI, including http:// scheme. Internally, appUIAddress uses < >. == [[getSparkUser]] Accessing Spark User -- getSparkUser Method","title":"appUIAddress: String"},{"location":"webui/SparkUI/#source-scala_3","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#getsparkuser-string","text":"getSparkUser returns the name of the user a Spark application runs as. Internally, getSparkUser requests user.name System property from spark-webui-EnvironmentListener.md[EnvironmentListener] Spark listener. NOTE: getSparkUser is used...FIXME == [[createLiveUI]] createLiveUI Method","title":"getSparkUser: String"},{"location":"webui/SparkUI/#source-scala_4","text":"createLiveUI( sc: SparkContext, conf: SparkConf, listenerBus: SparkListenerBus, jobProgressListener: JobProgressListener, securityManager: SecurityManager, appName: String, startTime: Long): SparkUI createLiveUI creates a SparkUI for a live running Spark application. Internally, createLiveUI simply forwards the call to < >. createLiveUI is used when SparkContext is created. == [[createHistoryUI]] createHistoryUI Method CAUTION: FIXME == [[appUIHostPort]] appUIHostPort Method","title":"[source, scala]"},{"location":"webui/SparkUI/#source-scala_5","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#appuihostport-string","text":"appUIHostPort returns the Spark application's web UI which is the public hostname and port, excluding the scheme. NOTE: < > uses appUIHostPort and adds http:// scheme. == [[getAppName]] getAppName Method","title":"appUIHostPort: String"},{"location":"webui/SparkUI/#source-scala_6","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#getappname-string","text":"getAppName returns the name of the Spark application (of a SparkUI instance). NOTE: getAppName is used when...FIXME == [[create]] Creating SparkUI Instance -- create Factory Method","title":"getAppName: String"},{"location":"webui/SparkUI/#source-scala_7","text":"create( sc: Option[SparkContext], store: AppStatusStore, conf: SparkConf, securityManager: SecurityManager, appName: String, basePath: String = \"\", startTime: Long, appSparkVersion: String = org.apache.spark.SPARK_VERSION): SparkUI create creates a SparkUI backed by a core:AppStatusStore.md[]. Internally, create simply creates a new < > (with the predefined Spark version). create is used when: SparkContext is created FsHistoryProvider is requested to spark-history-server:FsHistoryProvider.md#getAppUI[getAppUI] (for a Spark application that already finished)","title":"[source, scala]"},{"location":"webui/SparkUI/#creating-instance","text":"SparkUI takes the following when created: [[store]] core:AppStatusStore.md[] [[sc]] ROOT:SparkContext.md[] [[conf]] ROOT:SparkConf.md[SparkConf] [[securityManager]] SecurityManager [[appName]] Application name [[basePath]] basePath [[startTime]] Start time [[appSparkVersion]] appSparkVersion SparkUI initializes the < > and < >. == [[initialize]] Attaching Tabs and Context Handlers -- initialize Method","title":"Creating Instance"},{"location":"webui/SparkUI/#source-scala_8","text":"","title":"[source, scala]"},{"location":"webui/SparkUI/#initialize-unit","text":"NOTE: initialize is part of spark-webui-WebUI.md#initialize[WebUI Contract] to initialize web components. initialize creates and < > the following tabs (with the reference to the SparkUI and its < >): . spark-webui-JobsTab.md[JobsTab] . spark-webui-StagesTab.md[StagesTab] . spark-webui-StorageTab.md[StorageTab] . spark-webui-EnvironmentTab.md[EnvironmentTab] . spark-webui-ExecutorsTab.md[ExecutorsTab] In the end, initialize creates and spark-webui-WebUI.md#attachHandler[attaches] the following ServletContextHandlers : . spark-webui-JettyUtils.md#createStaticHandler[Creates a static handler] for serving files from a static directory, i.e. /static to serve static files from org/apache/spark/ui/static directory (on CLASSPATH) . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect / to /jobs/ (and so the spark-webui-jobs.md[Jobs tab] is the welcome tab when you open the web UI) . spark-api-ApiRootResource.md#getServletHandler[Creates the /api/* context handler] for the spark-api.md[Status REST API] . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /jobs/job/kill to /jobs/ and request the JobsTab to execute spark-webui-JobsTab.md#handleKillRequest[handleKillRequest] before redirection . spark-webui-JettyUtils.md#createRedirectHandler[Creates a redirect handler] to redirect /stages/stage/kill to /stages/ and request the StagesTab to execute spark-webui-StagesTab.md#handleKillRequest[handleKillRequest] before redirection","title":"initialize(): Unit"},{"location":"webui/SparkUITab/","text":"== [[SparkUITab]] SparkUITab SparkUITab is the < > of spark-webui-WebUITab.md[WebUITab] extensions with two additional properties: < > < > [[contract]] [source, scala] package org.apache.spark.ui abstract class SparkUITab(parent: SparkUI, prefix: String) extends WebUITab(parent, prefix) { def appName: String def appSparkVersion: String } NOTE: SparkUITab is a private[spark] contract. .SparkUITab Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | appName | [[appName]] Used when...FIXME | appSparkVersion | [[appSparkVersion]] Used when...FIXME |=== [[implementations]] .SparkUITabs [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | SparkUITab | Description | spark-webui-EnvironmentTab.md[EnvironmentTab] | [[EnvironmentTab]] | spark-webui-ExecutorsTab.md[ExecutorsTab] | [[ExecutorsTab]] | spark-webui-JobsTab.md[JobsTab] | [[JobsTab]] | spark-webui-StagesTab.md[StagesTab] | [[StagesTab]] | spark-webui-StorageTab.md[StorageTab] | [[StorageTab]] | SQLTab | [[SQLTab]] Used in Spark SQL module | StreamingTab | [[StreamingTab]] Used in Spark Streaming module | ThriftServerTab | [[ThriftServerTab]] Used in Spark Thrift Server |===","title":"SparkUITab"},{"location":"webui/StagePage/","text":"== [[StagePage]] StagePage -- Stage Details [[prefix]] StagePage is a spark-webui-WebUIPage.md[WebUIPage] with stage spark-webui-WebUIPage.md#prefix[prefix]. StagePage is < > exclusively when StagesTab is spark-webui-StagesTab.md#creating-instance[created]. StagePage shows the task details for a stage given its id and attempt id. .Details for Stage image::spark-webui-stage-details.png[align=\"center\"] StagePage renders a page available under /stage URL that requires two < > -- id and attempt , e.g. http://localhost:4040/stages/stage/?id=2&attempt=0 . StagePage uses the parent's spark-webui-JobProgressListener.md[JobProgressListener] and spark-webui-RDDOperationGraphListener.md[RDDOperationGraphListener] to calculate the < >. More specifically, StagePage uses JobProgressListener 's spark-webui-JobProgressListener.md#stageIdToData[stageIdToData] registry to access the stage for given stage id and attempt . StagePage uses spark-webui-executors-ExecutorsListener.md[ExecutorsListener] to display stdout and stderr logs of the executors in < >. === [[tasks]][[TaskPagedTable]] Tasks Section .Tasks Section image::spark-webui-stage-tasks.png[align=\"center\"] Tasks paged table displays spark-webui-JobProgressListener.md#StageUIData[StageUIData] that spark-webui-JobProgressListener.md#stageIdToData[ JobProgressListener collected for a stage and stage attempt]. NOTE: The section uses spark-webui-executors-ExecutorsListener.md[ExecutorsListener] to access stdout and stderr logs for Executor ID / Host column. === [[summary-task-metrics]] Summary Metrics for Completed Tasks in Stage The summary metrics table shows the metrics for the tasks in a given stage that have already finished with SUCCESS status and metrics available. The table consists of the following columns: Metric , Min , 25 th percentile , Median , 75 th percentile , Max . .Summary Metrics for Completed Tasks in Stage image::spark-webui-stage-summary-metrics-tasks.png[align=\"center\"] NOTE: All the quantiles are doubles using TaskUIData.metrics (sorted in ascending order). The 1 st row is Duration which includes the quantiles based on executorRunTime . The 2 nd row is the optional Scheduler Delay which includes the time to ship the task from the scheduler to executors, and the time to send the task result from the executors to the scheduler. It is not enabled by default and you should select Scheduler Delay checkbox under Show Additional Metrics to include it in the summary table. TIP: If Scheduler Delay is large, consider decreasing the size of tasks or decreasing the size of task results. The 3 rd row is the optional Task Deserialization Time which includes the quantiles based on executorDeserializeTime task metric. It is not enabled by default and you should select Task Deserialization Time checkbox under Show Additional Metrics to include it in the summary table. The 4 th row is GC Time which is the time that an executor spent paused for Java garbage collection while the task was running (using jvmGCTime task metric). The 5 th row is the optional Result Serialization Time which is the time spent serializing the task result on a executor before sending it back to the driver (using resultSerializationTime task metric). It is not enabled by default and you should select Result Serialization Time checkbox under Show Additional Metrics to include it in the summary table. The 6 th row is the optional Getting Result Time which is the time that the driver spends fetching task results from workers. It is not enabled by default and you should select Getting Result Time checkbox under Show Additional Metrics to include it in the summary table. TIP: If Getting Result Time is large, consider decreasing the amount of data returned from each task. If < > (it is by default), the 7 th row is the optional Peak Execution Memory which is the sum of the peak sizes of the internal data structures created during shuffles, aggregations and joins (using peakExecutionMemory task metric). For SQL jobs, this only tracks all unsafe operators, broadcast joins, and external sort. It is not enabled by default and you should select Peak Execution Memory checkbox under Show Additional Metrics to include it in the summary table. If the stage has an input, the 8 th row is Input Size / Records which is the bytes and records read from Hadoop or from a Spark storage (using inputMetrics.bytesRead and inputMetrics.recordsRead task metrics). If the stage has an output, the 9 th row is Output Size / Records which is the bytes and records written to Hadoop or to a Spark storage (using outputMetrics.bytesWritten and outputMetrics.recordsWritten task metrics). If the stage has shuffle read there will be three more rows in the table. The first row is Shuffle Read Blocked Time which is the time that tasks spent blocked waiting for shuffle data to be read from remote machines (using shuffleReadMetrics.fetchWaitTime task metric). The other row is Shuffle Read Size / Records which is the total shuffle bytes and records read (including both data read locally and data read from remote executors using shuffleReadMetrics.totalBytesRead and shuffleReadMetrics.recordsRead task metrics). And the last row is Shuffle Remote Reads which is the total shuffle bytes read from remote executors (which is a subset of the shuffle read bytes; the remaining shuffle data is read locally). It uses shuffleReadMetrics.remoteBytesRead task metric. If the stage has shuffle write, the following row is Shuffle Write Size / Records (using executor:ShuffleWriteMetrics.md#bytesWritten[shuffleWriteMetrics.bytesWritten] and executor:ShuffleWriteMetrics.md#recordsWritten[shuffleWriteMetrics.recordsWritten] task metrics). If the stage has bytes spilled, the following two rows are Shuffle spill (memory) (using memoryBytesSpilled task metric) and Shuffle spill (disk) (using diskBytesSpilled task metric). === [[parameters]] Request Parameters id is... attempt is... NOTE: id and attempt uniquely identify the stage in spark-webui-JobProgressListener.md#stageIdToData[JobProgressListener.stageIdToData] to retrieve StageUIData . task.page (default: 1 ) is... task.sort (default: Index ) task.desc (default: false ) task.pageSize (default: 100 ) task.prevPageSize (default: task.pageSize ) === [[metrics]] Metrics Scheduler Delay is...FIXME Task Deserialization Time is...FIXME Result Serialization Time is...FIXME Getting Result Time is...FIXME Peak Execution Memory is...FIXME Shuffle Read Time is...FIXME Executor Computing Time is...FIXME Shuffle Write Time is...FIXME .DAG Visualization image::spark-webui-stage-dagvisualization.png[align=\"center\"] .Event Timeline image::spark-webui-stage-eventtimeline.png[align=\"center\"] .Stage Task and Shuffle Stats image::spark-webui-stage-header.png[align=\"center\"] === [[aggregated-metrics-by-executor]][[ExecutorTable]] Aggregated Metrics by Executor ExecutorTable table shows the following columns: Executor ID Address Task Time Total Tasks Failed Tasks Killed Tasks Succeeded Tasks (optional) Input Size / Records (only when the stage has an input) (optional) Output Size / Records (only when the stage has an output) (optional) Shuffle Read Size / Records (only when the stage read bytes for a shuffle) (optional) Shuffle Write Size / Records (only when the stage wrote bytes for a shuffle) (optional) Shuffle Spill (Memory) (only when the stage spilled memory bytes) (optional) Shuffle Spill (Disk) (only when the stage spilled bytes to disk) .Aggregated Metrics by Executor image::spark-webui-stage-aggregated-metrics-by-executor.png[align=\"center\"] It gets executorSummary from StageUIData (for the stage and stage attempt id) and creates rows per executor. It also spark-webui-JobProgressListener.md#blockManagerIds[requests BlockManagers (from JobProgressListener)] to map executor ids to a pair of host and port to display in Address column. === [[accumulators]] Accumulators Stage page displays the table with spark-accumulators.md#named[named accumulators] (only if they exist). It contains the name and value of the accumulators. .Accumulators Section image::spark-webui-stage-accumulators.png[align=\"center\"] NOTE: The information with name and value is stored in spark-accumulators.md#AccumulableInfo[AccumulableInfo] (that is available in spark-webui-JobProgressListener.md#StageUIData[StageUIData]). === [[creating-instance]] Creating StagePage Instance StagePage takes the following when created: [[parent]] Parent spark-webui-StagesTab.md[StagesTab] [[store]] core:AppStatusStore.md[]","title":"StagePage"},{"location":"webui/StagesTab/","text":"== [[StagesTab]] StagesTab -- Stages for All Jobs [[prefix]] StagesTab is a spark-webui-SparkUITab.md[SparkUITab] with stages spark-webui-SparkUITab.md#prefix[prefix]. StagesTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. When < >, StagesTab creates the following pages and spark-webui-WebUITab.md#attachPage[attaches] them immediately: spark-webui-AllStagesPage.md[AllStagesPage] spark-webui-StagePage.md[StagePage] spark-webui-PoolPage.md[PoolPage] Stages tab in spark-webui.md[web UI] shows spark-webui-AllStagesPage.md[the current state of all stages of all jobs in a Spark application] (i.e. a ROOT:SparkContext.md[]) with two optional pages for spark-webui-StagePage.md[the tasks and statistics for a stage] (when a stage is selected) and spark-webui-PoolPage.md[pool details] (when the application works in spark-scheduler-SchedulingMode.md#FAIR[FAIR scheduling mode]). The title of the tab is Stages for All Jobs . You can access the Stages tab under /stages URL, i.e. http://localhost:4040/stages . With no jobs submitted yet (and hence no stages to display), the page shows nothing but the title. .Stages Page Empty image::spark-webui-stages-empty.png[align=\"center\"] The Stages page shows the stages in a Spark application per state in their respective sections -- Active Stages , Pending Stages , Completed Stages , and Failed Stages . .Stages Page With One Stage Completed image::spark-webui-stages-completed.png[align=\"center\"] NOTE: The state sections are only displayed when there are stages in a given state. Refer to spark-webui-AllStagesPage.md[Stages for All Jobs]. In spark-scheduler-SchedulingMode.md#FAIR[FAIR scheduling mode] you have access to the table showing the scheduler pools. .Fair Scheduler Pools Table image::spark-webui-stages-fairschedulerpools.png[align=\"center\"] Internally, the page is represented by https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala[org.apache.spark.ui.jobs.StagesTab ] class. The page uses the parent's spark-webui-SparkUI.md[SparkUI] to access required services, i.e. ROOT:SparkContext.md[], spark-sql-SQLConf.md[SparkConf], spark-webui-JobProgressListener.md[JobProgressListener], spark-webui-RDDOperationGraphListener.md[RDDOperationGraphListener], and to know whether < >. StagesTab is < > when...FIXME === [[killEnabled]] killEnabled flag CAUTION: FIXME === [[creating-instance]] Creating StagesTab Instance StagesTab takes the following when created: [[parent]] spark-webui-SparkUI.md[SparkUI] [[store]] core:AppStatusStore.md[] === [[handleKillRequest]] Handling Request to Kill Stage (from web UI) -- handleKillRequest Method [source, scala] \u00b6 handleKillRequest(request: HttpServletRequest): Unit \u00b6 handleKillRequest ...FIXME NOTE: handleKillRequest is used when...FIXME","title":"StagesTab"},{"location":"webui/StagesTab/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/StagesTab/#handlekillrequestrequest-httpservletrequest-unit","text":"handleKillRequest ...FIXME NOTE: handleKillRequest is used when...FIXME","title":"handleKillRequest(request: HttpServletRequest): Unit"},{"location":"webui/StorageListener/","text":"= StorageListener StorageListener is a webui:spark-webui-BlockStatusListener.md[BlockStatusListener] that uses < > to track changes in the persistence status of RDD blocks in a Spark application. [[SparkListener-callbacks]] .StorageListener's SparkListener Callbacks (in alphabetical order) [width=\"100%\",cols=\"1,2\",options=\"header\"] |=== | Callback | Description | < > | Updates <<_rddInfoMap, _rddInfoMap>> with the update to a single block. | < > | Removes storage:RDDInfo.md[RDDInfo] instances from <<_rddInfoMap, _rddInfoMap>> that participated in the completed stage as well as the ones that are no longer cached. | < > | Updates <<_rddInfoMap, _rddInfoMap>> registry with the names of every storage:RDDInfo.md[RDDInfo] in the submitted stage, possibly adding new storage:RDDInfo.md[RDDInfo] instances if they were not registered yet. | < > | Removes an storage:RDDInfo.md[RDDInfo] from <<_rddInfoMap, _rddInfoMap>> registry for the unpersisted RDD. |=== [[internal-registries]] .StorageListener's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[_rddInfoMap]] _rddInfoMap | storage:RDDInfo.md[RDDInfo] instances per IDs Used when...FIXME |=== == [[creating-instance]] Creating StorageListener Instance StorageListener takes the following when created: [[storageStatusListener]] spark-webui-StorageStatusListener.md[StorageStatusListener] StorageListener initializes the < >. NOTE: StorageListener is created when SparkUI spark-webui-SparkUI.md#create[is created]. == [[activeStorageStatusList]] Finding Active BlockManagers -- activeStorageStatusList Method [source, scala] \u00b6 activeStorageStatusList: Seq[StorageStatus] \u00b6 activeStorageStatusList requests < > for spark-webui-StorageStatusListener.md#storageStatusList[active BlockManagers (on executors)]. [NOTE] \u00b6 activeStorageStatusList is used when: AllRDDResource does rddList and getRDDStorageInfo StorageListener < > \u00b6 == [[onBlockUpdated]] Intercepting Block Status Update Events -- onBlockUpdated Callback [source, scala] \u00b6 onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit \u00b6 onBlockUpdated creates a BlockStatus (from the input SparkListenerBlockUpdated ) and < > (passing in storage:BlockId.md[] and BlockStatus as a single-element collection of updated blocks). NOTE: onBlockUpdated is part of ROOT:SparkListener.md#onBlockUpdated[SparkListener contract] to announce that there was a change in a block status (on a BlockManager on an executor). == [[onStageCompleted]] Intercepting Stage Completed Events -- onStageCompleted Callback [source, scala] \u00b6 onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit \u00b6 onStageCompleted finds the identifiers of the RDDs that have participated in the completed stage and removes them from <<_rddInfoMap, _rddInfoMap>> registry as well as the RDDs that are no longer cached. NOTE: onStageCompleted is part of ROOT:SparkListener.md#onStageCompleted[SparkListener contract] to announce that a stage has finished. == [[onStageSubmitted]] Intercepting Stage Submitted Events -- onStageSubmitted Callback [source, scala] \u00b6 onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit \u00b6 onStageSubmitted updates <<_rddInfoMap, _rddInfoMap>> registry with the names of every storage:RDDInfo.md[RDDInfo] in stageSubmitted , possibly adding new storage:RDDInfo.md[RDDInfo] instances if they were not registered yet. NOTE: onStageSubmitted is part of ROOT:SparkListener.md#onStageSubmitted[SparkListener contract] to announce that the missing tasks of a stage were submitted for execution. == [[onUnpersistRDD]] Intercepting Unpersist RDD Events -- onUnpersistRDD Callback [source, scala] \u00b6 onUnpersistRDD(unpersistRDD: SparkListenerUnpersistRDD): Unit \u00b6 onUnpersistRDD removes the storage:RDDInfo.md[RDDInfo] from <<_rddInfoMap, _rddInfoMap>> registry for the unpersisted RDD (from unpersistRDD ). NOTE: onUnpersistRDD is part of ROOT:SparkListener.md#onUnpersistRDD[SparkListener contract] to announce that an RDD has been unpersisted. == [[updateRDDInfo]] Updating Registered RDDInfos (with Block Updates from BlockManagers) [source, scala] \u00b6 updateRDDInfo(updatedBlocks: Seq[(BlockId, BlockStatus)]): Unit \u00b6 updateRDDInfo finds the RDDs for the input updatedBlocks (for storage:BlockId.md[]s). NOTE: updateRDDInfo finds BlockIds that are storage:BlockId.md#RDDBlockId[RDDBlockIds]. updateRDDInfo takes RDDInfo entries (in <<_rddInfoMap, _rddInfoMap>> registry) for which there are blocks in the input updatedBlocks and < > (from < >). NOTE: updateRDDInfo is used exclusively when StorageListener < BlockManager on an executor)>>. == [[StorageUtils.updateRddInfo]] Updating RDDInfos (using StorageStatus) -- StorageUtils.updateRddInfo Method [source, scala] \u00b6 updateRddInfo(rddInfos: Seq[RDDInfo], statuses: Seq[StorageStatus]): Unit \u00b6 CAUTION: FIXME [NOTE] \u00b6 updateRddInfo is used when: SparkContext ROOT:SparkContext.md#getRDDStorageInfo[is requested for storage status of cached RDDs] StorageListener < > \u00b6","title":"StorageListener"},{"location":"webui/StorageListener/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#activestoragestatuslist-seqstoragestatus","text":"activeStorageStatusList requests < > for spark-webui-StorageStatusListener.md#storageStatusList[active BlockManagers (on executors)].","title":"activeStorageStatusList: Seq[StorageStatus]"},{"location":"webui/StorageListener/#note","text":"activeStorageStatusList is used when: AllRDDResource does rddList and getRDDStorageInfo","title":"[NOTE]"},{"location":"webui/StorageListener/#storagelistener","text":"== [[onBlockUpdated]] Intercepting Block Status Update Events -- onBlockUpdated Callback","title":"StorageListener &lt;&gt;"},{"location":"webui/StorageListener/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#onblockupdatedblockupdated-sparklistenerblockupdated-unit","text":"onBlockUpdated creates a BlockStatus (from the input SparkListenerBlockUpdated ) and < > (passing in storage:BlockId.md[] and BlockStatus as a single-element collection of updated blocks). NOTE: onBlockUpdated is part of ROOT:SparkListener.md#onBlockUpdated[SparkListener contract] to announce that there was a change in a block status (on a BlockManager on an executor). == [[onStageCompleted]] Intercepting Stage Completed Events -- onStageCompleted Callback","title":"onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit"},{"location":"webui/StorageListener/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#onstagecompletedstagecompleted-sparklistenerstagecompleted-unit","text":"onStageCompleted finds the identifiers of the RDDs that have participated in the completed stage and removes them from <<_rddInfoMap, _rddInfoMap>> registry as well as the RDDs that are no longer cached. NOTE: onStageCompleted is part of ROOT:SparkListener.md#onStageCompleted[SparkListener contract] to announce that a stage has finished. == [[onStageSubmitted]] Intercepting Stage Submitted Events -- onStageSubmitted Callback","title":"onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit"},{"location":"webui/StorageListener/#source-scala_3","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#onstagesubmittedstagesubmitted-sparklistenerstagesubmitted-unit","text":"onStageSubmitted updates <<_rddInfoMap, _rddInfoMap>> registry with the names of every storage:RDDInfo.md[RDDInfo] in stageSubmitted , possibly adding new storage:RDDInfo.md[RDDInfo] instances if they were not registered yet. NOTE: onStageSubmitted is part of ROOT:SparkListener.md#onStageSubmitted[SparkListener contract] to announce that the missing tasks of a stage were submitted for execution. == [[onUnpersistRDD]] Intercepting Unpersist RDD Events -- onUnpersistRDD Callback","title":"onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit"},{"location":"webui/StorageListener/#source-scala_4","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#onunpersistrddunpersistrdd-sparklistenerunpersistrdd-unit","text":"onUnpersistRDD removes the storage:RDDInfo.md[RDDInfo] from <<_rddInfoMap, _rddInfoMap>> registry for the unpersisted RDD (from unpersistRDD ). NOTE: onUnpersistRDD is part of ROOT:SparkListener.md#onUnpersistRDD[SparkListener contract] to announce that an RDD has been unpersisted. == [[updateRDDInfo]] Updating Registered RDDInfos (with Block Updates from BlockManagers)","title":"onUnpersistRDD(unpersistRDD: SparkListenerUnpersistRDD): Unit"},{"location":"webui/StorageListener/#source-scala_5","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#updaterddinfoupdatedblocks-seqblockid-blockstatus-unit","text":"updateRDDInfo finds the RDDs for the input updatedBlocks (for storage:BlockId.md[]s). NOTE: updateRDDInfo finds BlockIds that are storage:BlockId.md#RDDBlockId[RDDBlockIds]. updateRDDInfo takes RDDInfo entries (in <<_rddInfoMap, _rddInfoMap>> registry) for which there are blocks in the input updatedBlocks and < > (from < >). NOTE: updateRDDInfo is used exclusively when StorageListener < BlockManager on an executor)>>. == [[StorageUtils.updateRddInfo]] Updating RDDInfos (using StorageStatus) -- StorageUtils.updateRddInfo Method","title":"updateRDDInfo(updatedBlocks: Seq[(BlockId, BlockStatus)]): Unit"},{"location":"webui/StorageListener/#source-scala_6","text":"","title":"[source, scala]"},{"location":"webui/StorageListener/#updaterddinforddinfos-seqrddinfo-statuses-seqstoragestatus-unit","text":"CAUTION: FIXME","title":"updateRddInfo(rddInfos: Seq[RDDInfo], statuses: Seq[StorageStatus]): Unit"},{"location":"webui/StorageListener/#note_1","text":"updateRddInfo is used when: SparkContext ROOT:SparkContext.md#getRDDStorageInfo[is requested for storage status of cached RDDs]","title":"[NOTE]"},{"location":"webui/StorageListener/#storagelistener_1","text":"","title":"StorageListener &lt;&gt;"},{"location":"webui/StoragePage/","text":"== [[StoragePage]] StoragePage [[prefix]] StoragePage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. StoragePage is < > exclusively when StorageTab is spark-webui-StorageTab.md#creating-instance[created]. [[creating-instance]] StoragePage takes the following when created: [[parent]] Parent spark-webui-SparkUITab.md[SparkUITab] [[store]] core:AppStatusStore.md[] === [[rddRow]] Rendering HTML Table Row for RDD Details -- rddRow Internal Method [source, scala] \u00b6 rddRow(rdd: v1.RDDStorageInfo): Seq[Node] \u00b6 rddRow ...FIXME NOTE: rddRow is used when...FIXME === [[rddTable]] Rendering HTML Table with RDD Details -- rddTable Method [source, scala] \u00b6 rddTable(rdds: Seq[v1.RDDStorageInfo]): Seq[Node] \u00b6 rddTable ...FIXME NOTE: rddTable is used when...FIXME === [[receiverBlockTables]] receiverBlockTables Method [source, scala] \u00b6 receiverBlockTables(blocks: Seq[StreamBlockData]): Seq[Node] \u00b6 receiverBlockTables ...FIXME NOTE: receiverBlockTables is used when...FIXME === [[render]] Rendering Page -- render Method [source, scala] \u00b6 render(request: HttpServletRequest): Seq[Node] \u00b6 NOTE: render is part of spark-webui-WebUIPage.md#render[WebUIPage Contract] to...FIXME. render requests the < > for core:AppStatusStore.md#rddList[rddList] and < > (if available). render requests the < > for core:AppStatusStore.md#streamBlocksList[streamBlocksList] and < > (if available). In the end, render requests UIUtils to headerSparkPage (with Storage title).","title":"StoragePage"},{"location":"webui/StoragePage/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/StoragePage/#rddrowrdd-v1rddstorageinfo-seqnode","text":"rddRow ...FIXME NOTE: rddRow is used when...FIXME === [[rddTable]] Rendering HTML Table with RDD Details -- rddTable Method","title":"rddRow(rdd: v1.RDDStorageInfo): Seq[Node]"},{"location":"webui/StoragePage/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/StoragePage/#rddtablerdds-seqv1rddstorageinfo-seqnode","text":"rddTable ...FIXME NOTE: rddTable is used when...FIXME === [[receiverBlockTables]] receiverBlockTables Method","title":"rddTable(rdds: Seq[v1.RDDStorageInfo]): Seq[Node]"},{"location":"webui/StoragePage/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/StoragePage/#receiverblocktablesblocks-seqstreamblockdata-seqnode","text":"receiverBlockTables ...FIXME NOTE: receiverBlockTables is used when...FIXME === [[render]] Rendering Page -- render Method","title":"receiverBlockTables(blocks: Seq[StreamBlockData]): Seq[Node]"},{"location":"webui/StoragePage/#source-scala_3","text":"","title":"[source, scala]"},{"location":"webui/StoragePage/#renderrequest-httpservletrequest-seqnode","text":"NOTE: render is part of spark-webui-WebUIPage.md#render[WebUIPage Contract] to...FIXME. render requests the < > for core:AppStatusStore.md#rddList[rddList] and < > (if available). render requests the < > for core:AppStatusStore.md#streamBlocksList[streamBlocksList] and < > (if available). In the end, render requests UIUtils to headerSparkPage (with Storage title).","title":"render(request: HttpServletRequest): Seq[Node]"},{"location":"webui/StorageStatusListener/","text":"== [[StorageStatusListener]] StorageStatusListener -- Spark Listener for Tracking BlockManagers StorageStatusListener is a ROOT:SparkListener.md[] that uses < > to track status of every storage:BlockManager.md[BlockManager] in a Spark application. StorageStatusListener is created and registered when SparkUI spark-webui-SparkUI.md#create[is created]. It is later used to create spark-webui-executors-ExecutorsListener.md[ExecutorsListener] and spark-webui-StorageListener.md[StorageListener] Spark listeners. [[SparkListener-callbacks]] .StorageStatusListener's SparkListener Callbacks (in alphabetical order) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Callback | Description | [[onBlockManagerAdded]] onBlockManagerAdded | Adds an executor id with spark-blockmanager-StorageStatus.md[StorageStatus] (with storage:BlockManager.md[BlockManager] and maximum memory on the executor) to < > internal registry. Removes any other BlockManager that may have been registered for the executor earlier in < > internal registry. | onBlockManagerRemoved | Removes an executor from < > internal registry and adds the removed spark-blockmanager-StorageStatus.md[StorageStatus] to < > internal registry. Removes the oldest spark-blockmanager-StorageStatus.md[StorageStatus] when the number of entries in < > is bigger than spark-webui-properties.md#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors]. | onBlockUpdated | Updates spark-blockmanager-StorageStatus.md[StorageStatus] for an executor in < > internal registry, i.e. removes a block for storage:StorageLevel.md[ NONE storage level] and updates otherwise. | [[onUnpersistRDD]] onUnpersistRDD | < > for an unpersisted RDD (on every BlockManager registered as spark-blockmanager-StorageStatus.md[StorageStatus] in < > internal registry). |=== [[internal-registries]] .StorageStatusListener's Internal Registries and Counters [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | [[deadExecutorStorageStatus]] deadExecutorStorageStatus | Collection of spark-blockmanager-StorageStatus.md[StorageStatus] of removed/inactive BlockManagers . Accessible using < > method. Adds an element when StorageStatusListener < > (possibly removing one element from the head when the number of elements are above spark-webui-properties.md#spark.ui.retainedDeadExecutors[spark.ui.retainedDeadExecutors] property). Removes an element when StorageStatusListener < > (per executor) so the executor is not longer dead. | [[executorIdToStorageStatus]] executorIdToStorageStatus | Lookup table of spark-blockmanager-StorageStatus.md[StorageStatus] per executor (including the driver). Adds an entry when StorageStatusListener < >. Removes an entry when StorageStatusListener < >. Updates StorageStatus of an executor when StorageStatusListener < >. |=== === [[updateStorageStatus-executor]] Updating Storage Status For Executor -- updateStorageStatus Method CAUTION: FIXME === [[storageStatusList]] Active BlockManagers (on Executors) -- storageStatusList Method [source, scala] \u00b6 storageStatusList: Seq[StorageStatus] \u00b6 storageStatusList gives a collection of spark-blockmanager-StorageStatus.md[StorageStatus] (from < > internal registry). [NOTE] \u00b6 storageStatusList is used when: StorageStatusListener < > ExecutorsListener does spark-webui-executors-ExecutorsListener.md#activeStorageStatusList[activeStorageStatusList] StorageListener does spark-webui-StorageListener.md#activeStorageStatusList[activeStorageStatusList] \u00b6 === [[deadStorageStatusList]] deadStorageStatusList Method [source, scala] \u00b6 deadStorageStatusList: Seq[StorageStatus] \u00b6 deadStorageStatusList gives < > internal registry. NOTE: deadStorageStatusList is used when ExecutorsListener spark-webui-executors-ExecutorsListener.md#deadStorageStatusList[is requested for inactive/dead BlockManagers]. === [[updateStorageStatus-unpersistedRDD]] Removing RDD Blocks for Unpersisted RDD -- updateStorageStatus Internal Method [source, scala] \u00b6 updateStorageStatus(unpersistedRDDId: Int) \u00b6 updateStorageStatus takes < >. updateStorageStatus then spark-blockmanager-StorageStatus.md#rddBlocksById[finds RDD blocks] for unpersistedRDDId RDD (for every BlockManager ) and spark-blockmanager-StorageStatus.md#removeBlock[removes the blocks]. NOTE: storageStatusList is used exclusively when StorageStatusListener < >.","title":"StorageStatusListener"},{"location":"webui/StorageStatusListener/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/StorageStatusListener/#storagestatuslist-seqstoragestatus","text":"storageStatusList gives a collection of spark-blockmanager-StorageStatus.md[StorageStatus] (from < > internal registry).","title":"storageStatusList: Seq[StorageStatus]"},{"location":"webui/StorageStatusListener/#note","text":"storageStatusList is used when: StorageStatusListener < > ExecutorsListener does spark-webui-executors-ExecutorsListener.md#activeStorageStatusList[activeStorageStatusList]","title":"[NOTE]"},{"location":"webui/StorageStatusListener/#storagelistener-does-spark-webui-storagelistenermdactivestoragestatuslistactivestoragestatuslist","text":"=== [[deadStorageStatusList]] deadStorageStatusList Method","title":"StorageListener does spark-webui-StorageListener.md#activeStorageStatusList[activeStorageStatusList]"},{"location":"webui/StorageStatusListener/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/StorageStatusListener/#deadstoragestatuslist-seqstoragestatus","text":"deadStorageStatusList gives < > internal registry. NOTE: deadStorageStatusList is used when ExecutorsListener spark-webui-executors-ExecutorsListener.md#deadStorageStatusList[is requested for inactive/dead BlockManagers]. === [[updateStorageStatus-unpersistedRDD]] Removing RDD Blocks for Unpersisted RDD -- updateStorageStatus Internal Method","title":"deadStorageStatusList: Seq[StorageStatus]"},{"location":"webui/StorageStatusListener/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/StorageStatusListener/#updatestoragestatusunpersistedrddid-int","text":"updateStorageStatus takes < >. updateStorageStatus then spark-blockmanager-StorageStatus.md#rddBlocksById[finds RDD blocks] for unpersistedRDDId RDD (for every BlockManager ) and spark-blockmanager-StorageStatus.md#removeBlock[removes the blocks]. NOTE: storageStatusList is used exclusively when StorageStatusListener < >.","title":"updateStorageStatus(unpersistedRDDId: Int)"},{"location":"webui/StorageTab/","text":"== [[StorageTab]] StorageTab [[prefix]] StorageTab is a spark-webui-SparkUITab.md[SparkUITab] with storage spark-webui-SparkUITab.md#prefix[prefix]. StorageTab is < > exclusively when SparkUI is spark-webui-SparkUI.md#initialize[initialized]. [[creating-instance]] StorageTab takes the following when created: [[parent]] Parent spark-webui-SparkUI.md[SparkUI] [[store]] core:AppStatusStore.md[] When < >, StorageTab creates the following pages and spark-webui-WebUITab.md#attachPage[attaches] them immediately: spark-webui-StoragePage.md[StoragePage] spark-webui-RDDPage.md[RDDPage]","title":"StorageTab"},{"location":"webui/UIUtils/","text":"== [[UIUtils]] UIUtils UIUtils is a utility object for...FIXME === [[headerSparkPage]] headerSparkPage Method [source, scala] \u00b6 headerSparkPage( request: HttpServletRequest, title: String, content: => Seq[Node], activeTab: SparkUITab, refreshInterval: Option[Int] = None, helpText: Option[String] = None, showVisualization: Boolean = false, useDataTables: Boolean = false): Seq[Node] headerSparkPage ...FIXME NOTE: headerSparkPage is used when...FIXME","title":"UIUtils"},{"location":"webui/UIUtils/#source-scala","text":"headerSparkPage( request: HttpServletRequest, title: String, content: => Seq[Node], activeTab: SparkUITab, refreshInterval: Option[Int] = None, helpText: Option[String] = None, showVisualization: Boolean = false, useDataTables: Boolean = false): Seq[Node] headerSparkPage ...FIXME NOTE: headerSparkPage is used when...FIXME","title":"[source, scala]"},{"location":"webui/WebUI/","text":"WebUI -- Base Web UI \u00b6 WebUI is the < > of the < > in Apache Spark: Active Spark applications Spark History Server Spark Standalone cluster manager Spark on Mesos cluster manager NOTE: Spark on YARN uses a different web framework for the web UI. WebUI is used as the parent of spark-webui-WebUITab.md#parent[WebUITabs]. [[contract]] [source, scala] package org.apache.spark.ui abstract class WebUI { // only required methods that have no implementation // the others follow def initialize(): Unit } NOTE: WebUI is a private[spark] contract. .(Subset of) WebUI Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | initialize a| [[initialize]] Used in < > only to let them initialize their web components NOTE: initialize does not add anything special to the Scala type hierarchy but a common name to use across WebUIs (that could also be possible without it). In other words, initialize does not participate in any design pattern or a type hierarchy. |=== WebUI is a Scala abstract class and cannot be < > directly, but only as one of the < >. [[implementations]] .WebUIs [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | WebUI | Description | spark-history-server:HistoryServer.md[HistoryServer] | [[HistoryServer]] Used in Spark History Server | MasterWebUI | [[MasterWebUI]] Used in Spark Standalone cluster manager | MesosClusterUI | [[MesosClusterUI]] Used in Spark on Mesos cluster manager | spark-webui-SparkUI.md[SparkUI] | [[SparkUI]] WebUI of a Spark application | WorkerWebUI | [[WorkerWebUI]] Used in Spark Standalone cluster manager |=== [[boundPort]] Once < >, WebUI is available at an HTTP port (and is used in the < > as boundPort ). [[webUrl]] WebUI is available at a web URL, i.e. http://[publicHostName]:[boundPort] . The < > is...FIXME and the < > is the port that the < >. [[internal-registries]] .WebUI's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | tabs | [[tabs]] spark-webui-WebUITab.md[WebUITabs] Used when...FIXME | handlers | [[handlers]] ServletContextHandlers Used when...FIXME | pageToHandlers | [[pageToHandlers]] ServletContextHandlers per spark-webui-WebUIPage.md[WebUIPage] Used when...FIXME | serverInfo | [[serverInfo]] Optional ServerInfo (default: None ) Used when...FIXME | publicHostName a| [[publicHostName]] Host name of the UI publicHostName is either SPARK_PUBLIC_DNS environment variable or spark.driver.host configuration property. Defaults to the following if defined (in order): . SPARK_LOCAL_HOSTNAME environment variable . Host name of SPARK_LOCAL_IP environment variable . Utils.findLocalInetAddress Used exclusively when WebUI is requested for the < > | className | [[className]] Used when...FIXME |=== [[logging]] [TIP] ==== Enable INFO or ERROR logging level for the corresponding loggers of the < >, e.g. org.apache.spark.ui.SparkUI , to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ui=INFO Refer to spark-logging.md[Logging]. \u00b6 === [[creating-instance]] Creating WebUI Instance WebUI takes the following when created: [[securityManager]] SecurityManager [[sslOptions]] SSLOptions [[port]] Port number [[conf]] ROOT:SparkConf.md[SparkConf] [[basePath]] basePath (default: empty) [[name]] Name (default: empty) WebUI initializes the < >. NOTE: WebUI is a Scala abstract class and cannot be created directly, but only as one of the < >. === [[detachPage]] Detaching Page And Associated Handlers from UI -- detachPage Method [source, scala] \u00b6 detachPage(page: WebUIPage): Unit \u00b6 detachPage ...FIXME NOTE: detachPage is used when...FIXME === [[detachTab]] Detaching Tab And Associated Pages from UI -- detachTab Method [source, scala] \u00b6 detachTab(tab: WebUITab): Unit \u00b6 detachTab ...FIXME NOTE: detachTab is used when...FIXME === [[detachHandler-ServletContextHandler]] Detaching Handler -- detachHandler Method [source, scala] \u00b6 detachHandler(handler: ServletContextHandler): Unit \u00b6 detachHandler ...FIXME NOTE: detachHandler is used when...FIXME === [[detachHandler-String]] Detaching Handler At Path -- detachHandler Method [source, scala] \u00b6 detachHandler(path: String): Unit \u00b6 detachHandler ...FIXME NOTE: detachHandler is used when...FIXME === [[attachPage]] Attaching Page to UI -- attachPage Method [source, scala] \u00b6 attachPage(page: WebUIPage): Unit \u00b6 Internally, attachPage creates the path of the spark-webui-WebUIPage.md[WebUIPage] that is / (forward slash) followed by the spark-webui-WebUIPage.md#prefix[prefix] of the page. attachPage spark-webui-JettyUtils.md#createServletHandler[creates a HTTP request handler]...FIXME [NOTE] \u00b6 attachPage is used when: WebUI is requested to < > (the spark-webui-WebUITab.md#pages[WebUIPages] actually) * spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI are requested to initialize \u00b6 === [[attachTab]] Attaching Tab And Associated Pages to UI -- attachTab Method [source, scala] \u00b6 attachTab(tab: WebUITab): Unit \u00b6 attachTab < > every WebUIPage of the input spark-webui-WebUITab.md#pages[WebUITab]. In the end, attachTab adds the input WebUITab to < >. NOTE: attachTab is used when...FIXME === [[addStaticHandler]] Attaching Static Handler -- addStaticHandler Method [source, scala] \u00b6 addStaticHandler(resourceBase: String, path: String): Unit \u00b6 addStaticHandler ...FIXME NOTE: addStaticHandler is used when...FIXME === [[attachHandler]] Attaching Handler to UI -- attachHandler Method [source, scala] \u00b6 attachHandler(handler: ServletContextHandler): Unit \u00b6 attachHandler simply adds the input Jetty ServletContextHandler to < > registry and requests the < > to addHandler (only if the ServerInfo is defined). attachHandler is used when: < > (i.e. spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI , spark-webui-SparkUI.md#initialize[SparkUI]) are requested to initialize WebUI is requested to < > and < > SparkContext is created HistoryServer is requested to spark-history-server:HistoryServer.md#attachSparkUI[attachSparkUI] Spark Standalone's Master and Worker are requested to onStart (and attach their metrics servlet handlers to the web ui) === [[getBasePath]] getBasePath Method [source, scala] \u00b6 getBasePath: String \u00b6 getBasePath simply returns the < >. NOTE: getBasePath is used exclusively when WebUITab is requested for the spark-webui-WebUITab.md#basePath[base path]. === [[getTabs]] Requesting Header Tabs -- getTabs Method [source, scala] \u00b6 getTabs: Seq[WebUITab] \u00b6 getTabs simply returns the < >. NOTE: getTabs is used exclusively when WebUITab is requested for the spark-webui-WebUITab.md#headerTabs[header tabs]. === [[getHandlers]] Requesting Handlers -- getHandlers Method [source, scala] \u00b6 getHandlers: Seq[ServletContextHandler] \u00b6 getHandlers simply returns the < >. NOTE: getHandlers is used when...FIXME === [[bind]] Binding UI to Jetty HTTP Server on Host -- bind Method [source, scala] \u00b6 bind(): Unit \u00b6 bind ...FIXME NOTE: bind is used when...FIXME === [[stop]] Stopping UI -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop is used when...FIXME","title":"WebUI"},{"location":"webui/WebUI/#webui-base-web-ui","text":"WebUI is the < > of the < > in Apache Spark: Active Spark applications Spark History Server Spark Standalone cluster manager Spark on Mesos cluster manager NOTE: Spark on YARN uses a different web framework for the web UI. WebUI is used as the parent of spark-webui-WebUITab.md#parent[WebUITabs]. [[contract]] [source, scala] package org.apache.spark.ui abstract class WebUI { // only required methods that have no implementation // the others follow def initialize(): Unit } NOTE: WebUI is a private[spark] contract. .(Subset of) WebUI Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | initialize a| [[initialize]] Used in < > only to let them initialize their web components NOTE: initialize does not add anything special to the Scala type hierarchy but a common name to use across WebUIs (that could also be possible without it). In other words, initialize does not participate in any design pattern or a type hierarchy. |=== WebUI is a Scala abstract class and cannot be < > directly, but only as one of the < >. [[implementations]] .WebUIs [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | WebUI | Description | spark-history-server:HistoryServer.md[HistoryServer] | [[HistoryServer]] Used in Spark History Server | MasterWebUI | [[MasterWebUI]] Used in Spark Standalone cluster manager | MesosClusterUI | [[MesosClusterUI]] Used in Spark on Mesos cluster manager | spark-webui-SparkUI.md[SparkUI] | [[SparkUI]] WebUI of a Spark application | WorkerWebUI | [[WorkerWebUI]] Used in Spark Standalone cluster manager |=== [[boundPort]] Once < >, WebUI is available at an HTTP port (and is used in the < > as boundPort ). [[webUrl]] WebUI is available at a web URL, i.e. http://[publicHostName]:[boundPort] . The < > is...FIXME and the < > is the port that the < >. [[internal-registries]] .WebUI's Internal Properties (e.g. Registries, Counters and Flags) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | tabs | [[tabs]] spark-webui-WebUITab.md[WebUITabs] Used when...FIXME | handlers | [[handlers]] ServletContextHandlers Used when...FIXME | pageToHandlers | [[pageToHandlers]] ServletContextHandlers per spark-webui-WebUIPage.md[WebUIPage] Used when...FIXME | serverInfo | [[serverInfo]] Optional ServerInfo (default: None ) Used when...FIXME | publicHostName a| [[publicHostName]] Host name of the UI publicHostName is either SPARK_PUBLIC_DNS environment variable or spark.driver.host configuration property. Defaults to the following if defined (in order): . SPARK_LOCAL_HOSTNAME environment variable . Host name of SPARK_LOCAL_IP environment variable . Utils.findLocalInetAddress Used exclusively when WebUI is requested for the < > | className | [[className]] Used when...FIXME |=== [[logging]] [TIP] ==== Enable INFO or ERROR logging level for the corresponding loggers of the < >, e.g. org.apache.spark.ui.SparkUI , to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.ui=INFO","title":"WebUI -- Base Web UI"},{"location":"webui/WebUI/#refer-to-spark-loggingmdlogging","text":"=== [[creating-instance]] Creating WebUI Instance WebUI takes the following when created: [[securityManager]] SecurityManager [[sslOptions]] SSLOptions [[port]] Port number [[conf]] ROOT:SparkConf.md[SparkConf] [[basePath]] basePath (default: empty) [[name]] Name (default: empty) WebUI initializes the < >. NOTE: WebUI is a Scala abstract class and cannot be created directly, but only as one of the < >. === [[detachPage]] Detaching Page And Associated Handlers from UI -- detachPage Method","title":"Refer to spark-logging.md[Logging]."},{"location":"webui/WebUI/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#detachpagepage-webuipage-unit","text":"detachPage ...FIXME NOTE: detachPage is used when...FIXME === [[detachTab]] Detaching Tab And Associated Pages from UI -- detachTab Method","title":"detachPage(page: WebUIPage): Unit"},{"location":"webui/WebUI/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#detachtabtab-webuitab-unit","text":"detachTab ...FIXME NOTE: detachTab is used when...FIXME === [[detachHandler-ServletContextHandler]] Detaching Handler -- detachHandler Method","title":"detachTab(tab: WebUITab): Unit"},{"location":"webui/WebUI/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#detachhandlerhandler-servletcontexthandler-unit","text":"detachHandler ...FIXME NOTE: detachHandler is used when...FIXME === [[detachHandler-String]] Detaching Handler At Path -- detachHandler Method","title":"detachHandler(handler: ServletContextHandler): Unit"},{"location":"webui/WebUI/#source-scala_3","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#detachhandlerpath-string-unit","text":"detachHandler ...FIXME NOTE: detachHandler is used when...FIXME === [[attachPage]] Attaching Page to UI -- attachPage Method","title":"detachHandler(path: String): Unit"},{"location":"webui/WebUI/#source-scala_4","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#attachpagepage-webuipage-unit","text":"Internally, attachPage creates the path of the spark-webui-WebUIPage.md[WebUIPage] that is / (forward slash) followed by the spark-webui-WebUIPage.md#prefix[prefix] of the page. attachPage spark-webui-JettyUtils.md#createServletHandler[creates a HTTP request handler]...FIXME","title":"attachPage(page: WebUIPage): Unit"},{"location":"webui/WebUI/#note","text":"attachPage is used when: WebUI is requested to < > (the spark-webui-WebUITab.md#pages[WebUIPages] actually)","title":"[NOTE]"},{"location":"webui/WebUI/#spark-history-serverhistoryservermdinitializehistoryserver-spark-standalones-masterwebui-and-workerwebui-spark-on-mesos-mesosclusterui-are-requested-to-initialize","text":"=== [[attachTab]] Attaching Tab And Associated Pages to UI -- attachTab Method","title":"* spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI, Spark on Mesos' MesosClusterUI are requested to initialize"},{"location":"webui/WebUI/#source-scala_5","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#attachtabtab-webuitab-unit","text":"attachTab < > every WebUIPage of the input spark-webui-WebUITab.md#pages[WebUITab]. In the end, attachTab adds the input WebUITab to < >. NOTE: attachTab is used when...FIXME === [[addStaticHandler]] Attaching Static Handler -- addStaticHandler Method","title":"attachTab(tab: WebUITab): Unit"},{"location":"webui/WebUI/#source-scala_6","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#addstatichandlerresourcebase-string-path-string-unit","text":"addStaticHandler ...FIXME NOTE: addStaticHandler is used when...FIXME === [[attachHandler]] Attaching Handler to UI -- attachHandler Method","title":"addStaticHandler(resourceBase: String, path: String): Unit"},{"location":"webui/WebUI/#source-scala_7","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#attachhandlerhandler-servletcontexthandler-unit","text":"attachHandler simply adds the input Jetty ServletContextHandler to < > registry and requests the < > to addHandler (only if the ServerInfo is defined). attachHandler is used when: < > (i.e. spark-history-server:HistoryServer.md#initialize[HistoryServer], Spark Standalone's MasterWebUI and WorkerWebUI , Spark on Mesos' MesosClusterUI , spark-webui-SparkUI.md#initialize[SparkUI]) are requested to initialize WebUI is requested to < > and < > SparkContext is created HistoryServer is requested to spark-history-server:HistoryServer.md#attachSparkUI[attachSparkUI] Spark Standalone's Master and Worker are requested to onStart (and attach their metrics servlet handlers to the web ui) === [[getBasePath]] getBasePath Method","title":"attachHandler(handler: ServletContextHandler): Unit"},{"location":"webui/WebUI/#source-scala_8","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#getbasepath-string","text":"getBasePath simply returns the < >. NOTE: getBasePath is used exclusively when WebUITab is requested for the spark-webui-WebUITab.md#basePath[base path]. === [[getTabs]] Requesting Header Tabs -- getTabs Method","title":"getBasePath: String"},{"location":"webui/WebUI/#source-scala_9","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#gettabs-seqwebuitab","text":"getTabs simply returns the < >. NOTE: getTabs is used exclusively when WebUITab is requested for the spark-webui-WebUITab.md#headerTabs[header tabs]. === [[getHandlers]] Requesting Handlers -- getHandlers Method","title":"getTabs: Seq[WebUITab]"},{"location":"webui/WebUI/#source-scala_10","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#gethandlers-seqservletcontexthandler","text":"getHandlers simply returns the < >. NOTE: getHandlers is used when...FIXME === [[bind]] Binding UI to Jetty HTTP Server on Host -- bind Method","title":"getHandlers: Seq[ServletContextHandler]"},{"location":"webui/WebUI/#source-scala_11","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#bind-unit","text":"bind ...FIXME NOTE: bind is used when...FIXME === [[stop]] Stopping UI -- stop Method","title":"bind(): Unit"},{"location":"webui/WebUI/#source-scala_12","text":"","title":"[source, scala]"},{"location":"webui/WebUI/#stop-unit","text":"stop ...FIXME NOTE: stop is used when...FIXME","title":"stop(): Unit"},{"location":"webui/WebUIPage/","text":"== [[WebUIPage]] WebUIPage -- Contract of Pages in Web UI WebUIPage is the < > of < > of a spark-webui-WebUI.md[WebUI] that can be rendered in < > and < >. WebUIPage can be: spark-webui-WebUI.md#attachPage[attached] or spark-webui-WebUI.md#detachPage[detached] from a WebUI spark-webui-WebUITab.md#attachPage[attached] to a WebUITab [[prefix]] WebUIPage has a prefix that...FIXME [[contract]] [source, scala] package org.apache.spark.ui abstract class WebUIPage(var prefix: String) { def render(request: HttpServletRequest): Seq[Node] def renderJson(request: HttpServletRequest): JValue = JNothing } NOTE: WebUIPage is a private[spark] contract. .WebUIPage Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | render | [[render]] Used exclusively when WebUI is requested to spark-webui-WebUI.md#attachPage[attach a page] (and...FIXME) | renderJson | [[renderJson]] Used when...FIXME |=== [[implementations]] .WebUIPages [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | WebUIPage | Description | AllExecutionsPage | [[AllExecutionsPage]] Used in Spark SQL module | spark-webui-AllJobsPage.md[AllJobsPage] | [[AllJobsPage]] | spark-webui-AllStagesPage.md[AllStagesPage] | [[AllStagesPage]] | spark-standalone-webui-ApplicationPage.md[ApplicationPage] | [[ApplicationPage]] Used in Spark Standalone cluster manager | BatchPage | [[BatchPage]] Used in Spark Streaming module | DriverPage | [[DriverPage]] Used in Spark on Mesos module | spark-webui-EnvironmentPage.md[EnvironmentPage] | [[EnvironmentPage]] | ExecutionPage | [[ExecutionPage]] Used in Spark SQL module | spark-webui-ExecutorsPage.md[ExecutorsPage] | [[ExecutorsPage]] | spark-webui-executors.md#ExecutorThreadDumpPage[ExecutorThreadDumpPage] | [[ExecutorThreadDumpPage]] | HistoryPage | [[HistoryPage]] Used in Spark History Server module | spark-webui-jobs.md[JobPage] | [[JobPage]] | LogPage | [[LogPage]] Used in Spark Standalone cluster manager | MasterPage | [[MasterPage]] Used in Spark Standalone cluster manager | MesosClusterPage | [[MesosClusterPage]] Used in Spark on Mesos module | spark-webui-PoolPage.md[PoolPage] | [[PoolPage]] | spark-webui-RDDPage.md[RDDPage] | [[RDDPage]] | spark-webui-StagePage.md[StagePage] | [[StagePage]] | spark-webui-StoragePage.md[StoragePage] | [[StoragePage]] | StreamingPage | [[StreamingPage]] Used in Spark Streaming module | ThriftServerPage | [[ThriftServerPage]] Used in Spark Thrift Server module | ThriftServerSessionPage | [[ThriftServerSessionPage]] Used in Spark Thrift Server module | WorkerPage | [[WorkerPage]] Used in Spark Standalone cluster manager |===","title":"WebUIPage"},{"location":"webui/WebUITab/","text":"== [[WebUITab]] WebUITab -- Contract of Tabs in Web UI WebUITab represents a < > in web UI with a < > and < >. WebUITab can be: spark-webui-WebUI.md#attachTab[attached] or spark-webui-WebUI.md#detachTab[detached] from a WebUI spark-webui-WebUITab.md#attachPage[attached] to a WebUITab [[pages]] WebUITab is simply a collection of spark-webui-WebUIPage.md[WebUIPages] that can be < > to the tab. [[name]] WebUITab has a name (and defaults to < > capitalized). [[implementations]] NOTE: spark-webui-SparkUITab.md[SparkUITab] is the one and only implementation of WebUITab contract. NOTE: WebUITab is a private[spark] contract. === [[attachPage]] Attaching Page to Tab -- attachPage Method [source, scala] \u00b6 attachPage(page: WebUIPage): Unit \u00b6 attachPage prepends the spark-webui-WebUIPage.md#prefix[page prefix] (of the input WebUIPage ) with the < > (with no ending slash, i.e. / , if the page prefix is undefined). In the end, attachPage adds the WebUIPage to < > registry. NOTE: attachPage is used when spark-webui-SparkUITab.md#implementations[web UI tabs] register their pages. === [[basePath]] Requesting Base URI Path -- basePath Method [source, scala] \u00b6 basePath: String \u00b6 basePath requests the < > for the spark-webui-WebUI.md#basePath[base path]. NOTE: basePath is used when...FIXME === [[headerTabs]] Requesting Header Tabs -- headerTabs Method [source, scala] \u00b6 headerTabs: Seq[WebUITab] \u00b6 headerTabs requests the < > for the spark-webui-WebUI.md#headerTabs[header tabs]. NOTE: headerTabs is used exclusively when UIUtils is requested to spark-webui-UIUtils.md#headerSparkPage[headerSparkPage]. === [[creating-instance]] Creating WebUITab Instance WebUITab takes the following when created: [[parent]] Parent spark-webui-WebUI.md[WebUI] [[prefix]] Prefix WebUITab initializes the < >. NOTE: WebUITab is a Scala abstract class and cannot be created directly, but only as one of the < >.","title":"WebUITab"},{"location":"webui/WebUITab/#source-scala","text":"","title":"[source, scala]"},{"location":"webui/WebUITab/#attachpagepage-webuipage-unit","text":"attachPage prepends the spark-webui-WebUIPage.md#prefix[page prefix] (of the input WebUIPage ) with the < > (with no ending slash, i.e. / , if the page prefix is undefined). In the end, attachPage adds the WebUIPage to < > registry. NOTE: attachPage is used when spark-webui-SparkUITab.md#implementations[web UI tabs] register their pages. === [[basePath]] Requesting Base URI Path -- basePath Method","title":"attachPage(page: WebUIPage): Unit"},{"location":"webui/WebUITab/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/WebUITab/#basepath-string","text":"basePath requests the < > for the spark-webui-WebUI.md#basePath[base path]. NOTE: basePath is used when...FIXME === [[headerTabs]] Requesting Header Tabs -- headerTabs Method","title":"basePath: String"},{"location":"webui/WebUITab/#source-scala_2","text":"","title":"[source, scala]"},{"location":"webui/WebUITab/#headertabs-seqwebuitab","text":"headerTabs requests the < > for the spark-webui-WebUI.md#headerTabs[header tabs]. NOTE: headerTabs is used exclusively when UIUtils is requested to spark-webui-UIUtils.md#headerSparkPage[headerSparkPage]. === [[creating-instance]] Creating WebUITab Instance WebUITab takes the following when created: [[parent]] Parent spark-webui-WebUI.md[WebUI] [[prefix]] Prefix WebUITab initializes the < >. NOTE: WebUITab is a Scala abstract class and cannot be created directly, but only as one of the < >.","title":"headerTabs: Seq[WebUITab]"},{"location":"webui/configuration-properties/","text":"web UI Configuration Properties \u00b6 [[properties]] .web UI Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description [[spark.ui.allowFramingFrom]] spark.ui.allowFramingFrom Defines the URL to use in ALLOW-FROM in X-Frame-Options header (as described in http://tools.ietf.org/html/rfc7034 ). Used exclusively when JettyUtils is requested to spark-webui-JettyUtils.md#createServlet[create an HttpServlet]. | [[spark.ui.consoleProgress.update.interval]] spark.ui.consoleProgress.update.interval | 200 (ms) | Update interval, i.e. how often to show the progress. | [[spark.ui.enabled]] spark.ui.enabled | true | The flag to control whether the web UI is started ( true ) or not ( false ). | [[spark.ui.port]] spark.ui.port | 4040 | The port web UI binds to. If multiple SparkContext s attempt to run on the same host (it is not possible to have two or more Spark contexts on a single JVM, though), they will bind to successive ports beginning with spark.ui.port . | [[spark.ui.killEnabled]] spark.ui.killEnabled | true | Enables jobs and stages to be killed from the web UI ( true ) or not ( false ). Used exclusively when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize] (and registers the redirect handlers for /jobs/job/kill and /stages/stage/kill URIs) | [[spark.ui.retainedDeadExecutors]] spark.ui.retainedDeadExecutors | 100 | The maximum number of entries in spark-webui-executors-ExecutorsListener.md#executorToTaskSummary[executorToTaskSummary] (in ExecutorsListener ) and spark-webui-StorageStatusListener.md#deadExecutorStorageStatus[deadExecutorStorageStatus] (in StorageStatusListener ) internal registries. | [[spark.ui.timeline.executors.maximum]] spark.ui.timeline.executors.maximum | 1000 | The maximum number of entries in < > registry. | [[spark.ui.timeline.tasks.maximum]] spark.ui.timeline.tasks.maximum | 1000 | |===","title":"Configuration Properties"},{"location":"webui/configuration-properties/#web-ui-configuration-properties","text":"[[properties]] .web UI Configuration Properties [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description [[spark.ui.allowFramingFrom]] spark.ui.allowFramingFrom Defines the URL to use in ALLOW-FROM in X-Frame-Options header (as described in http://tools.ietf.org/html/rfc7034 ). Used exclusively when JettyUtils is requested to spark-webui-JettyUtils.md#createServlet[create an HttpServlet]. | [[spark.ui.consoleProgress.update.interval]] spark.ui.consoleProgress.update.interval | 200 (ms) | Update interval, i.e. how often to show the progress. | [[spark.ui.enabled]] spark.ui.enabled | true | The flag to control whether the web UI is started ( true ) or not ( false ). | [[spark.ui.port]] spark.ui.port | 4040 | The port web UI binds to. If multiple SparkContext s attempt to run on the same host (it is not possible to have two or more Spark contexts on a single JVM, though), they will bind to successive ports beginning with spark.ui.port . | [[spark.ui.killEnabled]] spark.ui.killEnabled | true | Enables jobs and stages to be killed from the web UI ( true ) or not ( false ). Used exclusively when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize] (and registers the redirect handlers for /jobs/job/kill and /stages/stage/kill URIs) | [[spark.ui.retainedDeadExecutors]] spark.ui.retainedDeadExecutors | 100 | The maximum number of entries in spark-webui-executors-ExecutorsListener.md#executorToTaskSummary[executorToTaskSummary] (in ExecutorsListener ) and spark-webui-StorageStatusListener.md#deadExecutorStorageStatus[deadExecutorStorageStatus] (in StorageStatusListener ) internal registries. | [[spark.ui.timeline.executors.maximum]] spark.ui.timeline.executors.maximum | 1000 | The maximum number of entries in < > registry. | [[spark.ui.timeline.tasks.maximum]] spark.ui.timeline.tasks.maximum | 1000 | |===","title":"web UI Configuration Properties"},{"location":"webui/environment/","text":"== Environment Tab Environment tab in spark-webui.md[web UI] shows...FIXME .Environment Tab in Web UI image::spark-webui-environment.png[align=\"center\"] The Environment tab is available under /environment URL, i.e. http://localhost:4040/environment . Internally, the Environment tab is represented by spark-webui-EnvironmentTab.md[EnvironmentTab].","title":"Environment"},{"location":"webui/executors/","text":"== Executors Tab Executors tab in spark-webui.md[web UI] shows...FIXME .Executors Tab in web UI (local mode) image::spark-webui-executors.png[align=\"center\"] The Executors tab is available under /executors URL, i.e. http://localhost:4040/executors . Internally, the Executors tab is represented by spark-webui-ExecutorsTab.md[ExecutorsTab]. [NOTE] \u00b6 What's interesting in how Storage Memory is displayed in the Executors tab is that the default UnifiedMemoryManager memory:UnifiedMemoryManager.md#getMaxMemory[calculates the maximum memory] in a way that is different from what the page displays (using the custom JavaScript formatBytes function in utils.js ). [source, scala] \u00b6 // local mode with spark.driver.memory 2g // ./bin/spark-shell --conf spark.driver.memory=2g // UnifiedMemoryManager reports 912MB // You can see it after enabling INFO messages for BlockManagerMasterEndpoint INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.8:54503 with 912.3 MB RAM, BlockManagerId(driver, 192.168.1.8, 54503, None) // custom JavaScript formatBytes function (from utils.js) reports...956.6MB // See https://github.com/apache/spark/blob/master/core/src/main/resources/org/apache/spark/ui/static/utils.js#L40-L48 def formatBytes(bytes: Double) = { val k = 1000 val i = math.floor(math.log(bytes) / math.log(k)) val maxMemoryWebUI = bytes / math.pow(k, i) f\"$maxMemoryWebUI%1.1f\" } scala> println(formatBytes(maxMemory)) 956.6 ==== === [[getExecInfo]] getExecInfo Method [source, scala] \u00b6 getExecInfo( listener: ExecutorsListener, statusId: Int, isActive: Boolean): ExecutorSummary getExecInfo creates a ExecutorSummary . CAUTION: FIXME NOTE: getExecInfo is used when...FIXME === [[settings]] Settings ==== [[spark_ui_threadDumpsEnabled]] spark.ui.threadDumpsEnabled spark.ui.threadDumpsEnabled (default: true ) is to enable ( true ) or disable ( false ) < >.","title":"Executors"},{"location":"webui/executors/#note","text":"What's interesting in how Storage Memory is displayed in the Executors tab is that the default UnifiedMemoryManager memory:UnifiedMemoryManager.md#getMaxMemory[calculates the maximum memory] in a way that is different from what the page displays (using the custom JavaScript formatBytes function in utils.js ).","title":"[NOTE]"},{"location":"webui/executors/#source-scala","text":"// local mode with spark.driver.memory 2g // ./bin/spark-shell --conf spark.driver.memory=2g // UnifiedMemoryManager reports 912MB // You can see it after enabling INFO messages for BlockManagerMasterEndpoint INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.8:54503 with 912.3 MB RAM, BlockManagerId(driver, 192.168.1.8, 54503, None) // custom JavaScript formatBytes function (from utils.js) reports...956.6MB // See https://github.com/apache/spark/blob/master/core/src/main/resources/org/apache/spark/ui/static/utils.js#L40-L48 def formatBytes(bytes: Double) = { val k = 1000 val i = math.floor(math.log(bytes) / math.log(k)) val maxMemoryWebUI = bytes / math.pow(k, i) f\"$maxMemoryWebUI%1.1f\" } scala> println(formatBytes(maxMemory)) 956.6 ==== === [[getExecInfo]] getExecInfo Method","title":"[source, scala]"},{"location":"webui/executors/#source-scala_1","text":"getExecInfo( listener: ExecutorsListener, statusId: Int, isActive: Boolean): ExecutorSummary getExecInfo creates a ExecutorSummary . CAUTION: FIXME NOTE: getExecInfo is used when...FIXME === [[settings]] Settings ==== [[spark_ui_threadDumpsEnabled]] spark.ui.threadDumpsEnabled spark.ui.threadDumpsEnabled (default: true ) is to enable ( true ) or disable ( false ) < >.","title":"[source, scala]"},{"location":"webui/jobs/","text":"Jobs Tab \u00b6 Jobs tab in index.md[web UI] shows spark-webui-AllJobsPage.md[status of all Spark jobs] in a Spark application (ROOT:SparkContext.md[]). .Jobs Tab in Web UI image::spark-webui-jobs.png[align=\"center\"] Jobs tab is available under /jobs URL (e.g. http://localhost:4040/jobs ). .Event Timeline in Jobs Tab image::spark-webui-jobs-event-timeline.png[align=\"center\"] The Jobs tab consists of two pages, i.e. spark-webui-AllJobsPage.md[All Jobs] and < > pages. Internally, the Jobs tab is represented by spark-webui-JobsTab.md[JobsTab]. == [[JobPage]] Details for Job -- JobPage Page When you click a job in spark-webui-AllJobsPage.md[AllJobsPage], you see the Details for Job page. .Details for Job Page image::spark-webui-jobs-details-for-job.png[align=\"center\"] JobPage is a spark-webui-WebUIPage.md[WebUIPage] that shows statistics and stage list for a given job. Details for Job page is registered under /job URL, i.e. http://localhost:4040/jobs/job/?id=0 and accepts one mandatory id request parameter as a job identifier. When a job id is not found, you should see \"No information to display for job ID\" message. .\"No information to display for job\" in Details for Job Page image::spark-webui-jobs-details-for-job-no-job.png[align=\"center\"] JobPage displays the job's status, group (if available), and the stages per state: active, pending, completed, skipped, and failed. NOTE: A job can be in a running, succeeded, failed or unknown state. .Details for Job Page with Active and Pending Stages image::spark-webui-jobs-details-for-job-active-pending-stages.png[align=\"center\"] .Details for Job Page with Four Stages image::spark-webui-jobs-details-for-job-four-stages.png[align=\"center\"]","title":"Jobs"},{"location":"webui/jobs/#jobs-tab","text":"Jobs tab in index.md[web UI] shows spark-webui-AllJobsPage.md[status of all Spark jobs] in a Spark application (ROOT:SparkContext.md[]). .Jobs Tab in Web UI image::spark-webui-jobs.png[align=\"center\"] Jobs tab is available under /jobs URL (e.g. http://localhost:4040/jobs ). .Event Timeline in Jobs Tab image::spark-webui-jobs-event-timeline.png[align=\"center\"] The Jobs tab consists of two pages, i.e. spark-webui-AllJobsPage.md[All Jobs] and < > pages. Internally, the Jobs tab is represented by spark-webui-JobsTab.md[JobsTab]. == [[JobPage]] Details for Job -- JobPage Page When you click a job in spark-webui-AllJobsPage.md[AllJobsPage], you see the Details for Job page. .Details for Job Page image::spark-webui-jobs-details-for-job.png[align=\"center\"] JobPage is a spark-webui-WebUIPage.md[WebUIPage] that shows statistics and stage list for a given job. Details for Job page is registered under /job URL, i.e. http://localhost:4040/jobs/job/?id=0 and accepts one mandatory id request parameter as a job identifier. When a job id is not found, you should see \"No information to display for job ID\" message. .\"No information to display for job\" in Details for Job Page image::spark-webui-jobs-details-for-job-no-job.png[align=\"center\"] JobPage displays the job's status, group (if available), and the stages per state: active, pending, completed, skipped, and failed. NOTE: A job can be in a running, succeeded, failed or unknown state. .Details for Job Page with Active and Pending Stages image::spark-webui-jobs-details-for-job-active-pending-stages.png[align=\"center\"] .Details for Job Page with Four Stages image::spark-webui-jobs-details-for-job-four-stages.png[align=\"center\"]","title":"Jobs Tab"},{"location":"webui/spark-webui-AllJobsPage/","text":"== [[AllJobsPage]] AllJobsPage -- Showing All Jobs in Web UI [[prefix]] AllJobsPage is a spark-webui-WebUIPage.md[WebUIPage] with an empty spark-webui-WebUIPage.md#prefix[prefix]. AllJobsPage is < > exclusively when JobsTab is spark-webui-JobsTab.md#creating-instance[created]. AllJobsPage renders a summary, an event timeline, and active, completed, and failed jobs of a Spark application. TIP: Jobs (in any state) are displayed when their number is greater than 0 . AllJobsPage displays the Summary section with the spark-webui-SparkUI.md#getSparkUser[current Spark user], total uptime, scheduling mode, and the number of jobs per status. NOTE: AllJobsPage uses spark-webui-JobProgressListener.md[JobProgressListener] for Scheduling Mode . .Summary Section in Jobs Tab image::spark-webui-jobs-summary-section.png[align=\"center\"] Under the summary section is the Event Timeline section. .Event Timeline in Jobs Tab image::spark-webui-jobs-event-timeline.png[align=\"center\"] NOTE: spark-webui-AllJobsPage.md[AllJobsPage] uses spark-webui-executors-ExecutorsListener.md[ExecutorsListener] to build the event timeline. Active Jobs , Completed Jobs , and Failed Jobs sections follow. .Job Status Section in Jobs Tab image::spark-webui-jobs-status-section.png[align=\"center\"] Jobs are clickable, i.e. you can click on a job to < >. When you hover over a job in Event Timeline not only you see the job legend but also the job is highlighted in the Summary section. .Hovering Over Job in Event Timeline Highlights The Job in Status Section image::spark-webui-jobs-timeline-popup.png[align=\"center\"] The Event Timeline section shows not only jobs but also executors. .Executors in Event Timeline image::spark-webui-jobs-timeline-executors.png[align=\"center\"] TIP: Use ROOT:SparkContext.md#dynamic-allocation[Programmable Dynamic Allocation] (using SparkContext ) to manage executors for demo purposes. === [[creating-instance]] Creating AllJobsPage Instance AllJobsPage takes the following when created: [[parent]] Parent spark-webui-JobsTab.md[JobsTab] [[store]] core:AppStatusStore.md[]","title":"AllJobsPage"},{"location":"webui/spark-webui-JobPage/","text":"== [[JobPage]] JobPage [[prefix]] JobPage is a spark-webui-WebUIPage.md[WebUIPage] with job spark-webui-WebUIPage.md#prefix[prefix]. JobPage is < > exclusively when JobsTab is spark-webui-JobsTab.md#creating-instance[created]. === [[creating-instance]] Creating JobPage Instance JobPage takes the following when created: [[parent]] Parent spark-webui-JobsTab.md[JobsTab] [[store]] core:AppStatusStore.md[]","title":"JobPage"},{"location":"webui/spark-webui-JobsTab/","text":"= [[JobsTab]] JobsTab [[prefix]] JobsTab is a spark-webui-SparkUITab.md[SparkUITab] with the spark-webui-SparkUITab.md#prefix[prefix] as jobs and the following pages: spark-webui-AllJobsPage.md[AllJobsPage] spark-webui-JobPage.md[JobPage] JobsTab is < > when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize]. NOTE: The Jobs tab uses spark-webui-JobProgressListener.md[JobProgressListener] to access statistics of job executions in a Spark application to display. == [[creating-instance]] Creating JobsTab Instance JobsTab takes the following to be created: [[parent]] spark-webui-SparkUI.md[SparkUI] [[store]] core:AppStatusStore.md[] While < >, JobsTab creates and spark-webui-WebUITab.md#attachPage[attaches] the pages: spark-webui-AllJobsPage.md[AllJobsPage] spark-webui-JobPage.md[JobPage] == [[handleKillRequest]] handleKillRequest Method [source, scala] \u00b6 handleKillRequest( request: HttpServletRequest): Unit handleKillRequest ...FIXME NOTE: handleKillRequest is used when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize] (and registers the /jobs/job/kill URL handler). == [[isFairScheduler]] isFairScheduler Method [source, scala] \u00b6 isFairScheduler: Boolean \u00b6 isFairScheduler ...FIXME NOTE: isFairScheduler is used when JobPage is requested to spark-webui-JobPage.md#render[render itself (as a HTML page)].","title":"JobsTab"},{"location":"webui/spark-webui-JobsTab/#source-scala","text":"handleKillRequest( request: HttpServletRequest): Unit handleKillRequest ...FIXME NOTE: handleKillRequest is used when SparkUI is requested to spark-webui-SparkUI.md#initialize[initialize] (and registers the /jobs/job/kill URL handler). == [[isFairScheduler]] isFairScheduler Method","title":"[source, scala]"},{"location":"webui/spark-webui-JobsTab/#source-scala_1","text":"","title":"[source, scala]"},{"location":"webui/spark-webui-JobsTab/#isfairscheduler-boolean","text":"isFairScheduler ...FIXME NOTE: isFairScheduler is used when JobPage is requested to spark-webui-JobPage.md#render[render itself (as a HTML page)].","title":"isFairScheduler: Boolean"},{"location":"webui/stages/","text":"== Stages Tab Stages tab in spark-webui.md[web UI] shows...FIXME .Stages Tab in Web UI image::spark-webui-stages.png[align=\"center\"] The Stages tab is available under /stages URL, i.e. http://localhost:4040/stages . Internally, the Stages tab is represented by spark-webui-StagesTab.md[StagesTab].","title":"Stages"},{"location":"webui/storage/","text":"== Storage Tab Storage tab in spark-webui.md[web UI] shows...FIXME .Storage Tab in Web UI image::spark-webui-storage.png[align=\"center\"] The Storage tab is available under /storage URL, i.e. http://localhost:4040/storage . Internally, the Storage tab is represented by spark-webui-StorageTab.md[StorageTab].","title":"Storage"}]}